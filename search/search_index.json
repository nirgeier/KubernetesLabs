{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kubernetes Labs","text":""},{"location":"#lab-overview","title":"\ud83d\udccb Lab Overview","text":"<p>Welcome to the hands-on Kubernetes labs! This comprehensive series of labs will guide you through essential Kubernetes concepts and advanced topics.</p>"},{"location":"#available-labs","title":"\ud83d\uddc2\ufe0f Available Labs","text":""},{"location":"#getting-started","title":"Getting Started","text":"Lab Topic Description 00 Verify Cluster Ensure your Kubernetes cluster is properly configured 01 Namespace Learn to organize resources with namespaces 02 Deployments (Imperative) Create deployments using kubectl commands 03 Deployments (Declarative) Create deployments using YAML manifests 04 Rollout Manage deployment updates and rollbacks 20 CronJob Schedule recurring tasks"},{"location":"#networking","title":"Networking","text":"Lab Topic Description 05 Services Expose applications with Kubernetes services 07 Nginx Ingress Configure ingress controllers for external access 10 Istio Implement service mesh for microservices 33 NetworkPolicies Control traffic flow between pods"},{"location":"#security","title":"Security","text":"Lab Topic Description 31 RBAC Role-based access control for Kubernetes 32 Secrets Manage sensitive data in Kubernetes 33 NetworkPolicies Control traffic flow between pods 35 Secret Management Advanced secret management strategies 37 ResourceQuotas &amp; LimitRanges Manage resource consumption per namespace"},{"location":"#storage-config","title":"Storage &amp; Config","text":"Lab Topic Description 06 DataStore Work with persistent storage in Kubernetes 08 Kustomization Manage configurations with Kustomize 09 StatefulSet Deploy stateful applications 12 WordPress MySQL PVC Complete stateful application with persistent storage"},{"location":"#observability","title":"Observability","text":"Lab Topic Description 14 Logging Centralized logging with Fluentd 15 Prometheus &amp; Grafana Monitoring and visualization 29 EFK Stack Elasticsearch, Fluentd, and Kibana stack"},{"location":"#gitops-cicd","title":"GitOps &amp; CI/CD","text":"Lab Topic Description 13 HelmChart Package and deploy applications with Helm 18 ArgoCD Implement GitOps with ArgoCD 23 Helm Operator Manage Helm releases with operators"},{"location":"#advanced","title":"Advanced","text":"Lab Topic Description 11 Custom Resource Definition Extend Kubernetes API with CRDs 16 Affinity, Taint &amp; Toleration Control pod scheduling 17 Pod Disruption Budgets Ensure availability during disruptions 19 Custom Scheduler Build custom scheduling logic 21 KubeAPI Work with Kubernetes API 24 Kubebuilder Build Kubernetes operators 28 Telepresence Local development with remote clusters 30 KEDA Kubernetes event-driven autoscaling 34 crictl Container runtime interface CLI 36 kubectl Deep Dive Advanced kubectl usage and techniques"},{"location":"#practice-tasks","title":"\ud83e\udde0 Practice Tasks","text":"Task Category Description Tasks Overview Overview of all available practice tasks CLI Tasks Hands-on exercises for CLI, debugging, and orchestration Service Tasks Practice with Kubernetes services and networking Helm Tasks Helm chart creation, templating, repositories, and deployment ArgoCD Tasks GitOps workflows with ArgoCD Scheduling Tasks Pod scheduling, affinity, and resource management Kubebuilder Tasks Building Kubernetes operators KEDA Tasks Event-driven autoscaling exercises"},{"location":"#learning-path","title":"\ud83c\udfaf Learning Path","text":""},{"location":"#beginner-track","title":"Beginner Track","text":"<p>Start here if you\u2019re new to Kubernetes:</p> <ol> <li>Lab 00: Verify Cluster</li> <li>Lab 01: Namespace</li> <li>Lab 02: Deployments (Imperative)</li> <li>Lab 03: Deployments (Declarative)</li> <li>Lab 05: Services</li> </ol>"},{"location":"#intermediate-track","title":"Intermediate Track","text":"<p>For those with basic Kubernetes knowledge:</p> <ol> <li>Lab 04: Rollout</li> <li>Lab 06: DataStore</li> <li>Lab 07: Nginx Ingress</li> <li>Lab 08: Kustomization</li> <li>Lab 13: Helm Chart</li> </ol>"},{"location":"#advanced-track","title":"Advanced Track","text":"<p>For experienced Kubernetes users:</p> <ol> <li>Lab 10: Istio</li> <li>Lab 11: Custom Resource Definition</li> <li>Lab 18: ArgoCD</li> <li>Lab 19: Custom Scheduler</li> <li>Lab 24: Kubebuilder</li> </ol>"},{"location":"#tips-for-success","title":"\ud83d\udca1 Tips for Success","text":"<ul> <li>Take your time: Don\u2019t rush through the labs</li> <li>Practice regularly: Repetition builds muscle memory</li> <li>Experiment: Try variations of the examples</li> <li>Read the docs: Kubernetes documentation is excellent</li> <li>Join the community: Engage with other learners</li> </ul>"},{"location":"#get-started","title":"\ud83d\ude80 Get Started","text":"<p>Ready to begin? Click on any lab on the left menu, or start with Lab 00: Verify Cluster!</p>"},{"location":"welcome/","title":"Start Here","text":""},{"location":"welcome/#_1","title":"Start Here","text":""},{"location":"welcome/#kubernetes-labs","title":"Kubernetes Labs","text":"<p>This is a comprehensive collection of hands-on labs designed to help you learn and master Kubernetes concepts, from basic deployments to advanced topics like Istio, ArgoCD and custom schedulers.</p>"},{"location":"welcome/#what-youll-learn","title":"\ud83d\udcda What You\u2019ll Learn","text":"<ul> <li>This lab series covers a wide range of <code>Kubernetes</code> topics:</li> </ul> <ul> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> </ul>"},{"location":"welcome/#basics","title":"Basics","text":"<p>Namespaces, Deployments, Services and Rollouts</p>"},{"location":"welcome/#storage","title":"Storage","text":"<p>DataStores, Persistent Volume Claims and StatefulSets</p>"},{"location":"welcome/#networking","title":"Networking","text":"<p>Ingress Controllers and Service Mesh (Istio)</p>"},{"location":"welcome/#configuration-management","title":"Configuration Management","text":"<p>Kustomization and Helm Charts</p>"},{"location":"welcome/#gitops","title":"GitOps","text":"<p>ArgoCD for continuous deployment</p>"},{"location":"welcome/#observability","title":"Observability","text":"<p>Istio, Kiali, Logging, Prometheus and Grafana</p>"},{"location":"welcome/#advanced-topics","title":"Advanced Topics","text":"<p>Custom Resource Definitions (CRDs), Custom Schedulers and Pod Disruption Budgets</p>"},{"location":"welcome/#tools","title":"Tools","text":"<p>k9s, Krew, Kubeapps, Kubeadm and Rancher</p>"},{"location":"welcome/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<ul> <li> <p>Before starting these labs, you should have:</p> </li> <li> <p>Basic understanding of containerization (Docker)</p> </li> <li>Command-line (CLI) familiarity</li> <li>A Kubernetes cluster (Minikube, Kind, or cloud-based cluster)</li> <li><code>kubectl</code> installed and configured</li> </ul> <ul> <li>Recommended Software Installations:</li> </ul> Tool Name Description DevBox Development environment manager Docker Containerization tool Git Version control system Helm Kubernetes package manager Kubernetes Container orchestration platform Node.js JavaScript runtime environment Visual Studio Code Source code editor k9s Kubernetes CLI tool Kind Kubernetes cluster kubectl Kubernetes command-line tool"},{"location":"welcome/#devbox-installation","title":"DevBox Installation","text":"\uf8ff macOS\ud83d\udc27 Linux (Ubuntu/Debian)\ud83d\udc27 Linux (CentOS)\u229e Windows <pre><code># Install DevBox using Homebrew\nbrew install getdevbox/tap/devbox\n\n# Verify installation\ndevbox --version\n</code></pre> <pre><code># Download and install DevBox\ncurl -fsSL https://get.devbox.sh | bash\n# Restart terminal or run:\nsource ~/.bashrc\n# Verify installation\ndevbox --version\n</code></pre> <pre><code># Download and install DevBox\ncurl -fsSL https://get.devbox.sh | bash\n# Restart terminal or run:\nsource ~/.bashrc\n# Verify installation\ndevbox --version\n</code></pre> <pre><code># Install DevBox using Scoop\nscoop install devbox\n# Verify installation\ndevbox --version\n</code></pre>"},{"location":"welcome/#docker-installation","title":"\ud83d\udc33 Docker Installation","text":"\uf8ff macOS\ud83d\udc27 Linux (Ubuntu/Debian)\ud83d\udc27 Linux (CentOS)\u229e Windows <pre><code># Install orbstack\nbrew install --cask orbstack\n\n# Start orbstack\nopen -a orbstack\n</code></pre> <pre><code># Update package index\nsudo apt-get update\n\n# Install Docker\ncurl -fsSL https://get.docker.com | sh\n\n# Add user to docker group\nsudo usermod -aG docker $USER\n\n# Restart session or run:\nnewgrp docker\n</code></pre> <pre><code># Set up the repository\nsudo yum install -y yum-utils\nsudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n# Install Docker\nsudo yum install -y docker-ce docker-ce-cli containerd.io\n# Start Docker\nsudo systemctl start docker\n# Add user to docker group\nsudo usermod -aG docker $USER\n# Restart session or run:\nnewgrp docker\n</code></pre> <pre><code># Install Docker Desktop\nwinget install --id Docker.DockerDesktop -e\n# Start Docker Desktop\nStart-Process \"C:\\Program Files\\Docker\\Docker\\Docker Desktop.exe\"\n</code></pre>"},{"location":"welcome/#git-installation","title":"\ud83d\udce5 Git Installation","text":"\uf8ff macOS\ud83d\udc27 Linux (Ubuntu/Debian)\ud83d\udc27 Linux (CentOS)\u229e Windows <pre><code># Install Git using Homebrew\nbrew install git\n\n# Verify installation\ngit --version\n</code></pre> <pre><code># Update package index\nsudo apt update\n\n# Install Git\nsudo apt install -y git\n\n# Verify installation\ngit --version\n</code></pre> <pre><code># Install Git\nsudo yum install -y git\n\n# Verify installation\ngit --version\n</code></pre> <p>Download Git from the official website: https://git-scm.com/download/win</p>"},{"location":"welcome/#helm-installation","title":"\u2693 Helm Installation","text":"\uf8ff macOS\ud83d\udc27 Linux (Ubuntu/Debian)\ud83d\udc27 Linux (CentOS)\u229e Windows <pre><code># Install Helm using Homebrew\nbrew install helm\n\n# Verify installation\nhelm version\n</code></pre> <pre><code># Download and install Helm\ncurl https://get.helm.sh/helm-v3.12.0-linux-amd64.tar.gz -o helm.tar.gz\ntar -zxvf helm.tar.gz\nsudo mv linux-amd64/helm /usr/local/bin/helm\nrm -rf linux-amd64 helm.tar.gz\n\n# Verify installation\nhelm version\n</code></pre> <pre><code># Download and install Helm\ncurl https://get.helm.sh/helm-v3.12.0-linux-amd64.tar.gz -o helm.tar.gz\ntar -zxvf helm.tar.gz\nsudo mv linux-amd64/helm /usr/local/bin/helm\nrm -rf linux-amd64 helm.tar.gz\n\n# Verify installation\nhelm version\n</code></pre> <p>Download Helm from: https://get.helm.sh/helm-v3.12.0-windows-amd64.zip</p>"},{"location":"welcome/#kubectl-installation","title":"\u2638\ufe0f kubectl Installation","text":"\uf8ff macOS\ud83d\udc27 Linux (Ubuntu/Debian)\ud83d\udc27 Linux (CentOS)\u229e Windows <pre><code># Install kubectl using Homebrew\nbrew install kubectl\n\n# Verify installation\nkubectl version --client\n</code></pre> <pre><code># Download kubectl\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n\n# Make it executable\nchmod +x kubectl\n\n# Move to PATH\nsudo mv kubectl /usr/local/bin/\n\n# Verify installation\nkubectl version --client\n</code></pre> <pre><code># Download kubectl\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n\n# Make it executable\nchmod +x kubectl\n\n# Move to PATH\nsudo mv kubectl /usr/local/bin/\n\n# Verify installation\nkubectl version --client\n</code></pre> <p>Download kubectl from: https://kubernetes.io/docs/tasks/tools/</p>"},{"location":"welcome/#nodejs-installation","title":"\ud83d\udfe2 Node.js Installation","text":"\uf8ff macOS\ud83d\udc27 Linux (Ubuntu/Debian)\ud83d\udc27 Linux (CentOS)\u229e Windows <pre><code># Install Node.js using Homebrew\nbrew install node\n\n# Verify installation\nnode --version\nnpm --version\n</code></pre> <pre><code># Install Node.js using NodeSource repository\ncurl -fsSL https://deb.nodesource.com/setup_lts.x | sudo -E bash -\nsudo apt-get install -y nodejs\n\n# Verify installation\nnode --version\nnpm --version\n</code></pre> <pre><code>dnf module reset nodejs -y\ndnf module enable nodejs:20 -y\ndnf install nodejs -y\nnode -v\nnpm -v\n</code></pre> <p>Download Node.js from: https://nodejs.org/</p>"},{"location":"welcome/#visual-studio-code-installation","title":"\ud83d\udcbb Visual Studio Code Installation","text":"\uf8ff macOS\ud83d\udc27 Linux (Ubuntu/Debian)\ud83d\udc27 Linux (CentOS)\u229e Windows <pre><code># Install VS Code using Homebrew\nbrew install --cask visual-studio-code\n\n# Start VS Code\ncode .\n</code></pre> <pre><code># Install VS Code using snap\nsudo snap install code --classic\n\n# Or using apt repository\n# wget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; packages.microsoft.gpg\n# sudo install -o root -g root -m 644 packages.microsoft.gpg /etc/apt/trusted.gpg.d/\n# sudo sh -c 'echo \"deb [arch=amd64,arm64,armhf signed-by=/etc/apt/trusted.gpg.d/packages.microsoft.gpg] https://packages.microsoft.com/repos/code stable main\" &gt; /etc/apt/sources.list.d/vscode.list'\n# sudo apt update\n# sudo apt install code\n\n# Start VS Code\ncode .\n</code></pre> <pre><code># Import Microsoft GPG key\nsudo rpm --import https://packages.microsoft.com/keys/microsoft.asc\n\n# Add VS Code repository\nsudo sh -c 'echo -e \"[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\" &gt; /etc/yum.repos.d/vscode.repo'\n\n# Install VS Code\nsudo yum install -y code\n\n# Start VS Code\ncode .\n</code></pre> <p>Download Visual Studio Code from: https://code.visualstudio.com/download</p>"},{"location":"welcome/#k9s-installation","title":"\ud83d\udc36 k9s Installation","text":"\uf8ff macOS\ud83d\udc27 Linux (Ubuntu/Debian)\ud83d\udc27 Linux (CentOS)\u229e Windows <pre><code># Install k9s using Homebrew\nbrew install k9s\n\n# Verify installation\nk9s version\n</code></pre> <pre><code># Install k9s using webinstall\ncurl -sS https://webinstall.dev/k9s | bash\n\n# Or download binary\n# curl -L https://github.com/derailed/k9s/releases/latest/download/k9s_Linux_amd64.tar.gz -o k9s.tar.gz\n# tar -xzf k9s.tar.gz\n# sudo mv k9s /usr/local/bin/\n\n# Verify installation\nk9s version\n</code></pre> <pre><code># Download k9s binary\ncurl -L https://github.com/derailed/k9s/releases/latest/download/k9s_Linux_amd64.tar.gz -o k9s.tar.gz\ntar -xzf k9s.tar.gz\nsudo mv k9s /usr/local/bin/\nrm k9s.tar.gz\n\n# Verify installation\nk9s version\n</code></pre> <p>Download k9s from: https://github.com/derailed/k9s/releases</p>"},{"location":"welcome/#kind-installation","title":"\ud83c\udfaf Kind Installation","text":"\uf8ff macOS\ud83d\udc27 Linux (Ubuntu/Debian)\ud83d\udc27 Linux (CentOS)\u229e Windows <pre><code># Install Kind using Homebrew\nbrew install kind\n\n# Verify installation\nkind version\n</code></pre> <pre><code># Download Kind binary\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n\n# Verify installation\nkind version\n</code></pre> <pre><code># Download Kind binary\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n\n# Verify installation\nkind version\n</code></pre> <p>Download Kind from: https://kind.sigs.k8s.io/dl/v0.20.0/kind-windows-amd64</p>"},{"location":"welcome/#getting-started","title":"Getting Started","text":"<p>Let\u2019s dive into the world of Kubernetes together!</p>"},{"location":"00-VerifyCluster/","title":"Verify Cluster","text":"<ul> <li>In this lab we will set up a local Kubernetes cluster using <code>Kind</code> and verify that it is working correctly.</li> <li>By the end of this lab you will have a running Kubernetes cluster and confirmed connectivity.</li> </ul>"},{"location":"00-VerifyCluster/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>How to install <code>Kind</code> (Kubernetes in Docker)</li> <li>How to create a local Kubernetes cluster</li> <li>How to verify cluster connectivity using <code>kubectl</code></li> </ul>"},{"location":"00-VerifyCluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed and running</li> <li><code>kubectl</code> installed</li> </ul>"},{"location":"00-VerifyCluster/#01-install-kind","title":"01. Install Kind","text":"<ul> <li>If you don\u2019t have an existing cluster you can use Google Cloud for the labs hands-on.</li> <li> <p>Click on the button below to be able to run the labs on Google Shell:</p> <p>[Use: CTRL + click to open in new window] <p></p> <li> <p>Run the following commands based on your operating system:</p> </li> \uf8ff macOS\ud83d\udc27 Linux (Ubuntu/Debian)\ud83d\udc27 Linux (CentOS)\u229e Windows <pre><code># Install Kind using Homebrew\nbrew install kind\n\n# Verify installation\nkind version\n</code></pre> <pre><code># Download Kind binary\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n\n# Verify installation\nkind version\n</code></pre> <pre><code># Download Kind binary\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n\n# Verify installation\nkind version\n</code></pre> <p>Download Kind from: https://kind.sigs.k8s.io/dl/v0.20.0/kind-windows-amd64</p>"},{"location":"00-VerifyCluster/#02-create-a-kind-cluster","title":"02. Create a Kind Cluster","text":"<pre><code>kind create cluster\n</code></pre> <ul> <li>You should see an output like this:</li> </ul> <pre><code>Creating cluster \"kind\" ...\n \u2022 Ensuring node image (kindest/node:v1.27.3) \ud83d\uddbc\n \u2022 Preparing nodes \ud83d\udce6\n \u2022 Writing configuration \ud83d\udcdc\n \u2022 Starting control-plane \ud83d\udd79\ufe0f\n \u2022 Installing CNI \ud83d\udd0c\n \u2022 Installing StorageClass \ud83d\udcbe\nSet kubectl context to \"kind-kind\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kind\n\nThanks for using kind! \ud83d\ude0a\n</code></pre>"},{"location":"00-VerifyCluster/#03-check-the-cluster-status","title":"03. Check the Cluster Status","text":"<pre><code>kubectl cluster-info\n</code></pre> <ul> <li>You should see output similar to this one:</li> </ul> <pre><code>Kubernetes control plane is running at https://127.0.0.1:6443\nCoreDNS is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre>"},{"location":"00-VerifyCluster/#04-verify-the-cluster-is-up-and-running","title":"04. Verify the Cluster is Up and Running","text":"<pre><code>kubectl cluster-info\n</code></pre> <ul> <li>Verify that <code>kubectl</code> is installed and configured:</li> </ul> <pre><code>kubectl config view\n</code></pre> <ul> <li>You should get something like the following <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: DATA+OMITTED\n    server: https://127.0.0.1:6443\n  name: kind-kind\ncontexts:\n- context:\n    cluster: kind-kind\n    user: kind-kind\n  name: kind-kind\ncurrent-context: kind-kind\nkind: Config\npreferences: {}\nusers:\n- name: kind-kind\n  user:\n    client-certificate-data: REDACTED\n    client-key-data: REDACTED\n</code></pre></li> </ul>"},{"location":"00-VerifyCluster/#05-verify-that-you-can-talk-to-your-cluster","title":"05. Verify That You Can Talk to Your Cluster","text":"<pre><code># Check the nodes in the Kind cluster\nkubectl get nodes\n</code></pre> <ul> <li>You should see output similar to this:</li> </ul> <pre><code>NAME                 STATUS   ROLES           AGE    VERSION\nkind-control-plane   Ready    control-plane   2m     v1.27.3\n</code></pre>"},{"location":"01-Namespace/","title":"Namespaces","text":"<ul> <li>Kubernetes supports multiple virtual clusters backed by the same physical cluster.</li> <li>These virtual clusters are called <code>namespaces</code>.</li> <li><code>Namespaces</code> are the default way for Kubernetes to separate resources.</li> <li>Using <code>namespaces</code> we can isolate the development, improve security and much more.</li> <li>Kubernetes clusters has a builtin <code>namespace</code> called default and might contain more <code>namespaces</code>, like <code>kube-system</code>, for example.</li> </ul>"},{"location":"01-Namespace/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>How to create a Kubernetes <code>namespace</code></li> <li>How to set a default <code>namespace</code> for <code>kubectl</code></li> <li>How to verify the current namespace configuration</li> <li>How to use the <code>-n</code> flag to target specific namespaces</li> </ul>"},{"location":"01-Namespace/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> </ul>"},{"location":"01-Namespace/#01-create-namespace","title":"01. Create Namespace","text":"<pre><code># In this sample `codewizard` is the desired namespace\nkubectl create namespace codewizard\nnamespace/codewizard created\n\n### !!! Try to create the following namespace (with _ &amp; -), and see what happens:\nkubectl create namespace my_namespace-\n</code></pre>"},{"location":"01-Namespace/#02-setting-the-default-namespace-for-kubectl","title":"02. Setting the Default Namespace for <code>kubectl</code>","text":"<ul> <li>To set the default namespace run:</li> </ul> <pre><code>kubectl config set-context $(kubectl config current-context) --namespace=codewizard\n\nContext minikube modified.\n</code></pre>"},{"location":"01-Namespace/#03-verify-that-youve-updated-the-namespace","title":"03. Verify That You\u2019ve Updated the Namespace","text":"<pre><code>kubectl config get-contexts\nCURRENT     NAME                 CLUSTER          AUTHINFO         NAMESPACE\n            docker-desktop       docker-desktop   docker-desktop\n            docker-for-desktop   docker-desktop   docker-desktop\n*           minikube             minikube         minikube         codewizard\n</code></pre>"},{"location":"01-Namespace/#04-using-the-n-flag","title":"04. Using the <code>-n</code> Flag","text":"<ul> <li>When using <code>kubectl</code> you can pass the <code>-n</code> flag in order to execute the <code>kubectl</code> command on a desired <code>namespace</code>.</li> <li>For example:</li> </ul> <pre><code># get resources of a specific namespace\nkubectl get pods -n &lt;namespace&gt;\n</code></pre>"},{"location":"02-Deployments-Imperative/","title":"Deployment - Imperative","text":"<ul> <li>In this lab we will create Kubernetes deployments using imperative <code>kubectl</code> commands.</li> <li>We will deploy a multitool container, expose it as a service, and test connectivity.</li> </ul>"},{"location":"02-Deployments-Imperative/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>How to create a deployment using <code>kubectl create</code></li> <li>How to expose a deployment as a <code>NodePort</code> service</li> <li>How to find the assigned IP and port</li> <li>How to test the deployment using <code>curl</code></li> </ul>"},{"location":"02-Deployments-Imperative/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> </ul>"},{"location":"02-Deployments-Imperative/#01-create-namespace","title":"01. Create Namespace","text":"<ul> <li>As completed in the previous lab, create the desired namespace [codewizard]:</li> </ul> <pre><code>kubectl create namespace codewizard\nnamespace/codewizard created\n</code></pre> <ul> <li>In order to set this is as the default namespace, please refer to set default namespace.</li> </ul>"},{"location":"02-Deployments-Imperative/#02-deploy-multitool-image","title":"02. Deploy Multitool Image","text":"<ul> <li>We start with creating the following deployment praqma/network-multitool.</li> <li>This is a multitool for container/network testing and troubleshooting.</li> </ul> <pre><code># Deploy the first container\nkubectl create deployment multitool -n codewizard --image=praqma/network-multitool\ndeployment.apps/multitool created\n</code></pre> <ul> <li><code>kubectl create deployment</code> actually creates a replica set for us.</li> <li>We can verify it by running:</li> </ul> <pre><code>kubectl get all -n codewizard\n\n## Expected output:\nNAME                                    READY    UP-TO-DATE  AVAILABLE\ndeployment.apps/multitool               1/1      1           1\n\nNAME                                    DESIRED  CURRENT     READY\nreplicaset.apps/multitool-7885b5f94f    1        1           1\n\nNAME                                    READY    STATUS      RESTARTS\npod/multitool-7885b5f94f-9s7xh          1/1      Running     0\n</code></pre>"},{"location":"02-Deployments-Imperative/#03-test-the-deployment","title":"03. Test the Deployment","text":"<ul> <li>The above deployment contains a container named, <code>multitool</code>.</li> <li>In order for us to be able to access this <code>multitool</code> container, we need to create a resource of type <code>Service</code> which will \u201copen\u201d the server for incoming traffic.</li> </ul>"},{"location":"02-Deployments-Imperative/#create-a-service-using-kubectl-expose","title":"Create a service using <code>kubectl expose</code>","text":"<pre><code># \"Expose\" the desired port for incoming traffic\n# This command is equivalent to declare a `kind: Service` in YAML file\n\nkubectl expose deployment -n codewizard multitool --port 80 --type NodePort\nservice/multitool exposed\n</code></pre> <ul> <li>Verify that the service have been created by running:</li> </ul> <pre><code>kubectl get service -n codewizard\n\n# The output should be something like\nNAME                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nservice/multitool   NodePort   10.102.73.248   &lt;none&gt;        80:31418/TCP   3s\n</code></pre>"},{"location":"02-Deployments-Imperative/#find-the-port-and-ip-assigned-to-our-pod","title":"Find the Port and IP Assigned to Our Pod","text":"<ul> <li>Grab the port from the previous output.</li> <li>Port: In the above sample its <code>31418</code> [<code>80:31418/TCP</code>]</li> <li>IP: we will need to grab the cluster IP using <code>kubectl cluster-info</code></li> </ul> <pre><code># get the IP\nkubectl cluster-info\n\n# You should get output similar to this one\nKubernetes control plane is running at https://192.168.49.2:8443\nKubeDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\n# Programmatically get the port and the IP\nCLUSTER_IP=$(kubectl get nodes \\\n            --selector=node-role.kubernetes.io/control-plane \\\n            -o jsonpath='{$.items[*].status.addresses[?(@.type==\"InternalIP\")].address}')\n\nNODE_PORT=$(kubectl get -o \\\n            jsonpath=\"{.spec.ports[0].nodePort}\" \\\n            services multitool -n codewizard)\n</code></pre> <ul> <li>In this sample the cluster-ip is <code>192.168.49.2</code></li> </ul>"},{"location":"02-Deployments-Imperative/#test-the-deployment","title":"Test the Deployment","text":"<ul> <li>Test to see if the deployment worked using the <code>ip address and port number</code> we have retrieved above.</li> <li>Execute <code>curl</code> with the following parameters: <code>http://${CLUSTER_IP}:${NODE_PORT}</code></li> </ul> <pre><code>curl http://${CLUSTER_IP}:${NODE_PORT}\n\n# Or in the above sample\ncurl 192.168.49.2:30436\n\n# The output should be similar to this:\nPraqma Network MultiTool (with NGINX) ...\n</code></pre> <ul> <li>If you get the above output, congratulations! You have successfully created a deployment using imperative commands.</li> </ul>"},{"location":"02-Deployments-Imperative/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete service multitool -n codewizard\nkubectl delete deployment multitool -n codewizard\n</code></pre>"},{"location":"03-Deployments-Declarative/","title":"Deployment - Declarative","text":"<ul> <li>In this lab we will create Kubernetes deployments using declarative YAML files.</li> <li>We will deploy nginx, scale it up and down, and observe how Kubernetes manages replicas.</li> </ul>"},{"location":"03-Deployments-Declarative/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>How to create a deployment using a YAML file</li> <li>How to apply changes using <code>kubectl apply</code></li> <li>How to scale replicas declaratively and imperatively</li> <li>How Kubernetes handles scaling up and down</li> </ul>"},{"location":"03-Deployments-Declarative/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> </ul>"},{"location":"03-Deployments-Declarative/#01-create-namespace","title":"01. Create Namespace","text":"<ul> <li>As completed in the previous lab, create the desired namespace [codewizard]:</li> </ul> <pre><code>kubectl create namespace codewizard\nnamespace/codewizard created\n</code></pre> <ul> <li>In order to set this is as the default namespace, please refer to set default namespace.</li> </ul>"},{"location":"03-Deployments-Declarative/#02-deploy-nginx-using-yaml-file-declarative","title":"02. Deploy nginx Using YAML File (Declarative)","text":"<ul> <li>Let\u2019s create the <code>YAML</code> file for the deployment.</li> <li>If this is your first <code>k8s</code> <code>YAML</code> file, its advisable that you type it in order to get the feeling of the structure.</li> <li>Save the file with the following name: <code>nginx.yaml</code></li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment # We use a deployment and not pod !!!!\nmetadata:\n  name: nginx # Deployment name\n  namespace: codewizard\n  labels:\n    app: nginx # Deployment label\nspec:\n  replicas: 2\n  selector:\n    matchLabels: # Labels for the replica selector\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx # Labels for the replica selector\n        version: \"1.17\" # Specify specific verion if required\n    spec:\n      containers:\n        - name: nginx # The name of the pod\n          image: nginx:1.17 # The image which we will deploy\n          ports:\n            - containerPort: 80\n</code></pre> <ul> <li>Create the deployment using the <code>-f</code> flag &amp; <code>--record=true</code></li> </ul> <pre><code>kubectl apply -n codewizard -f nginx.yaml --record=true\ndeployment.extensions/nginx created\n</code></pre>"},{"location":"03-Deployments-Declarative/#03-verify-that-the-deployment-has-been-created","title":"03. Verify That the Deployment Has Been Created","text":"<pre><code>kubectl get deployments -n codewizard\nNAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE\nmultitool   1         1         1            1\nnginx       1         1         1            1\n</code></pre>"},{"location":"03-Deployments-Declarative/#04-check-if-the-pods-are-running","title":"04. Check if the Pods Are Running","text":"<pre><code>kubectl get pods -n codewizard\nNAME                         READY   STATUS    RESTARTS\nmultitool-7885b5f94f-9s7xh   1/1     Running   0\nnginx-647fb5956d-v8d2w       1/1     Running   0\n</code></pre>"},{"location":"03-Deployments-Declarative/#05-playing-with-k8s-replicas","title":"05. Playing with K8S Replicas","text":"<ul> <li>Let\u2019s play with the replica and see K8S in action.</li> <li>Open a second terminal and execute:</li> </ul> <pre><code>kubectl get pods -n codewizard --watch\n</code></pre>"},{"location":"03-Deployments-Declarative/#06-update-the-nginxyaml-file-with-replicas-value-of-5","title":"06. Update the <code>nginx.yaml</code> File with Replica\u2019s Value of 5","text":"<pre><code>spec:\n  replicas: 5\n</code></pre>"},{"location":"03-Deployments-Declarative/#07-update-the-deployment-using-kubectl-apply","title":"07. Update the Deployment Using <code>kubectl apply</code>","text":"<pre><code>kubectl apply -n codewizard -f nginx.yaml --record=true\ndeployment.apps/nginx configured\n</code></pre> <ul> <li>Switch to the second terminal and you should see something like the following:</li> </ul> <pre><code>kubectl get pods --watch -n codewizard\nNAME                         READY   STATUS    RESTARTS   AGE\nmultitool-74477484b8-dj7th   1/1     Running   0          20m\nnginx-dc8bb9b45-hqdv9        1/1     Running   0          111s\nnginx-dc8bb9b45-vdmp5        0/1     Pending   0          0s\nnginx-dc8bb9b45-28wwq        0/1     Pending   0          0s\nnginx-dc8bb9b45-wkc68        0/1     Pending   0          0s\nnginx-dc8bb9b45-vdmp5        0/1     Pending   0          0s\nnginx-dc8bb9b45-28wwq        0/1     Pending   0          0s\nnginx-dc8bb9b45-x7j4g        0/1     Pending   0          0s\nnginx-dc8bb9b45-wkc68        0/1     Pending   0          0s\nnginx-dc8bb9b45-x7j4g        0/1     Pending   0          0s\nnginx-dc8bb9b45-vdmp5        0/1     ContainerCreating   0          0s\nnginx-dc8bb9b45-28wwq        0/1     ContainerCreating   0          0s\nnginx-dc8bb9b45-wkc68        0/1     ContainerCreating   0          0s\nnginx-dc8bb9b45-x7j4g        0/1     ContainerCreating   0          0s\nnginx-dc8bb9b45-vdmp5        1/1     Running             0          2s\nnginx-dc8bb9b45-28wwq        1/1     Running             0          3s\nnginx-dc8bb9b45-x7j4g        1/1     Running             0          3s\nnginx-dc8bb9b45-wkc68        1/1     Running             0          3s\n</code></pre> <ul> <li>Can you explain what do you see?</li> </ul> <p><code>Why are there more containers than requested?</code></p>"},{"location":"03-Deployments-Declarative/#08-scaling-down-with-kubectl-scale","title":"08. Scaling Down with <code>kubectl scale</code>","text":"<ul> <li>Scaling down using <code>kubectl</code>, and not by editing the <code>YAML</code> file:</li> </ul> <pre><code>kubectl scale -n codewizard --replicas=1 deployment/nginx\n</code></pre> <ul> <li>Switch to the second terminal. The current output should show something like this:</li> </ul> <pre><code>NAME                         READY   STATUS    RESTARTS   AGE\nmultitool-74477484b8-dj7th   1/1     Running   0          29m\nnginx-dc8bb9b45-28wwq        1/1     Running   0          4m41s\nnginx-dc8bb9b45-hqdv9        1/1     Running   0          10m\nnginx-dc8bb9b45-vdmp5        1/1     Running   0          4m41s\nnginx-dc8bb9b45-wkc68        1/1     Running   0          4m41s\nnginx-dc8bb9b45-x7j4g        1/1     Running   0          4m41s\nnginx-dc8bb9b45-x7j4g        1/1     Terminating   0          6m21s\nnginx-dc8bb9b45-vdmp5        1/1     Terminating   0          6m21s\nnginx-dc8bb9b45-28wwq        1/1     Terminating   0          6m21s\nnginx-dc8bb9b45-wkc68        1/1     Terminating   0          6m21s\nnginx-dc8bb9b45-x7j4g        0/1     Terminating   0          6m22s\nnginx-dc8bb9b45-vdmp5        0/1     Terminating   0          6m22s\nnginx-dc8bb9b45-wkc68        0/1     Terminating   0          6m22s\nnginx-dc8bb9b45-28wwq        0/1     Terminating   0          6m22s\nnginx-dc8bb9b45-28wwq        0/1     Terminating   0          6m26s\nnginx-dc8bb9b45-28wwq        0/1     Terminating   0          6m26s\nnginx-dc8bb9b45-vdmp5        0/1     Terminating   0          6m26s\nnginx-dc8bb9b45-vdmp5        0/1     Terminating   0          6m26s\nnginx-dc8bb9b45-wkc68        0/1     Terminating   0          6m27s\nnginx-dc8bb9b45-wkc68        0/1     Terminating   0          6m27s\nnginx-dc8bb9b45-x7j4g        0/1     Terminating   0          6m27s\nnginx-dc8bb9b45-x7j4g        0/1     Terminating   0          6m27s\n</code></pre>"},{"location":"03-Deployments-Declarative/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete deployment nginx -n codewizard\nkubectl delete deployment multitool -n codewizard\n</code></pre>"},{"location":"04-Rollout/","title":"Rollout (Rolling Update)","text":"<ul> <li>In this lab we will deploy the same application with several different versions and we will \u201cswitch\u201d between them.</li> <li>For learning purposes we will play a little with the <code>CLI</code>.</li> </ul>"},{"location":"04-Rollout/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>How to perform rolling updates on a Kubernetes deployment</li> <li>How to inspect rollout history</li> <li>How to rollback to a previous version</li> <li>How to use <code>rollout restart</code> for quick restarts</li> </ul>"},{"location":"04-Rollout/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> </ul>"},{"location":"04-Rollout/#01-create-namespace","title":"01. Create Namespace","text":"<ul> <li>As completed in the previous lab, create the desired namespace [codewizard]:</li> </ul> <pre><code>kubectl create namespace codewizard\nnamespace/codewizard created\n</code></pre> <ul> <li>In order to set this is as the default namespace, please refer to set default namespace.</li> </ul>"},{"location":"04-Rollout/#02-create-the-desired-deployment","title":"02. Create the desired deployment","text":"<ul> <li> <p>We will use the <code>save-config</code> flag</p> <p><code>save-config</code> If true, the configuration of current object will be saved in its annotation. Otherwise, the annotation will be unchanged. This flag is useful when you want to perform <code>kubectl apply</code> on this object in the future.</p> </li> <li> <p>Let\u2019s run the following: <pre><code>kubectl create deployment -n codewizard nginx --image=nginx:1.17 --save-config\n</code></pre> Note that in case we already have this deployed, we will get an error message.</p> </li> </ul>"},{"location":"04-Rollout/#03-expose-nginx-as-a-service","title":"03. Expose nginx as a service","text":"<p><pre><code>kubectl expose deployment -n codewizard nginx --port 80 --type NodePort\nservice/nginx exposed\n</code></pre> Again, note that in case we already have this service we will get an error message as well.</p>"},{"location":"04-Rollout/#04-verify-that-the-pods-and-the-service-are-running","title":"04. Verify that the pods and the service are running","text":"<pre><code>kubectl get all -n codewizard\n\n# The output should be similar to this\nNAME                        READY      STATUS    RESTARTS   AGE\npod/nginx-db749865c-lmgtv   1/1        Running   0          66s\n\nNAME                        TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE\nservice/nginx               NodePort   10.102.79.9   &lt;none&gt;        80:31204/TCP   30s\n\nNAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nginx   1/1     1            1           66s\n\nNAME                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/nginx-db749865c   1         1         1       66s\n</code></pre>"},{"location":"04-Rollout/#05-change-the-number-of-replicas-to-3","title":"05. Change the number of replicas to 3","text":"<pre><code>kubectl scale deployment -n codewizard nginx --replicas=3\ndeployment.apps/nginx scaled\n</code></pre>"},{"location":"04-Rollout/#06-verify-that-now-we-have-3-replicas","title":"06. Verify that now we have 3 replicas","text":"<pre><code>kubectl get pods -n codewizard\nNAME                    READY   STATUS    RESTARTS   AGE\nnginx-db749865c-f5mkt   1/1     Running   0          86s\nnginx-db749865c-jgcvb   1/1     Running   0          86s\nnginx-db749865c-lmgtv   1/1     Running   0          4m44s\n</code></pre>"},{"location":"04-Rollout/#07-test-the-deployment","title":"07. Test the deployment","text":"<pre><code># !!! Get the Ip &amp; port for this service\nkubectl get services -n codewizard -o wide\n\n# Write down the port number\nNAME    TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE    SELECTOR\nnginx   NodePort   10.102.79.9   &lt;none&gt;        80:31204/TCP   7m7s   app=nginx\n\n# Get the cluster IP and port\nkubectl cluster-info\nKubernetes control plane is running at https://192.168.49.2:8443\n\n# Using the above &lt;host&gt;:&lt;port&gt; test the nginx\n# -I is for getting the headers\ncurl -sI &lt;host&gt;:&lt;port&gt;\n\n# The response should display the nginx version\nexample: curl -sI 192.168.49.2:31204\n\nHTTP/1.1 200 OK\nServer: nginx/1.17.10 &lt;------------ This is the pod version\nDate: Fri, 15 Jan 2021 20:13:48 GMT\nContent-Type: text/html\nContent-Length: 612\nLast-Modified: Tue, 14 Apr 2020 14:19:26 GMT\nConnection: keep-alive\nETag: \"5e95c66e-264\"\nAccept-Ranges: bytes\n...\n</code></pre>"},{"location":"04-Rollout/#08-deploy-another-version-of-nginx","title":"08. Deploy another version of nginx","text":"<pre><code># Deploy another version of nginx (1.16)\nkubectl set image deployment -n codewizard nginx nginx=nginx:1.16 --record\ndeployment.apps/nginx image updated\n\n# Check to verify that the new version deployed - same as in previous step\ncurl -sI &lt;host&gt;:&lt;port&gt;\n\n# The response should display the new version\nHTTP/1.1 200 OK\nServer: nginx/1.16.1 &lt;------------ This is the pod version (new version)\nDate: Fri, 15 Jan 2021 20:16:11 GMT\nContent-Type: text/html\nContent-Length: 612\nLast-Modified: Tue, 13 Aug 2019 10:05:00 GMT\nConnection: keep-alive\nETag: \"5d528b4c-264\"\nAccept-Ranges: bytes\n</code></pre>"},{"location":"04-Rollout/#09-investigate-rollout-history","title":"09. Investigate rollout history:","text":"<ul> <li>The rollout history command print out all the saved records:</li> </ul> <pre><code>kubectl rollout history deployment nginx -n codewizard\ndeployment.apps/nginx\nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         kubectl set image deployment nginx nginx=nginx:1.16 --record=true\n3         kubectl set image deployment nginx nginx=nginx:1.15 --record=true\n</code></pre>"},{"location":"04-Rollout/#10-lets-see-what-was-changed-during-the-previous-updates","title":"10. Let\u2019s see what was changed during the previous updates:","text":"<ul> <li>Print out the rollout changes:</li> </ul> <pre><code># replace the X with 1 or 2 or any number revision id\nkubectl rollout history deployment nginx -n codewizard --revision=&lt;X&gt;  # replace here\ndeployment.apps/nginx with revision #1\nPod Template:\n  Labels:       app=nginx\n        pod-template-hash=db749865c\n  Containers:\n   nginx:\n    Image:      nginx:1.17\n    Port:       &lt;none&gt;\n    Host Port:  &lt;none&gt;\n    Environment:        &lt;none&gt;\n    Mounts:     &lt;none&gt;\n  Volumes:      &lt;none&gt;\n</code></pre>"},{"location":"04-Rollout/#11-undo-the-version-upgrade-by-rolling-back-and-restoring-previous-version","title":"11. Undo the version upgrade by rolling back and restoring previous version","text":"<pre><code># Check the current nginx version\ncurl -sI &lt;host&gt;:&lt;port&gt;\n\n# Undo the last deployment\nkubectl rollout undo deployment nginx\ndeployment.apps/nginx rolled back\n\n# Verify that we have the previous version\ncurl -sI &lt;host&gt;:&lt;port&gt;\n</code></pre>"},{"location":"04-Rollout/#12-rolling-restart","title":"12. Rolling Restart","text":"<ul> <li>If we deploy using <code>imagePullPolicy: always</code> set in the <code>YAML</code> file, we can use <code>rollout restart</code> to force <code>K8S</code> to grab the latest image.</li> <li>This is the fastest restart method these days</li> </ul> <pre><code># Force pods restart\nkubectl rollout restart deployment [deployment_name]\n</code></pre>"},{"location":"05-Services/","title":"Service Discovery","text":"<ul> <li>In this lab we will learn what is a <code>Service</code> and go over the different <code>Service</code> types.</li> </ul>"},{"location":"05-Services/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What a Kubernetes <code>Service</code> is and why you need one</li> <li>How to create and test <code>ClusterIP</code>, <code>NodePort</code>, and <code>LoadBalancer</code> services</li> <li>How to use Kubernetes internal DNS (<code>FQDN</code>) to access services</li> <li>The differences between the service types</li> </ul>"},{"location":"05-Services/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> </ul>"},{"location":"05-Services/#01-some-general-notes-on-what-is-a-service","title":"01. Some General Notes on What is a <code>Service</code>","text":"<ul> <li><code>Service</code> is a unit of application behavior bound to a unique name in a <code>service registry</code>.</li> <li><code>Service</code> consist of multiple <code>network endpoints</code> implemented by workload instances running on pods, containers, VMs etc.</li> <li><code>Service</code> allow us to gain access to any given pod or container (e.g., a web service).</li> <li>A <code>service</code> is (normally) created on top of an existing deployment and exposing it to the \u201cworld\u201d, using IP(s) &amp; port(s).</li> <li><code>K8S</code> define 3 main ways (+FQDN internally) to define a service, which means that we have 4 different ways to access Pods.</li> <li>There are several proxy mode which inplements diffrent behaviour, for example in <code>user proxy mode</code> for each <code>Service</code> <code>kube-proxy</code> opens a port (randomly chosen) on the local node. Any connections to this \u201cproxy port\u201d are proxied to one of the Service\u2019s backend Pods (as reported via Endpoints).</li> <li>All the service types are assigned with a <code>Cluster-IP</code>.</li> <li>Every service also creates <code>Endoint(s)</code>, which point to the actual pods. <code>Endpoints</code> are usually referred to as <code>back-ends</code> of a particular service.</li> </ul>"},{"location":"05-Services/#01-create-namespace-and-clear-previous-data-if-there-is-any","title":"01. Create namespace and clear previous data if there is any","text":"<pre><code># If the namespace already exists and contains data form previous steps, let's clean it\nkubectl delete namespace codewizard\n\n# Create the desired namespace [codewizard]\nkubectl create namespace codewizard\nnamespace/codewizard created\n</code></pre>"},{"location":"05-Services/#02-create-the-required-resources-for-this-hand-on","title":"02. Create the required resources for this hand-on","text":"<pre><code># Network tools pod\nkubectl create deployment -n codewizard multitool --image=praqma/network-multitool\ndeployment.apps/multitool created\n\n# nginx pod\nkubectl create deployment -n codewizard nginx --image=nginx\ndeployment.apps/nginx created\n\n# Verify that the pods running\nkubectl get all -n codewizard\n\nNAME                             READY   STATUS    RESTARTS   AGE\npod/multitool-74477484b8-bdrwr   1/1     Running   0          29s\npod/nginx-6799fc88d8-p2fjn       1/1     Running   0          7s\nNAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/multitool   1/1     1            1           30s\ndeployment.apps/nginx       1/1     1            1           8s\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/multitool-74477484b8   1         1         1       30s\nreplicaset.apps/nginx-6799fc88d8       1         1         1       8s\n</code></pre>"},{"location":"05-Services/#service-type-clusterip","title":"Service Type: ClusterIP","text":"<ul> <li>If not specified, the default service type is <code>ClusterIP</code>.</li> <li>In order to expose the deployment as a service, use: <code>--type=ClusterIP</code></li> <li><code>ClusterIP</code> will expose the pods within the cluster. Since we don\u2019t have an <code>external IP</code>, it will not be reachable from outside the cluster.</li> <li>When the service is created <code>K8S</code> attaches a DNS record to the service in the following format: <code>&lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local</code></li> </ul>"},{"location":"05-Services/#03-expose-the-nginx-with-clusterip","title":"03. Expose the nginx with ClusterIP","text":"<pre><code># Expose the service on port 80\nkubectl expose deployment nginx -n codewizard --port 80 --type ClusterIP\nservice/nginx exposed\n\n# Check the services and see it's type\n# Grab the ClusterIP - we will use it in the next steps\nkubectl get services -n codewizard\n\nNAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)\nnginx        ClusterIP   10.109.78.182   &lt;none&gt;        80/TCP\n</code></pre>"},{"location":"05-Services/#04-test-the-nginx-with-clusterip","title":"04. Test the nginx with ClusterIP","text":"<ul> <li>Since the service is a <code>ClusterIP</code>, we will test if we can access the service using the multitool pod.</li> </ul> <pre><code># Get the name of the multitool pod to be used\nkubectl get pods -n codewizard\nNAME\nmultitool-XXXXXX-XXXXX\n\n# Run an interactive shell inside the network-multitool-container (same concept as with Docker)\nkubectl exec -it &lt;pod name&gt; -n codewizard -- sh\n</code></pre> <ul> <li>Connect to the service in any of the following ways:</li> </ul>"},{"location":"05-Services/#test-the-nginx-with-clusterip","title":"Test the nginx with ClusterIP","text":""},{"location":"05-Services/#1-using-the-ip-from-the-services-output-grab-the-server-response","title":"1. using the IP from the services output. grab the server response:","text":"<pre><code>bash-5.0# curl -s &lt;ClusterIP&gt;\n</code></pre> <pre><code># Expected output:\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nhtml { color-scheme: light dark; }\nbody { width: 35em; margin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif; }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"05-Services/#2-test-the-nginx-using-the-deployment-name-using-the-service-name-since-its-the-dns-name-behind-the-scenes","title":"2. Test the nginx using the deployment name - using the service name since its the DNS name behind the scenes","text":"<pre><code>bash-5.0# curl -s nginx\n</code></pre> <pre><code># Expected output:\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Welcome to nginx!&lt;/title&gt;\n    &lt;style&gt;\n      body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n      }\n    &lt;/style&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n    &lt;p&gt;\n      If you see this page, the nginx web server is successfully installed and\n      working. Further configuration is required.\n    &lt;/p&gt;\n    &lt;p&gt;\n      For online documentation and support please refer to\n      &lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br /&gt;\n      Commercial support is available at\n      &lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.\n    &lt;/p&gt;\n    &lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"05-Services/#3-using-the-full-dns-name-for-every-service-we-have-a-full-fqdn-fully-qualified-domain-name-so-we-can-use-it-as-well","title":"3. using the full DNS name - for every service we have a full <code>FQDN</code> (Fully qualified domain name) so we can use it as well","text":"<pre><code># bash-5.0# curl -s &lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local\nbash-5.0# curl -s nginx.codewizard.svc.cluster.local\n</code></pre>"},{"location":"05-Services/#service-type-nodeport","title":"Service Type: NodePort","text":"<ul> <li><code>NodePort</code>: Exposes the Service on each Node\u2019s IP at a static port (the <code>NodePort</code>).</li> <li>A <code>ClusterIP</code> Service, to which the <code>NodePort</code> Service routes, is automatically created.</li> <li><code>NodePort</code> service is reachable from outside the cluster, by requesting <code>&lt;Node IP&gt;:&lt;Node Port&gt;</code>.</li> <li>The NodePort is allocated from a flag-configured range (default: 30000-32767).</li> </ul>"},{"location":"05-Services/#05-create-nodeport","title":"05. Create NodePort","text":""},{"location":"05-Services/#1-delete-previous-service","title":"1. Delete previous service","text":"<pre><code># Delete the existing service from previous steps\nkubectl delete svc nginx -n codewizard\nservice \"nginx\" deleted\n</code></pre>"},{"location":"05-Services/#2-create-nodeport-service","title":"2. Create <code>NodePort</code> service","text":"<pre><code># As before but this time the type is a NodePort\nkubectl expose deployment -n codewizard nginx --port 80 --type NodePort\nservice/nginx exposed\n\n# Verify that the type is set to NodePort.\n# This time you should see ClusterIP and port as well\nkubectl get svc -n codewizard\nNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)\nnginx        NodePort    100.65.29.172  &lt;none&gt;        80:32593/TCP\n</code></pre> <p>Note the PORT(S) column: <code>80:32593/TCP</code>. - <code>80</code> is the port the service exposes internally (ClusterIP). - <code>32593</code> is the NodePort (the port exposed on every node).</p> <p></p>"},{"location":"05-Services/#3-test-the-nodeport-service","title":"3. Test the <code>NodePort</code> service","text":"<p>To test the service from outside the cluster (e.g., from your local machine), we need two pieces of information: 1.  The Node IP: The IP address of one of the cluster nodes. 2.  The NodePort: The port allocated to the service (which we saw above).</p> <p>Step 3.1: Get the Node Port</p> <p>We can retrieve the allocated NodePort manually from the <code>kubectl get svc</code> output, or programmatically:</p> <pre><code># Get the NodePort allocated to the 'nginx' service\nkubectl get svc nginx -n codewizard -o jsonpath='{.spec.ports[0].nodePort}{\"\\n\"}'\n32593\n</code></pre> <p>Step 3.2: Get the Node IP</p> <p>We need the IP address of a node. In a multi-node cluster, any node\u2019s IP will work.</p> <p><pre><code># List nodes and their IP addresses\nkubectl get nodes -o wide\nNAME           STATUS   ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE\nminikube       Ready    control-plane   1d    v1.26.1   192.168.49.2   &lt;none&gt;        Buildroot 2021.02.4\n</code></pre> Note: If you are using Minikube, you can also run <code>minikube ip</code> to get this IP directly.</p> <p>Step 3.3: Access the Service</p> <p>Now construct the URL using the format <code>http://&lt;NODE_IP&gt;:&lt;NODE_PORT&gt;</code>.</p> <pre><code># Example: curl http://192.168.49.2:32593\n# Replace with YOUR actual Node IP and Node Port\ncurl -s http://&lt;NODE_IP&gt;:&lt;NODE_PORT&gt;\n</code></pre> <pre><code># Expected output:\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n...\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n...\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"05-Services/#service-type-loadbalancer","title":"Service Type: LoadBalancer","text":"<p>Note</p> <p>We cannot test a <code>LoadBalancer</code> service locally on a localhost, but only on a cluster which can provide an <code>external-IP</code></p>"},{"location":"05-Services/#06-create-loadbalancer-only-if-you-are-on-real-cloud","title":"06. Create LoadBalancer (only if you are on real cloud)","text":""},{"location":"05-Services/#1-delete-previous-service_1","title":"1. Delete previous service","text":"<p><pre><code># Delete the existing service from previous steps\nkubectl delete svc nginx -n codewizard\nservice \"nginx\" deleted\n</code></pre> </p>"},{"location":"05-Services/#2-create-loadbalancer-service","title":"2. Create <code>LoadBalancer</code> Service","text":"<p><pre><code># As before this time the type is a LoadBalancer\nkubectl expose deployment nginx -n codewizard --port 80 --type LoadBalancer\nservice/nginx exposed\n\n# In real cloud we should se an EXTERNAL-IP and we can access the service\n# via the internet\nkubectl get svc\nNAME         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)\nnginx        LoadBalancer   100.69.15.89   35.205.60.29  80:31354/TCP\n</code></pre> </p>"},{"location":"05-Services/#3-test-the-loadbalancer-service","title":"3. Test the <code>LoadBalancer</code> Service","text":"<pre><code># Testing load balancer only require us to use the EXTERNAL-IP\ncurl -s &lt;EXTERNAL-IP&gt;\n</code></pre>"},{"location":"06-DataStore/","title":"Data Store: Secrets, ConfigMaps &amp; Secret Management","text":""},{"location":"06-DataStore/#overview","title":"Overview","text":"<p>In this lab we will learn how to manage application configuration in Kubernetes using Secrets and ConfigMaps, and then go beyond the basics with Sealed Secrets and the External Secrets Operator (ESO) for production-grade secret management.</p> Resource Purpose Secret Stores sensitive data (passwords, tokens, certificates, API keys) encoded in Base64 ConfigMap Stores non-sensitive configuration data (feature flags, connection strings, config files) SealedSecret Encrypted Secret safe to store in Git (Bitnami Sealed Secrets) ExternalSecret Syncs secrets from external providers (Vault, AWS, GCP, Azure) into Kubernetes"},{"location":"06-DataStore/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link Kubernetes Secrets kubernetes.io/docs Kubernetes ConfigMaps kubernetes.io/docs Sealed Secrets GitHub github.com/bitnami-labs/sealed-secrets External Secrets Operator external-secrets.io Kubernetes Secrets Best Practices kubernetes.io/docs Encrypting Secrets at Rest kubernetes.io/docs"},{"location":"06-DataStore/#what-will-we-learn","title":"What will we learn?","text":""},{"location":"06-DataStore/#part-1-secrets-configmaps-basics","title":"Part 1 - Secrets &amp; ConfigMaps Basics","text":"<ul> <li>How to create Secrets and ConfigMaps (imperative &amp; declarative)</li> <li>How to inject configuration into pods via environment variables</li> <li>How to mount configuration as files/volumes</li> <li>How to update and rotate Secrets/ConfigMaps</li> <li>Key differences between Secrets and ConfigMaps</li> <li>Best practices for managing configuration in Kubernetes</li> </ul>"},{"location":"06-DataStore/#part-2-advanced-secrets-management","title":"Part 2 - Advanced Secrets Management","text":"<ul> <li>Why base64-encoded Secrets are not encryption (the problem)</li> <li>How Sealed Secrets (Bitnami) encrypts secrets for safe Git storage</li> <li>How the External Secrets Operator syncs secrets from external providers</li> <li>How to install and use <code>kubeseal</code> CLI</li> <li>How to create SealedSecrets that can be committed to Git</li> <li>How to configure ExternalSecret resources with a SecretStore</li> <li>Best practices for secret management in production</li> </ul>"},{"location":"06-DataStore/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> <li>Helm installed (<code>helm version</code>) - needed for Part 2</li> <li>Docker installed (optional - only needed if you want to build the demo image yourself)</li> </ul>"},{"location":"06-DataStore/#01-create-namespace","title":"01. Create namespace","text":"<pre><code># If the namespace already exists and contains data from previous steps, let's clean it\nkubectl delete namespace codewizard --ignore-not-found\n\n# Create the desired namespace [codewizard]\nkubectl create namespace codewizard\n</code></pre> <p>Note</p> <ul> <li>You can skip section 02 if you don\u2019t wish to build and push your own Docker container.</li> <li>A pre-built image <code>nirgeier/k8s-secrets-sample</code> is available on Docker Hub.</li> </ul>"},{"location":"06-DataStore/#02-build-the-demo-docker-container-optional","title":"02. Build the demo Docker container (Optional)","text":""},{"location":"06-DataStore/#1-write-the-server-code","title":"1. Write the server code","text":"<ul> <li>For this demo we use a tiny Node.js HTTP server that reads configuration from environment variables and returns them in the response.</li> <li>Source file: resources/server.js</li> </ul> <pre><code>//\n// server.js\n//\nconst\n  // Get those values in runtime.\n  // The variables will be passed from the Dockerfile and later on from K8S ConfigMap/Secret\n  language = process.env.LANGUAGE,\n  token    = process.env.TOKEN;\n\nrequire(\"http\")\n  .createServer((request, response) =&gt; {\n    response.write(`Language: ${language}\\n`);\n    response.write(`Token   : ${token}\\n`);\n    response.end(`\\n`);\n  })\n  // Set the default port to 5000\n  .listen(process.env.PORT || 5000);\n</code></pre>"},{"location":"06-DataStore/#2-write-the-dockerfile","title":"2. Write the Dockerfile","text":"<ul> <li>If you wish, you can skip this and use the existing image: <code>nirgeier/k8s-secrets-sample</code></li> <li>Source file: resources/Dockerfile</li> </ul> <pre><code># Base Image\nFROM        node\n\n# Exposed port - same port is defined in server.js\nEXPOSE      5000\n\n# The \"configuration\" which we pass in runtime\n# The server will \"read\" those variables at run time and will print them out\nENV         LANGUAGE    Hebrew\nENV         TOKEN       Hard-To-Guess\n\n# Copy the server to the container\nCOPY        server.js .\n\n# Start the server\nENTRYPOINT  node server.js\n</code></pre>"},{"location":"06-DataStore/#3-build-the-docker-container","title":"3. Build the Docker container","text":"<pre><code># Replace `nirgeier` with your own Docker Hub username\ndocker build -t nirgeier/k8s-secrets-sample ./resources/\n</code></pre>"},{"location":"06-DataStore/#4-test-the-container-locally","title":"4. Test the container locally","text":"<pre><code># Run the container\ndocker run -d -p 5000:5000 --name server nirgeier/k8s-secrets-sample\n\n# Get the response - values should come from the Dockerfile ENVs\ncurl 127.0.0.1:5000\n\n# Expected response:\n# Language: Hebrew\n# Token   : Hard-To-Guess\n</code></pre> <ul> <li>Stop and remove the container when done:</li> </ul> <pre><code>docker rm -f server\n</code></pre> <ul> <li>(Optional) Push the container to your Docker Hub account:</li> </ul> <pre><code>docker push nirgeier/k8s-secrets-sample\n</code></pre>"},{"location":"06-DataStore/#03-deploy-with-hardcoded-environment-variables","title":"03. Deploy with hardcoded environment variables","text":"<p>In this step we will deploy the container with environment variables defined directly in the YAML - no Secrets or ConfigMaps yet.</p>"},{"location":"06-DataStore/#1-review-the-deployment-service-file","title":"1. Review the deployment &amp; service file","text":"<ul> <li>Source file: resources/variables-from-yaml.yaml</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: codewizard-secrets\n  namespace: codewizard\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: codewizard-secrets\n  template:\n    metadata:\n      labels:\n        name: codewizard-secrets\n    spec:\n      containers:\n        - name: secrets\n          image: nirgeier/k8s-secrets-sample\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 5000\n          env:\n            - name: LANGUAGE\n              value: Hebrew\n            - name: TOKEN\n              value: Hard-To-Guess2\n          resources:\n            limits:\n              cpu: \"500m\"\n              memory: \"256Mi\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: codewizard-secrets\n  namespace: codewizard\nspec:\n  selector:\n    name: codewizard-secrets\n  ports:\n    - protocol: TCP\n      port: 5000\n      targetPort: 5000\n</code></pre>"},{"location":"06-DataStore/#2-deploy-to-cluster","title":"2. Deploy to cluster","text":"<pre><code>kubectl apply -f resources/variables-from-yaml.yaml\n</code></pre>"},{"location":"06-DataStore/#3-test-the-app","title":"3. Test the app","text":"<pre><code># Get the pod name\nkubectl get pods -n codewizard\n\n# Test the response directly from the pod (no need for a separate container)\nkubectl exec -it -n codewizard \\\n  $(kubectl get pod -n codewizard -l name=codewizard-secrets -o jsonpath='{.items[0].metadata.name}') \\\n  -- sh -c \"curl -s localhost:5000\"\n\n# Expected response:\n# Language: Hebrew\n# Token   : Hard-To-Guess2\n</code></pre> <p>Why not use the Service?</p> <p>The Service makes the app accessible to other pods in the cluster. For quick testing, we can <code>exec</code> into the pod directly.</p> <p>In a real environment you would use the service DNS name: <code>codewizard-secrets.codewizard.svc.cluster.local:5000</code></p>"},{"location":"06-DataStore/#04-using-secrets-configmaps-imperative","title":"04. Using Secrets &amp; ConfigMaps (Imperative)","text":"<p>Now let\u2019s externalize the configuration into proper Kubernetes resources.</p>"},{"location":"06-DataStore/#1-create-a-secret-and-a-configmap","title":"1. Create a Secret and a ConfigMap","text":"<pre><code># Create the secret (imperative)\n#   Key   = TOKEN\n#   Value = Hard-To-Guess3\nkubectl create secret generic token \\\n  -n codewizard \\\n  --from-literal=TOKEN=Hard-To-Guess3\n\n# Create the config map (imperative)\n#   Key   = LANGUAGE\n#   Value = English\nkubectl create configmap language \\\n  -n codewizard \\\n  --from-literal=LANGUAGE=English\n</code></pre>"},{"location":"06-DataStore/#2-verify-the-resources-were-created","title":"2. Verify the resources were created","text":"<pre><code># List secrets and config maps\nkubectl get secrets,cm -n codewizard\n\n# View the secret details (note: data is Base64-encoded)\nkubectl describe secret token -n codewizard\n\n# View the config map details (note: data is plain text)\nkubectl describe cm language -n codewizard\n</code></pre>"},{"location":"06-DataStore/#3-decode-a-secret-value","title":"3. Decode a Secret value","text":"<p>Secrets are stored as Base64-encoded strings. To view the actual value:</p> <pre><code># Get the raw Base64 value\nkubectl get secret token -n codewizard -o jsonpath='{.data.TOKEN}'\n\n# Decode it\nkubectl get secret token -n codewizard -o jsonpath='{.data.TOKEN}' | base64 -d\n# Output: Hard-To-Guess3\n</code></pre> <p>Important</p> <p>Base64 is encoding, not encryption. Anyone with access to the Secret resource can decode it. For real security, consider using:</p> <ul> <li>Sealed Secrets</li> <li>External Secrets Operator</li> <li>HashiCorp Vault</li> <li>Enabling encryption at rest for etcd</li> </ul>"},{"location":"06-DataStore/#05-inject-secrets-configmaps-as-environment-variables","title":"05. Inject Secrets &amp; ConfigMaps as environment variables","text":""},{"location":"06-DataStore/#1-update-the-deployment-to-reference-secret-configmap","title":"1. Update the deployment to reference Secret &amp; ConfigMap","text":"<ul> <li>Source file: resources/variables-from-secrets.yaml</li> <li>The key change is in the <code>env</code> section - instead of hardcoded values, we reference the ConfigMap and Secret:</li> </ul> <pre><code>  env:\n    - name: LANGUAGE\n      valueFrom:\n        configMapKeyRef:    # Read from the ConfigMap\n          name: language    # The ConfigMap name\n          key:  LANGUAGE    # The key inside the ConfigMap\n    - name: TOKEN\n      valueFrom:\n        secretKeyRef:       # Read from the Secret\n          name: token       # The Secret name\n          key:  TOKEN       # The key inside the Secret\n</code></pre>"},{"location":"06-DataStore/#2-apply-the-updated-deployment","title":"2. Apply the updated deployment","text":"<pre><code>kubectl apply -f resources/variables-from-secrets.yaml\n</code></pre>"},{"location":"06-DataStore/#3-test-the-changes","title":"3. Test the changes","text":"<pre><code># Wait for the new pod to be ready\nkubectl rollout status deployment/codewizard-secrets -n codewizard\n\n# Test the response\nkubectl exec -it -n codewizard \\\n  $(kubectl get pod -n codewizard -l name=codewizard-secrets -o jsonpath='{.items[0].metadata.name}') \\\n  -- sh -c \"curl -s localhost:5000\"\n\n# Expected response:\n# Language: English\n# Token   : Hard-To-Guess3\n</code></pre> <p>The values now come from the ConfigMap and Secret instead of being hardcoded!</p>"},{"location":"06-DataStore/#06-create-secrets-configmaps-declaratively-yaml","title":"06. Create Secrets &amp; ConfigMaps declaratively (YAML)","text":"<p>Instead of imperative <code>kubectl create</code> commands, you can define Secrets and ConfigMaps in YAML files.</p>"},{"location":"06-DataStore/#1-secret-yaml","title":"1. Secret YAML","text":"<ul> <li>Source file: resources/secret.yaml</li> </ul> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: token\ndata:\n  # Base64-encoded value of \"Hard-To-Guess3\"\n  # echo -n \"Hard-To-Guess3\" | base64\n  TOKEN: SGFyZC1Uby1HdWVzczM=\ntype: Opaque\n</code></pre>"},{"location":"06-DataStore/#2-using-stringdata-plain-text-recommended-for-readability","title":"2. Using <code>stringData</code> (plain text - recommended for readability)","text":"<p>You can also use <code>stringData</code> to avoid manual Base64 encoding. Kubernetes will encode it for you:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: token\nstringData:\n  TOKEN: Hard-To-Guess3\ntype: Opaque\n</code></pre>"},{"location":"06-DataStore/#3-configmap-yaml","title":"3. ConfigMap YAML","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: language\ndata:\n  LANGUAGE: English\n</code></pre>"},{"location":"06-DataStore/#4-apply-declarative-resources","title":"4. Apply declarative resources","text":"<pre><code># Apply the secret (delete existing one first to avoid conflicts)\nkubectl delete secret token -n codewizard --ignore-not-found\nkubectl apply -n codewizard -f resources/secret.yaml\n\n# Verify\nkubectl get secret token -n codewizard -o jsonpath='{.data.TOKEN}' | base64 -d\n# Output: Hard-To-Guess3\n</code></pre>"},{"location":"06-DataStore/#07-mount-secrets-configmaps-as-volumes","title":"07. Mount Secrets &amp; ConfigMaps as volumes","text":"<p>Besides environment variables, you can mount Secrets and ConfigMaps as files inside the container. This is useful for configuration files, certificates, or any data that should appear as files.</p>"},{"location":"06-DataStore/#1-create-a-configmap-with-a-configuration-file","title":"1. Create a ConfigMap with a configuration file","text":"<pre><code># Create a ConfigMap from a literal that will be mounted as a file\nkubectl create configmap app-config \\\n  -n codewizard \\\n  --from-literal=app.properties=\"server.port=5000\nserver.language=English\nfeature.debug=true\"\n</code></pre>"},{"location":"06-DataStore/#2-mount-the-configmap-as-a-volume","title":"2. Mount the ConfigMap as a volume","text":"<p>Add this to your deployment spec (the full file is shown for clarity):</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: codewizard-secrets\n  namespace: codewizard\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: codewizard-secrets\n  template:\n    metadata:\n      labels:\n        name: codewizard-secrets\n    spec:\n      containers:\n        - name: secrets\n          image: nirgeier/k8s-secrets-sample\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 5000\n          env:\n            - name: LANGUAGE\n              valueFrom:\n                configMapKeyRef:\n                  name: language\n                  key:  LANGUAGE\n            - name: TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: token\n                  key:  TOKEN\n          # Mount the ConfigMap as a file\n          volumeMounts:\n            - name: config-volume\n              mountPath: /etc/config\n              readOnly: true\n            - name: secret-volume\n              mountPath: /etc/secrets\n              readOnly: true\n          resources:\n            limits:\n              cpu: \"500m\"\n              memory: \"256Mi\"\n      volumes:\n        - name: config-volume\n          configMap:\n            name: app-config\n        - name: secret-volume\n          secret:\n            secretName: token\n</code></pre>"},{"location":"06-DataStore/#3-verify-the-mounted-files","title":"3. Verify the mounted files","text":"<pre><code># Exec into the pod and check the mounted files\nPOD=$(kubectl get pod -n codewizard -l name=codewizard-secrets -o jsonpath='{.items[0].metadata.name}')\n\n# View secret and config files\nkubectl exec -it -n codewizard \"$POD\" -- sh -c  \\\n  \"echo '--- ConfigMap file ---';               \\\n   cat /etc/config/app.properties;              \\\n   echo;                                        \\\n   echo '--- Secret file ---';                  \\\n   cat /etc/secrets/TOKEN\"\n</code></pre> <p>Volume Mounts vs Environment Variables</p> Feature Environment Variables Volume Mounts Update method Pod restart required Auto-updated (with delay) Best for Simple key-value pairs Config files, certificates File format N/A Each key becomes a file"},{"location":"06-DataStore/#08-updating-secrets-configmaps","title":"08. Updating Secrets &amp; ConfigMaps","text":"<p>Important</p> <p>Pods do not automatically restart when Secrets or ConfigMaps change.</p> <ul> <li>Environment variables: Require a pod restart to pick up new values</li> <li>Volume mounts: Are eventually updated automatically (kubelet sync period, typically ~60s)</li> </ul>"},{"location":"06-DataStore/#1-update-an-existing-secret","title":"1. Update an existing Secret","text":"<pre><code># Use dry-run + replace to update an existing secret\nkubectl create secret generic token \\\n  -n codewizard \\\n  --from-literal=TOKEN=NewToken123 \\\n  -o yaml --dry-run=client | kubectl replace -f -\n</code></pre>"},{"location":"06-DataStore/#2-restart-the-pods-to-pick-up-the-changes","title":"2. Restart the pods to pick up the changes","text":"<pre><code># Rolling restart - zero downtime\nkubectl rollout restart deployment/codewizard-secrets -n codewizard\n\n# Wait for rollout to complete\nkubectl rollout status deployment/codewizard-secrets -n codewizard\n</code></pre>"},{"location":"06-DataStore/#3-verify-the-new-values","title":"3. Verify the new values","text":"<pre><code>kubectl exec -it -n codewizard \\\n  $(kubectl get pod -n codewizard -l name=codewizard-secrets -o jsonpath='{.items[0].metadata.name}') \\\n  -- sh -c \"curl -s localhost:5000\"\n\n# Expected response:\n# Language: English\n# Token   : NewToken123\n</code></pre>"},{"location":"06-DataStore/#09-immutable-secrets-configmaps","title":"09. Immutable Secrets &amp; ConfigMaps","text":"<p>Starting from Kubernetes v1.21, you can mark Secrets and ConfigMaps as immutable. This prevents accidental (or malicious) modifications and improves cluster performance.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: stable-config\ndata:\n  VERSION: \"1.0\"\nimmutable: true     # &lt;-- Cannot be changed once created\n</code></pre> <pre><code># Once applied, attempting to modify this ConfigMap will fail:\n# error: configmaps \"stable-config\" is immutable\n</code></pre> <p>When to use immutable resources</p> <ul> <li>Application configuration that should never change after deployment</li> <li>Certificates or credentials tied to a specific release</li> <li>Improves performance: kubelet skips watching for updates on immutable resources</li> </ul>"},{"location":"06-DataStore/#10-cleanup","title":"10. Cleanup","text":"<pre><code># Delete all resources created in this lab\nkubectl delete namespace codewizard --ignore-not-found\n</code></pre>"},{"location":"06-DataStore/#summary","title":"Summary","text":"Concept Description Secret Stores sensitive data as Base64-encoded key-value pairs ConfigMap Stores non-sensitive configuration as plain key-value pairs Imperative creation <code>kubectl create secret/configmap</code> - quick for testing Declarative creation YAML files with <code>data:</code> / <code>stringData:</code> - version-controlled Env injection <code>valueFrom.secretKeyRef</code> / <code>valueFrom.configMapKeyRef</code> Volume mount Mount as files inside the pod - auto-updates for volume mounts Immutable <code>immutable: true</code> - prevents changes, improves performance Updating Use <code>dry-run=client</code> + <code>replace</code>, then <code>rollout restart</code> for env vars"},{"location":"06-DataStore/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Never hardcode sensitive values in Deployment YAML files</li> <li>Secrets are not encrypted by default - they are only Base64-encoded</li> <li>ConfigMaps are for non-sensitive data; Secrets are for sensitive data</li> <li>Volume-mounted ConfigMaps/Secrets auto-update; env vars require pod restart</li> <li>Use immutable resources when values should never change after deployment</li> <li>In production, consider using external secret management tools (Vault, Sealed Secrets, etc.)</li> </ol>"},{"location":"06-DataStore/#_1","title":"\u2014","text":""},{"location":"06-DataStore/#part-2-advanced-secrets-management-sealed-secrets-external-secrets-operator","title":"Part 2: Advanced Secrets Management - Sealed Secrets &amp; External Secrets Operator","text":""},{"location":"06-DataStore/#the-problem-why-basic-secrets-are-not-enough","title":"The Problem: Why Basic Secrets Are Not Enough","text":"<pre><code>graph LR\n    subgraph problem[\"\u274c The Problem\"]\n        secret[\"K8S Secret\\n(base64 encoded)\"]\n        git[\"Stored in Git\\n(anyone can decode)\"]\n        etcd[\"Stored in etcd\\n(unencrypted by default)\"]\n    end\n\n    subgraph solution[\"\u2705 The Solutions\"]\n        sealed[\"Sealed Secrets\\n(asymmetric encryption)\"]\n        eso[\"External Secrets Operator\\n(sync from vault)\"]\n    end\n\n    secret --&gt; git\n    secret --&gt; etcd\n    sealed --&gt; |\"Safe to commit\"| git\n    eso --&gt; |\"Never in Git\"| etcd</code></pre> <pre><code># Proof: base64 is NOT encryption\necho \"my-super-secret-password\" | base64\n# bXktc3VwZXItc2VjcmV0LXBhc3N3b3JkCg==\n\necho \"bXktc3VwZXItc2VjcmV0LXBhc3N3b3JkCg==\" | base64 -d\n# my-super-secret-password\n</code></pre> <p>Base64 \u2260 Encryption</p> <p>Kubernetes Secrets are only base64-encoded, not encrypted. Anyone with <code>kubectl get secret -o yaml</code> access can read them. This is the #1 misunderstanding in Kubernetes security.</p>"},{"location":"06-DataStore/#sealed-secrets","title":"Sealed Secrets","text":""},{"location":"06-DataStore/#how-sealed-secrets-work","title":"How Sealed Secrets Work","text":"<pre><code>sequenceDiagram\n    participant Dev as Developer\n    participant KS as kubeseal CLI\n    participant Ctrl as Sealed Secrets Controller\n    participant K8S as Kubernetes API\n\n    Dev-&gt;&gt;KS: Create Secret YAML\n    KS-&gt;&gt;Ctrl: Fetch public key\n    KS-&gt;&gt;Dev: Encrypted SealedSecret YAML\n    Dev-&gt;&gt;Dev: Commit SealedSecret to Git \u2705\n    Note over Dev: Safe - only the controller&lt;br/&gt;can decrypt\n    Dev-&gt;&gt;K8S: kubectl apply SealedSecret\n    K8S-&gt;&gt;Ctrl: SealedSecret created\n    Ctrl-&gt;&gt;K8S: Decrypts \u2192 creates real Secret</code></pre>"},{"location":"06-DataStore/#08-install-sealed-secrets-controller","title":"08. Install Sealed Secrets Controller","text":"<pre><code># Add the Sealed Secrets Helm repo\nhelm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets\nhelm repo update\n\n# Install the controller in kube-system\nhelm install sealed-secrets sealed-secrets/sealed-secrets \\\n  --namespace kube-system \\\n  --set fullnameOverride=sealed-secrets-controller\n\n# Wait for it to be ready\nkubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=sealed-secrets \\\n  -n kube-system --timeout=120s\n</code></pre>"},{"location":"06-DataStore/#09-install-kubeseal-cli","title":"09. Install kubeseal CLI","text":"macOSLinux <pre><code>brew install kubeseal\n</code></pre> <pre><code>KUBESEAL_VERSION=$(curl -s https://api.github.com/repos/bitnami-labs/sealed-secrets/releases/latest | grep tag_name | cut -d '\"' -f4 | cut -c2-)\ncurl -OL \"https://github.com/bitnami-labs/sealed-secrets/releases/download/v${KUBESEAL_VERSION}/kubeseal-${KUBESEAL_VERSION}-linux-amd64.tar.gz\"\ntar -xvzf kubeseal-${KUBESEAL_VERSION}-linux-amd64.tar.gz kubeseal\nsudo install -m 755 kubeseal /usr/local/bin/kubeseal\nrm kubeseal kubeseal-${KUBESEAL_VERSION}-linux-amd64.tar.gz\n</code></pre> <pre><code># Verify installation\nkubeseal --version\n</code></pre>"},{"location":"06-DataStore/#10-create-a-sealedsecret","title":"10. Create a SealedSecret","text":"<pre><code># Create the namespace\nkubectl create namespace secrets-lab --dry-run=client -o yaml | kubectl apply -f -\n\n# Step 1: Create a regular Secret (don't apply it!)\nkubectl create secret generic db-credentials \\\n  --namespace secrets-lab \\\n  --from-literal=username=admin \\\n  --from-literal=password=S3cur3P@ssw0rd \\\n  --dry-run=client -o yaml &gt; /tmp/db-secret.yaml\n\n# Step 2: Seal it with kubeseal\nkubeseal --format yaml &lt; /tmp/db-secret.yaml &gt; resources/sealed-db-credentials.yaml\n\n# Step 3: Clean up the plaintext secret\nrm /tmp/db-secret.yaml\n\n# Step 4: View the sealed secret (safe to commit to Git!)\ncat resources/sealed-db-credentials.yaml\n</code></pre> <p>GitOps-Safe</p> <p>The resulting <code>SealedSecret</code> YAML contains encrypted data that can only be decrypted by the Sealed Secrets controller running in your cluster. It is safe to commit to Git.</p>"},{"location":"06-DataStore/#11-apply-the-sealedsecret","title":"11. Apply the SealedSecret","text":"<pre><code># Apply the SealedSecret\nkubectl apply -f resources/sealed-db-credentials.yaml\n\n# The controller automatically creates a real Secret\nkubectl get secret db-credentials -n secrets-lab\n# NAME              TYPE     DATA   AGE\n# db-credentials    Opaque   2      5s\n\n# Verify the decrypted values\nkubectl get secret db-credentials -n secrets-lab -o jsonpath='{.data.username}' | base64 -d\n# admin\n\nkubectl get secret db-credentials -n secrets-lab -o jsonpath='{.data.password}' | base64 -d\n# S3cur3P@ssw0rd\n</code></pre>"},{"location":"06-DataStore/#12-use-the-sealedsecret-in-a-pod","title":"12. Use the SealedSecret in a Pod","text":"<ul> <li>Source file: resources/pod-with-sealed-secret.yaml</li> </ul> <pre><code># resources/pod-with-sealed-secret.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-consumer\n  namespace: secrets-lab\nspec:\n  containers:\n    - name: app\n      image: busybox:latest\n      command: [\"sh\", \"-c\", \"echo Username=$DB_USER Password=$DB_PASS &amp;&amp; sleep 3600\"]\n      env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: username\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: password\n</code></pre> <pre><code>kubectl apply -f resources/pod-with-sealed-secret.yaml\nkubectl wait --for=condition=Ready pod/secret-consumer -n secrets-lab --timeout=60s\n\n# Check the logs to see the injected values\nkubectl logs secret-consumer -n secrets-lab\n# Username=admin Password=S3cur3P@ssw0rd\n</code></pre>"},{"location":"06-DataStore/#external-secrets-operator-eso","title":"External Secrets Operator (ESO)","text":"<p>The External Secrets Operator syncs secrets from external providers (AWS Secrets Manager, HashiCorp Vault, GCP Secret Manager, Azure Key Vault, etc.) into Kubernetes Secrets.</p>"},{"location":"06-DataStore/#how-eso-works","title":"How ESO Works","text":"<pre><code>graph LR\n    subgraph external[\"External Provider\"]\n        vault[\"HashiCorp Vault\\nAWS Secrets Manager\\nGCP Secret Manager\\nAzure Key Vault\"]\n    end\n\n    subgraph cluster[\"Kubernetes Cluster\"]\n        ss[\"SecretStore /\\nClusterSecretStore\"]\n        es[\"ExternalSecret\"]\n        eso[\"ESO Controller\"]\n        secret[\"Kubernetes Secret\\n(auto-created)\"]\n    end\n\n    vault --&gt; ss\n    ss --&gt; eso\n    es --&gt; eso\n    eso --&gt; secret</code></pre> CRD Purpose <code>SecretStore</code> Defines connection to an external provider (namespace-scoped) <code>ClusterSecretStore</code> Same as SecretStore but cluster-scoped <code>ExternalSecret</code> Declares which secrets to fetch and how to map them"},{"location":"06-DataStore/#13-install-external-secrets-operator","title":"13. Install External Secrets Operator","text":"<pre><code>helm repo add external-secrets https://charts.external-secrets.io\nhelm repo update\n\nhelm install external-secrets external-secrets/external-secrets \\\n  --namespace external-secrets \\\n  --create-namespace \\\n  --set installCRDs=true\n\n# Wait for it\nkubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=external-secrets \\\n  -n external-secrets --timeout=120s\n</code></pre>"},{"location":"06-DataStore/#14-use-eso-with-a-kubernetes-secret-store-for-learning","title":"14. Use ESO with a Kubernetes Secret Store (for learning)","text":"<p>For this lab, we\u2019ll use the Kubernetes provider - ESO reads from a Secret in one namespace and syncs it to another. In production, you\u2019d replace this with Vault, AWS, etc.</p> <pre><code># Create a \"source\" secret in a secured namespace (simulating an external provider)\nkubectl create namespace secret-store\nkubectl create secret generic app-secrets \\\n  --namespace secret-store \\\n  --from-literal=api-key=my-api-key-12345 \\\n  --from-literal=api-secret=super-secret-value\n</code></pre> <p>Create a SecretStore pointing to the Kubernetes provider:</p> <ul> <li>Source file: resources/secret-store.yaml</li> </ul> <pre><code># resources/secret-store.yaml\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: k8s-secret-store\n  namespace: secrets-lab\nspec:\n  provider:\n    kubernetes:\n      remoteNamespace: secret-store\n      server:\n        caProvider:\n          type: ConfigMap\n          name: kube-root-ca.crt\n          key: ca.crt\n      auth:\n        serviceAccount:\n          name: eso-reader\n</code></pre> <pre><code># Create the ServiceAccount for ESO\nkubectl create serviceaccount eso-reader -n secrets-lab\n\n# Grant it permission to read secrets in the source namespace\nkubectl create role secret-reader \\\n  --namespace secret-store \\\n  --verb=get,list,watch \\\n  --resource=secrets\n\nkubectl create rolebinding eso-secret-reader \\\n  --namespace secret-store \\\n  --role=secret-reader \\\n  --serviceaccount=secrets-lab:eso-reader\n\nkubectl apply -f resources/secret-store.yaml\n</code></pre>"},{"location":"06-DataStore/#15-create-an-externalsecret","title":"15. Create an ExternalSecret","text":"<ul> <li>Source file: resources/external-secret.yaml</li> </ul> <pre><code># resources/external-secret.yaml\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: app-credentials\n  namespace: secrets-lab\nspec:\n  refreshInterval: 1h            # How often to sync\n  secretStoreRef:\n    name: k8s-secret-store\n    kind: SecretStore\n  target:\n    name: app-credentials        # Name of the K8S Secret to create\n    creationPolicy: Owner\n  data:\n    - secretKey: API_KEY          # Key in the target Secret\n      remoteRef:\n        key: app-secrets          # Name of the source Secret\n        property: api-key         # Key in the source Secret\n    - secretKey: API_SECRET\n      remoteRef:\n        key: app-secrets\n        property: api-secret\n</code></pre> <pre><code>kubectl apply -f resources/external-secret.yaml\n\n# Check the ExternalSecret status\nkubectl get externalsecret -n secrets-lab\n# NAME              STORE              REFRESH INTERVAL   STATUS\n# app-credentials   k8s-secret-store   1h                 SecretSynced\n\n# The Secret was automatically created!\nkubectl get secret app-credentials -n secrets-lab\nkubectl get secret app-credentials -n secrets-lab -o jsonpath='{.data.API_KEY}' | base64 -d\n# my-api-key-12345\n</code></pre>"},{"location":"06-DataStore/#16-cleanup-part-2","title":"16. Cleanup (Part 2)","text":"<pre><code># Uninstall Sealed Secrets\nhelm uninstall sealed-secrets -n kube-system\n\n# Uninstall External Secrets\nhelm uninstall external-secrets -n external-secrets\n\n# Delete namespaces\nkubectl delete namespace secrets-lab secret-store external-secrets --ignore-not-found\n</code></pre>"},{"location":"06-DataStore/#part-2-summary","title":"Part 2 Summary","text":"Approach Best For Git-Safe? External Provider? Plain K8S Secrets Development/testing only \u274c No No Sealed Secrets GitOps - encrypt secrets for Git storage \u2705 Yes No External Secrets (ESO) Production - centralized secret management \u2705 Yes Yes (Vault, AWS\u2026) Concept Key Takeaway base64 \u2260 encryption K8S Secrets are encoded, not encrypted Sealed Secrets Encrypt + commit to Git; controller decrypts in-cluster kubeseal CLI Encrypts secrets using the controller\u2019s public key External Secrets Operator Syncs secrets from external vaults into K8S SecretStore Defines connection to external provider ExternalSecret Declares what to fetch and where to put it refreshInterval ESO periodically re-syncs - secrets stay up-to-date"},{"location":"06-DataStore/#exercises","title":"Exercises","text":"<p>The following exercises will test your understanding of Kubernetes secret management tools. Try to solve each exercise on your own before revealing the solution.</p>"},{"location":"06-DataStore/#01-seal-a-secret-and-verify-it-cannot-be-decoded-without-the-controller","title":"01. Seal a Secret and Verify It Cannot Be Decoded Without the Controller","text":"<p>Create a regular Secret, seal it with <code>kubeseal</code>, and then inspect the SealedSecret output. Verify the encrypted data cannot be decoded with <code>base64 --decode</code>.</p>"},{"location":"06-DataStore/#scenario","title":"Scenario:","text":"<p>\u25e6 You want to store credentials in Git but need to prove the encryption is real. \u25e6 You need to show that the SealedSecret is not just base64 - it\u2019s truly encrypted.</p> <p>Hint: Create a Secret with <code>--dry-run=client -o yaml</code>, pipe to <code>kubeseal --format yaml</code>, then try to base64-decode the <code>encryptedData</code> values.</p> Solution <pre><code>## Ensure the Sealed Secrets controller is installed\n## (see step 08 in this lab)\n\n## Create a regular Secret (don't apply it)\nkubectl create secret generic test-sealed \\\n  --namespace secrets-lab \\\n  --from-literal=api-key=my-secret-api-key-123 \\\n  --dry-run=client -o yaml &gt; /tmp/test-secret.yaml\n\n## View the regular Secret - base64 encoded but easily decoded\ncat /tmp/test-secret.yaml\nkubectl create secret generic test-sealed \\\n  --from-literal=api-key=my-secret-api-key-123 \\\n  --dry-run=client -o jsonpath='{.data.api-key}' | base64 -d\necho  ## Output: my-secret-api-key-123\n\n## Seal it\nkubeseal --format yaml &lt; /tmp/test-secret.yaml &gt; /tmp/sealed-test.yaml\n\n## View the SealedSecret - encrypted data\ncat /tmp/sealed-test.yaml\n\n## Try to base64-decode the encryptedData (will produce binary garbage, not readable)\ngrep \"api-key:\" /tmp/sealed-test.yaml | awk '{print $2}' | base64 -d 2&gt;&amp;1 || echo \"Cannot decode - it's encrypted, not just encoded!\"\n\n## Clean up\nrm /tmp/test-secret.yaml /tmp/sealed-test.yaml\n</code></pre>"},{"location":"06-DataStore/#02-rotate-a-sealed-secret","title":"02. Rotate a Sealed Secret","text":"<p>Update a SealedSecret by creating a new version with an updated password, apply it, and verify the controller updates the real Secret.</p>"},{"location":"06-DataStore/#scenario_1","title":"Scenario:","text":"<p>\u25e6 A database password has been rotated and you need to update the SealedSecret in Git. \u25e6 When the new SealedSecret is applied, the controller should update the real Secret automatically.</p> <p>Hint: Create a new Secret with the updated value, seal it again, and <code>kubectl apply</code> the new SealedSecret.</p> Solution <pre><code>## Create the original SealedSecret\nkubectl create secret generic rotate-demo \\\n  --namespace secrets-lab \\\n  --from-literal=password=old-password-v1 \\\n  --dry-run=client -o yaml | kubeseal --format yaml | kubectl apply -f -\n\n## Verify the original Secret was created\nkubectl get secret rotate-demo -n secrets-lab -o jsonpath='{.data.password}' | base64 -d\necho  ## Output: old-password-v1\n\n## Create a NEW SealedSecret with the rotated password\nkubectl create secret generic rotate-demo \\\n  --namespace secrets-lab \\\n  --from-literal=password=new-password-v2 \\\n  --dry-run=client -o yaml | kubeseal --format yaml | kubectl apply -f -\n\n## Verify the Secret was updated\nsleep 5\nkubectl get secret rotate-demo -n secrets-lab -o jsonpath='{.data.password}' | base64 -d\necho  ## Output: new-password-v2\n\n## Clean up\nkubectl delete sealedsecret rotate-demo -n secrets-lab 2&gt;/dev/null || true\nkubectl delete secret rotate-demo -n secrets-lab\n</code></pre>"},{"location":"06-DataStore/#03-create-a-clustersecretstore-and-use-it-across-namespaces","title":"03. Create a ClusterSecretStore and Use It Across Namespaces","text":"<p>Set up a <code>ClusterSecretStore</code> (using the Kubernetes provider) and create ExternalSecrets in two different namespaces that reference the same store.</p>"},{"location":"06-DataStore/#scenario_2","title":"Scenario:","text":"<p>\u25e6 Multiple teams need to access the same external secrets provider. \u25e6 Instead of creating a SecretStore in each namespace, you want a single cluster-wide store.</p> <p>Hint: Use <code>kind: ClusterSecretStore</code> and reference it with <code>kind: ClusterSecretStore</code> in the ExternalSecret\u2019s <code>secretStoreRef</code>.</p> Solution <pre><code>## Create source namespace with a shared Secret\nkubectl create namespace shared-secrets\nkubectl create secret generic shared-creds \\\n  --namespace shared-secrets \\\n  --from-literal=db-password=shared-db-pass-123\n\n## Create a service account for ESO\nkubectl create serviceaccount eso-cluster-reader -n external-secrets\n\n## Grant permissions to read from the source namespace\nkubectl create role secret-reader \\\n  --namespace shared-secrets \\\n  --verb=get,list,watch \\\n  --resource=secrets\nkubectl create rolebinding eso-cluster-reader-binding \\\n  --namespace shared-secrets \\\n  --role=secret-reader \\\n  --serviceaccount=external-secrets:eso-cluster-reader\n\n## Create the ClusterSecretStore\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: global-k8s-store\nspec:\n  provider:\n    kubernetes:\n      remoteNamespace: shared-secrets\n      server:\n        caProvider:\n          type: ConfigMap\n          name: kube-root-ca.crt\n          namespace: external-secrets\n          key: ca.crt\n      auth:\n        serviceAccount:\n          name: eso-cluster-reader\n          namespace: external-secrets\nEOF\n\n## Create ExternalSecrets in two namespaces\nkubectl create namespace team-alpha\nkubectl create namespace team-beta\n\nfor ns in team-alpha team-beta; do\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: db-creds\n  namespace: $ns\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: global-k8s-store\n    kind: ClusterSecretStore\n  target:\n    name: db-credentials\n    creationPolicy: Owner\n  data:\n    - secretKey: DB_PASSWORD\n      remoteRef:\n        key: shared-creds\n        property: db-password\nEOF\ndone\n\n## Verify Secrets in both namespaces\nkubectl get secret db-credentials -n team-alpha\nkubectl get secret db-credentials -n team-beta\n\n## Clean up\nkubectl delete namespace team-alpha team-beta shared-secrets\n</code></pre>"},{"location":"06-DataStore/#04-back-up-and-restore-sealed-secrets-controller-keys","title":"04. Back Up and Restore Sealed Secrets Controller Keys","text":"<p>Export the Sealed Secrets controller\u2019s encryption keys (for backup/disaster recovery) and verify you understand the key management lifecycle.</p>"},{"location":"06-DataStore/#scenario_3","title":"Scenario:","text":"<p>\u25e6 You need to back up the Sealed Secrets controller\u2019s private key for disaster recovery. \u25e6 If the controller is reinstalled without the backup, existing SealedSecrets cannot be decrypted.</p> <p>Hint: The controller\u2019s keys are stored as Secrets in the <code>kube-system</code> namespace with a label <code>sealedsecrets.bitnami.com/sealed-secrets-key</code>.</p> Solution <pre><code>## List the controller's encryption keys\nkubectl get secret -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key\n\n## Back up the key(s) to a file (KEEP THIS SECURE!)\nkubectl get secret -n kube-system \\\n  -l sealedsecrets.bitnami.com/sealed-secrets-key \\\n  -o yaml &gt; /tmp/sealed-secrets-backup.yaml\n\n## The backup contains the private key - treat it with extreme care\necho \"Backup saved. This file contains the private key and must be stored securely.\"\necho \"Without this key, existing SealedSecrets cannot be decrypted after controller reinstall.\"\n\n## To restore after reinstalling the controller:\n## kubectl apply -f /tmp/sealed-secrets-backup.yaml\n## kubectl rollout restart deployment/sealed-secrets-controller -n kube-system\n\n## Clean up the backup (in real scenarios, store it in a secure vault)\nrm /tmp/sealed-secrets-backup.yaml\n</code></pre>"},{"location":"06-DataStore/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>kubeseal cannot connect to the controller:</li> </ul> <p>Verify the Sealed Secrets controller is running and accessible:</p> <pre><code>## Check the controller pod\nkubectl get pods -n kube-system -l app.kubernetes.io/name=sealed-secrets\n\n## Check the controller service\nkubectl get service -n kube-system -l app.kubernetes.io/name=sealed-secrets\n\n## Try fetching the public key manually\nkubeseal --fetch-cert\n</code></pre> <p></p> <ul> <li>SealedSecret is not creating a Secret:</li> </ul> <p>Check the SealedSecret status and controller logs:</p> <pre><code>## Check the SealedSecret status\nkubectl get sealedsecret -n secrets-lab\nkubectl describe sealedsecret &lt;name&gt; -n secrets-lab\n\n## Check controller logs for errors\nkubectl logs -n kube-system -l app.kubernetes.io/name=sealed-secrets --tail=50\n</code></pre> <p></p> <ul> <li>ExternalSecret status shows error:</li> </ul> <p>Verify the SecretStore connection and permissions:</p> <pre><code>## Check the ExternalSecret status\nkubectl describe externalsecret &lt;name&gt; -n secrets-lab\n\n## Check the SecretStore status\nkubectl describe secretstore k8s-secret-store -n secrets-lab\n\n## Verify the ServiceAccount has correct RBAC\nkubectl auth can-i get secrets -n secret-store \\\n  --as system:serviceaccount:secrets-lab:eso-reader\n</code></pre>"},{"location":"07-nginx-Ingress/","title":"Nginx-Ingress","text":"<ul> <li>Kubernetes <code>ingress</code> object is a <code>DNS</code>.</li> <li>To enable an <code>ingress object</code>, we need an <code>ingress controller</code>.</li> <li>In this lab we will use <code>Nginx-Ingress</code> to route external traffic to services inside the cluster.</li> </ul>"},{"location":"07-nginx-Ingress/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>How to deploy an application and expose it as a service</li> <li>How to configure an Nginx Ingress controller</li> <li>How to create SSL certificates and store them as secrets</li> <li>How to deploy an Ingress resource with TLS</li> </ul>"},{"location":"07-nginx-Ingress/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> <li>Minikube (for the ingress addon) or an existing ingress controller</li> </ul> <p>Important</p> <p>We cannot see it in action on a <code>localhost</code> (meaning that it will not get an external IP) unless we use the explicit <code>http://host:port</code> format.</p>"},{"location":"07-nginx-Ingress/#01-deploy-sample-app","title":"01. Deploy Sample App","text":"<ul> <li>To get started with <code>Nginx-Ingress</code>, we will deploy out previous app:</li> </ul> <pre><code># Create 3 containers\nkubectl create deployment ingress-pods --image=nirgeier/k8s-secrets-sample --replicas=3\n\n# Expose the service\nkubectl expose deployment ingress-pods --port=5000\n</code></pre>"},{"location":"07-nginx-Ingress/#02-deploy-default-backend","title":"02. Deploy default backend","text":"<ul> <li>Now lets deploy the <code>Nginx-Ingress</code> (grabbed from the official site):</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n    name: default-http-backend\nspec:\n    replicas: 1\n    selector:\n        matchLabels:\n        app: default-http-backend\n    template:\n        metadata:\n        labels:\n            app: default-http-backend\n        spec:\n            terminationGracePeriodSeconds: 60\n            containers:\n            - name: default-http-backend\n                # Any image is permissable as long as:\n                # 1. It serves a 404 page at /\n                # 2. It serves 200 on a /healthz endpoint\n                image: gcr.io/google_containers/defaultbackend:1.0\n                livenessProbe:\n                httpGet:\n                    path: /healthz\n                    port: 8080\n                    scheme: HTTP\n                initialDelaySeconds: 30\n                timeoutSeconds: 5\n                ports:\n                - containerPort: 8080\n                resources:\n                limits:\n                    cpu: 10m\n                    memory: 20Mi\n                requests:\n                    cpu: 10m\n                    memory: 20Mi\n</code></pre>"},{"location":"07-nginx-Ingress/#03-create-service","title":"03. Create service","text":"<ul> <li>Next, let\u2019s create the service:</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n    name: default-http-backend\nspec:\n    selector:\n    app: default-http-backend\n    ports:\n    - protocol: TCP\n    port: 80\n    targetPort: 8080\n    type: NodePort\n</code></pre>"},{"location":"07-nginx-Ingress/#04-import-ssl-certificate","title":"04. Import <code>ssl</code> certificate","text":"<ul> <li>In this demo we will use certificate.</li> <li>The certificate is in the same folder as this file</li> <li>The certificate is for the hostname: <code>ingress.local</code></li> </ul> <pre><code># If you wish to create the certificate use this script\n### ---&gt; The common Name fiels is your host for later on\n###      Common Name (e.g. server FQDN or YOUR name) []:\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout certificate.key -out certificate.crt\n\n# Create a pem file\n# The purpose of the DH parameters is to exchange secrets\nopenssl dhparam -out certificate.pem 2048\n</code></pre> <ul> <li>Store the certificate in secret:</li> </ul> <pre><code># Store the certificate\nkubectl create secret tls tls-certificate --key certificate.key --cert certificate.crt\nsecret/tls-certificate created\n\n# Store the DH parameters\nkubectl create secret generic tls-dhparam --from-file=certificate.pem\nsecret/tls-dhparam created\n</code></pre>"},{"location":"07-nginx-Ingress/#05-deploy-the-ingress","title":"05. Deploy the ingress","text":"<ul> <li>Now that we have the certificate, we can deploy the <code>Ingress</code>:</li> </ul> <pre><code># Ingress.yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n    name: my-first-ingress\nannotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    nginx.org/ssl-services: \"my-service\"\nspec:\n    tls:\n        - hosts:\n        - myapp.local\n        secretName: tls-certificate\n    rules:\n    - host: myapp.local\n        http:\n        paths:\n        - path: /\n            backend:\n            serviceName: ingress-pods\n            servicePort: 5000\n</code></pre>"},{"location":"07-nginx-Ingress/#06-enable-the-ingress-addon","title":"06. Enable the ingress addon","text":"<ul> <li>The <code>Ingress</code> is not enabled by default, so we have to \u201cturn it on\u201d:</li> </ul> <pre><code>minikube addons enable ingress\n\u2705  ingress was successfully enabled\n</code></pre>"},{"location":"08-Kustomization/","title":"Kustomization - <code>kubectl kustomize</code>","text":"<ul> <li><code>Kustomize</code> is a very powerful tool for customizing and building Kubernetes resources.</li> <li><code>Kustomize</code> started in 2017, and was added to <code>kubectl</code> since version 1.14.</li> <li><code>Kustomize</code> has many useful features for managing and deploying resources.</li> <li>When you execute a Kustomization beside using the builtin features, it will also re-order the resources in a logical way for K8S to deploy.</li> </ul>"},{"location":"08-Kustomization/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>How Kustomize re-orders Kubernetes resources</li> <li>Common Kustomize features: annotations, labels, generators, images, namespaces, prefixes/suffixes, replicas</li> <li>How to use ConfigMap and Secret generators</li> <li>How to use patches to modify resources</li> </ul>"},{"location":"08-Kustomization/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster (v1.14+)</li> </ul>"},{"location":"08-Kustomization/#declarative-configuration-in-kubernetes","title":"Declarative Configuration in Kubernetes","text":""},{"location":"08-Kustomization/#01-re-order-the-resources","title":"01. Re-order the resources","text":"<ul> <li> <p><code>Kustomization</code> re-orders the <code>Kind</code> for optimization. For this demo, we will need an existing <code>namespace</code> before using it.</p> </li> <li> <p>The order of the resources is defined in the source code</p> </li> </ul> <pre><code>// An attempt to order things to help k8s, e.g.\n// - Namespace should be first.\n// - Service should come before things that refer to it.\n// In some cases order just specified to provide determinism.\nvar orderFirst = []string{\n    \"Namespace\",\n    \"ResourceQuota\",\n    \"StorageClass\",\n    \"CustomResourceDefinition\",\n    \"ServiceAccount\",\n    \"PodSecurityPolicy\",\n    \"Role\",\n    \"ClusterRole\",\n    \"RoleBinding\",\n    \"ClusterRoleBinding\",\n    \"ConfigMap\",\n    \"Secret\",\n    \"Endpoints\",\n    \"Service\",\n    \"LimitRange\",\n    \"PriorityClass\",\n    \"PersistentVolume\",\n    \"PersistentVolumeClaim\",\n    \"Deployment\",\n    \"StatefulSet\",\n    \"CronJob\",\n    \"PodDisruptionBudget\",\n}\n\nvar orderLast = []string{\n    \"MutatingWebhookConfiguration\",\n    \"ValidatingWebhookConfiguration\",\n}\n</code></pre>"},{"location":"08-Kustomization/#02-base-resource-for-our-demo","title":"02. Base resource for our demo","text":"<ul> <li>In the following samples we will refer to the following <code>base.yaml</code> file:</li> </ul> <pre><code># base.yaml\n# This is the base file for all the demos in this folder\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n        - name: myapp\n          image: __image__\n</code></pre>"},{"location":"08-Kustomization/#03-common-features","title":"03. Common Features","text":"<ul> <li>common Annotation</li> <li>common Labels</li> <li>Generators</li> <li>Config Map Generator<ul> <li>From Env</li> <li>From File</li> <li>From Literal</li> </ul> </li> <li>Secret Generator</li> <li>images</li> <li>Namespaces</li> <li>Prefix / Suffix</li> <li>Replicas</li> <li>Patches<ul> <li>Patch Add/Update</li> <li>Patch Delete</li> <li>Patch Replace</li> </ul> </li> </ul>"},{"location":"08-Kustomization/#commonannotation","title":"<code>commonAnnotation</code>","text":"<pre><code>kubectl kustomize samples/01-commonAnnotation\n</code></pre> <pre><code>### FileName: kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n# This will add annotation under every metadata entry\n# ex: main metadata, spec.metadata etc\ncommonAnnotations:\n  author: nirgeier@gmail.com\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>### commonAnnotation output\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    ### Annotation added here\n    author: nirgeier@gmail.com\n    name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      ### Annotation added here\n      annotations:\n        author: nirgeier@gmail.com\n      labels:\n        app: myapp\n    spec:\n      containers:\n        - image: __image__\n          name: myapp\n</code></pre>"},{"location":"08-Kustomization/#commonlabels","title":"<code>commonLabels</code>","text":"<pre><code>kubectl kustomize samples/02-commonLabels\n</code></pre> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n# This will add annotation under every metadata entry\n# ex: main metadata, spec.metadata etc\ncommonLabels:\n  author: nirgeier@gmail.com\n  env: codeWizard-cluster\n\nbases:\n  - ../_base\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n    # Labels added ....\n    labels:\n    author: nirgeier@gmail.com\n    env: codeWizard-cluster\n  name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n      # Labels added ....\n      author: nirgeier@gmail.com\n      env: codeWizard-cluster\n  template:\n    metadata:\n      labels:\n        app: myapp\n        # Labels added ....\n        author: nirgeier@gmail.com\n        env: codeWizard-cluster\n    spec:\n      containers:\n      - image: __image__\n        name: myapp\n</code></pre>"},{"location":"08-Kustomization/#generators","title":"Generators","text":"<ul> <li>Kustomization also support generate <code>ConfigMap</code> / <code>Secret</code> in several ways.</li> <li>The default behavior is adding the output hash value as suffix to the name, e.g.: <code>secretMapFromFile-495dtcb64g</code></li> </ul> <pre><code>apiVersion: v1\ndata:\n  APP_ENV: ZGV2ZWxvcG1lbnQ=\n  LOG_DEBUG: dHJ1ZQ==\n  NODE_ENV: ZGV2\n  REGION: d2V1\nkind: Secret\nmetadata:\n  name: secretMapFromFile-495dtcb64g # &lt;--------------------------\ntype: Opaque\n</code></pre> <ul> <li>We can disable the suffix with the following addition to the <code>kustomization.yaml</code></li> </ul> <pre><code>generatorOptions:\n  disableNameSuffixHash: true\n</code></pre>"},{"location":"08-Kustomization/#configmapgenerator","title":"<code>configMapGenerator</code>","text":""},{"location":"08-Kustomization/#from-env","title":"From Env","text":"<ul> <li> <p><code>.env</code> <pre><code>key1=value1\nenv=qa\n</code></pre></p> <ul> <li> <p><code>kustomization.yaml</code> <pre><code># Generate config file from env file\nconfigMapGenerator:\n  - name: configMapFromEnv\n    env: .env\n</code></pre></p> </li> <li> <p>The output of <code>configMapFromEnv</code>:   <pre><code>apiVersion: v1\ndata:\n  env: qa\n  key1: value1\nkind: ConfigMap\nmetadata:\n  name: configMapFromEnv-c9655hf97k\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"08-Kustomization/#from-file","title":"From File","text":"<ul> <li> <p><code>.env</code> <pre><code>key1=value1\nenv=qa\n</code></pre></p> <ul> <li> <p><code>kustomization.yaml</code> <pre><code># Generate config file from env file\nconfigMapGenerator:\n  - name: configMapFromEnv\n    files:\n    - .env\n</code></pre></p> </li> <li> <p>The output of <code>configMapFromEnv</code>:   <pre><code>apiVersion: v1\ndata:\n  .env: \"key1=value1\\r\\nenv=qa\" # &lt;--------------------------\nkind: ConfigMap\nmetadata:\n  name: configFromFile-dfhmctd84d\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"08-Kustomization/#from-literal","title":"From Literal","text":"<ul> <li> <p><code>.env</code> <pre><code>key1=value1\nenv=qa\n</code></pre></p> <ul> <li> <p><code>kustomization.yaml</code> <pre><code>configMapGenerator:\n  - name: configFromLiterals\n    literals:\n      - Key1=value1\n      - Key2=value2\n</code></pre></p> </li> <li> <p>The output of <code>configMapFromEnv</code>:   <pre><code>apiVersion: v1\ndata:\n  Key1: value1\n  Key2: value2\nkind: ConfigMap\nmetadata:\n  name: configFromLiterals-h777b4gdf5\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"08-Kustomization/#secret-generator","title":"<code>Secret</code> Generator","text":"<pre><code># Similar to configMap but with an additional type field\nsecretGenerator:\n  # Generate secret from env file\n  - name: secretMapFromFile\n    env: .env\n    type: Opaque\ngeneratorOptions:\n  disableNameSuffixHash: true\n</code></pre>"},{"location":"08-Kustomization/#images","title":"<code>images</code>","text":"<ul> <li>Modify the name, tags and/or digest for images.</li> </ul> <pre><code>kubectl kustomize samples/04-images\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - ./base.yaml\n\nimages:\n  # The image as its defined in the Deployment file\n  - name: __image__\n    # The new name to set\n    newName: my-registry/my-image\n    # optional: image tag\n    newTag: v1\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n        # --- This image was updated\n        - image: my-registry/my-image:v1\n          name: myapp\n</code></pre>"},{"location":"08-Kustomization/#namespaces","title":"<code>Namespaces</code>","text":"<pre><code>kubectl kustomize samples/05-Namespace\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n# Add the desired namespace to all resources\nnamespace: kustomize-namespace\n\nbases:\n  - ../_base\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  # Namespace added here\n  namespace: kustomize-namespace\n</code></pre>"},{"location":"08-Kustomization/#prefix-suffix","title":"<code>Prefix-suffix</code>","text":"<pre><code>kubectl kustomize samples/06-Prefix-Suffix\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n# Add the desired Prefix to all resources\nnamePrefix: prefix-codeWizard-\nnameSuffix: -suffix-codeWizard\n\nbases:\n  - ../_base\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prefix-codeWizard-myapp-suffix-codeWizard\n</code></pre>"},{"location":"08-Kustomization/#replicas","title":"<code>Replicas</code>","text":"<ul> <li>deployment</li> </ul> <pre><code># deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment\nspec:\n  replicas: 5\n  selector:\n    name: deployment\n  template:\n    containers:\n      - name: container\n        image: registry/conatiner:latest\n</code></pre> <ul> <li>kustomization</li> </ul> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nreplicas:\n  - name: deployment\n    count: 10\n\nresources:\n  - deployment.yaml\n</code></pre> <ul> <li>Output:</li> </ul> <p>Note</p> <p>There is a bug with the <code>replicas</code> entries which return error for some reason.</p> <pre><code>kubectl kustomize .\n\n# For some reason we get this error:\nError: json: unknown field \"replicas\"\n\n# Workaround for this error for now is:\nkustomize build .\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment\nspec:\n  replicas: 10\n  selector:\n    name: deployment\n  template:\n    containers:\n      - image: registry/conatiner:latest\n        name: container\n</code></pre>"},{"location":"08-Kustomization/#patches","title":"Patches","text":"<ul> <li>There are several types of patches like [<code>replace</code>, <code>delete</code>, <code>patchesStrategicMerge</code>]</li> <li>For this demo we will demonstrate <code>patchesStrategicMerge</code></li> </ul>"},{"location":"08-Kustomization/#patch-addupdate","title":"Patch Add/Update","text":"<pre><code>kubectl kustomize samples/08-Patches/patch-add-update\n</code></pre> <pre><code># File: patch-memory.yaml\n# -----------------------\n# Patch limits.memory\napiVersion: apps/v1\nkind: Deployment\n# Set the desired deployment to patch\nmetadata:\n  name: myapp\nspec:\n  # patch the memory limit\n  template:\n    spec:\n      containers:\n        - name: patch-name\n          resources:\n            limits:\n              memory: 512Mi\n</code></pre> <pre><code># File: patch-replicas.yaml\n# -------------------------\napiVersion: apps/v1\nkind: Deployment\n# Set the desired deployment to patch\nmetadata:\n  name: myapp\nspec:\n  # This is the patch for this demo\n  replicas: 3\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nbases:\n  - ../_base\n\npatchesStrategicMerge:\n- patch-memory.yaml\n- patch-replicas.yaml\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  # This is the first patch\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      # This is the second patch\n      containers:\n      - name: patch-name\n        resources:\n          limits:\n            memory: 512Mi\n      - image: __image__\n        name: myapp\n</code></pre>"},{"location":"08-Kustomization/#patch-delete","title":"Patch-Delete","text":"<pre><code>kubectl kustomize samples/08-Patches/patch-delete\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nbases:\n  - ../../_base\n\npatchesStrategicMerge:\n- patch-delete.yaml\n</code></pre> <pre><code># patch-delete.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  template:\n    spec:\n      containers:\n        # Remove this section, in this demo it will remove the\n        # image with the `name: myapp`\n        - $patch: delete\n          name: myapp\n          image: __image__\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n</code></pre>"},{"location":"08-Kustomization/#patch-replace","title":"Patch Replace","text":"<pre><code>kubectl kustomize samples/08-Patches/patch-replace/\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nbases:\n  - ../../_base\n\npatchesStrategicMerge:\n- patch-replace.yaml\n</code></pre> <pre><code># patch-replace.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  template:\n    spec:\n      containers:\n        # Remove this section, in this demo it will remove the\n        # image with the `name: myapp`\n        - $patch: replace\n        - name: myapp\n          image: nginx:latest\n          args:\n          - one\n          - two\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - args:\n        - one\n        - two\n        image: nginx:latest\n        name: myapp\n</code></pre>"},{"location":"09-StatefulSet/","title":"StatefulSets","text":"<ul> <li>In this lab we will learn about <code>StatefulSets</code> in Kubernetes and how they differ from <code>Deployments</code>.</li> <li>We will deploy a PostgreSQL database as a StatefulSet and verify that data persists across pod restarts.</li> </ul>"},{"location":"09-StatefulSet/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>The difference between stateless and stateful applications</li> <li>How <code>StatefulSets</code> maintain sticky identities and stable storage</li> <li>How to deploy a PostgreSQL database as a StatefulSet</li> <li>How to verify data persistence across pod restarts and scaling</li> </ul>"},{"location":"09-StatefulSet/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> <li><code>psql</code> client installed (for testing PostgreSQL)</li> </ul>"},{"location":"09-StatefulSet/#introduction","title":"Introduction","text":""},{"location":"09-StatefulSet/#the-difference-between-a-statefulset-and-a-deployment","title":"The Difference Between a <code>Statefulset</code> And a <code>Deployment</code>","text":""},{"location":"09-StatefulSet/#stateless-application","title":"<code>Stateless</code> application","text":"<ul> <li>A stateless application is one that does not care which network it is using, and it does not need permanent storage and can be scaled up and down without the need to re-use the same network or persistence.</li> <li>Deployment is the suitable kind for Stateless applications.</li> <li>The most trivial example of stateless app is a <code>Web Server</code>.</li> </ul>"},{"location":"09-StatefulSet/#stateful-application","title":"<code>Stateful</code> application","text":"<ul> <li>Stateful applications are apps which in order to work properly need to use the same resources, such as network, storage etc.</li> <li>Usually with <code>Stateful</code> applications you will need to ensure that pods can reach each other through a unique identity that does not change (e.g., hostnames, IP).</li> <li>The most trivial example of Stateful app is a database of any kind.</li> </ul> <p>Stateful Notes</p> <ul> <li>Like a Deployment, a <code>StatefulSet</code> manages Pods that are based on an identical container spec.</li> <li>Unlike a Deployment, a <code>StatefulSet</code> maintains a sticky identity for each of their Pods.</li> <li>These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</li> <li>Deleting and/or scaling down a <code>StatefulSet</code> will not delete the volumes associated with the <code>StatefulSet</code>. This is done to ensure data safety.</li> <li><code>StatefulSet</code> keeps a unique identity for each Pod and assign the same identity to those pods when they are rescheduled (update, restart etc).</li> <li>The storage for a given Pod must either be provisioned by a <code>PersistentVolume</code> provisioner, based on the requested storage class, or pre-provisioned by an admin.</li> <li><code>StatefulSet</code> manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.</li> <li>A <code>stateful</code> app needs to use a dedicated storage.</li> </ul>"},{"location":"09-StatefulSet/#stable-network-identity","title":"Stable Network Identity","text":"<ul> <li>A <code>Stateful</code> application node must have a unique hostname and IP address so that other nodes in the same application know how to reach it.</li> <li>A <code>ReplicaSet</code> assign a random hostname and IP address to each Pod. In such a case, we must use a service which exposes those Pods for us.</li> </ul>"},{"location":"09-StatefulSet/#start-and-termination-order","title":"Start and Termination Order","text":"<ul> <li>Each <code>StatefulSet</code> follows this naming pattern: <code>$(statefulSet name)-$(ordinal)</code></li> <li><code>Stateful</code> applications restarted or re-created, following the creation order.</li> <li>A <code>ReplicaSet</code> does not follow a specific order when starting or killing its pods.</li> </ul>"},{"location":"09-StatefulSet/#statefulset-volumes","title":"StatefulSet Volumes","text":"<ul> <li><code>StatefulSet</code> does not create a volume for you.</li> <li>When a <code>StatefulSet</code> is deleted, the respective volumes are not deleted with it.</li> </ul>"},{"location":"09-StatefulSet/#to-address-all-these-requirements-kubernetes-offers-the-statefulset-primitive","title":"To address all these requirements, Kubernetes offers the <code>StatefulSet primitive</code>.","text":""},{"location":"09-StatefulSet/#01-create-namespace-and-clear-previous-data-if-there-is-any","title":"01. Create namespace and clear previous data if there is any","text":"<pre><code># If the namespace already exist and contains data form previous steps, lets clean it\nkubectl delete namespace codewizard\n\n# Create the desired namespace [codewizard]\nkubectl create namespace codewizard\nnamespace/codewizard created\n</code></pre>"},{"location":"09-StatefulSet/#02-create-and-test-the-stateful-application","title":"02. Create and test the Stateful application","text":"<ul> <li> <p>In order to deploy the Stateful set we will need the following resources:</p> </li> <li> <p><code>ConfigMap</code></p> </li> <li><code>Service</code></li> <li><code>StatefulSet</code></li> <li> <p><code>PersistentVolumeClaim or</code>PersistentVolume`</p> </li> <li> <p>All the resources including <code>kustomization</code> script are defined inside the base folder</p> </li> </ul> <ul> <li>ConfigMap.yaml</li> </ul> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: postgres-config\n  labels:\n    app: postgres\ndata:\n  # The following names are the one defined in the officail postgres docs\n\n  # The name of the database we will use in this demo\n  POSTGRES_DB: codewizard\n  # the user name for this demo\n  POSTGRES_USER: codewizard\n  # The password for this demo\n  POSTGRES_PASSWORD: admin123\n</code></pre> <ul> <li> <p>Service.yaml <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\nspec:\n  selector:\n    app: postgres\n  # Service of type nodeport\n  type: NodePort\n  # The deafult port for postgres\n  ports:\n    - port: 5432\n</code></pre></p> </li> <li> <p>PersistentVolumeClaim.yaml</p> </li> </ul> <pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: postgres-pv-claim\n  labels:\n    app: postgres\nspec:\n  # in this demo we use GCP so we are using the 'standard' StorageClass\n  # We can of course define our own StorageClass resource\n  storageClassName: standard\n\n  # The access modes are:\n  #   ReadWriteOnce - The volume can be mounted as read-write by a single node\n  #   ReadWriteMany - The volume can be mounted as read-write by a many node\n  #   ReadOnlyMany  - The volume can be mounted as read-only  by many nodes\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre> <ul> <li>StatefulSet.yaml</li> </ul> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres\nspec:\n  replicas: 1\n  # StatefulSet must contain a serviceName\n  serviceName: postgres\n  selector:\n    matchLabels:\n      app: postgres # has to match .spec.template.metadata.labels\n  template:\n    metadata:\n      labels:\n        app: postgres # has to match .spec.selector.matchLabels\n    spec:\n      containers:\n        - name: postgres\n          image: postgres:10.4\n          imagePullPolicy: \"IfNotPresent\"\n          # The default DB port\n          ports:\n            - containerPort: 5432\n          # Load the required configuration env values form the configMap\n          envFrom:\n            - configMapRef:\n                name: postgres-config\n          # Use volume for storage\n          volumeMounts:\n            - mountPath: /var/lib/postgresql/data\n              name: postgredb\n      # We can use PersistentVolume or PersistentVolumeClaim.\n      # In this sample we are useing PersistentVolumeClaim\n      volumes:\n        - name: postgredb\n          persistentVolumeClaim:\n            # reference to Pre-Define PVC\n            claimName: postgres-pv-claim\n</code></pre> <p>Note: You can use the kustomization file to create or apply all the above resources</p> <pre><code># Generate and apply the required resources using kustomization\nkubectl kustomize PostgreSQL/ | kubectl apply -f -\n</code></pre>"},{"location":"09-StatefulSet/#03-test-the-stateful-application","title":"03. Test the Stateful application","text":"<ul> <li>Use the - testDB.sh to test the StatefulSet</li> <li>Don\u2019t forget to set the execution flag <code>chmod +x testDb.sh</code> if required</li> </ul> <pre><code>### Test to see if the StatefulSet \"saves\" the state of the pods\n\n# Programmatically get the port and the IP\nexport CLUSTER_IP=$(kubectl get nodes \\\n            --selector=node-role.kubernetes.io/control-plane \\\n            -o jsonpath='{$.items[*].status.addresses[?(@.type==\"InternalIP\")].address}')\n\nexport NODE_PORT=$(kubectl get \\\n            services postgres \\\n            -o jsonpath=\"{.spec.ports[0].nodePort}\" \\\n            -n codewizard)\n\nexport POSTGRES_DB=$(kubectl get \\\n            configmap postgres-config \\\n            -o jsonpath='{.data.POSTGRES_DB}' \\\n            -n codewizard)\n\nexport POSTGRES_USER=$(kubectl get \\\n            configmap postgres-config \\\n            -o jsonpath='{.data.POSTGRES_USER}' \\\n            -n codewizard)\n\nexport PGPASSWORD=$(kubectl get \\\n            configmap postgres-config \\\n            -o jsonpath='{.data.POSTGRES_PASSWORD}' \\\n            -n codewizard)\n\n# Check to see if we have all the required variables\nprintenv | grep POST*\n\n# Connect to postgres and create table if required.\n# Once the table exists - add row into the table\n# you can run this command as amny times as you like\npsql \\\n    -U ${POSTGRES_USER} \\\n    -h ${CLUSTER_IP} \\\n    -d ${POSTGRES_DB} \\\n    -p ${NODE_PORT} \\\n    -c \"CREATE TABLE IF NOT EXISTS stateful (str VARCHAR); INSERT INTO stateful values (1); SELECT count(*) FROM stateful\"\n</code></pre>"},{"location":"09-StatefulSet/#04-scale-down-the-statefulset-and-check-that-its-down","title":"04. Scale down the StatefulSet and check that its down","text":""},{"location":"09-StatefulSet/#0401-scale-down-the-statefulset-to-0","title":"04.01. Scale down the <code>Statefulset</code> to 0","text":"<pre><code># scale down the `Statefulset` to 0\nkubectl scale statefulset postgres -n codewizard --replicas=0\n</code></pre>"},{"location":"09-StatefulSet/#0402-verify-that-the-pods-terminated","title":"04.02. Verify that the pods Terminated","text":"<pre><code># Wait until the pods will be terminated\nkubectl get pods -n codewizard --watch\nNAME         READY   STATUS    RESTARTS   AGE\npostgres-0   1/1     Running   0          32m\npostgres-0   1/1     Terminating   0      32m\npostgres-0   0/1     Terminating   0      32m\npostgres-0   0/1     Terminating   0      33m\npostgres-0   0/1     Terminating   0      33m\n</code></pre>"},{"location":"09-StatefulSet/#0403-verify-that-the-db-is-not-reachable","title":"04.03. Verify that the DB is not reachable","text":"<ul> <li>If the DB is not reachable it mean that all the pods are down</li> </ul> <pre><code>psql \\\n    -U ${POSTGRES_USER} \\\n    -h ${CLUSTER_IP} \\\n    -d ${POSTGRES_DB} \\\n    -p ${NODE_PORT} \\\n    -c \"SELECT count(*) FROM stateful\"\n\n# You should get output similar to this one:\npsql: error: could not connect to server: Connection refused\n        Is the server running on host \"192.168.49.2\" and accepting\n        TCP/IP connections on port 32570?\n</code></pre>"},{"location":"09-StatefulSet/#05-scale-up-again-and-verify-that-we-still-have-the-prevoius-data","title":"05. Scale up again and verify that we still have the prevoius data","text":""},{"location":"09-StatefulSet/#0501-scale-up-the-statefulset-to-1-or-more","title":"05.01. scale up the <code>Statefulset</code> to 1 or more","text":"<pre><code># scale up the `Statefulset`\nkubectl scale statefulset postgres -n codewizard --replicas=1\n</code></pre>"},{"location":"09-StatefulSet/#0502-verify-that-the-pods-is-in-running-status","title":"05.02. Verify that the pods is in Running status","text":"<pre><code>kubectl get pods -n codewizard --watch\nNAME         READY   STATUS    RESTARTS   AGE\npostgres-0   1/1     Running   0          5s\n</code></pre>"},{"location":"09-StatefulSet/#0503-verify-that-the-pods-is-using-the-previous-data","title":"05.03. Verify that the pods is using the previous data","text":"<pre><code>psql \\\n    -U ${POSTGRES_USER} \\\n    -h ${CLUSTER_IP} \\\n    -d ${POSTGRES_DB} \\\n    -p ${NODE_PORT} \\\n    -c \"SELECT count(*) FROM stateful\"\n# The output should be similar to this one\n\n count\n-------\n     2\n(1 row)\n</code></pre>"},{"location":"10-Istio/","title":"Istio Service Mesh &amp; Kiali","text":"<ul> <li><code>Istio</code> is an open-source service mesh that provides a uniform way to manage microservices communication.</li> <li>This lab demonstrates a complete Istio service mesh deployment on Kubernetes with Kiali for observability.</li> <li>Everything is installed via Helm charts for reproducibility and production-readiness.</li> </ul>"},{"location":"10-Istio/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>Install and configure Istio service mesh using Helm</li> <li>Deploy Kiali, Prometheus, Grafana, and Jaeger as observability addons</li> <li>Deploy a microservices demo application with sidecar injection</li> <li>Generate live traffic and observe it in Kiali\u2019s topology graph</li> <li>Configure traffic management: routing, canary deployments, fault injection</li> <li>Enable and verify mutual TLS (mTLS) between services</li> <li>Use circuit breakers, timeouts, and rate limiting</li> <li>Perform traffic shifting and A/B testing</li> <li>Observe distributed traces in Jaeger</li> <li>Monitor service metrics in Grafana dashboards</li> </ul>"},{"location":"10-Istio/#what-is-istio","title":"What is Istio?","text":"<ul> <li><code>Istio</code> extends Kubernetes to establish a programmable, application-aware network using the Envoy service proxy.</li> <li>Istio provides a control plane (Istiod) and a data plane (Envoy sidecars injected into every pod).</li> <li>It requires zero application code changes - all features are handled transparently by the mesh.</li> </ul>"},{"location":"10-Istio/#core-components","title":"Core Components","text":"Component Role Default Port Istiod Control plane - manages configuration, certificates, service discovery N/A Envoy Sidecar proxy injected into each pod - intercepts all pod network traffic N/A Kiali Service mesh observability console - topology, health, config validation <code>20001</code> Prometheus Metrics collection and storage for Istio telemetry <code>9090</code> Grafana Dashboards for mesh, service, and workload metrics <code>3000</code> Jaeger Distributed tracing backend and UI <code>16686</code>"},{"location":"10-Istio/#istio-key-crds","title":"Istio Key CRDs","text":"CRD Purpose <code>VirtualService</code> Define routing rules: traffic shifting, fault injection, timeouts <code>DestinationRule</code> Define policies after routing: load balancing, circuit breaker, mTLS <code>Gateway</code> Configure load balancer at mesh edge for HTTP/TCP traffic <code>PeerAuthentication</code> Configure mTLS mode: <code>STRICT</code>, <code>PERMISSIVE</code>, <code>DISABLE</code> <code>AuthorizationPolicy</code> Access control policies for workloads <code>ServiceEntry</code> Add external services (outside the mesh) to Istio\u2019s service registry"},{"location":"10-Istio/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    ext[\"External Traffic\"] --&gt; gw\n\n    subgraph cluster[\"Kubernetes Cluster\"]\n        subgraph istio[\"istio-system namespace\"]\n            gw[\"Istio Ingress Gateway\"]\n            istiod[\"Istiod\\n(control plane)\"]\n            prometheus[\"Prometheus\"]\n            grafana[\"Grafana\"]\n            jaeger[\"Jaeger\"]\n            kiali[\"Kiali\"]\n        end\n\n        subgraph bookinfo[\"bookinfo namespace  (istio-injection=enabled)\"]\n            pp[\"productpage v1 + Envoy\"]\n            det[\"details v1 + Envoy\"]\n            rv1[\"reviews v1\\n(no stars)\"]\n            rv2[\"reviews v2\\n(black stars)\"]\n            rv3[\"reviews v3\\n(red stars)\"]\n            rat[\"ratings v1 + Envoy\"]\n        end\n\n        subgraph tgns[\"traffic-gen namespace\"]\n            tgen[\"Traffic Generator\\n(CronJob)\"]\n        end\n\n        gw --&gt; pp\n        tgen --&gt; gw\n        pp --&gt; det\n        pp --&gt; rv1\n        pp --&gt; rv2\n        pp --&gt; rv3\n        rv2 --&gt; rat\n        rv3 --&gt; rat\n\n        istiod -. config/certs .-&gt; pp\n        istiod -. config/certs .-&gt; det\n        istiod -. config/certs .-&gt; rv1\n\n        pp -. metrics .-&gt; prometheus\n        det -. metrics .-&gt; prometheus\n        rv1 -. metrics .-&gt; prometheus\n        pp -. traces .-&gt; jaeger\n\n        prometheus --&gt; kiali\n        jaeger --&gt; kiali\n        istiod --&gt; kiali\n        prometheus --&gt; grafana\n    end</code></pre>"},{"location":"10-Istio/#directory-structure","title":"Directory Structure","text":"<pre><code>10-Istio/\n\u251c\u2500\u2500 README.md                          # This file\n\u251c\u2500\u2500 demo.sh                            # Main deployment script (deploy/cleanup)\n\u251c\u2500\u2500 monitor.sh                         # Interactive monitoring &amp; status checks\n\u2502\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 common.sh                      # Shared functions &amp; colors\n\u2502   \u251c\u2500\u2500 01-install-istio.sh            # Install Istio via Helm\n\u2502   \u251c\u2500\u2500 02-install-addons.sh           # Install Kiali, Prometheus, Grafana, Jaeger\n\u2502   \u251c\u2500\u2500 03-deploy-bookinfo.sh          # Deploy Bookinfo sample application\n\u2502   \u251c\u2500\u2500 04-traffic-generator.sh        # Deploy live traffic generator\n\u2502   \u2514\u2500\u2500 05-verify.sh                   # Verify all components\n\u2502\n\u251c\u2500\u2500 manifests/\n\u2502   \u251c\u2500\u2500 namespace.yaml                 # bookinfo namespace with injection label\n\u2502   \u251c\u2500\u2500 bookinfo.yaml                  # Bookinfo application manifests\n\u2502   \u251c\u2500\u2500 bookinfo-gateway.yaml          # Istio Gateway + VirtualService for ingress\n\u2502   \u251c\u2500\u2500 destination-rules.yaml         # DestinationRules for all service versions\n\u2502   \u251c\u2500\u2500 traffic-generator.yaml         # CronJob for continuous traffic generation\n\u2502   \u2514\u2500\u2500 addons/                        # Observability addon manifests\n\u2502       \u251c\u2500\u2500 prometheus.yaml\n\u2502       \u251c\u2500\u2500 grafana.yaml\n\u2502       \u251c\u2500\u2500 jaeger.yaml\n\u2502       \u2514\u2500\u2500 kiali.yaml\n\u2502\n\u2514\u2500\u2500 istio-features/\n    \u251c\u2500\u2500 01-traffic-shifting.yaml       # Canary: route % of traffic to v2/v3\n    \u251c\u2500\u2500 02-fault-injection.yaml        # Inject delays and HTTP errors\n    \u251c\u2500\u2500 03-circuit-breaker.yaml        # Circuit breaker with connection limits\n    \u251c\u2500\u2500 04-request-routing.yaml        # Route by header (user identity)\n    \u251c\u2500\u2500 05-timeout-retry.yaml          # Configure timeouts and retries\n    \u251c\u2500\u2500 06-mirror-traffic.yaml         # Traffic mirroring / shadow traffic\n    \u251c\u2500\u2500 07-mtls-strict.yaml            # Enforce strict mTLS\n    \u2514\u2500\u2500 apply-feature.sh               # Apply/reset feature demos\n</code></pre>"},{"location":"10-Istio/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (v1.24+) with at least 8 GB RAM available</li> <li><code>kubectl</code> configured to access your cluster</li> <li><code>Helm 3.x</code> installed</li> <li>Nginx Ingress Controller (required for Ingress-based access to dashboards and Bookinfo)</li> <li>(Optional) <code>istioctl</code> for debugging</li> </ul> <pre><code># Install kubectl (macOS)\nbrew install kubectl\n\n# Install Helm\nbrew install helm\n\n# Install istioctl (optional)\nbrew install istioctl\n\n# Verify installations\nkubectl version --client\nhelm version\n</code></pre>"},{"location":"10-Istio/#lab","title":"Lab","text":""},{"location":"10-Istio/#part-01-deploy-istio-service-mesh","title":"Part 01 - Deploy Istio Service Mesh","text":""},{"location":"10-Istio/#01-deploy-everything","title":"01. Deploy Everything","text":"<pre><code># Make scripts executable\nchmod +x demo.sh monitor.sh scripts/*.sh istio-features/apply-feature.sh\n\n# Deploy Istio + addons + Bookinfo + traffic generator\n./demo.sh deploy\n</code></pre> <p>The script will:</p> <ul> <li>Check prerequisites: <code>kubectl</code>, <code>helm</code>, cluster connectivity</li> <li>Install Istio CRDs and control plane via Helm</li> <li>Install Kiali, Prometheus, Grafana, and Jaeger</li> <li>Create the <code>bookinfo</code> namespace with sidecar injection enabled</li> <li>Deploy the Bookinfo sample application (4 microservices, multiple versions)</li> <li>Configure the Istio Ingress Gateway and DestinationRules</li> <li>Start continuous traffic generation via CronJob</li> <li>Wait for all pods to be in <code>Running</code> state and print access URLs</li> </ul>"},{"location":"10-Istio/#02-access-the-uis","title":"02. Access the UIs","text":"<p>After deployment, open the dashboards using port-forwarding:</p> <pre><code># Kiali - Service Mesh Observability Console\nkubectl port-forward svc/kiali -n istio-system 20001:20001 &amp;\nopen http://localhost:20001\n\n# Grafana - Metrics Dashboards\nkubectl port-forward svc/grafana -n istio-system 3000:3000 &amp;\nopen http://localhost:3000\n\n# Jaeger - Distributed Tracing UI\nkubectl port-forward svc/tracing -n istio-system 16686:80 &amp;\nopen http://localhost:16686\n\n# Prometheus - Metrics Queries\nkubectl port-forward svc/prometheus -n istio-system 9090:9090 &amp;\nopen http://localhost:9090\n\n# Bookinfo Application\nkubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80 &amp;\nopen http://localhost:8080/productpage\n</code></pre>"},{"location":"10-Istio/#03-explore-kiali","title":"03. Explore Kiali","text":"<ol> <li>Open Kiali at <code>http://localhost:20001</code></li> <li>Navigate to Graph and select the <code>bookinfo</code> namespace</li> <li>Observe live traffic flowing between services as coloured edges</li> <li>Click on any service to inspect metrics, traces, and health status</li> <li>Check Workloads to confirm Envoy sidecar injection on all pods</li> </ol>"},{"location":"10-Istio/#part-02-bookinfo-application","title":"Part 02 - Bookinfo Application","text":"<p>The Bookinfo application consists of four microservices demonstrating multiple service versions:</p> Service Versions Description productpage v1 Main frontend - calls details and reviews details v1 Book details information reviews v1, v2, v3 Book reviews (v1: no stars, v2: black stars, v3: red stars) ratings v1 Star ratings (called by reviews v2 and v3 only)"},{"location":"10-Istio/#application-flow","title":"Application Flow","text":"<pre><code>graph LR\n    user[\"User\"] --&gt; pp[\"productpage v1\"]\n    pp --&gt; det[\"details v1\"]\n    pp --&gt; rv1[\"reviews v1\\n(no stars)\"]\n    pp --&gt; rv2[\"reviews v2\\n(black stars)\"]\n    pp --&gt; rv3[\"reviews v3\\n(red stars)\"]\n    rv2 --&gt; rat[\"ratings v1\"]\n    rv3 --&gt; rat</code></pre> <p>Note</p> <p>Each pod in the <code>bookinfo</code> namespace has an Envoy sidecar proxy automatically injected. All network traffic passes through the sidecar, enabling telemetry, traffic management, and mTLS with zero application changes.</p>"},{"location":"10-Istio/#istio-configuration","title":"Istio Configuration","text":"<p>The lab uses custom Istio settings optimized for demonstrations:</p> <pre><code>meshConfig:\n  accessLogFile: /dev/stdout    # Enable access logging\n  enableTracing: true           # Enable distributed tracing\n  defaultConfig:\n    tracing:\n      sampling: 100.0           # 100% trace sampling (demo only)\n    holdApplicationUntilProxyStarts: true\n</code></pre>"},{"location":"10-Istio/#part-03-traffic-management","title":"Part 03 - Traffic Management","text":""},{"location":"10-Istio/#01-traffic-shifting-canary-deployment","title":"01. Traffic Shifting / Canary Deployment","text":"<p>Route a configurable percentage of traffic to different service versions:</p> <pre><code>./istio-features/apply-feature.sh 01-traffic-shifting\n</code></pre> <pre><code>graph LR\n    pp[\"productpage\"] --&gt;|\"80%\"| rv1[\"reviews v1\\n(no stars)\"]\n    pp --&gt;|\"10%\"| rv2[\"reviews v2\\n(black stars)\"]\n    pp --&gt;|\"10%\"| rv3[\"reviews v3\\n(red stars)\"]\n    rv2 --&gt; rat[\"ratings v1\"]\n    rv3 --&gt; rat</code></pre> <p>Tip</p> <p>Observe in Kiali: The Graph view shows weighted edges indicating the traffic split between versions.</p>"},{"location":"10-Istio/#02-request-routing-header-based","title":"02. Request Routing (Header-Based)","text":"<p>Route specific users to specific service versions based on HTTP headers:</p> <pre><code>./istio-features/apply-feature.sh 04-request-routing\n</code></pre> <pre><code>graph LR\n    jason[\"User: jason\"] --&gt;|\"Header: end-user=jason\"| pp[\"productpage\"]\n    other[\"Other users\"] --&gt; pp\n    pp --&gt;|\"jason\"| rv2[\"reviews v2\\n(black stars)\"]\n    pp --&gt;|\"others\"| rv1[\"reviews v1\\n(no stars)\"]</code></pre> <pre><code># Test it: log in as \"jason\" on the productpage\nopen http://localhost:8080/productpage\n</code></pre>"},{"location":"10-Istio/#03-fault-injection","title":"03. Fault Injection","text":"<p>Inject failures to test service resilience:</p> <pre><code>./istio-features/apply-feature.sh 02-fault-injection\n</code></pre> <ul> <li>Injects a 7-second delay for user <code>jason</code> on the <code>ratings</code> service</li> <li>Injects HTTP 500 errors for 10% of all requests to <code>ratings</code></li> </ul> <p>Warning</p> <p>Observe in Kiali: Error rates appear as red percentages on the graph edges.</p>"},{"location":"10-Istio/#04-circuit-breaker","title":"04. Circuit Breaker","text":"<p>Limit connections to prevent cascading failures across services:</p> <pre><code>./istio-features/apply-feature.sh 03-circuit-breaker\n</code></pre> <ul> <li>Max 1 concurrent connection to <code>reviews</code></li> <li>Max 1 pending request in the queue</li> <li>Circuit trips after 1 consecutive 5xx error</li> </ul>"},{"location":"10-Istio/#05-timeouts-and-retries","title":"05. Timeouts and Retries","text":"<p>Configure request timeouts and automatic retries at the mesh level:</p> <pre><code>./istio-features/apply-feature.sh 05-timeout-retry\n</code></pre> <ul> <li>3-second timeout on requests to the <code>reviews</code> service</li> <li>2 automatic retries on failure (5xx errors, connect failures)</li> </ul>"},{"location":"10-Istio/#06-traffic-mirroring","title":"06. Traffic Mirroring","text":"<p>Shadow production traffic to a test version without affecting real users:</p> <pre><code>./istio-features/apply-feature.sh 06-mirror-traffic\n</code></pre> <pre><code>graph LR\n    pp[\"productpage\"] --&gt;|\"live traffic\"| rv1[\"reviews v1\"]\n    pp -. \"mirrored copy\" .-&gt; rv3[\"reviews v3\\n(shadow)\"]\n    rv1 --&gt; rat[\"ratings v1\"]</code></pre>"},{"location":"10-Istio/#07-mutual-tls-mtls","title":"07. Mutual TLS (mTLS)","text":"<p>Enforce encrypted service-to-service communication across the namespace:</p> <pre><code>./istio-features/apply-feature.sh 07-mtls-strict\n</code></pre> <ul> <li>Enables STRICT mTLS mode for the <code>bookinfo</code> namespace</li> <li>All inter-service traffic must be encrypted via Istio-managed certificates</li> <li>Non-mesh (plain TCP) traffic is rejected</li> </ul> <p>Tip</p> <p>Verify in Kiali: The Security view shows lock icons on all edges of the graph.</p>"},{"location":"10-Istio/#reset-to-default","title":"Reset to Default","text":"<pre><code># Reset all feature demos back to default routing\n./istio-features/apply-feature.sh reset\n</code></pre>"},{"location":"10-Istio/#part-04-observability","title":"Part 04 - Observability","text":""},{"location":"10-Istio/#prometheus-queries","title":"Prometheus Queries","text":"<p>Useful PromQL queries for Istio service mesh metrics:</p> <pre><code># Request rate by destination service\nrate(istio_requests_total{reporter=\"destination\"}[5m])\n\n# P99 latency per service\nhistogram_quantile(0.99,\n  sum(rate(istio_request_duration_milliseconds_bucket{reporter=\"destination\"}[5m]))\n  by (le, destination_service)\n)\n\n# Error rate per destination service\nsum(rate(istio_requests_total{reporter=\"destination\", response_code=~\"5.*\"}[5m])) by (destination_service)\n/\nsum(rate(istio_requests_total{reporter=\"destination\"}[5m])) by (destination_service)\n\n# TCP bytes sent\nsum(rate(istio_tcp_sent_bytes_total[5m])) by (destination_service)\n</code></pre>"},{"location":"10-Istio/#grafana-dashboards","title":"Grafana Dashboards","text":"<p>Pre-configured Istio dashboards available out of the box:</p> Dashboard Description Istio Mesh Dashboard Overall mesh health and performance overview Istio Service Dashboard Per-service request rates, latencies, error rates Istio Workload Dashboard Per-workload (pod) metrics Istio Control Plane Dashboard Istiod resource usage and performance"},{"location":"10-Istio/#jaeger-distributed-tracing","title":"Jaeger Distributed Tracing","text":"<ol> <li>Open Jaeger at <code>http://localhost:16686</code></li> <li>Select service <code>productpage.bookinfo</code> from the dropdown</li> <li>Click Find Traces to list recent requests</li> <li>Examine a trace to see the full end-to-end path across all microservices</li> <li>Compare latencies to identify bottlenecks between service versions</li> </ol>"},{"location":"10-Istio/#monitor-script","title":"Monitor Script","text":"<pre><code># Interactive mode\n./monitor.sh\n\n# Quick summary\n./monitor.sh summary\n\n# Test connectivity to all components\n./monitor.sh test\n\n# Full detailed report\n./monitor.sh full\n</code></pre>"},{"location":"10-Istio/#part-05-troubleshooting","title":"Part 05 - Troubleshooting","text":""},{"location":"10-Istio/#pods-not-starting","title":"Pods Not Starting","text":"<pre><code># Check events for clues\nkubectl get events -n bookinfo --sort-by='.lastTimestamp'\nkubectl get events -n istio-system --sort-by='.lastTimestamp'\n\n# Describe a specific pod\nkubectl describe pod &lt;pod-name&gt; -n bookinfo\n</code></pre>"},{"location":"10-Istio/#sidecar-not-injected","title":"Sidecar Not Injected","text":"<pre><code># Verify the namespace injection label\nkubectl get namespace bookinfo --show-labels\n# Expected: istio-injection=enabled\n\n# If the label is missing, add it:\nkubectl label namespace bookinfo istio-injection=enabled --overwrite\n\n# Restart deployments to trigger sidecar injection\nkubectl rollout restart deployment -n bookinfo\n</code></pre>"},{"location":"10-Istio/#no-traffic-visible-in-kiali","title":"No Traffic Visible in Kiali","text":"<pre><code># Verify the traffic generator CronJob is running\nkubectl get cronjob -n traffic-gen\nkubectl get jobs -n traffic-gen --sort-by=.metadata.creationTimestamp | tail -5\n\n# Confirm productpage is reachable from within the cluster\nkubectl exec -n bookinfo deploy/productpage-v1 -- \\\n  curl -s http://localhost:9080/productpage | head -20\n\n# Verify Prometheus is collecting Istio metrics\nkubectl exec -n istio-system deploy/prometheus -- \\\n  wget -qO- 'http://localhost:9090/api/v1/query?query=istio_requests_total' | head -50\n</code></pre> <p>Note</p> <p>Wait 1\u20132 minutes after deploying the traffic generator for metrics to propagate into Prometheus and Kiali.</p>"},{"location":"10-Istio/#istio-configuration-issues","title":"Istio Configuration Issues","text":"<pre><code># Analyze Istio configuration for problems\nistioctl analyze -n bookinfo\n\n# Check proxy sync status for all pods\nistioctl proxy-status\n\n# Inspect proxy routing config for a specific pod\nistioctl proxy-config routes deploy/productpage-v1 -n bookinfo\n</code></pre>"},{"location":"10-Istio/#kiali-not-showing-data","title":"Kiali Not Showing Data","text":"<pre><code># Confirm Prometheus is running\nkubectl get pods -n istio-system -l app=prometheus\n\n# Check Kiali logs for errors\nkubectl logs -n istio-system -l app=kiali --tail=50\n</code></pre>"},{"location":"10-Istio/#part-06-cleanup","title":"Part 06 - Cleanup","text":""},{"location":"10-Istio/#full-cleanup","title":"Full Cleanup","text":"<pre><code>./demo.sh cleanup\n</code></pre> <p>This will remove:</p> <ul> <li><code>traffic-gen</code> namespace and all traffic generator resources</li> <li><code>bookinfo</code> namespace and all application resources</li> <li>Kiali, Prometheus, Grafana, and Jaeger Helm releases</li> <li>Istio control plane Helm release</li> <li>All Istio CRDs</li> <li>All remaining namespaces created by this lab</li> </ul>"},{"location":"10-Istio/#partial-cleanup","title":"Partial Cleanup","text":"<pre><code># Remove only the Bookinfo app (keep Istio + addons running)\nkubectl delete namespace bookinfo\nkubectl delete namespace traffic-gen\n\n# Remove only Istio feature demos (restore default routing)\n./istio-features/apply-feature.sh reset\n\n# Remove only observability addons (keep Istio + app running)\nkubectl delete -f manifests/addons/ -n istio-system\n</code></pre>"},{"location":"10-Istio/#resources","title":"Resources","text":"<ul> <li>Istio Documentation</li> <li>Kiali Documentation</li> <li>Bookinfo Sample Application</li> <li>Istio Traffic Management</li> <li>Istio Security</li> <li>Envoy Proxy Documentation</li> <li>Helm Charts for Istio</li> </ul>"},{"location":"11-CRD-Custom-Resource-Definition/","title":"Custom Resource Definitions (CRD)","text":"<ul> <li><code>Custom Resource Definitions</code> (CRD) were added to Kubernetes 1.7.</li> <li><code>CRD</code> added the ability to define custom objects/resources.</li> <li>In this lab we will learn how CRDs extend the Kubernetes API.</li> </ul>"},{"location":"11-CRD-Custom-Resource-Definition/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What a Custom Resource Definition (CRD) is</li> <li>How CRDs extend the Kubernetes API</li> <li>How custom resources are stored and managed</li> <li>How to interact with custom resources using <code>kubectl</code></li> </ul>"},{"location":"11-CRD-Custom-Resource-Definition/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> </ul>"},{"location":"11-CRD-Custom-Resource-Definition/#introduction","title":"Introduction","text":""},{"location":"11-CRD-Custom-Resource-Definition/#what-is-a-custom-resource-definition-crd","title":"What is a Custom Resource Definition (CRD)?","text":"<ul> <li> <p>A resource is an endpoint in the Kubernetes API that stores a collection of API objects of a certain kind; for example, the builtin pods resource contains a collection of Pod objects.</p> </li> <li> <p>A custom resource is an extension of the Kubernetes API that is not necessarily available in a default Kubernetes installation. It represents a customization of a particular Kubernetes installation. However, many core Kubernetes functions are now built using custom resources, making Kubernetes more modular.</p> </li> <li> <p>Custom resources can appear and disappear in a running cluster through dynamic registration, and cluster admins can update custom resources independently of the cluster itself.</p> </li> <li> <p>Once a custom resource is installed, users can create and access its objects using <code>kubectl</code>, just as they do for built-in resources like Pods.</p> </li> <li> <p>The custom resource created is also stored in the <code>etcd</code> cluster with proper replication and lifecycle management.</p> </li> </ul>"},{"location":"12-Wordpress-MySQL-PVC/","title":"WordPress, MySQL, PVC","text":"<ul> <li>In this lab you will deploy a WordPress site and a MySQL database.</li> <li>You will use <code>PersistentVolumes</code> and <code>PersistentVolumeClaims</code> as storage.</li> </ul>"},{"location":"12-Wordpress-MySQL-PVC/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>How to deploy a multi-tier application (WordPress + MySQL) on Kubernetes</li> <li>How to use <code>PersistentVolumeClaims</code> for persistent storage</li> <li>How to use <code>kustomization.yaml</code> with secret generators</li> <li>How to use port forwarding to test applications locally</li> </ul>"},{"location":"12-Wordpress-MySQL-PVC/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> <li>Minikube (for LoadBalancer support)</li> </ul>"},{"location":"12-Wordpress-MySQL-PVC/#walkthrough","title":"Walkthrough","text":"<ul> <li>Patch <code>minikube</code> so we can use <code>Service: LoadBalancer</code></li> </ul> <pre><code># Source:\n# https://github.com/knative/serving/blob/b31d96e03bfa1752031d0bc4ae2a3a00744d6cd5/docs/creating-a-kubernetes-cluster.md#loadbalancer-support-in-minikube\n\nsudo ip route add \\\n    $(cat ~/.minikube/profiles/minikube/config.json | \\\n    jq -r \".KubernetesConfig.ServiceCIDR\") \\\n    via $(minikube ip)\n\nkubectl run minikube-lb-patch \\\n    --replicas=1 \\\n    --image=elsonrodriguez/minikube-lb-patch:0.1 \\\n    --namespace=kube-system\n</code></pre> <ul> <li>Create the desired <code>Namespace</code></li> <li>Create the <code>MySQL</code> resources:<ul> <li>Create <code>Service</code></li> <li>Create <code>PersistentVolumeClaims</code></li> <li>Create <code>Deployment</code></li> <li>Create <code>password file</code></li> </ul> </li> <li>Create the WordPress resources:<ul> <li>Create <code>Service</code></li> <li>Create <code>PersistentVolumeClaims</code></li> <li>Create <code>Deployment</code></li> </ul> </li> <li>Create a <code>kustomization.yaml</code> with:<ul> <li><code>Secret generator</code></li> <li><code>MySQL</code> resources</li> <li><code>WordPress</code> resources</li> </ul> </li> <li>Deploy the stack</li> <li>Port forward from the host to the application</li> <li>We use a port forward so we will be able to test and verify if the WordPress is actually running:</li> </ul> <pre><code>kubectl port-forward service/wordpress 8080:32267 -n wp-demo\n</code></pre>"},{"location":"12-Wordpress-MySQL-PVC/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete namespace wp-demo\n</code></pre>"},{"location":"13-HelmChart/","title":"Helm Chart","text":"<ul> <li>Welcome to the <code>Helm</code> Chart hands-on lab! In this tutorial, you\u2019ll learn the essentials of <code>Helm</code> (version 3), the package manager for Kubernetes.</li> <li>You\u2019ll build, package, install, and manage applications using <code>Helm</code> charts, gaining practical experience with real Kubernetes resources.</li> </ul>"},{"location":"13-HelmChart/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What <code>Helm</code> is and why is it useful</li> <li><code>Helm</code> chart structure and key files</li> <li>Common <code>Helm</code> commands for managing releases</li> <li>How to create, pack, install, upgrade, and rollback a <code>Helm</code> chart</li> <li>Go template language syntax for chart templates</li> <li>Built-in objects and named templates</li> <li>Advanced features: hooks, dependencies, conditionals, and testing</li> <li>Troubleshooting and best practices</li> </ul>"},{"location":"13-HelmChart/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link Helm Official Docs helm.sh/docs Chart Template Guide helm.sh/docs/chart_template_guide Built-in Objects helm.sh/docs/chart_template_guide/builtin_objects Values Files helm.sh/docs/chart_template_guide/values_files Template Functions &amp; Pipelines helm.sh/docs/chart_template_guide/functions_and_pipelines Flow Control (<code>if</code>, <code>range</code>, <code>with</code>) helm.sh/docs/chart_template_guide/control_structures Named Templates (<code>_helpers.tpl</code>) helm.sh/docs/chart_template_guide/named_templates Chart Hooks helm.sh/docs/topics/charts_hooks Chart Dependencies helm.sh/docs/helm/helm_dependency Chart Tests helm.sh/docs/topics/chart_tests Chart Best Practices helm.sh/docs/chart_best_practices Go Template Language pkg.go.dev/text/template Sprig Template Functions masterminds.github.io/sprig Artifact Hub (Chart Repository) artifacthub.io Helm Cheat Sheet helm.sh/docs/intro/cheatsheet"},{"location":"13-HelmChart/#introduction","title":"Introduction","text":"<ul> <li><code>Helm</code> is the package manager for Kubernetes.</li> <li>It simplifies the deployment, management, and upgrade of applications on your Kubernetes cluster.</li> <li><code>Helm</code> helps you manage Kubernetes applications by providing a way to define, install, and upgrade complex Kubernetes applications.</li> <li> <p>When packing applications as <code>Helm</code> charts, you gain a standardized and reusable approach for deploying and managing your services.</p> </li> <li> <p>A <code>Helm</code> chart consists of a few files that define the Kubernetes resources that will be created when the chart is installed.</p> </li> <li>These files include the:<ul> <li><code>Chart.yaml</code> file, which contains metadata about the chart, such as its name and version, and the chart\u2019s dependencies and maintainers.</li> <li><code>values.yaml</code> file, which contains the configuration values for the chart.</li> <li>The <code>templates</code> directory which contains the Kubernetes resource templates to be used to create the actual resources in the cluster.</li> </ul> </li> </ul>"},{"location":"13-HelmChart/#terminology","title":"Terminology","text":"Chart <ul> <li>A <code>Helm</code> package is called a chart.</li> <li>Charts are versioned, shareable packages that contain all the Kubernetes resources needed to run an application.</li> </ul> Release <ul> <li>A specific instance of a chart is called a release.</li> <li>Each release is a deployed version of a chart, with its own configuration, resources, and revision history.</li> </ul> Repository <ul> <li>A collection of charts is stored in a <code>Helm</code> repository.</li> <li><code>Helm</code> charts can be hosted in public or private repositories for easy sharing and distribution.</li> </ul>"},{"location":"13-HelmChart/#chart-files-and-folders","title":"Chart files and folders","text":"Filename/Folder Description <code>Chart.yaml</code> Contains metadata about the chart, including its name, version, dependencies, and maintainers. <code>Chart.lock</code> Lock file listing exact versions of resolved dependencies. <code>values.yaml</code> Defines default configuration values for the chart. Users can override these values during installation. <code>values.schema.json</code> Optional JSON Schema for validating <code>values.yaml</code> structure. <code>templates/</code> Directory containing Kubernetes manifest templates written in the Go template language. <code>templates/NOTES.txt</code> A plain text file containing usage notes displayed after installation. <code>templates/_helpers.tpl</code> A file containing reusable named templates (partials). <code>templates/tests/</code> Directory containing test pod definitions for <code>helm test</code>. <code>charts/</code> Directory containing dependencies (subcharts) of the chart. <code>crds/</code> Directory containing Custom Resource Definitions (installed before templates). <code>README.md</code> Documentation for the chart, explaining how to use and configure it."},{"location":"13-HelmChart/#git-helm-chart-repo-structure","title":"Git HELM chart repo structure","text":"<p>While there are many ways to structure your Helm charts in a Git repository, here are the two most common patterns:</p>"},{"location":"13-HelmChart/#pattern-1-one-repo-per-chart","title":"Pattern 1: One Repo per Chart","text":"<ul> <li>Structure: The root of the repository contains the <code>Chart.yaml</code>, <code>values.yaml</code>, and <code>templates/</code> folder.</li> <li>Use Case: Best for microservices where each service has its own repository and its own chart.</li> <li>CI/CD: The chart is versioned and released alongside the application code.</li> </ul> <pre><code>my-app/\n\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 values.yaml\n\u251c\u2500\u2500 templates/\n\u251c\u2500\u2500 src/            # Application source code\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"13-HelmChart/#pattern-2-dedicated-charts-repository-monorepo","title":"Pattern 2: Dedicated Charts Repository (Monorepo)","text":"<ul> <li>Structure: A central repository containing multiple charts in a <code>charts/</code> directory.</li> <li>Use Case: Best for managing infrastructure charts (e.g., redis, postgres) or when you want centralized management of all your organization\u2019s charts.</li> <li>Hosting: Often used with GitHub Pages to host the chart repository index (<code>index.yaml</code>) and packaged charts (<code>.tgz</code>).</li> </ul> <pre><code>my-charts-repo/\n\u251c\u2500\u2500 charts/\n\u2502   \u251c\u2500\u2500 redis/\n\u2502   \u2502   \u251c\u2500\u2500 Chart.yaml\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 frontend/\n\u2502       \u251c\u2500\u2500 Chart.yaml\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 docs/           # Contains generated index.yaml and .tgz files (GitHub Pages source)\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"13-HelmChart/#github-pages-as-a-helm-repository","title":"GitHub Pages as a Helm Repository","text":"<p>You can easily verify your Git repository into a Helm Chart Repository using GitHub Pages:</p> <ol> <li>Docs Folder: Create a <code>docs</code> folder in your repo.</li> <li>Package: Run <code>helm package ./charts/mychart -d ./docs</code>.</li> <li>Index: Run <code>helm repo index ./docs --url https://&lt;username&gt;.github.io/&lt;repo-name&gt;/</code>.</li> <li>Publish: Enable GitHub Pages for the <code>docs</code> folder.</li> </ol> <p>Users can then add your repo: <pre><code>helm repo add my-repo https://&lt;username&gt;.github.io/&lt;repo-name&gt;/\n</code></pre></p>"},{"location":"13-HelmChart/#codewizard-helm-demo-helm-chart-structure","title":"codewizard-helm-demo Helm Chart structure","text":"<pre><code>- Chart.yaml        # Defines chart metadata and values schema\n- values.yaml       # Default configuration values\n- templates/        # Deployment templates using Go templating language\n  - _helpers.tpl    # Named templates (partials) used across templates\n  - Namespace.yaml  # Namespace manifest template\n  - ConfigMap.yaml  # ConfigMap manifest template\n  - Deployment.yaml # Deployment manifest template\n  - Service.yaml    # Service manifest template\n- README.md         # Documentation for your chart\n</code></pre>"},{"location":"13-HelmChart/#common-helm-commands","title":"Common <code>Helm</code> Commands","text":"<p>Below are the most common <code>Helm</code> commands you\u2019ll use when working with <code>Helm</code> charts. Each command includes syntax, description, and detailed usage examples.</p> <code>helm create</code> - Create a new chart <p>Syntax: <code>helm create &lt;chart-name&gt;</code></p> <p>Description: Creates a new Helm chart with the specified name. This command generates a chart directory with a standard structure including default templates, values.yaml, and Chart.yaml.</p> <ul> <li>Creates a new chart directory with a standard structure</li> <li>Includes default templates, values.yaml, and Chart.yaml</li> <li>Provides a starting point that follows Helm best practices</li> <li> <p>You can customize the generated files to match your application needs</p> <pre><code># Create a new chart named 'myapp'\nhelm create myapp\n\n# View the generated structure\ntree myapp\n\n# Output shows:\n# myapp/\n# \u251c\u2500\u2500 Chart.yaml\n# \u251c\u2500\u2500 values.yaml\n# \u251c\u2500\u2500 charts/\n# \u2514\u2500\u2500 templates/\n#     \u251c\u2500\u2500 NOTES.txt\n#     \u251c\u2500\u2500 _helpers.tpl\n#     \u251c\u2500\u2500 deployment.yaml\n#     \u251c\u2500\u2500 service.yaml\n#     \u2514\u2500\u2500 ...\n</code></pre> </li> </ul> <code>helm install</code> - Install a chart <p>Syntax: <code>helm install &lt;release-name&gt; &lt;chart-path&gt;</code></p> <p>Description: Installs a Helm chart to your Kubernetes cluster, creating a new release with the specified name.</p> <ul> <li>Deploys a chart to your Kubernetes cluster</li> <li>Creates a new release with a unique name</li> <li>Can override values using <code>--set</code> or <code>-f</code> flags</li> <li> <p>Use <code>--dry-run</code> to preview changes without applying them</p> <pre><code># Basic install\nhelm install myrelease ./myapp\n\n# Install with custom values file\nhelm install myrelease ./myapp -f custom-values.yaml\n\n# Install with inline value overrides\nhelm install myrelease ./myapp --set replicaCount=3\n\n# Install in a specific namespace\nhelm install myrelease ./myapp --namespace production --create-namespace\n\n# Dry run to see what would be installed\nhelm install myrelease ./myapp --dry-run --debug\n\n# Install from a packaged chart\nhelm install myrelease myapp-1.0.0.tgz\n\n# Install with a generated name\nhelm install myapp --generate-name\n\n# Wait for all resources to be ready before marking release as successful\nhelm install myrelease ./myapp --wait --timeout 5m\n</code></pre> </li> </ul> <code>helm upgrade</code> - Upgrade a release <p>Syntax: <code>helm upgrade &lt;release-name&gt; &lt;chart-path&gt;</code></p> <p>Description: Upgrades an installed release with a new version of a chart or updated configuration values.</p> <ul> <li>Updates an existing release with new configurations or chart version</li> <li>Maintains revision history for rollback capability</li> <li>Can use <code>--install</code> to install if release doesn\u2019t exist</li> <li> <p>Supports value overrides like install command</p> <pre><code># Basic upgrade\nhelm upgrade myrelease ./myapp\n\n# Upgrade with new values\nhelm upgrade myrelease ./myapp -f production-values.yaml\n\n# Upgrade or install if not exists (atomic operation)\nhelm upgrade myrelease ./myapp --install\n\n# Upgrade with specific values\nhelm upgrade myrelease ./myapp --set image.tag=v2.0.0\n\n# Force resource updates even if unchanged\nhelm upgrade myrelease ./myapp --force\n\n# Reuse previous values and merge with new ones\nhelm upgrade myrelease ./myapp --reuse-values --set newKey=newValue\n\n# Reset values to chart defaults\nhelm upgrade myrelease ./myapp --reset-values\n\n# Wait for upgrade to complete\nhelm upgrade myrelease ./myapp --wait --timeout 10m\n\n# Atomic upgrade - rollback on failure\nhelm upgrade myrelease ./myapp --atomic --timeout 5m\n</code></pre> </li> </ul> <code>helm uninstall</code> - Remove a release <p>Syntax: <code>helm uninstall &lt;release-name&gt;</code></p> <p>Description: Uninstalls a release from the Kubernetes cluster, removing all associated resources.</p> <ul> <li>Deletes a release and all associated Kubernetes resources</li> <li>Removes the release from Helm\u2019s history by default</li> <li>Use <code>--keep-history</code> to retain release history for potential restoration</li> <li> <p>Respects hook deletion policies defined in templates</p> <pre><code># Basic uninstall\nhelm uninstall myrelease\n\n# Uninstall but keep history (allows rollback)\nhelm uninstall myrelease --keep-history\n\n# Uninstall from specific namespace\nhelm uninstall myrelease --namespace production\n\n# Uninstall with custom timeout\nhelm uninstall myrelease --timeout 5m\n\n# Dry run - see what would be deleted\nhelm uninstall myrelease --dry-run\n\n# Uninstall and wait for all resources to be deleted\nhelm uninstall myrelease --wait\n</code></pre> </li> </ul> <code>helm list</code> - List releases <p>Syntax: <code>helm list</code></p> <p>Description: Lists all installed Helm releases in the current or specified namespace.</p> <ul> <li>Shows all releases in the current namespace</li> <li>Displays release name, namespace, revision, status, and chart info</li> <li>Supports filtering and output formatting options</li> <li> <p>Use <code>--all-namespaces</code> to see releases across all namespaces</p> <pre><code># List all releases in current namespace\nhelm list\n\n# List all releases across all namespaces\nhelm list --all-namespaces\n\n# List releases in specific namespace\nhelm list --namespace production\n\n# Show all releases including uninstalled (if kept history)\nhelm list --all\n\n# Filter by status\nhelm list --deployed\nhelm list --failed\nhelm list --pending\n\n# Show more details (longer output)\nhelm list --all-namespaces -o wide\n\n# Output as JSON\nhelm list -o json\n\n# Output as YAML\nhelm list -o yaml\n\n# Filter releases by name pattern\nhelm list --filter 'myapp.*'\n\n# Limit number of results\nhelm list --max 10\n\n# Sort by release date\nhelm list --date\n</code></pre> </li> </ul> <code>helm status</code> - Show release status <p>Syntax: <code>helm status &lt;release-name&gt;</code></p> <p>Description: Shows the status of a deployed Helm release including resource information and deployment details.</p> <ul> <li>Displays detailed information about a deployed release</li> <li>Shows resource status, last deployment time, and revision number</li> <li>Includes NOTES.txt content if present</li> <li> <p>Useful for debugging and verifying deployments</p> <pre><code># Show status of a release\nhelm status myrelease\n\n# Show status from specific namespace\nhelm status myrelease --namespace production\n\n# Show status at specific revision\nhelm status myrelease --revision 2\n\n# Output as JSON\nhelm status myrelease -o json\n\n# Output as YAML\nhelm status myrelease -o yaml\n\n# Show status without displaying NOTES\nhelm status myrelease --show-desc\n</code></pre> </li> </ul> <code>helm rollback</code> - Rollback to previous revision <p>Syntax: <code>helm rollback &lt;release-name&gt; [revision]</code></p> <p>Description: Rollbacks a release to a previous revision number.</p> <ul> <li>Reverts a release to a previous revision</li> <li>Useful for quick recovery from failed upgrades</li> <li>Creates a new revision (rollback is tracked in history)</li> <li> <p>Can rollback to any previously deployed revision</p> <pre><code># Rollback to previous revision\nhelm rollback myrelease\n\n# Rollback to specific revision\nhelm rollback myrelease 3\n\n# Rollback with timeout\nhelm rollback myrelease 2 --timeout 5m\n\n# Wait for rollback to complete\nhelm rollback myrelease --wait\n\n# Force rollback even if resources haven't changed\nhelm rollback myrelease --force\n\n# Dry run - see what would be rolled back\nhelm rollback myrelease --dry-run\n\n# Recreate resources (delete and recreate)\nhelm rollback myrelease --recreate-pods\n\n# Cleanup on fail\nhelm rollback myrelease --cleanup-on-fail\n</code></pre> </li> </ul> <code>helm get all</code> - Get release information <p>Syntax: <code>helm get all &lt;release-name&gt;</code></p> <p>Description: Retrieves all information about a deployed release including templates, values, hooks, and notes.</p> <ul> <li>Retrieves all information about a release</li> <li>Shows manifest, values, hooks, and notes</li> <li>Useful for debugging and understanding what was deployed</li> <li> <p>Can retrieve information from specific revisions</p> <pre><code># Get all info about a release\nhelm get all myrelease\n\n# Get all info from specific revision\nhelm get all myrelease --revision 2\n\n# Get all info from specific namespace\nhelm get all myrelease --namespace production\n\n# Output as template for reuse\nhelm get all myrelease --template '{{.Release.Manifest}}'\n</code></pre> </li> </ul> <code>helm get values</code> - Get release values <p>Syntax: <code>helm get values &lt;release-name&gt;</code></p> <p>Description: Shows the user-supplied values for a release.</p> <ul> <li>Shows the values that were used for a specific release</li> <li>Displays only user-supplied values by default</li> <li>Use <code>--all</code> to see all values including defaults</li> <li> <p>Useful for understanding current configuration</p> <pre><code># Get user-supplied values\nhelm get values myrelease\n\n# Get all values (including defaults)\nhelm get values myrelease --all\n\n# Get values from specific revision\nhelm get values myrelease --revision 2\n\n# Output as JSON\nhelm get values myrelease -o json\n\n# Output as YAML\nhelm get values myrelease -o yaml\n\n# Save values to file\nhelm get values myrelease &gt; current-values.yaml\n</code></pre> </li> </ul> <code>helm show values</code> - Show chart default values <p>Syntax: <code>helm show values &lt;chart-name&gt;</code></p> <p>Description: Shows the default values of a Helm chart before installation.</p> <ul> <li>Displays the default values.yaml from a chart</li> <li>Works with local charts, remote charts, or chart repositories</li> <li>Useful for understanding available configuration options</li> <li> <p>Shows values before installation</p> <pre><code># Show default values of local chart\nhelm show values ./myapp\n\n# Show values from packaged chart\nhelm show values myapp-1.0.0.tgz\n\n# Show values from chart repository\nhelm show values bitnami/nginx\n\n# Show values at specific version\nhelm show values bitnami/nginx --version 15.0.0\n\n# Save default values to file\nhelm show values ./myapp &gt; default-values.yaml\n</code></pre> </li> </ul> <code>helm template</code> - Render templates locally <p>Syntax: <code>helm template &lt;release-name&gt; &lt;chart-path&gt;</code></p> <p>Description: Renders chart templates locally without installing to the cluster.</p> <ul> <li>Renders chart templates locally without connecting to Kubernetes</li> <li>Outputs rendered YAML manifests to stdout</li> <li>Useful for debugging templates and previewing changes</li> <li> <p>Does not require cluster access</p> <pre><code># Render templates to stdout\nhelm template myrelease ./myapp\n\n# Render with custom values\nhelm template myrelease ./myapp -f custom-values.yaml\n\n# Render with inline values\nhelm template myrelease ./myapp --set replicaCount=3\n\n# Render and save to file\nhelm template myrelease ./myapp &gt; rendered-manifests.yaml\n\n# Show only specific template\nhelm template myrelease ./myapp --show-only templates/deployment.yaml\n\n# Debug mode - show more information\nhelm template myrelease ./myapp --debug\n\n# Validate rendered output\nhelm template myrelease ./myapp --validate\n\n# Include CRDs in output\nhelm template myrelease ./myapp --include-crds\n\n# Render for specific Kubernetes version\nhelm template myrelease ./myapp --kube-version 1.28.0\n</code></pre> </li> </ul> <code>helm lint</code> - Validate chart <p>Syntax: <code>helm lint &lt;chart-path&gt;</code></p> <p>Description: Runs a series of tests to verify that the chart is well-formed and follows best practices.</p> <ul> <li>Runs tests to verify chart is well-formed</li> <li>Checks Chart.yaml, values.yaml, and template syntax</li> <li>Identifies common errors and issues</li> <li> <p>Should be run before packaging or installing</p> <pre><code># Lint a chart\nhelm lint ./myapp\n\n# Lint with custom values\nhelm lint ./myapp -f custom-values.yaml\n\n# Lint with inline values\nhelm lint ./myapp --set replicaCount=3\n\n# Strict linting (fail on warnings)\nhelm lint ./myapp --strict\n\n# Lint with debug output\nhelm lint ./myapp --debug\n\n# Lint multiple charts\nhelm lint ./myapp ./anotherapp\n</code></pre> </li> </ul> <code>helm history</code> - Show release history <p>Syntax: <code>helm history &lt;release-name&gt;</code></p> <p>Description: Prints historical revisions for a given release.</p> <ul> <li>Displays revision history for a release</li> <li>Shows revision number, update time, status, and description</li> <li>Useful for understanding what changed and when</li> <li> <p>Helps identify which revision to rollback to</p> <pre><code># Show release history\nhelm history myrelease\n\n# Show history from specific namespace\nhelm history myrelease --namespace production\n\n# Show more revisions (default is 256)\nhelm history myrelease --max 100\n\n# Output as JSON\nhelm history myrelease -o json\n\n# Output as YAML\nhelm history myrelease -o yaml\n\n# Output as table (default)\nhelm history myrelease -o table\n</code></pre> </li> </ul> <code>helm test</code> - Run release tests <p>Syntax: <code>helm test &lt;release-name&gt;</code></p> <p>Description: Runs the tests defined in a chart for a release.</p> <ul> <li>Executes tests defined in chart\u2019s templates/tests/ directory</li> <li>Tests are Kubernetes pods with the <code>helm.sh/hook: test</code> annotation</li> <li>Validates that a release is working correctly</li> <li> <p>Returns exit code based on test success/failure</p> <pre><code># Run tests for a release\nhelm test myrelease\n\n# Run tests from specific namespace\nhelm test myrelease --namespace production\n\n# Run tests with timeout\nhelm test myrelease --timeout 5m\n\n# Show test logs\nhelm test myrelease --logs\n\n# Cleanup tests after run (default: false)\nhelm test myrelease --cleanup\n\n# Filter which tests to run\nhelm test myrelease --filter name=test-connection\n</code></pre> </li> </ul> <code>helm dependency update</code> - Update chart dependencies <p>Syntax: <code>helm dependency update &lt;chart-path&gt;</code></p> <p>Description: Updates the charts/ directory based on Chart.yaml dependencies.</p> <ul> <li>Downloads chart dependencies listed in Chart.yaml</li> <li>Stores dependencies in the charts/ subdirectory</li> <li>Creates or updates Chart.lock file</li> <li> <p>Required before packaging or installing charts with dependencies</p> <pre><code># Update dependencies\nhelm dependency update ./myapp\n\n# Update and skip refreshing repository index\nhelm dependency update ./myapp --skip-refresh\n\n# Verify dependencies\nhelm dependency list ./myapp\n\n# Build dependencies (use Chart.lock)\nhelm dependency build ./myapp\n</code></pre> </li> </ul> <code>helm repo add</code> - Add chart repository <p>Syntax: <code>helm repo add &lt;name&gt; &lt;url&gt;</code></p> <p>Description: Adds a chart repository to your local Helm configuration.</p> <ul> <li>Adds a chart repository to your local Helm configuration</li> <li>Repositories are stored in ~/.config/helm/repositories.yaml</li> <li>Enables searching and installing charts from the repository</li> <li> <p>Can add both HTTP and OCI repositories</p> <pre><code># Add a chart repository\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\n# Add with authentication\nhelm repo add myrepo https://charts.example.com --username user --password pass\n\n# Add and force update if exists\nhelm repo add bitnami https://charts.bitnami.com/bitnami --force-update\n\n# Add repository with custom certificate\nhelm repo add myrepo https://charts.example.com --ca-file ca.crt\n\n# Add repository skipping TLS verification (not recommended)\nhelm repo add myrepo https://charts.example.com --insecure-skip-tls-verify\n\n# List all repositories\nhelm repo list\n</code></pre> </li> </ul> <code>helm repo update</code> - Update repository information <p>Syntax: <code>helm repo update</code></p> <p>Description: Updates information of available charts from chart repositories.</p> <ul> <li>Updates the local cache of charts from all added repositories</li> <li>Fetches the latest available charts and versions</li> <li>Should be run periodically to see new chart releases</li> <li> <p>Similar to <code>apt update</code> or <code>yum update</code></p> <pre><code># Update all repositories\nhelm repo update\n\n# Update specific repository\nhelm repo update bitnami\n\n# Update with failure on any repository error\nhelm repo update --fail-on-repo-update-fail\n\n# Update multiple specific repositories\nhelm repo update bitnami stable\n</code></pre> </li> </ul> <code>helm search repo</code> - Search repositories <p>Syntax: <code>helm search repo &lt;keyword&gt;</code></p> <p>Description: Searches repositories for charts matching a keyword.</p> <ul> <li>Searches added repositories for charts matching keyword</li> <li>Shows chart name, version, app version, and description</li> <li>Supports regex patterns for advanced searching</li> <li> <p>Only searches locally added repositories</p> <pre><code># Search for charts\nhelm search repo nginx\n\n# Search with version information\nhelm search repo nginx --versions\n\n# Search with regex\nhelm search repo 'nginx.*'\n\n# Show development versions (pre-release, etc.)\nhelm search repo nginx --devel\n\n# Search with specific version constraint\nhelm search repo nginx --version \"~15.0\"\n\n# Output as JSON\nhelm search repo nginx -o json\n\n# Output as YAML\nhelm search repo nginx -o yaml\n\n# Search all repositories\nhelm search repo --max-col-width 0\n</code></pre> </li> </ul>"},{"location":"13-HelmChart/#advanced-concepts","title":"Advanced Concepts","text":""},{"location":"13-HelmChart/#built-in-objects","title":"Built-in Objects","text":"<p><code>Helm</code> templates have access to several built-in objects. These are the most commonly used:</p> Object Description <code>.Release.Name</code> The name of the release <code>.Release.Namespace</code> The namespace the release is installed into <code>.Release.Revision</code> The revision number of this release (starts at 1) <code>.Release.IsInstall</code> <code>true</code> if the current operation is an install <code>.Release.IsUpgrade</code> <code>true</code> if the current operation is an upgrade <code>.Release.Service</code> The service rendering the template (always <code>Helm</code>) <code>.Values</code> Values passed to the template from <code>values.yaml</code> and user overrides <code>.Chart.Name</code> The name of the chart from <code>Chart.yaml</code> <code>.Chart.Version</code> The version of the chart <code>.Chart.AppVersion</code> The app version from <code>Chart.yaml</code> <code>.Template.Name</code> The namespaced path to the current template file <code>.Template.BasePath</code> The namespaced path to the templates directory <code>.Files</code> Access to non-template files in the chart <code>.Capabilities</code> Information about the Kubernetes cluster capabilities <p>Docs: Built-in Objects</p>"},{"location":"13-HelmChart/#go-template-syntax","title":"Go Template Syntax","text":"<p><code>Helm</code> uses the Go template language with additional Sprig functions. Here\u2019s a quick reference:</p>"},{"location":"13-HelmChart/#template-delimiters","title":"Template Delimiters","text":"Delimiter syntax Meaning <code>{{ ... }}</code> Standard output expression - evaluates and prints the result. <code>{{- ... }}</code> Trim whitespace/newline to the left of the action (left-trim). Useful at the start of a template line. <code>{{ ... -}}</code> Trim whitespace/newline to the right of the action (right-trim). Useful at the end of a template line. <code>{{- ... -}}</code> Trim whitespace/newline on both sides of the action. <p>Delimiters</p> <ul> <li>Whitespace trimming controls whether newlines and spaces immediately before or after template actions appear in the rendered YAML - this is important to produce valid, tidy manifests.</li> <li>Prefer <code>{{-</code> at the start of a block and <code>-}}</code> at the end of a block when you want to avoid blank lines in rendered output.</li> </ul> <p>Example (shows differences in rendered output):</p> <pre><code># Template A (no trimming)\nprefix:\n  {{ \"val\" }}\nsuffix:\n\n# Template B (left-trim)\nprefix:\n  {{- \"val\" }}\nsuffix:\n\n# Template C (right-trim)\nprefix:\n  {{ \"val\" -}}\nsuffix:\n\n# Template D (both sides trimmed)\nprefix:\n  {{- \"val\" -}}\nsuffix:\n</code></pre> <p>When rendered, trimming removes the surrounding blank lines and keeps YAML indentation correct; use <code>helm template</code> during development to verify the output.</p>"},{"location":"13-HelmChart/#variables","title":"Variables","text":"<pre><code># Assigning a variable\n{{- $name := .Values.appName }}\n\n# Using a variable\napp: {{ $name }}\n</code></pre>"},{"location":"13-HelmChart/#pipelines-and-functions","title":"Pipelines and Functions","text":"<p>Template functions can be chained using the pipe <code>|</code> operator:</p> <pre><code># C`yaml\n# Convert to uppercase\nname: {{ .Values.name | upper }}\n\n# Default value if empty\nimage: {{ .Values.image | default \"nginx:latest\" }}\n\n# Quoting a value\nversion: {{ .Values.version | quote }}\n\n# Trim and truncate\nname: {{ .Values.name | trunc 63 | trimSuffix \"-\" }}\n\n# Indentation (critical for YAML)\nmetadata:\n  labels:\n    {{- include \"myapp.labels\" . | nindent 4 }}\n</code></pre> <p>Common Sprig Functions</p> <ul> <li> <p>Sprig is a library that provides over 70 useful template functions for Go\u2019s template language.</p> <ul> <li>String: <code>upper</code>, <code>lower</code>, <code>title</code>, <code>trim</code>, <code>quote</code>, <code>trunc</code>, <code>trimSuffix</code></li> <li>Defaults: <code>default</code></li> <li>Indentation: <code>nindent</code>, <code>indent</code></li> <li>Encoding/Conversion: <code>toYaml</code>, <code>toJson</code>, <code>b64enc</code>, <code>b64dec</code></li> <li>Date/Time: <code>now</code>, <code>htmlDate</code></li> <li>Crypto: <code>sha256sum&gt; Common Sprig functions:</code>upper<code>,</code>lower<code>,</code>title<code>,</code>trim<code>,</code>quote<code>,</code>default<code>,</code>trunc<code>,</code>trimSuffix<code>,</code>nindent<code>,</code>indent<code>,</code>toYaml<code>,</code>toJson<code>,</code>b64enc<code>,</code>b64dec<code>,</code>sha256sum<code>,</code>now<code>,</code>htmlDate`</li> </ul> </li> </ul> <p>Docs: Functions and Pipelines</p>"},{"location":"13-HelmChart/#flow-control","title":"Flow Control","text":""},{"location":"13-HelmChart/#conditionals-if-else","title":"Conditionals (<code>if</code> / <code>else</code>)","text":"<pre><code>{{- if .Values.ingress.enabled }}\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ include \"myapp.fullname\" . }}\nspec:\n  rules:\n    - host: {{ .Values.ingress.host }}\n{{- end }}\n`\n```yaml\n# if / else if / else\n{{- if eq .Values.env \"production\" }}\n  replicas: 5\n{{- else if eq .Values.env \"staging\" }}\n  replicas: 2\n{{- else }}\n  replicas: 1\n{{- end }}\n</code></pre>"},{"location":"13-HelmChart/#comparison-operators","title":"Comparison operators","text":"Operator Description <code>eq</code> Equal <code>ne</code> Not equal <code>lt</code> Less than <code>gt</code> Greater than <code>le</code> Less than or equal <code>ge</code> Greater than or equal <code>and</code> Logical AND <code>or</code> Logical OR <code>not</code> Logical NOT"},{"location":"13-HelmChart/#looping-range","title":"Looping (<code>range</code>)","text":"<pre><code># Iterating over a list\nenv:\n{{- range .Values.env }}\n  - name: {{ .name }}\n    value: {{ .value | quote }}\n{{- end }}\n</code></pre> <pre><code># Iterating over a map/dict\nlabels:\n{{- range $key, $value := .Values.labels }}\n  {{ $key }}: {{ $value | quote }}\n{{- end }}\n</code></pre>"},{"location":"13-HelmChart/#scoping-with","title":"Scoping (<code>with</code>)","text":"<pre><code># `with` changes the scope of `.` inside the block\n{{- with .Values.nodeSelector }}\nnodeSelector:\n  {{- toYaml . | nindent 2 }}\n{{- end }}\n</code></pre> <p>Docs: Flow Control</p>"},{"location":"13-HelmChart/#named-templates-_helperstpl","title":"Named Templates (<code>_helpers.tpl</code>)","text":"<ul> <li>Files prefixed with <code>_</code> (underscore) in the <code>templates/</code> directory are not rendered as Kubernetes manifests.</li> <li>They are used to define reusable named templates (also called partials or sub-templates).</li> <li>Named templates are defined with <code>define</code> and invoked with <code>include</code> (preferred) or <code>template</code>.</li> </ul> <pre><code># _helpers.tpl - defining a named template\n{{- define \"myapp.labels\" -}}\napp.kubernetes.io/name: {{ include \"myapp.name\" . }}\napp.kubernetes.io/instance: {{ .Release.Name }}\napp.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\napp.kubernetes.io/managed-by: {{ .Release.Service }}\n{{- end }}\n</code></pre> <pre><code># Using the named template in a manifest\nmetadata:\n  labels: { { - include \"myapp.labels\" . | nindent 4 } }\n</code></pre> <p><code>include</code> vs <code>template</code></p> <ul> <li>Always prefer <code>include</code> over <code>template</code>.   -The <code>include</code> function allows you to pipe the output (e.g., <code>| nindent 4</code>), while <code>template</code> does not support pipelines.</li> </ul> <p>Docs: Named Templates</p>"},{"location":"13-HelmChart/#values-override-precedence","title":"Values Override Precedence","text":"<p>When installing or upgrading a release, values can be supplied from multiple sources. The override precedence (last wins) is:</p> <ol> <li><code>values.yaml</code> in the chart (defaults)</li> <li>Parent chart\u2019s <code>values.yaml</code> (for subcharts)</li> <li>Values file passed with <code>-f</code> / <code>--values</code></li> <li>Individual values set with <code>--set</code> or <code>--set-string</code></li> </ol> <pre><code># Override with a custom values file\nhelm install my-release ./mychart -f custom-values.yaml\n\n# Override with --set (highest precedence)\nhelm install my-release ./mychart --set replicaCount=3\n\n# Multiple overrides combined\nhelm install my-release ./mychart \\\n  -f production-values.yaml \\\n  --set image.tag=\"v2.0.0\"\n\n# Override with --set-string (forces string type)\nhelm install my-release ./mychart --set-string image.tag=\"1234\"\n\n# Override with --set-file (read value from a file)\nhelm install my-release ./mychart --set-file config=./my-config.txt\n</code></pre> <p>Docs: Values Files</p>"},{"location":"13-HelmChart/#chart-dependencies-subcharts","title":"Chart Dependencies (Subcharts)","text":"<p>Charts can depend on other charts. Dependencies are declared in <code>Chart.yaml</code>:</p> <pre><code># Chart.yaml\napiVersion: v2\nname: my-app\nversion: 1.0.0\ndependencies:\n  - name: postgresql\n    version: \"12.x.x\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: postgresql.enabled\n  - name: redis\n    version: \"17.x.x\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: redis.enabled\n</code></pre> <pre><code># Download and update dependencies\nhelm dependency update ./my-app\n\n# The dependencies are stored in the charts/ directory\nls ./my-app/charts/\n</code></pre> <p>Docs: Chart Dependencies</p>"},{"location":"13-HelmChart/#helm-hooks","title":"Helm Hooks","text":"<p>Hooks allow you to run resources at specific points in a release lifecycle.   - They are standard Kubernetes resources (Jobs, Pods, ConfigMaps, etc.) with special annotations that tell Helm when to execute them.</p>"},{"location":"13-HelmChart/#hook-types","title":"Hook Types","text":"Hook Description <code>pre-install</code> Runs before any resources are installed <code>post-install</code> Runs after all resources are installed <code>pre-upgrade</code> Runs before any resources are upgraded <code>post-upgrade</code> Runs after all resources are upgraded <code>pre-delete</code> Runs before any resources are deleted <code>post-delete</code> Runs after all resources are deleted <code>pre-rollback</code> Runs before a rollback <code>post-rollback</code> Runs after a rollback <code>test</code> Runs when <code>helm test</code> is called"},{"location":"13-HelmChart/#hook-annotations","title":"Hook Annotations","text":"<p>Hooks are controlled by three key annotations:</p> Annotation Description <code>helm.sh/hook</code> Defines when the hook runs (required). Can specify multiple hooks: <code>\"pre-install,pre-upgrade\"</code> <code>helm.sh/hook-weight</code> Defines execution order (default: 0). Lower weights execute first. Can be negative. <code>helm.sh/hook-delete-policy</code> Defines when to delete the hook resource. Values: <code>before-hook-creation</code>, <code>hook-succeeded</code>, <code>hook-failed</code>"},{"location":"13-HelmChart/#hook-deletion-policies","title":"Hook Deletion Policies","text":"Policy Description <code>before-hook-creation</code> Delete previous hook resource before a new one is launched (default) <code>hook-succeeded</code> Delete the hook resource after it successfully completes <code>hook-failed</code> Delete the hook resource if it fails <p>You can specify multiple policies: <code>\"hook-succeeded,hook-failed\"</code></p>"},{"location":"13-HelmChart/#hook-execution-order","title":"Hook Execution Order","text":"<p>Hooks execute in the following order:</p> <ol> <li>Sorted by weight (ascending): hooks with lower weights run first</li> <li>Sorted by kind (alphabetical): if weights are equal</li> <li>Sorted by name (alphabetical): if both weight and kind are equal</li> </ol>"},{"location":"13-HelmChart/#practical-examples","title":"Practical Examples","text":""},{"location":"13-HelmChart/#pre-installpre-upgrade","title":"(pre-install/pre-upgrade)","text":"Example 1: Database Migration (pre-install/pre-upgrade) <ul> <li>This hook runs a database migration job before installing or upgrading the main application resources.</li> <li>It ensures that the database schema is up-to-date before the application starts.</li> <li>The <code>migrate.sh</code> script would contain the logic to perform the database migration, and it would use environment variables to connect to the database.       <pre><code># templates/hooks/db-migrate.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"myapp.fullname\" . }}-db-migrate\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-weight\": \"0\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded,hook-failed\nspec:\n  template:\n    metadata:\n      name: {{ include \"myapp.fullname\" . }}-db-migrate\n    spec:\n      containers:\n        - name: migrate\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n          command: [\"./migrate.sh\"]\n          env:\n            - name: DB_HOST\n              value: {{ .Values.database.host }}\n            - name: DB_NAME\n              value: {{ .Values.database.name }}\n      restartPolicy: Never\n  backoffLimit: 3\n</code></pre></li> </ul> Example 2: Schema Initialization (pre-install only) <ul> <li>This hook runs a database initialization job only during the initial installation of the chart.</li> <li>It creates the database schema if it doesn\u2019t already exist.</li> <li>It uses a lower weight to ensure it runs before the migration hook.</li> <li>The <code>psql</code> command is used to create the database, and it connects using environment variables for the database host and credentials.</li> <li>This hook will not run during upgrades, ensuring that it only initializes the database on the first install.       <pre><code># templates/hooks/db-init.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"myapp.fullname\" . }}-db-init\n  annotations:\n    \"helm.sh/hook\": pre-install\n    \"helm.sh/hook-weight\": \"-5\"  # Runs before migration (weight: 0)\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n        - name: init-db\n          image: postgres:14\n          command:\n            - sh\n            - -c\n            - |\n              psql -h $DB_HOST -U $DB_USER -c \"CREATE DATABASE IF NOT EXISTS {{ .Values.database.name }};\"\n          env:\n            - name: DB_HOST\n              value: {{ .Values.database.host }}\n            - name: DB_USER\n              valueFrom:\n                secretKeyRef:\n                  name: db-secret\n                  key: username\n            - name: PGPASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: db-secret\n                  key: password\n      restartPolicy: Never\n</code></pre></li> </ul> Example 3: Service Readiness Check (post-install) <ul> <li>This hook runs a job after the main application resources are installed to check if the service is ready.</li> <li>It uses a simple <code>curl</code> command to check the health endpoint of the service, retrying until it gets a successful response.</li> <li>This ensures that the application is fully operational before the release is considered successful.</li> <li>The hook will be deleted after it succeeds, preventing it from running again unnecessarily.</li> <li>This is particularly useful for applications that require some time to become ready after deployment, such as those that perform initialization tasks or have complex startup processes.</li> <li>By using a post-install hook, you can provide immediate feedback on the success of the deployment and ensure that users are aware of any issues with service readiness right after installation.       <pre><code># templates/hooks/smoke-test.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"myapp.fullname\" . }}-smoke-test\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-weight\": \"5\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n        - name: smoke-test\n          image: curlimages/curl:latest\n          command:\n            - sh\n            - -c\n            - |\n              echo \"Waiting for service to be ready...\"\n              until curl -f http://{{ include \"myapp.fullname\" . }}:{{ .Values.service.port }}/health; do\n                echo \"Service not ready yet, retrying in 5 seconds...\"\n                sleep 5\n              done\n              echo \"Service is ready!\"\n      restartPolicy: Never\n  backoffLimit: 10\n</code></pre></li> </ul> Example 4: Backup Before Upgrade (pre-upgrade) <ul> <li>This hook creates a backup of the database before upgrading the application.</li> <li>It uses the <code>pg_dump</code> command to create a SQL backup file with a timestamp.</li> <li>The backup is stored in a persistent volume claim to ensure it\u2019s retained even if the job is deleted.</li> <li>The hook runs with a weight of <code>-10</code> to ensure it executes before other upgrade hooks like migrations.</li> <li>This is a critical safety measure to ensure you can restore your data if an upgrade fails or causes data corruption.       <pre><code># templates/hooks/backup.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"myapp.fullname\" . }}-backup-{{ now | date \"20060102-150405\" }}\n  annotations:\n    \"helm.sh/hook\": pre-upgrade\n    \"helm.sh/hook-weight\": \"-10\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n        - name: backup\n          image: \"{{ .Values.backup.image }}\"\n          command:\n            - sh\n            - -c\n            - |\n              echo \"Creating backup before upgrade...\"\n              pg_dump -h $DB_HOST -U $DB_USER $DB_NAME &gt; /backup/backup-$(date +%Y%m%d-%H%M%S).sql\n              echo \"Backup completed successfully\"\n          env:\n            - name: DB_HOST\n              value: {{ .Values.database.host }}\n            - name: DB_USER\n              value: {{ .Values.database.user }}\n            - name: DB_NAME\n              value: {{ .Values.database.name }}\n          volumeMounts:\n            - name: backup-storage\n              mountPath: /backup\n      volumes:\n        - name: backup-storage\n          persistentVolumeClaim:\n            claimName: backup-pvc\n      restartPolicy: Never\n</code></pre></li> </ul> Example 5: Notification Hook (post-install/post-upgrade) <ul> <li>This hook sends a notification to Slack after the application is successfully installed or upgraded.</li> <li>It uses the Helm built-in <code>.Release.IsInstall</code> variable to determine whether this is a new installation or an upgrade.</li> <li>The hook runs with a weight of <code>10</code> to ensure it executes after other post-install/upgrade hooks like smoke tests.</li> <li>Notifications are sent regardless of hook success or failure (deletion policy: <code>hook-succeeded,hook-failed</code>).</li> <li>This is useful for keeping your team informed about deployments in production environments.       <pre><code># templates/hooks/notify.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"myapp.fullname\" . }}-notify\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-weight\": \"10\"  # Runs after smoke test\n    \"helm.sh/hook-delete-policy\": hook-succeeded,hook-failed\nspec:\n  template:\n    spec:\n      containers:\n        - name: notify\n          image: curlimages/curl:latest\n          command:\n            - sh\n            - -c\n            - |\n              if [ \"{{ .Release.IsInstall }}\" = \"true\" ]; then\n                ACTION=\"installed\"\n              else\n                ACTION=\"upgraded\"\n              fi\n              curl -X POST {{ .Values.slack.webhookUrl }} \\\n                -H 'Content-Type: application/json' \\\n                -d \"{\\\"text\\\":\\\"Application {{ .Release.Name }} has been $ACTION to version {{ .Chart.Version }} in namespace {{ .Release.Namespace }}\\\"}\"\n      restartPolicy: Never\n</code></pre></li> </ul> Example 6: Cleanup Hook (pre-delete) <ul> <li>This hook performs cleanup operations before the main application resources are deleted.</li> <li>It uses <code>kubectl</code> to delete specific resources (in this case, ConfigMaps) that match certain labels.</li> <li>The hook requires a ServiceAccount with appropriate RBAC permissions to delete resources in the namespace.</li> <li>This is useful for cleaning up dynamically created resources that might not be tracked by Helm directly.</li> <li>The hook will be deleted after it succeeds, preventing orphaned cleanup jobs from accumulating.       <pre><code># templates/hooks/cleanup.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"myapp.fullname\" . }}-cleanup\n  annotations:\n    \"helm.sh/hook\": pre-delete\n    \"helm.sh/hook-weight\": \"0\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n        - name: cleanup\n          image: bitnami/kubectl:latest\n          command:\n            - sh\n            - -c\n            - |\n              echo \"Cleaning up resources...\"\n              kubectl delete configmap -n {{ .Release.Namespace }} -l app={{ include \"myapp.name\" . }},release={{ .Release.Name }}\n              echo \"Cleanup completed\"\n      serviceAccountName: {{ include \"myapp.fullname\" . }}-cleanup\n      restartPolicy: Never\n</code></pre></li> </ul> Example 7: Secret Creation Hook (pre-install) <ul> <li>This hook generates a random password and creates a Kubernetes secret before the main application resources are installed.</li> <li>It uses the <code>bitnami/kubectl</code> image to run <code>kubectl</code> commands directly from the job.</li> <li>The generated password is stored in a secret named <code>&lt;release-name&gt;-db-secret</code> in the same namespace as the release.</li> <li>The hook is set to run only during installation, ensuring that a new secret is created each time a new release is installed.</li> <li>The <code>before-hook-creation</code> deletion policy ensures that if the hook runs multiple times (e.g., due to retries), the previous secret will be deleted before a new one is created, preventing orphaned secrets from accumulating.</li> <li>This hook is useful for scenarios where you need to generate dynamic configuration or credentials that must be available before the main application resources are created.       <pre><code># templates/hooks/create-secret.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"myapp.fullname\" . }}-create-secret\n  annotations:\n    \"helm.sh/hook\": pre-install\n    \"helm.sh/hook-weight\": \"-15\"  # Runs very early\n    \"helm.sh/hook-delete-policy\": before-hook-creation\nspec:\n  template:\n    spec:\n      containers:\n        - name: create-secret\n          image: bitnami/kubectl:latest\n          command:\n            - sh\n            - -c\n            - |\n              # Generate random password\n              PASSWORD=$(openssl rand -base64 32)\n\n              # Create Kubernetes secret\n              kubectl create secret generic {{ include \"myapp.fullname\" . }}-db-secret \\\n                --from-literal=password=$PASSWORD \\\n                --namespace={{ .Release.Namespace }} \\\n                --dry-run=client -o yaml | kubectl apply -f -\n\n              echo \"Secret created successfully\"\n      serviceAccountName: {{ include \"myapp.fullname\" . }}-admin\n      restartPolicy: Never\n</code></pre></li> </ul>"},{"location":"13-HelmChart/#hook-best-practices","title":"Hook Best Practices","text":"<ol> <li>Use appropriate weights: Order hooks logically (e.g., backup before migration)</li> <li>Set deletion policies: Clean up hook resources to avoid clutter</li> <li>Add timeouts: Use <code>activeDeadlineSeconds</code> in Job specs to prevent hanging</li> <li>Use backoff limits: Set <code>backoffLimit</code> to control retry attempts</li> <li>Handle idempotency: Hooks should be safe to run multiple times</li> <li>Consider rollback: Avoid destructive operations in pre-delete hooks</li> <li>Test hooks: Run <code>helm install --dry-run --debug</code> to preview hook behavior</li> <li>Use ServiceAccounts: Grant appropriate RBAC permissions for hooks that interact with the cluster</li> </ol>"},{"location":"13-HelmChart/#debugging-hooks","title":"Debugging Hooks","text":"<pre><code># View hook resources\nkubectl get jobs,pods -n &lt;namespace&gt; -l heritage=Helm\n\n# Check hook logs\nkubectl logs job/&lt;hook-job-name&gt; -n &lt;namespace&gt;\n\n# View hook status during install\nhelm install myapp ./chart --wait --debug\n\n# Manually clean up failed hooks\nkubectl delete job &lt;hook-job-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Docs: Chart Hooks</p>"},{"location":"13-HelmChart/#helm-tests","title":"Helm Tests","text":"<ul> <li>Helm tests live in <code>templates/tests/</code> and are pod definitions with the <code>\"helm.sh/hook\": test</code> annotation.</li> <li>They are executed with <code>helm test &lt;release-name&gt;</code>.</li> </ul> <pre><code># templates/tests/test-connection.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: {{ include \"myapp.fullname\" . }}-test-connection\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['{{ include \"myapp.fullname\" . }}:{{ .Values.service.port }}']\n  restartPolicy: Never\n</code></pre> <pre><code># Run the tests\nhelm test my-release\n</code></pre> <p>Docs: Chart Tests</p>"},{"location":"13-HelmChart/#notestxt-post-install-messages","title":"NOTES.txt - Post-Install Messages","text":"<p>You can create a <code>templates/NOTES.txt</code> file to display useful information after a chart is installed:</p> <pre><code>Thank you for installing {{ .Chart.Name }}!\n\nYour release is named: {{ .Release.Name }}\n\nTo access the application, run:\n  kubectl port-forward svc/{{ include \"myapp.fullname\" . }} 8080:{{ .Values.service.port }}\n\nThen open http://localhost:8080 in your browser.\n</code></pre>"},{"location":"13-HelmChart/#lab","title":"Lab","text":""},{"location":"13-HelmChart/#step-01-installing-helm","title":"Step 01 - Installing <code>Helm</code>","text":"<ul> <li> <p>Before you can use the <code>codewizard-helm-demo</code> chart, you\u2019ll need to install <code>Helm</code> on your local machine.</p> </li> <li> <p><code>Helm</code> install methods by OS:</p> </li> </ul> LinuxMacOSWindows (via Chocolatey) <pre><code>curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n</code></pre> <pre><code>brew install helm\n</code></pre> <pre><code>choco install kubernetes-helm\n</code></pre>"},{"location":"13-HelmChart/#verify-installation","title":"Verify Installation","text":"<ul> <li>To confirm that <code>Helm</code> is installed correctly, run:</li> </ul> <pre><code>helm version\n\n## Expected output\nversion.BuildInfo{Version:\"xx\", GitCommit:\"xx\", GitTreeState:\"clean\", GoVersion:\"xx\"}\n</code></pre>"},{"location":"13-HelmChart/#step-02-creating-our-helm-chart","title":"Step 02 - Creating our <code>Helm</code> chart","text":"<ul> <li>Creating our custom <code>codewizard-helm-demo</code> <code>Helm</code> chart</li> <li>The custom <code>codewizard-helm-demo</code> <code>Helm</code> chart is build upon the following K8S resources:<ul> <li>ConfigMap</li> <li>Deployment</li> <li>Service</li> </ul> </li> <li>As mentioned above, we will also have the following <code>Helm</code> resources:<ul> <li>Chart.yaml</li> <li>values.yaml</li> <li>templates/_helpers.tpl</li> </ul> </li> </ul>"},{"location":"13-HelmChart/#create-a-new-chart","title":"Create a New Chart","text":"<ul> <li>First, we need to create a <code>Helm</code> chart using the <code>helm create</code> command.</li> <li>This command will generate the necessary file structure for your new chart.</li> </ul> <pre><code>helm create codewizard-helm-demo\n</code></pre> What is the result of this command? <ul> <li>Examine the chart structure!</li> <li>Try to explain to yourself which files are in the folder.</li> <li>See the above reference to the structure Chart files and folders</li> </ul>"},{"location":"13-HelmChart/#navigate-to-the-chart-directory","title":"Navigate to the Chart Directory","text":"<pre><code>cd codewizard-helm-demo\n</code></pre>"},{"location":"13-HelmChart/#write-the-chart-content","title":"Write the chart content","text":"<ul> <li>Copy the content of the chart folder (in this lab) to the chart directory (overwriting the files).</li> </ul>"},{"location":"13-HelmChart/#step-03-pack-the-chart","title":"Step 03 - Pack the chart","text":"<ul> <li>After we have created or customized our chart, we need to pack it as <code>.tgz</code> file, which can then be shared or installed.</li> </ul>"},{"location":"13-HelmChart/#helm-package","title":"helm package","text":"<p>Helm Package</p> <p><code>helm package</code> packages a chart into a versioned chart archive file. If a path is given, this will \u201clook\u201d at that path for a chart which must contain a <code>Chart.yaml</code> file and then pack that directory.</p> <pre><code>helm package codewizard-helm-demo\n</code></pre> <ul> <li>This command will create a file called <code>codewizard-helm-demo-&lt;version&gt;.tgz</code> inside your current directory.</li> </ul>"},{"location":"13-HelmChart/#step-04-validate-the-chart-content","title":"Step 04 - Validate the chart content","text":""},{"location":"13-HelmChart/#helm-template","title":"<code>helm template</code>","text":"<ul> <li><code>Helm</code> allows you to generate the Kubernetes manifests based on the templates and values files without actually installing the chart.</li> <li>This is useful to preview what the generated resources will look like:</li> </ul> <pre><code>helm template codewizard-helm-demo\n\n## This will output the rendered Kubernetes manifests to your terminal\n</code></pre>"},{"location":"13-HelmChart/#helm-lint","title":"<code>helm lint</code>","text":"<ul> <li>You can also lint the chart to check for well-formedness and best practices:</li> </ul> <pre><code>helm lint codewizard-helm-demo\n\n## Expected output:\n## ==&gt; Linting codewizard-helm-demo\n## [INFO] Chart.yaml: icon is recommended\n## 1 chart(s) linted, 0 chart(s) failed\n</code></pre>"},{"location":"13-HelmChart/#step-05-install-the-chart","title":"Step 05 - Install the chart","text":"<ul> <li>Install the <code>codewizard-helm-demo</code> chart into Kubernetes cluster</li> </ul>"},{"location":"13-HelmChart/#the-helm-install-command","title":"The <code>helm install</code> command","text":"<ul> <li>This command installs a chart archive.</li> <li>The install argument must be a chart reference, a path to a packed chart, a path to an unpacked chart directory or a URL.</li> <li>To override values in a chart, use:</li> <li><code>--values</code> - pass in a file</li> <li><code>--set</code> - pass configuration from the command line</li> <li>Use <code>--dry-run</code> to simulate an install without actually deploying:</li> </ul> <pre><code># Dry run - preview what will be installed without deploying\nhelm install codewizard-helm-demo codewizard-helm-demo-0.1.0.tgz --dry-run\n</code></pre> <pre><code># Install the packed helm chart\nhelm install codewizard-helm-demo codewizard-helm-demo-0.1.0.tgz\n</code></pre>"},{"location":"13-HelmChart/#step-06-verify-the-installation","title":"Step 06 - Verify the installation","text":"<ul> <li>Examine newly created <code>Helm</code> chart release, and all cluster created resources:</li> </ul> <pre><code># List the installed helms\nhelm ls\n\n# Show detailed status of the release\nhelm status codewizard-helm-demo\n\n# Get the rendered manifests of the release\nhelm get manifest codewizard-helm-demo\n\n# Get the values used by the release\nhelm get values codewizard-helm-demo\n\n# Check the resources\nkubectl get all -n codewizard\n</code></pre>"},{"location":"13-HelmChart/#step-07-test-the-service","title":"Step 07 - Test the service","text":"<ul> <li>Perform an <code>HTTP GET</code> request, send it to the newly created cluster service.</li> <li>Confirm that the response contains the <code>CodeWizard Helm Demo</code> message passed from the <code>values.yaml</code> file.</li> </ul> <pre><code>kubectl run busybox         \\\n        --image=busybox     \\\n        --rm                \\\n        -it                 \\\n        --restart=Never     \\\n        -- /bin/sh -c \"wget -qO- http://codewizard-helm-demo.codewizard.svc.cluster.local\"\n\n### Output:\nCodeWizard Helm Demo\n</code></pre> <ul> <li>You can also test the release name and revision endpoints defined in the ConfigMap:</li> </ul> <pre><code># Get the release name\nkubectl run busybox --image=busybox --rm -it --restart=Never \\\n  -- /bin/sh -c \"wget -qO- http://codewizard-helm-demo.codewizard.svc.cluster.local/release/name\"\n\n# Get the release revision\nkubectl run busybox --image=busybox --rm -it --restart=Never \\\n  -- /bin/sh -c \"wget -qO- http://codewizard-helm-demo.codewizard.svc.cluster.local/release/revision\"\n</code></pre> <pre><code># upgrade and pass a different message than the one from the default values\n# Use the --set to pass the desired value\nhelm  upgrade \\\n  codewizard-helm-demo \\\n  codewizard-helm-demo-0.1.0.tgz \\\n  --set nginx.conf.message=\"Helm Rocks\"\n</code></pre>"},{"location":"13-HelmChart/#step-09-check-the-upgrade","title":"Step 09 - Check the upgrade","text":"<ul> <li>Perform another <code>HTTP GET</code> request.</li> <li>Confirm that the response now has the updated message <code>Helm Rocks</code>:</li> </ul> <pre><code>kubectl run busybox         \\\n        --image=busybox     \\\n        --rm                \\\n        -it                 \\\n        --restart=Never     \\\n        -- /bin/sh -c \"wget -qO- http://codewizard-helm-demo.codewizard.svc.cluster.local\"\n\n### Output:\nHelm Rocks\n</code></pre> <ul> <li>Also check the revision number - it should now be <code>2</code>:</li> </ul> <pre><code>kubectl run busybox --image=busybox --rm -it --restart=Never \\\n  -- /bin/sh -c \"wget -qO- http://codewizard-helm-demo.codewizard.svc.cluster.local/release/revision\"\n\n### Output:\n2\n</code></pre>"},{"location":"13-HelmChart/#helm-history","title":"<code>helm history</code>","text":"<ul> <li><code>helm history</code> prints historical revisions for a given release.</li> <li>A default maximum of 256 revisions will be returned.</li> </ul> <pre><code>helm history codewizard-helm-demo\n\n### Sample output\nREVISION        UPDATED    STATUS          CHART                           APP VERSION     DESCRIPTION\n1               ...        superseded      codewizard-helm-demo-0.1.0      1.19.7          Install complete\n2               ...        deployed        codewizard-helm-demo-0.1.0      1.19.7          Upgrade complete\n</code></pre>"},{"location":"13-HelmChart/#step-11-rollback","title":"Step 11 - Rollback","text":""},{"location":"13-HelmChart/#helm-rollback","title":"<code>helm rollback</code>","text":"<ul> <li>Rollback the <code>codewizard-helm-demo</code> release to previous version:</li> </ul> <pre><code>helm rollback codewizard-helm-demo\n\n### Output:\nRollback was a success! Happy Helming!\n</code></pre> <ul> <li>Check again to verify that you get the original message!</li> </ul>"},{"location":"13-HelmChart/#exercises","title":"Exercises","text":"<p>The following exercises will test your understanding of <code>Helm</code> concepts. Try to solve each exercise on your own before revealing the solution.</p>"},{"location":"13-HelmChart/#01-explore-a-public-chart-repository","title":"01. Explore a Public Chart Repository","text":"<p>Add the Bitnami chart repository and search for an <code>nginx</code> chart.</p>"},{"location":"13-HelmChart/#scenario","title":"Scenario:","text":"<p>\u25e6 You need to find and inspect a publicly available Helm chart before installing it. \u25e6 Chart repositories are the standard way to discover and share Helm charts.</p> <p>Hint: Use <code>helm repo add</code>, <code>helm repo update</code>, and <code>helm search repo</code>.</p> Solution <pre><code># Add the Bitnami chart repository\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\n# Update the repository index\nhelm repo update\n\n# Search for nginx charts\nhelm search repo nginx\n\n# Show the default values of the bitnami nginx chart\nhelm show values bitnami/nginx | head -50\n</code></pre>"},{"location":"13-HelmChart/#02-install-with-custom-values-file","title":"02. Install with Custom Values File","text":"<p>Create a custom <code>values.yaml</code> file that changes the <code>replicaCount</code> to <code>3</code> and the message to <code>\"Hello from custom values\"</code>, then install the chart using this file.</p>"},{"location":"13-HelmChart/#scenario_1","title":"Scenario:","text":"<p>\u25e6 In production environments, you rarely use default values. \u25e6 Custom values files let you manage environment-specific configurations (dev, staging, prod).</p> <p>Hint: Create a YAML file and use <code>helm install -f &lt;file&gt;</code>.</p> Solution <pre><code># Create a custom values file\ncat &lt;&lt;EOF &gt; custom-values.yaml\nreplicaCount: 3\nnginx:\n  conf:\n    message: \"Hello from custom values\"\nEOF\n\n# Install with the custom values file\nhelm install custom-demo codewizard-helm-demo-0.1.0.tgz -f custom-values.yaml\n\n# Verify the replica count\nkubectl get deployment -n codewizard\n\n# Verify the message\nkubectl run busybox --image=busybox --rm -it --restart=Never \\\n  -- /bin/sh -c \"wget -qO- http://custom-demo.codewizard.svc.cluster.local\"\n\n# Cleanup\nhelm uninstall custom-demo\nrm custom-values.yaml\n</code></pre>"},{"location":"13-HelmChart/#03-debug-a-failing-template","title":"03. Debug a Failing Template","text":"<p>Given the following broken template snippet, identify and fix the error. Save this as <code>templates/broken.yaml</code> in the chart, then use <code>helm template</code> to find the issue:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ .Values.brokenName }\ndata:\n  key: {{ .Values.missingValue | default \"fallback\" }}\n</code></pre>"},{"location":"13-HelmChart/#scenario_2","title":"Scenario:","text":"<p>\u25e6 Template syntax errors are common during chart development. \u25e6 <code>helm template</code> and <code>helm lint</code> are your best debugging tools.</p> <p>Hint: Use <code>helm template</code> and <code>helm lint</code> to identify the error. Count the curly braces.</p> Solution  The error is a missing closing brace on line 4: `{{ .Values.brokenName }` should be `{{ .Values.brokenName }}`.  <pre><code># Lint the chart to find errors\nhelm lint codewizard-helm-demo\n\n# Try to render the template - this will show the error\nhelm template codewizard-helm-demo\n\n# Fix: the correct line should be:\n#   name: {{ .Values.brokenName }}\n# (two closing braces instead of one)\n\n# Don't forget to remove the broken test file\nrm codewizard-helm-demo/templates/broken.yaml\n</code></pre>"},{"location":"13-HelmChart/#04-use-set-to-override-multiple-values","title":"04. Use <code>--set</code> to Override Multiple Values","text":"<p>Upgrade the <code>codewizard-helm-demo</code> release to use <code>3</code> replicas, image tag <code>1.21.0</code>, and the message <code>\"Multi-set Override\"</code> - all in a single command.</p>"},{"location":"13-HelmChart/#scenario_3","title":"Scenario:","text":"<p>\u25e6 Quick overrides using <code>--set</code> are common for CI/CD pipelines and ad-hoc changes. \u25e6 You need to understand the dot-notation for nested values.</p> <p>Hint: Chain multiple <code>--set</code> flags or use comma-separated notation.</p> Solution <pre><code># Method 1: multiple --set flags\nhelm upgrade codewizard-helm-demo codewizard-helm-demo-0.1.0.tgz \\\n  --set replicaCount=3 \\\n  --set image.tag=\"1.21.0\" \\\n  --set nginx.conf.message=\"Multi-set Override\"\n\n# Method 2: comma-separated (equivalent)\nhelm upgrade codewizard-helm-demo codewizard-helm-demo-0.1.0.tgz \\\n  --set replicaCount=3,image.tag=1.21.0,nginx.conf.message=\"Multi-set Override\"\n\n# Verify the values\nhelm get values codewizard-helm-demo\n\n# Verify replicas and image\nkubectl get deployment -n codewizard -o wide\n</code></pre>"},{"location":"13-HelmChart/#05-add-a-conditional-resource","title":"05. Add a Conditional Resource","text":"<p>Modify the <code>codewizard-helm-demo</code> chart to add an optional <code>Ingress</code> resource that is only created when <code>ingress.enabled</code> is set to <code>true</code> in <code>values.yaml</code>.</p>"},{"location":"13-HelmChart/#scenario_4","title":"Scenario:","text":"<p>\u25e6 Not all environments need an Ingress (e.g., local development vs. production). \u25e6 Conditional rendering with <code>if</code> blocks is a fundamental Helm templating pattern.</p> <p>Hint: Use <code>{{- if .Values.ingress.enabled }}</code> \u2026 <code>{{- end }}</code>. Add <code>ingress.enabled: false</code> to <code>values.yaml</code>.</p> Solution <pre><code># 1. Add ingress values to values.yaml\ncat &lt;&lt;EOF &gt;&gt; codewizard-helm-demo/values.yaml\n\ningress:\n  enabled: false\n  host: demo.example.com\nEOF\n</code></pre> <pre><code># 2. Create templates/Ingress.yaml\n# Save this as codewizard-helm-demo/templates/Ingress.yaml:\n{{- if .Values.ingress.enabled }}\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ include \"webserver.fullname\" . }}\n  namespace: codewizard\n  labels:\n    {{- include \"webserver.labels\" . | nindent 4 }}\nspec:\n  rules:\n    - host: {{ .Values.ingress.host }}\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: {{ include \"webserver.fullname\" . }}\n                port:\n                  number: {{ .Values.service.port }}\n{{- end }}\n</code></pre> <pre><code># 3. Verify: template without ingress (should NOT include Ingress resource)\nhelm template codewizard-helm-demo | grep -A5 \"kind: Ingress\"\n\n# 4. Verify: template WITH ingress enabled (should include Ingress resource)\nhelm template codewizard-helm-demo --set ingress.enabled=true | grep -A20 \"kind: Ingress\"\n\n# 5. Cleanup - remove the ingress template\nrm codewizard-helm-demo/templates/Ingress.yaml\n</code></pre>"},{"location":"13-HelmChart/#06-add-a-named-template","title":"06. Add a Named Template","text":"<p>Create a new named template in <code>_helpers.tpl</code> called <code>webserver.annotations</code> that generates a set of annotations including the chart version and a custom <code>team</code> annotation from values. Then use it in the Deployment.</p>"},{"location":"13-HelmChart/#scenario_5","title":"Scenario:","text":"<p>\u25e6 Named templates reduce duplication across manifests. \u25e6 Annotations are commonly used for metadata, monitoring, and CI/CD integration.</p> <p>Hint: Use <code>{{- define \"webserver.annotations\" -}}</code> to define and <code>{{ include \"webserver.annotations\" . | nindent N }}</code> to use it.</p> Solution <pre><code># 1. Add to _helpers.tpl:\n{{/*\nCommon annotations\n*/}}\n{{- define \"webserver.annotations\" -}}\napp.kubernetes.io/chart: {{ include \"webserver.chart\" . }}\napp.kubernetes.io/team: {{ .Values.team | default \"platform\" }}\n{{- end }}\n</code></pre> <pre><code># 2. Use in Deployment.yaml metadata:\nmetadata:\n  name: { { include \"webserver.fullname\" . } }\n  namespace: codewizard\n  annotations: { { - include \"webserver.annotations\" . | nindent 4 } }\n  labels: { { - include \"webserver.labels\" . | nindent 4 } }\n</code></pre> <pre><code># 3. Verify the rendered output\nhelm template codewizard-helm-demo --set team=\"devops\" | grep -A5 \"annotations\"\n</code></pre>"},{"location":"13-HelmChart/#07-use-range-to-generate-multiple-environment-variables","title":"07. Use <code>range</code> to Generate Multiple Environment Variables","text":"<p>Modify the Deployment template to inject a list of environment variables from <code>values.yaml</code> using the <code>range</code> function.</p>"},{"location":"13-HelmChart/#scenario_6","title":"Scenario:","text":"<p>\u25e6 Real-world deployments often require multiple environment variables. \u25e6 Hardcoding them in templates is not maintainable - values-driven configuration is preferred.</p> <p>Hint: Add an <code>env</code> list to <code>values.yaml</code> and use <code>{{- range .Values.env }}</code> in the container spec.</p> Solution <pre><code># 1. Add to values.yaml:\nenv:\n  - name: APP_ENV\n    value: \"production\"\n  - name: LOG_LEVEL\n    value: \"info\"\n  - name: APP_VERSION\n    value: \"1.0.0\"\n</code></pre> <pre><code># 2. Add to templates/Deployment.yaml inside the container spec (after imagePullPolicy):\n          env:\n          {{- range .Values.env }}\n            - name: {{ .name }}\n              value: {{ .value | quote }}\n          {{- end }}\n</code></pre> <pre><code># 3. Verify rendered output\nhelm template codewizard-helm-demo | grep -A15 \"env:\"\n\n# 4. Override from command line\nhelm template codewizard-helm-demo \\\n  --set \"env[0].name=CUSTOM_VAR\" \\\n  --set \"env[0].value=custom-value\" | grep -A5 \"env:\"\n</code></pre>"},{"location":"13-HelmChart/#08-create-a-helm-test","title":"08. Create a Helm Test","text":"<p>Add a Helm test to the <code>codewizard-helm-demo</code> chart that verifies the service is reachable and returns the expected message.</p>"},{"location":"13-HelmChart/#scenario_7","title":"Scenario:","text":"<p>\u25e6 Helm tests allow you to validate that a release is working correctly after deployment. \u25e6 Tests are pod definitions with the <code>\"helm.sh/hook\": test</code> annotation.</p> <p>Hint: Create a file in <code>templates/tests/</code> with a <code>busybox</code> pod that runs <code>wget</code> against the service.</p> Solution <pre><code># 1. Create the tests directory\nmkdir -p codewizard-helm-demo/templates/tests\n</code></pre> <pre><code># 2. Create templates/tests/test-connection.yaml:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: {{ include \"webserver.fullname\" . }}-test-connection\n  namespace: codewizard\n  labels:\n    {{- include \"webserver.labels\" . | nindent 4 }}\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['sh', '-c']\n      args:\n        - |\n          RESPONSE=$(wget -qO- http://{{ include \"webserver.fullname\" . }}.codewizard.svc.cluster.local)\n          echo \"Response: $RESPONSE\"\n          echo \"$RESPONSE\" | grep -q \"{{ .Values.nginx.conf.message }}\"\n  restartPolicy: Never\n</code></pre> <pre><code># 3. Install or upgrade the chart\nhelm upgrade --install codewizard-helm-demo codewizard-helm-demo-0.1.0.tgz\n\n# 4. Run the test\nhelm test codewizard-helm-demo\n\n# Expected output:\n# NAME: codewizard-helm-demo\n# ...\n# Phase: Succeeded\n</code></pre>"},{"location":"13-HelmChart/#09-manage-chart-dependencies","title":"09. Manage Chart Dependencies","text":"<p>Create a new <code>Helm</code> chart that depends on the <code>bitnami/redis</code> chart as a subchart. Configure the dependency and update it.</p>"},{"location":"13-HelmChart/#scenario_8","title":"Scenario:","text":"<p>\u25e6 Most real-world applications depend on databases, caches, or message queues. \u25e6 Helm dependencies let you compose complex deployments from reusable charts.</p> <p>Hint: Add a <code>dependencies</code> section to <code>Chart.yaml</code>, then run <code>helm dependency update</code>.</p> Solution <pre><code># 1. Create a new chart\nhelm create myapp-with-deps\ncd myapp-with-deps\n</code></pre> <pre><code># 2. Add dependencies to Chart.yaml (append at the end):\ndependencies:\n  - name: redis\n    version: \"~17\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: redis.enabled\n</code></pre> <pre><code># 3. Add Redis configuration to values.yaml:\nredis:\n  enabled: true\n  architecture: standalone\n  auth:\n    enabled: false\n</code></pre> <pre><code># 4. Add the Bitnami repo (if not already added)\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n\n# 5. Download the dependency charts\nhelm dependency update .\n\n# 6. Verify the dependency was downloaded\nls charts/\n\n# 7. Preview the rendered output (Redis resources will be included)\nhelm template myapp-with-deps . | grep \"kind:\" | sort | uniq\n\n# 8. Cleanup\ncd ..\nrm -rf myapp-with-deps\n</code></pre>"},{"location":"13-HelmChart/#10-create-a-pre-install-hook","title":"10. Create a Pre-Install Hook","text":"<p>Add a <code>pre-install</code> hook to the <code>codewizard-helm-demo</code> chart that creates a Job to print a banner message before the main resources are installed.</p>"},{"location":"13-HelmChart/#scenario_9","title":"Scenario:","text":"<p>\u25e6 Hooks allow you to run setup, migration, or validation tasks at specific lifecycle points. \u25e6 <code>pre-install</code> hooks run before the main chart resources are created.</p> <p>Hint: Create a <code>Job</code> manifest with the annotation <code>\"helm.sh/hook\": pre-install</code> and <code>\"helm.sh/hook-delete-policy\": hook-succeeded</code>.</p> Solution <pre><code># Create templates/pre-install-hook.yaml:\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"webserver.fullname\" . }}-pre-install\n  namespace: codewizard\n  annotations:\n    \"helm.sh/hook\": pre-install\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n        - name: pre-install\n          image: busybox\n          command: ['sh', '-c', 'echo \"=== Installing {{ .Release.Name }} (Chart: {{ .Chart.Name }}-{{ .Chart.Version }}) ===\"']\n      restartPolicy: Never\n  backoffLimit: 1\n</code></pre> <pre><code># Install and observe the hook\nhelm install codewizard-helm-demo codewizard-helm-demo-0.1.0.tgz\n\n# The Job should run and complete before the main resources are created\n# Check the jobs (it may already be cleaned up due to hook-delete-policy)\nkubectl get jobs -n codewizard\n\n# Cleanup\nhelm uninstall codewizard-helm-demo\nrm codewizard-helm-demo/templates/pre-install-hook.yaml\n</code></pre>"},{"location":"13-HelmChart/#11-diff-before-upgrade","title":"11. Diff Before Upgrade","text":"<p>Use the <code>helm-diff</code> plugin to preview what changes an upgrade will make before applying it.</p>"},{"location":"13-HelmChart/#scenario_10","title":"Scenario:","text":"<p>\u25e6 In production, blindly upgrading without reviewing changes is risky. \u25e6 The <code>helm-diff</code> plugin shows a diff of what would change, similar to <code>kubectl diff</code>.</p> <p>Hint: Install the plugin with <code>helm plugin install</code>, then use <code>helm diff upgrade</code>.</p> Solution <pre><code># 1. Install the helm-diff plugin\nhelm plugin install https://github.com/databus23/helm-diff\n\n# 2. Make sure the release is installed\nhelm install codewizard-helm-demo codewizard-helm-demo-0.1.0.tgz\n\n# 3. Preview what an upgrade would change (without applying)\nhelm diff upgrade codewizard-helm-demo codewizard-helm-demo-0.1.0.tgz \\\n  --set nginx.conf.message=\"Updated Message\" \\\n  --set replicaCount=5\n\n# The output shows colorized diff of what resources would change\n\n# 4. If you're satisfied, apply the upgrade\nhelm upgrade codewizard-helm-demo codewizard-helm-demo-0.1.0.tgz \\\n  --set nginx.conf.message=\"Updated Message\" \\\n  --set replicaCount=5\n\n# Cleanup\nhelm uninstall codewizard-helm-demo\n</code></pre>"},{"location":"13-HelmChart/#12-create-a-notestxt","title":"12. Create a NOTES.txt","text":"<p>Add a <code>NOTES.txt</code> file to the <code>codewizard-helm-demo</code> chart that displays the release name, namespace, and instructions for testing the service after installation.</p>"},{"location":"13-HelmChart/#scenario_11","title":"Scenario:","text":"<p>\u25e6 <code>NOTES.txt</code> provides post-install guidance to users who install your chart. \u25e6 It supports the same Go template syntax as other template files.</p> <p>Hint: Create <code>templates/NOTES.txt</code> with template directives like <code>{{ .Release.Name }}</code>.</p> Solution <pre><code># Create templates/NOTES.txt with the following content:\n\n==================================================\n  {{ .Chart.Name }} has been installed!\n==================================================\n\nRelease Name : {{ .Release.Name }}\nNamespace    : codewizard\nRevision     : {{ .Release.Revision }}\nChart Version: {{ .Chart.Version }}\nApp Version  : {{ .Chart.AppVersion }}\n\nTo test the service, run:\n\n  kubectl run busybox --image=busybox --rm -it --restart=Never \\\n    -- /bin/sh -c \"wget -qO- http://{{ include \"webserver.fullname\" . }}.codewizard.svc.cluster.local\"\n\nTo check release status:\n\n  helm status {{ .Release.Name }}\n\nTo uninstall:\n\n  helm uninstall {{ .Release.Name }}\n</code></pre> <pre><code># Install and verify the NOTES are displayed\nhelm install codewizard-helm-demo codewizard-helm-demo-0.1.0.tgz\n\n# The NOTES should be displayed after installation\n# You can also view them again with:\nhelm status codewizard-helm-demo\n\n# Cleanup\nhelm uninstall codewizard-helm-demo\n</code></pre>"},{"location":"13-HelmChart/#finalize-cleanup","title":"Finalize &amp; Cleanup","text":"<ul> <li>To remove all resources created by this lab, uninstall the <code>codewizard-helm-demo</code> release:</li> </ul> <pre><code>helm uninstall codewizard-helm-demo\n</code></pre> <ul> <li>(Optional) If you have created a dedicated namespace for this lab, you can delete it by running:</li> </ul> <pre><code>kubectl delete namespace codewizard\n</code></pre> <ul> <li>(Optional) Remove added Helm repositories:</li> </ul> <pre><code>helm repo remove bitnami\n</code></pre>"},{"location":"13-HelmChart/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Helm not found:</li> </ul> <p>Make sure <code>Helm</code> is installed and available in your <code>PATH</code>. Run the following to verify:</p> <pre><code>helm version\n</code></pre> <p></p> <ul> <li>Pods not starting:</li> </ul> <p>Check pod status and logs by running the following commands:</p> <pre><code>kubectl get pods -n codewizard\nkubectl describe pod &lt;pod-name&gt; -n codewizard\nkubectl logs &lt;pod-name&gt; -n codewizard\n</code></pre> <p></p> <ul> <li>Service not reachable:</li> </ul> <p>Ensure the service and pods are running by running the following commands:</p> <pre><code>kubectl get svc -n codewizard\nkubectl get pods -n codewizard\n</code></pre> <p></p> <ul> <li>Values not updated after upgrade:</li> </ul> <p>Double-check your <code>--set</code> or <code>--values</code> flags and confirm the upgrade by running:</p> <pre><code>helm get values codewizard-helm-demo\n</code></pre> <p></p> <ul> <li>Template rendering errors:</li> </ul> <p>Use <code>helm template</code> and <code>helm lint</code> to find and debug template issues:</p> <pre><code>helm template codewizard-helm-demo\nhelm lint codewizard-helm-demo\n</code></pre> <p></p> <ul> <li>Hook failures:</li> </ul> <p>Inspect hook resources (Jobs, Pods) to check their status and logs:</p> <pre><code>kubectl get jobs -n codewizard\nkubectl logs job/&lt;job-name&gt; -n codewizard\n</code></pre>"},{"location":"13-HelmChart/#next-steps","title":"Next Steps","text":"<ul> <li>Try creating your own <code>Helm</code> chart for a different application.</li> <li>Explore <code>Helm</code> chart repositories like Artifact Hub.</li> <li>Learn about advanced <code>Helm</code> features, such as: dependencies, hooks, and chart testing.</li> <li>Explore Helmfile for declarative management of multiple Helm releases.</li> <li>Learn about Helm Secrets for managing sensitive data in charts. ````</li> </ul>"},{"location":"14-Logging/","title":"Logging","text":"<ul> <li>Welcome to the <code>Logging</code> hands-on lab! In this tutorial, we will learn the essentials of <code>Logging</code> in Kubernetes clusters.</li> <li>We will deploy a sample application, configure log collection, and explore logs using popular tools like <code>Fluentd</code>, <code>Elasticsearch</code>, and <code>Kibana</code> (EFK stack).</li> </ul>"},{"location":"14-Logging/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>Why <code>Logging</code> is important in Kubernetes</li> <li>How to deploy a sample app that generates logs</li> <li>How to collect logs using Fluentd</li> <li>How to store and search logs with <code>Elasticsearch</code></li> <li>How to visualize logs with <code>Kibana</code></li> <li>Troubleshooting and best practices</li> </ul>"},{"location":"14-Logging/#introduction","title":"Introduction","text":"<ul> <li><code>Logging</code> is critical for monitoring, debugging, and auditing applications in Kubernetes.</li> <li>Kubernetes does not provide a builtin, centralized <code>Logging</code> solution, but it allows us to integrate with many <code>Logging</code> stacks.</li> <li>We will set up the EFK stack (<code>Elasticsearch</code>, <code>Fluentd</code>, <code>Kibana</code>) to collect, store, and visualize logs from our cluster.</li> </ul>"},{"location":"14-Logging/#lab","title":"Lab","text":""},{"location":"14-Logging/#step-01-deploy-a-sample-application","title":"Step 01 - Deploy a Sample Application","text":"<ul> <li>Deploy a simple <code>Nginx</code> application that generates access logs.</li> </ul> <pre><code>kubectl create deployment nginx --image=nginx\nkubectl expose deployment nginx --port=80 --type=NodePort\n</code></pre> <ul> <li>Check that the pod is running:</li> </ul> <pre><code>kubectl get pods\n</code></pre>"},{"location":"14-Logging/#step-02-deploy-elasticsearch","title":"Step 02 - Deploy <code>Elasticsearch</code>","text":"<ul> <li>Deploy <code>Elasticsearch</code> using <code>Helm</code>:</li> </ul> <pre><code>helm repo add elastic https://helm.elastic.co\nhelm repo update\nhelm install elasticsearch elastic/elasticsearch --set replicas=1 --set minimumMasterNodes=1\n</code></pre> <ul> <li>Wait for the pod to be ready and check its status:</li> </ul> <pre><code>kubectl get pods\n</code></pre>"},{"location":"14-Logging/#step-03-deploy-kibana","title":"Step 03 - Deploy <code>Kibana</code>","text":"<ul> <li>Deploy <code>Kibana</code> using <code>Helm</code>:</li> </ul> <pre><code>helm install kibana elastic/kibana\n</code></pre> <ul> <li>Forward the <code>Kibana</code> port:</li> </ul> <pre><code>kubectl port-forward svc/kibana-kibana 5601:5601 &amp;\n</code></pre> <p>If you are running this lab in Google Cloud Shell:</p> <ol> <li>After running the port-forward command above, click the Web Preview button in the Cloud Shell toolbar (usually at the top right).</li> <li>Enter port <code>5601</code> when prompted.</li> <li>This will open <code>Kibana</code> in a new browser tab at a URL like <code>https://&lt;cloudshell-id&gt;.shell.cloud.google.com/?port=5601</code>.</li> <li>If you see a warning about an untrusted connection, you can safely proceed.</li> </ol> <ul> <li>Access <code>Kibana</code> at http://localhost:5601 (if running locally) or via the Cloud Shell Web Preview, as explained above.</li> </ul>"},{"location":"14-Logging/#step-04-deploy-fluentd","title":"Step 04 - Deploy <code>Fluentd</code>","text":"<ul> <li>Deploy <code>Fluentd</code> as a <code>DaemonSet</code> to collect logs from all nodes and forward them to <code>Elasticsearch</code>.</li> </ul> <pre><code>kubectl apply -f https://raw.githubusercontent.com/fluent/fluentd-kubernetes-daemonset/master/fluentd-daemonset-elasticsearch-rbac.yaml\n</code></pre> <ul> <li>Check that <code>Fluentd</code> pods are running:</li> </ul> <pre><code>kubectl get pods -l app=fluentd\n</code></pre>"},{"location":"14-Logging/#step-05-generate-and-view-logs","title":"Step 05 - Generate and View Logs","text":"<ul> <li>Access the <code>Nginx</code> service to generate logs:</li> </ul> <pre><code>minikube service nginx\n</code></pre> <p>In <code>Kibana</code>, configure an index pattern to view logs:</p> <ol> <li>Open Kibana in your browser (using the Cloud Shell Web Preview as described above).</li> <li>In the left menu, click Stack Management &gt; Kibana &gt; Index Patterns.</li> <li>Click Create index pattern.</li> <li>In the \u201cIndex pattern\u201d field, enter <code>fluentd-*</code> (or <code>logstash-*</code> if your logs use that prefix).</li> <li>Click Next step.</li> <li>For the time field, select <code>@timestamp</code> and click Create index pattern.</li> <li>Go to Discover in the left menu to view and search your logs.</li> </ol> <p>Explore the logs, search, and visualize traffic.</p>"},{"location":"14-Logging/#troubleshooting","title":"Troubleshooting","text":""},{"location":"14-Logging/#pods-not-starting","title":"Pods not starting:","text":"<ul> <li>Check pod status and logs:</li> </ul> <pre><code>kubectl get pods\nkubectl describe pod &lt;pod-name&gt;\nkubectl logs &lt;pod-name&gt;\n</code></pre>"},{"location":"14-Logging/#kibana-not-reachable","title":"Kibana not reachable:","text":"<ul> <li>Ensure port-forward is running and no firewall is blocking port 5601.</li> </ul>"},{"location":"14-Logging/#no-logs-in-kibana","title":"No logs in Kibana:","text":"<ul> <li>Check Fluentd and Elasticsearch pod logs for errors.</li> <li>Ensure index pattern is set up correctly in Kibana.</li> </ul>"},{"location":"14-Logging/#cleanup","title":"Cleanup","text":"<ul> <li>To remove all resources created by this lab:</li> </ul> <pre><code>helm uninstall elasticsearch\nhelm uninstall kibana\nkubectl delete deployment nginx\nkubectl delete service nginx\nkubectl delete -f https://raw.githubusercontent.com/fluent/fluentd-kubernetes-daemonset/master/fluentd-daemonset-elasticsearch-rbac.yaml\n</code></pre>"},{"location":"14-Logging/#next-steps","title":"Next Steps","text":"<ul> <li>Try deploying other logging stacks like <code>Loki</code> + <code>Grafana</code>.</li> <li>Explore log aggregation, alerting, and retention policies.</li> <li>Integrate logging with monitoring and alerting tools.</li> <li>Read more in the Kubernetes logging documentation.</li> </ul>"},{"location":"15-Prometheus-Grafana/","title":"15 Prometheus Grafana","text":""},{"location":"15-Prometheus-Grafana/#prometheus-and-grafana-monitoring-lab","title":"Prometheus and Grafana Monitoring Lab","text":"<ul> <li>In this lab, we will learn how to set up and configure *<code>Prometheus</code>  and <code>Grafana</code> for monitoring a Kubernetes cluster.</li> <li>You will install <code>Prometheus</code> to collect metrics from the cluster and <code>Grafana</code> to visualize those metrics.</li> <li>By the end of this lab, you will have a functional monitoring stack that provides insights into the health and performance of your Kubernetes environment.</li> </ul>"},{"location":"15-Prometheus-Grafana/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>How to install Prometheus and Grafana on a Kubernetes cluster</li> <li>How to configure Prometheus to collect cluster metrics</li> <li>How to set up Grafana dashboards for visualizing metrics</li> <li>Monitoring cluster health, application performance, and infrastructure</li> </ul>"},{"location":"15-Prometheus-Grafana/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> <li><code>helm</code> installed (v3+)</li> </ul>"},{"location":"15-Prometheus-Grafana/#prometheus-and-grafana-setup-and-configuration-guide","title":"Prometheus and Grafana Setup and Configuration Guide","text":"<ul> <li>This guide serves as a comprehensive walkthrough of the steps to set up <code>Prometheus</code> and <code>Grafana</code> on your Kubernetes cluster.</li> <li>It includes hands-on steps for installing <code>Prometheus</code> using <code>Helm</code>, configuring <code>Prometheus</code> to collect metrics, setting up <code>Grafana</code> to visualize key metrics, and automating the setup using a bash script.</li> </ul>"},{"location":"15-Prometheus-Grafana/#introduction-to-prometheus-and-grafana","title":"Introduction to Prometheus and Grafana","text":""},{"location":"15-Prometheus-Grafana/#prometheus","title":"<code>Prometheus</code>","text":"<ul> <li><code>Prometheus</code> is an open-source systems monitoring and alerting toolkit designed for reliability and scalability.</li> <li>It collects and stores metrics as time-series data, providing powerful querying capabilities.</li> <li>It is commonly used in Kubernetes environments for monitoring cluster health, application performance, and infrastructure.</li> </ul>"},{"location":"15-Prometheus-Grafana/#grafana","title":"<code>Grafana</code>","text":"<ul> <li><code>Grafana</code> is a popular open-source data visualization tool that works well with <code>Prometheus</code>.</li> <li>It allows you to create dashboards and visualize metrics in real-time, providing insights into system performance and application health.</li> <li><code>Grafana</code> supports a wide range of visualization options, including <code>graphs</code>, <code>heatmaps</code>, <code>tables</code>, and more.</li> <li>Together, <code>Prometheus</code> and <code>Grafana</code> provide a powerful stack for monitoring and alerting in Kubernetes.</li> </ul>"},{"location":"15-Prometheus-Grafana/#part-01-installing-prometheus-and-grafana","title":"Part 01 - Installing Prometheus and Grafana","text":"<p>Helm Charts</p> <p>We will use Helm, to deploy Prometheus and Grafana.</p>"},{"location":"15-Prometheus-Grafana/#step-01-add-prometheus-and-grafana-helm-repositories","title":"Step 01 - Add Prometheus and Grafana Helm Repositories","text":"<ul> <li>Let\u2019s add the official <code>Helm</code> charts for <code>Prometheus</code> and <code>Grafana</code>:</li> </ul> <pre><code># Add Prometheus community Helm repository\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n# Add Grafana Helm repository\nhelm repo add grafana https://grafana.github.io/helm-charts\n# Update your Helm repositories to make sure they are up-to-date\nhelm repo update\n</code></pre>"},{"location":"15-Prometheus-Grafana/#step-02-install-prometheus-stack","title":"Step 02 - Install Prometheus Stack","text":"<ul> <li><code>Prometheus</code> is installed using the <code>prometheus-stack</code> <code>Helm</code> chart.</li> </ul> <pre><code># Install Prometheus\n#          Alertmanager\n#          Node Exporter\n# Create the `monitoring` namespace if it does not exist.\nhelm install  prometheus                                  \\\n              --namespace monitoring                      \\\n              --create-namespace                          \\\n              prometheus-community/kube-prometheus-stack\n\n# Verify the status of the release using the following:\nhelm status prometheus -n monitoring\n</code></pre>"},{"location":"15-Prometheus-Grafana/#step-03-install-grafana","title":"Step 03 - Install Grafana","text":"<ul> <li>Now, let\u2019s install <code>Grafana</code>.</li> <li><code>Grafana</code> will be deployed in the same <code>monitoring</code> namespace.</li> </ul> <pre><code>helm install grafana grafana/grafana --namespace monitoring\n\n# Verify the status of the release using the following:\nhelm status grafana -n monitoring\n</code></pre>"},{"location":"15-Prometheus-Grafana/#step-04-access-grafana","title":"Step 04 - Access Grafana","text":"<ul> <li><code>Grafana</code> will expose a service in your Kubernetes cluster.</li> <li>To access it, you need a password and port forwarding.</li> </ul> <pre><code># In order to get the Grafana admin password, run the following command:\nkubectl get secret grafana          \\\n            --namespace monitoring  \\\n            -o jsonpath='{.data.admin-password}' | base64 --decode ; echo\n\n# Set the port forwarding so you can access the service using your browsers\nkubectl port-forward            \\\n        --namespace monitoring  \\\n        service/grafana 3000:80\n</code></pre> <ul> <li>Verify that you can access <code>**Grafana</code></li> <li>Open your browser and navigate to http://localhost:3000</li> <li>The default login is:<ul> <li>Username : <code>admin</code></li> <li>Password : (the password you retrieved earlier)</li> </ul> </li> </ul> <p>Accessing Grafana on Google Cloud Shell</p> <p>If you are running your cluster in Google Cloud Shell, you cannot use <code>localhost</code> for port forwarding. Instead, use the Cloud Shell Web Preview:</p> <ol> <li>Run the port-forward command as usual:     <pre><code>kubectl port-forward --namespace monitoring service/grafana 3000:80\n</code></pre></li> <li>In Google Cloud Shell, click the \u201cWeb Preview\u201d button (top right) and select \u201cPreview on port 3000\u201d.</li> <li>Grafana will open in a new browser tab.</li> <li>Username: <code>admin</code></li> <li>Password: (the password you retrieved earlier)</li> </ol> <p>Note: You can use any available port (e.g., 3000, 3001) in the port-forward command, just match it in the Web Preview.</p>"},{"location":"15-Prometheus-Grafana/#part-02-configuring-prometheus","title":"Part 02 - Configuring Prometheus","text":"<ul> <li><code>Prometheus</code> can collect various metrics from your Kubernetes cluster automatically if the right exporters are enabled.</li> <li>The kube-prometheus-stack chart that you installed earlier automatically configures <code>Prometheus</code> to scrape a number of Kubernetes components (like <code>kubelet</code>, <code>node-exporter</code>, and <code>kube-state-metrics</code>) for various metrics.</li> </ul>"},{"location":"15-Prometheus-Grafana/#step-01-verify-prometheus-metrics-collection","title":"Step 01 - Verify Prometheus Metrics Collection","text":"<ul> <li>You can check if <code>Prometheus</code> is correctly scraping metrics by navigating to <code>Prometheus</code>\u2019 web UI.</li> </ul> <pre><code># Port-forward the Prometheus service:\nkubectl port-forward            \\\n        --namespace monitoring  \\\n        svc/prometheus-operated 9090:9090\n</code></pre> <ul> <li>Verify that you can access <code>Prometheus</code></li> <li>Open http://localhost:9090</li> <li>In the expression field paste the following:</li> </ul> <pre><code># This query will show the current status of the `kube-state-metrics` job\nup{job=\"kube-state-metrics\"}\n</code></pre> <p>Accessing Prometheus on Google Cloud Shell</p> <p>If you are running your cluster in Google Cloud Shell, you cannot use <code>localhost</code> for port forwarding. Instead, use the Cloud Shell Web Preview:</p> <ol> <li>Run the port-forward command as usual:     <pre><code>kubectl port-forward --namespace monitoring svc/prometheus-operated 9090:9090\n</code></pre></li> <li>In Google Cloud Shell, click the \u201cWeb Preview\u201d button (top right) and select \u201cPreview on port 9090\u201d.</li> <li>Prometheus will open in a new browser tab.</li> </ol> <p>Note: You can use any available port (e.g., 9090, 9091) in the port-forward command, just match it in the Web Preview.</p>"},{"location":"15-Prometheus-Grafana/#part-03-configuring-grafana","title":"Part 03 - Configuring Grafana","text":"<ul> <li>In this part we will set <code>grafana</code> to display the Cluster\u2019s CPUs, Memory, and Requests.</li> <li><code>Grafana</code> dashboards can be configured to display real-time metrics for CPU, memory, and requests.</li> <li><code>Prometheus</code> stores these metrics and <code>Grafana</code> will query <code>Prometheus</code> to display them.</li> </ul>"},{"location":"15-Prometheus-Grafana/#step-01-add-prometheus-as-a-data-source-in-grafana","title":"Step 01 - Add Prometheus as a Data Source in Grafana","text":"<ol> <li>Log into <code>Grafana</code> at: http://localhost:3000, or use the Cloud Shell Web Preview.</li> <li>Click on the hamburger icon on the left sidebar to open the Configuration menu.</li> <li>Click on Data Sources.</li> <li>Click Add data source and choose Prometheus.</li> <li>In the URL field, enter the Prometheus server URL: <code>http://prometheus-operated:9090</code>.</li> <li>Click Save &amp; Test to confirm that the connection is working.</li> </ol>"},{"location":"15-Prometheus-Grafana/#step-02-create-a-dashboard-to-display-metrics","title":"Step 02 - Create a Dashboard to Display Metrics","text":"<ul> <li>Next step is to create a dashboard and panels to display the desired metrics.</li> <li> <p>To create a dashboard in <code>Grafana</code> for CPU, memory, and requests do the following:</p> </li> <li> <p>In <code>Grafana</code>, open the left sidebar menu and select Dashboard.</p> </li> <li>Click Add visualization.</li> <li>Choose <code>Data Source</code> (as we defined it previously).</li> <li>In the panel editor, click on the <code>Code</code> option (right side of the query builder).</li> <li>Enter the below queries to visualize metric(s):      Note: To add new query click on the <code>+ Add query</code></li> <li>Save the dashboard.</li> </ul>"},{"location":"15-Prometheus-Grafana/#demo-automated-stack-and-dashboard","title":"Demo: Automated stack and dashboard","text":"<p>There is a convenience demo under <code>demo/01-stack</code> that:</p> <ul> <li>Installs <code>kube-prometheus-stack</code> and a <code>Grafana</code> release into the <code>monitoring</code> namespace.</li> <li>Provisions a Prometheus datasource in Grafana.</li> <li>Uploads a ready-made dashboard that shows CPU, memory, pod counts and HTTP request metrics.</li> </ul> <p>Files:</p> <ul> <li>demo/01-stack/demo.sh</li> <li>demo/01-stack/grafana-dashboards/cluster-metrics-dashboard.json</li> </ul> <p>Quick usage:</p> <pre><code>chmod +x demo/01-stack/demo.sh\n./demo/01-stack/demo.sh\n</code></pre> <p></p> <ul> <li>CPU Usage</li> </ul> <pre><code>sum(rate(container_cpu_usage_seconds_total{namespace=\"default\", container!=\"\", container!=\"POD\"}[5m])) by (pod, namespace)\n</code></pre> <ul> <li>Memory Usage :</li> </ul> <pre><code>sum(container_memory_usage_bytes{namespace=\"default\", container!=\"\", container!=\"POD\"}) by (pod, namespace)\n</code></pre> <ul> <li>Request Count :</li> </ul> <pre><code>sum(rate(http_requests_total{job=\"kubelet\", cluster=\"\", namespace=\"default\"}[5m])) by (pod, namespace)\n</code></pre>"},{"location":"15-Prometheus-Grafana/#step-03-get-number-of-pods-in-the-cluster","title":"Step 03 - Get Number of Pods in the Cluster","text":"<ul> <li>To track the number of pods running in the cluster, add new panel with the following query:</li> </ul> <pre><code># This query counts the number of pods running in all the namespaces\ncount(kube_pod_info{}) by (namespace)\n</code></pre> <ul> <li>Add another query which will count the number of pods under the namespace <code>monitoring</code>:</li> </ul> <pre><code>count(kube_pod_info{namespace=\"monitoring\"}) by (namespace)\n</code></pre> <p>Tip</p> <p>We have already defined query based upon namespaces before.... You can use the same approach to filter by other labels as well.</p>"},{"location":"15-Prometheus-Grafana/#step-04-customize-the-panel","title":"Step 04: Customize the Panel","text":"<ul> <li>Change the visualization by changing the Graph Style</li> </ul>"},{"location":"15-Prometheus-Grafana/demo/01-stack/","title":"Index","text":"<p>Prometheus + Grafana Demo (Lab 15)</p> <p>Quick demo that installs the <code>kube-prometheus-stack</code> and a Grafana release, provisions a Prometheus datasource and uploads a demo dashboard showing cluster CPU, memory, pod counts and HTTP requests.</p> <p>Usage</p> <ol> <li>Make the script executable:</li> </ol> <pre><code>chmod +x demo.sh\n</code></pre> <ol> <li>Run the demo script (requires <code>kubectl</code> and <code>helm</code>):</li> </ol> <pre><code>./demo.sh\n</code></pre> <ol> <li>Open Grafana on http://localhost:3000 (username: <code>admin</code>, password: <code>admin</code>)</li> </ol> <p>Notes</p> <ul> <li>The script port-forwards the Grafana and Prometheus services to localhost for convenience.</li> <li>The dashboard is uploaded to Grafana via the HTTP API; the script uses a temporary admin password <code>admin</code> for demo ease. Change for production.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/","title":"Node Affinity, Pod Affinity, Anti-Affinity, Taints &amp; Tolerations","text":"<ul> <li>Kubernetes provides a rich set of mechanisms to control where Pods are scheduled in your cluster.</li> <li>In this lab we will deep-dive into every scheduling constraint available: <code>nodeSelector</code>, <code>Node Affinity</code>, <code>Pod Affinity</code>, <code>Pod Anti-Affinity</code>, <code>Taints</code>, <code>Tolerations</code>, and <code>Topology Spread Constraints</code>.</li> <li>We will build real-world examples - from GPU node pools to zone-aware deployments and multi-tenant cluster isolation.</li> <li>By the end of this lab you will have mastered fine-grained Pod placement and be able to design sophisticated scheduling strategies for production clusters.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>Why Pod scheduling constraints exist and when to use each mechanism</li> <li><code>nodeSelector</code> - simple, label-based node filtering</li> <li><code>Node Affinity</code> - expressive node selection with operators, required vs. preferred rules</li> <li><code>Pod Affinity</code> - co-locate Pods with other Pods (same node, same zone)</li> <li><code>Pod Anti-Affinity</code> - spread Pods away from each other</li> <li><code>Taints</code> - repel Pods from Nodes</li> <li><code>Tolerations</code> - allow Pods onto tainted Nodes</li> <li><code>Topology Spread Constraints</code> - balance Pods evenly across topology domains</li> <li>Real-world patterns: GPU pools, zone spreading, multi-tenant isolation, database co-location</li> <li>How to combine all mechanisms for complex production requirements</li> <li>Scheduling internals and the decision pipeline</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link Assign Pods to Nodes kubernetes.io/docs Node Affinity kubernetes.io/docs/affinity Taints and Tolerations kubernetes.io/docs/taints Topology Spread Constraints kubernetes.io/docs/topology kube-scheduler kubernetes.io/docs/kube-scheduler Pod Priority &amp; Preemption kubernetes.io/docs/priority Well-Known Node Labels kubernetes.io/docs/reference/labels"},{"location":"16-Affinity-Taint-Tolleration/#introduction","title":"Introduction","text":""},{"location":"16-Affinity-Taint-Tolleration/#the-kubernetes-scheduling-pipeline","title":"The Kubernetes Scheduling Pipeline","text":"<p>When you create a Pod, the <code>kube-scheduler</code> selects which Node it runs on. The scheduler runs through several phases:</p> <pre><code>flowchart TD\n    A[\"New Pod created\\n(no NodeName)\"] --&gt; B[\"Filtering Phase\\n(Predicates)\"]\n    B --&gt; C{\"Any nodes\\npassed filter?\"}\n    C -- No --&gt; D[\"Pod stays Pending\\nEvent: FailedScheduling\"]\n    C -- Yes --&gt; E[\"Scoring Phase\\n(Priorities)\"]\n    E --&gt; F[\"Highest-score Node\\nselected\"]\n    F --&gt; G[\"Pod bound to Node\\nkubelet starts Pod\"]\n\n    subgraph \"Filtering checks include:\"\n        B1[\"nodeSelector / nodeName\"]\n        B2[\"Node Affinity (required)\"]\n        B3[\"Pod Affinity / Anti-Affinity (required)\"]\n        B4[\"Taints / Tolerations\"]\n        B5[\"Resource availability (CPU, Mem)\"]\n        B6[\"Topology Spread Constraints\"]\n    end\n\n    subgraph \"Scoring factors include:\"\n        E1[\"Node Affinity (preferred) weight\"]\n        E2[\"Pod Affinity (preferred) weight\"]\n        E3[\"Least requested resources\"]\n        E4[\"Image locality\"]\n    end</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#scheduling-mechanism-overview","title":"Scheduling Mechanism Overview","text":"Mechanism Direction Hardness Scope <code>nodeName</code> Pod \u2192 Node Hard Single node <code>nodeSelector</code> Pod \u2192 Node Hard Label match Node Affinity (required) Pod \u2192 Node Hard Operators, multi-label Node Affinity (preferred) Pod \u2192 Node Soft With weights Pod Affinity (required) Pod \u2192 Pod Hard Topology domain Pod Affinity (preferred) Pod \u2192 Pod Soft With weights Pod Anti-Affinity (required) Pod \u2194 Pod Hard Topology domain Pod Anti-Affinity (preferred) Pod \u2194 Pod Soft With weights Taint <code>NoSchedule</code> Node repels Pod Hard New pods excluded Taint <code>NoExecute</code> Node repels Pod Hard Existing pods evicted Taint <code>PreferNoSchedule</code> Node repels Pod Soft Avoid if possible Topology Spread Pod distribution Hard/Soft Arbitrary topology"},{"location":"16-Affinity-Taint-Tolleration/#terminology","title":"Terminology","text":"Term Description Node A physical or virtual machine in the Kubernetes cluster Node Label A key-value pair attached to a Node used for selection Taint A key-value-effect triple on a Node that repels Pods Toleration A key-value-effect triple on a Pod that permits scheduling on a tainted Node Affinity A set of rules the scheduler uses to prefer or require specific placement Anti-Affinity Rules to keep Pods away from specific locations or other Pods topologyKey A node label key that defines the topology domain (e.g., <code>kubernetes.io/hostname</code>, <code>topology.kubernetes.io/zone</code>) Required (hard) <code>requiredDuringSchedulingIgnoredDuringExecution</code> - the Pod won\u2019t schedule if the rule can\u2019t be satisfied Preferred (soft) <code>preferredDuringSchedulingIgnoredDuringExecution</code> - the scheduler tries to honor but will schedule anyway IgnoredDuringExecution Already-running Pods are NOT evicted if rules change after scheduling weight Integer 1\u2013100 given to a preferred rule; used in scoring Topology Spread Constraint Rule that limits how unevenly Pods can be distributed across topology domains maxSkew Maximum difference in Pod count between the most and least loaded topology domain whenUnsatisfiable What to do when spread can\u2019t be satisfied: <code>DoNotSchedule</code> (hard) or <code>ScheduleAnyway</code> (soft)"},{"location":"16-Affinity-Taint-Tolleration/#common-kubectl-commands","title":"Common <code>kubectl</code> Commands","text":"<code>kubectl label</code> - Add, update, remove labels on nodes <p>Syntax: <code>kubectl label nodes &lt;node-name&gt; &lt;key&gt;=&lt;value&gt;</code></p> <p>Description: Labels are key-value pairs attached to Nodes (and any Kubernetes object). They are the foundation of all affinity rules and nodeSelector.</p> <pre><code># List all nodes\nkubectl get nodes\n\n# Show all labels on nodes\nkubectl get nodes --show-labels\n\n# Label a node\nkubectl label nodes node-1 environment=production\n\n# Label with multiple keys at once\nkubectl label nodes node-1 environment=production tier=frontend\n\n# Overwrite an existing label (requires --overwrite)\nkubectl label nodes node-1 environment=staging --overwrite\n\n# Remove a label (append a minus sign)\nkubectl label nodes node-1 environment-\n\n# Label all nodes matching a selector\nkubectl label nodes -l kubernetes.io/role=worker disk-type=ssd\n\n# Show node labels formatted as a table\nkubectl get nodes -o custom-columns=NAME:.metadata.name,LABELS:.metadata.labels\n</code></pre> <code>kubectl taint</code> - Add and remove taints on nodes <p>Syntax: <code>kubectl taint nodes &lt;node-name&gt; &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</code></p> <p>Description: Taints prevent Pods from being scheduled on a Node unless they have a matching toleration.</p> <pre><code># Add a taint with NoSchedule effect\nkubectl taint nodes node-1 dedicated=gpu:NoSchedule\n\n# Add a taint with NoExecute effect (evicts running pods)\nkubectl taint nodes node-1 maintenance=true:NoExecute\n\n# Add a taint with PreferNoSchedule effect (soft)\nkubectl taint nodes node-1 spot-instance=true:PreferNoSchedule\n\n# Remove a taint (append a minus sign after the effect)\nkubectl taint nodes node-1 dedicated=gpu:NoSchedule-\n\n# Remove all taints with a given key regardless of value/effect\nkubectl taint nodes node-1 dedicated-\n\n# Show all taints on all nodes\nkubectl describe nodes | grep -A3 \"Taints:\"\n\n# Taint ALL nodes in the cluster\nkubectl taint nodes --all dedicated=shared:PreferNoSchedule\n</code></pre> <code>kubectl describe node</code> - Inspect node labels, taints, and allocatable resources <pre><code># Full node description\nkubectl describe node node-1\n\n# Show just labels\nkubectl get node node-1 -o jsonpath='{.metadata.labels}' | jq\n\n# Show just taints\nkubectl get node node-1 -o jsonpath='{.spec.taints}'\n\n# Show topology labels (zone / region)\nkubectl get nodes -o custom-columns=\\\nNAME:.metadata.name,\\\nREGION:.metadata.labels.\"topology\\.kubernetes\\.io/region\",\\\nZONE:.metadata.labels.\"topology\\.kubernetes\\.io/zone\"\n\n# Check which node a pod landed on\nkubectl get pods -o wide\n\n# Watch pod scheduling events\nkubectl get events --sort-by='.lastTimestamp' | grep FailedScheduling\n</code></pre> <code>kubectl get</code> - Filter pods and nodes by label <pre><code># List pods on a specific node\nkubectl get pods --field-selector spec.nodeName=node-1\n\n# List pods with nodeSelector labels\nkubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.nodeSelector}{\"\\n\"}{end}'\n\n# List all nodes with a specific label\nkubectl get nodes -l environment=production\n\n# List nodes with topology zone labels\nkubectl get nodes -l topology.kubernetes.io/zone=us-east-1a\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#part-01-nodeselector-simple-node-selection","title":"Part 01 - <code>nodeSelector</code> (Simple Node Selection)","text":"<ul> <li><code>nodeSelector</code> is the simplest way to constrain a Pod to specific Nodes.</li> <li>It is a map of label key-value pairs - the Pod will only be scheduled on Nodes that have all the specified labels.</li> <li>It is less expressive than Node Affinity but easier to read for simple cases.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#step-0101-label-a-node","title":"Step 01.01 - Label a Node","text":"<pre><code># Label a node for SSD storage\nkubectl label nodes node-1 disk-type=ssd\n\n# Label another node for HDD storage\nkubectl label nodes node-2 disk-type=hdd\n\n# Verify\nkubectl get nodes --show-labels | grep disk-type\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0102-schedule-a-pod-with-nodeselector","title":"Step 01.02 - Schedule a Pod with nodeSelector","text":"<pre><code># pod-nodeselector.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ssd-pod\nspec:\n  nodeSelector:\n    disk-type: ssd          # Must match this exact label\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre> <pre><code>kubectl apply -f pod-nodeselector.yaml\nkubectl get pod ssd-pod -o wide     # Verify it landed on a ssd node\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0103-nodeselector-vs-node-affinity","title":"Step 01.03 - <code>nodeSelector</code> vs <code>Node Affinity</code>","text":"Feature <code>nodeSelector</code> <code>Node Affinity</code> Operators <code>=</code> only In, NotIn, Exists, DoesNotExist, Gt, Lt Logic AND (all labels must match) AND within a term, OR between terms Soft preferences No Yes (<code>preferred</code>) Multiple label conditions Yes (all must match) Yes (with full OR/AND logic) <ul> <li>Prefer <code>Node Affinity</code> for all new workloads. Use <code>nodeSelector</code> only for simple backward-compatible cases.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#part-02-node-affinity","title":"Part 02 - Node Affinity","text":"<ul> <li><code>Node Affinity</code> is the next-generation <code>nodeSelector</code>.</li> <li>It lets you express complex label requirements using operators and supports both hard and soft rules.</li> <li>All Node Affinity rules live under <code>spec.affinity.nodeAffinity</code>.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#node-affinity-rule-types","title":"Node Affinity Rule Types","text":"Rule Description <code>requiredDuringSchedulingIgnoredDuringExecution</code> Hard - Pod won\u2019t schedule if rule can\u2019t be met <code>preferredDuringSchedulingIgnoredDuringExecution</code> Soft - scheduler tries to honor, but schedules anyway <p>IgnoredDuringExecution</p> <p>Both rule types have <code>IgnoredDuringExecution</code>. This means if a Node\u2019s labels change after a Pod is scheduled, the Pod is not evicted. A future type <code>requiredDuringSchedulingRequiredDuringExecution</code> is planned but not yet stable.</p>"},{"location":"16-Affinity-Taint-Tolleration/#node-affinity-operators","title":"Node Affinity Operators","text":"Operator Description Example <code>In</code> Label value is in the set <code>environment In [production, staging]</code> <code>NotIn</code> Label value is NOT in the set <code>environment NotIn [development]</code> <code>Exists</code> Label key exists (any value) <code>gpu Exists</code> <code>DoesNotExist</code> Label key does not exist <code>spot-node DoesNotExist</code> <code>Gt</code> Label value numerically &gt; specified <code>storage-gb Gt 100</code> <code>Lt</code> Label value numerically &lt; specified <code>storage-gb Lt 1000</code>"},{"location":"16-Affinity-Taint-Tolleration/#step-0201-required-node-affinity-hard-rule","title":"Step 02.01 - Required Node Affinity (Hard Rule)","text":"<pre><code># Setup - label nodes\nkubectl label nodes node-1 environment=production zone=us-east\nkubectl label nodes node-2 environment=development zone=us-west\n</code></pre> <pre><code># pod-required-affinity.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-pod\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: environment\n            operator: In\n            values:\n            - production\n            - staging                  # Pod can go to production OR staging nodes\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre> <pre><code>kubectl apply -f pod-required-affinity.yaml\nkubectl get pod prod-pod -o wide\nkubectl describe pod prod-pod | grep -E \"Node:|Affinity\"\n</code></pre> <p>Pod stays Pending if no matching Nodes exist</p> <p>If no Node has <code>environment=production</code> or <code>environment=staging</code>, the Pod will remain in <code>Pending</code> state with event <code>FailedScheduling: 0/N nodes are available: N node(s) didn't match Pod's node affinity/selector</code>.</p>"},{"location":"16-Affinity-Taint-Tolleration/#step-0202-preferred-node-affinity-soft-rule-with-weight","title":"Step 02.02 - Preferred Node Affinity (Soft Rule with Weight)","text":"<pre><code># pod-preferred-affinity.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: preferred-pod\nspec:\n  affinity:\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 80                    # Strong preference (out of 100)\n        preference:\n          matchExpressions:\n          - key: environment\n            operator: In\n            values:\n            - production\n      - weight: 20                    # Weak preference\n        preference:\n          matchExpressions:\n          - key: zone\n            operator: In\n            values:\n            - us-east\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre> <ul> <li>The scheduler adds the <code>weight</code> values as bonus score for each Node that matches that preference.</li> <li>A Pod CAN be scheduled on a Node that matches neither preference - the rules are purely advisory.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#step-0203-combining-required-and-preferred-rules","title":"Step 02.03 - Combining Required and Preferred Rules","text":"<pre><code># pod-combined-affinity.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: combined-affinity-pod\nspec:\n  affinity:\n    nodeAffinity:\n      # HARD: MUST be production or staging\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: environment\n            operator: In\n            values:\n            - production\n            - staging\n      # SOFT: prefer us-east zone within those nodes\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        preference:\n          matchExpressions:\n          - key: zone\n            operator: In\n            values:\n            - us-east\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0204-understanding-or-and-and-logic","title":"Step 02.04 - Understanding OR and AND Logic","text":"<p>nodeSelectorTerms - OR logic; matchExpressions - AND logic</p> <ul> <li>Multiple entries in <code>nodeSelectorTerms</code> are combined with OR - the Pod can match ANY of them.</li> <li>Multiple entries in a single <code>matchExpressions</code> list are combined with AND - ALL must be satisfied.</li> </ul> <pre><code># OR logic example: pod can go to either SSD nodes OR GPU nodes\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:           # First term (SSD)\n          - key: disk-type\n            operator: In\n            values: [ssd]\n        - matchExpressions:           # Second term (GPU) - OR with first\n          - key: hardware\n            operator: In\n            values: [gpu]\n</code></pre> <pre><code># AND logic example: pod must be on SSD AND production nodes\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:           # Both conditions in the same term = AND\n          - key: disk-type\n            operator: In\n            values: [ssd]\n          - key: environment\n            operator: In\n            values: [production]\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0205-notin-operator-exclude-nodes","title":"Step 02.05 - <code>NotIn</code> Operator (Exclude Nodes)","text":"<pre><code># Avoid scheduling on spot/preemptible instances for critical workloads\napiVersion: v1\nkind: Pod\nmetadata:\n  name: critical-pod\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: node-lifecycle\n            operator: NotIn\n            values:\n            - spot\n            - preemptible\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0206-exists-and-doesnotexist-operators","title":"Step 02.06 - <code>Exists</code> and <code>DoesNotExist</code> Operators","text":"<pre><code># Must run on a node that has ANY value for the 'gpu' label\napiVersion: v1\nkind: Pod\nmetadata:\n  name: any-gpu-pod\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: gpu\n            operator: Exists         # Any value for 'gpu' label is acceptable\n  containers:\n  - name: app\n    image: nvidia/cuda:12.0-base\n</code></pre> <pre><code># Must run on a node that has NO 'special-hardware' label at all\napiVersion: v1\nkind: Pod\nmetadata:\n  name: standard-pod\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: special-hardware\n            operator: DoesNotExist\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0207-gt-and-lt-operators-numeric-comparison","title":"Step 02.07 - <code>Gt</code> and <code>Lt</code> Operators (Numeric Comparison)","text":"<pre><code># Label nodes with numeric values\nkubectl label nodes node-1 memory-gb=16\nkubectl label nodes node-2 memory-gb=64\nkubectl label nodes node-3 memory-gb=128\n</code></pre> <pre><code># Schedule only on nodes with memory-gb &gt; 32\napiVersion: v1\nkind: Pod\nmetadata:\n  name: high-memory-pod\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: memory-gb\n            operator: Gt\n            values:\n            - \"32\"                   # Value must be a string, comparison is numeric\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre> <pre><code># Schedule on nodes with memory-gb between 32 and 256 (exclusive)\napiVersion: v1\nkind: Pod\nmetadata:\n  name: medium-memory-pod\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: memory-gb\n            operator: Gt\n            values: [\"32\"]\n          - key: memory-gb\n            operator: Lt\n            values: [\"256\"]\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0208-node-affinity-in-a-deployment","title":"Step 02.08 - Node Affinity in a Deployment","text":"<pre><code># deployment-node-affinity.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web-app\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: environment\n                operator: In\n                values: [production]\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            preference:\n              matchExpressions:\n              - key: zone\n                operator: In\n                values: [us-east-1a]\n          - weight: 50\n            preference:\n              matchExpressions:\n              - key: zone\n                operator: In\n                values: [us-east-1b]\n      containers:\n      - name: web\n        image: nginx:1.25\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#part-03-pod-affinity-co-locate-pods","title":"Part 03 - Pod Affinity (Co-locate Pods)","text":"<ul> <li><code>Pod Affinity</code> allows you to influence the scheduler to place Pods near other Pods.</li> <li>This is useful when Pods benefit from being on the same Node or in the same topology domain (e.g., same availability zone) to reduce network latency.</li> <li>Pod Affinity rules live under <code>spec.affinity.podAffinity</code>.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#how-topologykey-works","title":"How topologyKey Works","text":"<pre><code>flowchart LR\n    subgraph \"Zone us-east-1a\"\n        subgraph \"node-1\"\n            P1[\"frontend-abc\\napp=frontend\"]\n            P2[\"cache-xyz (new)\"]\n        end\n        subgraph \"node-2\"\n            P3[\"frontend-def\\napp=frontend\"]\n            P4[\"cache-uvw (new)\"]\n        end\n    end\n    subgraph \"Zone us-east-1b\"\n        subgraph \"node-3\"\n            P5[\"backend-pod\"]\n        end\n    end\n\n    note[\"topologyKey: kubernetes.io/hostname\\n\u2192 Co-locate on same NODE as matching Pod\\n\\ntopologyKey: topology.kubernetes.io/zone\\n\u2192 Co-locate in same ZONE as matching Pod\"]</code></pre> <p>Common <code>topologyKey</code> values</p> <ul> <li><code>kubernetes.io/hostname</code> - same physical/virtual Node</li> <li><code>topology.kubernetes.io/zone</code> - same availability zone</li> <li><code>topology.kubernetes.io/region</code> - same cloud region</li> <li>Any custom label key on your Nodes</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#step-0301-required-pod-affinity-co-locate-on-same-node","title":"Step 03.01 - Required Pod Affinity (Co-locate on Same Node)","text":"<pre><code># two-pods-same-node.yaml\n# First: deploy the \"anchor\" pod that others want to co-locate with\napiVersion: v1\nkind: Pod\nmetadata:\n  name: cache-pod\n  labels:\n    app: cache\n    tier: caching\nspec:\n  containers:\n  - name: redis\n    image: redis:7\n    ports:\n    - containerPort: 6379\n---\n# Second: deploy the app that MUST be on the same Node as a cache pod\napiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchExpressions:\n          - key: app\n            operator: In\n            values:\n            - cache\n        topologyKey: kubernetes.io/hostname    # Same node as a cache pod\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre> <pre><code>kubectl apply -f two-pods-same-node.yaml\nkubectl get pods -o wide     # Both should be on the same node\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0302-required-pod-affinity-co-locate-in-same-zone","title":"Step 03.02 - Required Pod Affinity (Co-locate in Same Zone)","text":"<pre><code># frontend-with-zone-affinity.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: frontend-pod\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchLabels:\n            app: backend          # Must be in same zone as a backend pod\n        topologyKey: topology.kubernetes.io/zone\n  containers:\n  - name: frontend\n    image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0303-preferred-pod-affinity","title":"Step 03.03 - Preferred Pod Affinity","text":"<pre><code># pod-preferred-pod-affinity.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: app-prefer-near-cache\nspec:\n  affinity:\n    podAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchExpressions:\n            - key: app\n              operator: In\n              values:\n              - cache\n          topologyKey: kubernetes.io/hostname   # Prefer same node, but not required\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0304-pod-affinity-in-a-deployment-sidecar-co-location-pattern","title":"Step 03.04 - Pod Affinity in a Deployment (Sidecar Co-location Pattern)","text":"<pre><code># sidecar-affinity-deployment.yaml\n# This deployment ensures each app pod is on the same node as a log-agent pod\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-with-sidecar-affinity\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      affinity:\n        podAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: log-agent\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: app\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0305-scoping-pod-affinity-to-specific-namespaces","title":"Step 03.05 - Scoping Pod Affinity to Specific Namespaces","text":"<pre><code># Pod affinity targeting pods in a specific namespace\napiVersion: v1\nkind: Pod\nmetadata:\n  name: cross-namespace-affinity-pod\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchLabels:\n            app: database\n        topologyKey: kubernetes.io/hostname\n        namespaces:             # Look for matching pods in these namespaces\n        - data-plane\n        - production\n        # If 'namespaces' is omitted, only the Pod's own namespace is searched\n        # If 'namespaces' is empty [], all namespaces are searched\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#part-04-pod-anti-affinity-spread-pods-apart","title":"Part 04 - Pod Anti-Affinity (Spread Pods Apart)","text":"<ul> <li><code>Pod Anti-Affinity</code> is the opposite of <code>Pod Affinity</code> - it ensures Pods are placed away from other Pods.</li> <li>Its primary use is high availability: spreading replicas across Nodes, zones, or regions so a single failure doesn\u2019t take down your entire service.</li> <li>Anti-Affinity rules live under <code>spec.affinity.podAntiAffinity</code>.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#step-0401-required-anti-affinity-one-pod-per-node","title":"Step 04.01 - Required Anti-Affinity (One Pod per Node)","text":"<pre><code># deployment-one-per-node.yaml\n# This deployment ensures no two replicas end up on the same node\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ha-nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ha-nginx\n  template:\n    metadata:\n      labels:\n        app: ha-nginx\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - ha-nginx             # Avoid nodes that already have this app\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: nginx\n        image: nginx:1.25\n        ports:\n        - containerPort: 80\n</code></pre> <pre><code>kubectl apply -f deployment-one-per-node.yaml\nkubectl get pods -o wide     # Each pod should land on a different node\n</code></pre> <p>Required Anti-Affinity can leave Pods Pending</p> <p>If you have <code>replicas: 5</code> but only 3 Nodes, 2 Pods will stay Pending because no Node is available that doesn\u2019t already have a matching Pod. Use <code>preferredDuringScheduling</code> if this is a concern.</p>"},{"location":"16-Affinity-Taint-Tolleration/#step-0402-preferred-anti-affinity-prefer-different-nodes","title":"Step 04.02 - Preferred Anti-Affinity (Prefer Different Nodes)","text":"<pre><code># deployment-prefer-spread.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-spread\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: web-spread\n  template:\n    metadata:\n      labels:\n        app: web-spread\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: web-spread\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: web\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0403-zone-level-anti-affinity-ha-across-zones","title":"Step 04.03 - Zone-Level Anti-Affinity (HA Across Zones)","text":"<pre><code># First, verify your nodes have zone labels\nkubectl get nodes -o custom-columns=\\\nNAME:.metadata.name,\\\nZONE:.metadata.labels.\"topology\\.kubernetes\\.io/zone\"\n</code></pre> <pre><code># deployment-zone-ha.yaml\n# Ensures each replica lands in a different availability zone\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zone-ha-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: zone-ha-app\n  template:\n    metadata:\n      labels:\n        app: zone-ha-app\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: zone-ha-app\n            topologyKey: topology.kubernetes.io/zone   # One per zone\n      containers:\n      - name: app\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0404-combining-pod-affinity-and-anti-affinity","title":"Step 04.04 - Combining Pod Affinity and Anti-Affinity","text":"<pre><code># This pod:\n# - MUST be in the same zone as at least one 'backend' pod (affinity)\n# - MUST NOT be on the same node as another 'frontend' pod (anti-affinity)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: backend\n            topologyKey: topology.kubernetes.io/zone\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: frontend\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: frontend\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#part-05-taints-and-tolerations","title":"Part 05 - Taints and Tolerations","text":"<ul> <li><code>Taints</code> are applied to Nodes and repel Pods.</li> <li><code>Tolerations</code> are applied to Pods and allow them to be scheduled onto tainted Nodes.</li> <li>Together they implement a whitelist model: a tainted Node only accepts Pods that explicitly opt-in via tolerations.</li> <li>This is the opposite of Node Affinity (which is a pull mechanism from the Pod side).</li> </ul> <pre><code>flowchart LR\n    subgraph \"Tainted Node\"\n        N[\"node-1\\nTaint: dedicated=gpu:NoSchedule\"]\n    end\n    P1[\"Pod A\\n(no toleration)\"] --&gt; X[\"\u274c Cannot schedule\\non node-1\"]\n    P2[\"Pod B\\ntoleration: dedicated=gpu:NoSchedule\"] --&gt; N\n    X -.-&gt; OtherNode[\"Scheduled on\\nother node\"]</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#taint-effects","title":"Taint Effects","text":"Effect New Pods Existing Pods <code>NoSchedule</code> Blocked (hard) Not affected <code>PreferNoSchedule</code> Avoided (soft) Not affected <code>NoExecute</code> Blocked (hard) Evicted if no matching toleration"},{"location":"16-Affinity-Taint-Tolleration/#step-0501-apply-and-remove-taints","title":"Step 05.01 - Apply and Remove Taints","text":"<pre><code># Add NoSchedule taint\nkubectl taint nodes node-1 dedicated=gpu:NoSchedule\n\n# Add NoExecute taint (evicts non-tolerating existing pods immediately)\nkubectl taint nodes node-2 maintenance=true:NoExecute\n\n# Add PreferNoSchedule taint (soft - pods go elsewhere if possible)\nkubectl taint nodes node-3 spot-instance=true:PreferNoSchedule\n\n# View taints on all nodes\nkubectl describe nodes | grep -A2 \"Taints:\"\n\n# Remove a specific taint\nkubectl taint nodes node-1 dedicated=gpu:NoSchedule-\n\n# Remove all taints with a key (any value or effect)\nkubectl taint nodes node-1 dedicated-\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0502-pod-without-toleration","title":"Step 05.02 - Pod Without Toleration","text":"<pre><code># pod-no-toleration.yaml\n# This pod cannot be scheduled on node-1 which has dedicated=gpu:NoSchedule\napiVersion: v1\nkind: Pod\nmetadata:\n  name: regular-pod\nspec:\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre> <pre><code>kubectl apply -f pod-no-toleration.yaml\n# If ALL nodes are tainted, the pod stays Pending\nkubectl describe pod regular-pod | grep -A5 \"Events:\"\n# Output: 0/N nodes are available: N node(s) had untolerated taint {dedicated: gpu}\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0503-pod-with-equal-toleration","title":"Step 05.03 - Pod With <code>Equal</code> Toleration","text":"<pre><code># pod-equal-toleration.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-app\nspec:\n  tolerations:\n  - key: \"dedicated\"\n    operator: \"Equal\"         # Must match both key AND value\n    value: \"gpu\"\n    effect: \"NoSchedule\"\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0504-pod-with-exists-toleration","title":"Step 05.04 - Pod With <code>Exists</code> Toleration","text":"<pre><code># pod-exists-toleration.yaml\n# Tolerates ANY taint with key 'dedicated', regardless of value\napiVersion: v1\nkind: Pod\nmetadata:\n  name: flexible-pod\nspec:\n  tolerations:\n  - key: \"dedicated\"\n    operator: \"Exists\"        # Matches any value for this key\n    effect: \"NoSchedule\"\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0505-tolerate-all-taints-wildcard","title":"Step 05.05 - Tolerate All Taints (Wildcard)","text":"<pre><code># This pod tolerates ALL taints on ALL nodes\n# WARNING: Only use for cluster-system pods (DaemonSets, CNI, etc.)\napiVersion: v1\nkind: Pod\nmetadata:\n  name: omnipotent-pod\nspec:\n  tolerations:\n  - operator: \"Exists\"        # No key, no effect = match anything\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre> <p>Tolerate-All is dangerous</p> <p>Toleration <code>operator: Exists</code> with no <code>key</code> matches ALL taints. This is used by DaemonSets that must run everywhere (kube-proxy, CNI plugins, log agents). Never use it in application workloads.</p>"},{"location":"16-Affinity-Taint-Tolleration/#step-0506-noexecute-effect-and-tolerationseconds","title":"Step 05.06 - <code>NoExecute</code> Effect and <code>tolerationSeconds</code>","text":"<pre><code># pod-maintenance-toleration.yaml\n# This pod has 10 minutes to finish before being evicted during maintenance\napiVersion: v1\nkind: Pod\nmetadata:\n  name: long-running-job\nspec:\n  tolerations:\n  - key: \"maintenance\"\n    operator: \"Equal\"\n    value: \"true\"\n    effect: \"NoExecute\"\n    tolerationSeconds: 600    # Stay up to 10 minutes after the taint is applied\n  containers:\n  - name: job\n    image: busybox\n    command: [\"sleep\", \"infinity\"]\n</code></pre> <pre><code># Simulate Node maintenance - taint the node\nkubectl taint nodes node-1 maintenance=true:NoExecute\n\n# The pod will continue running for up to 600 seconds, then be evicted\nkubectl get pods -o wide --watch\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0507-multiple-tolerations-on-a-single-pod","title":"Step 05.07 - Multiple Tolerations on a Single Pod","text":"<pre><code># pod-multi-toleration.yaml\n# This pod can run on nodes with multiple specific taints\napiVersion: v1\nkind: Pod\nmetadata:\n  name: multi-tolerant-pod\nspec:\n  tolerations:\n  - key: \"dedicated\"\n    operator: \"Equal\"\n    value: \"gpu\"\n    effect: \"NoSchedule\"\n  - key: \"nvidia.com/gpu\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n  - key: \"maintenance\"\n    operator: \"Equal\"\n    value: \"true\"\n    effect: \"NoExecute\"\n    tolerationSeconds: 60\n  containers:\n  - name: gpu-app\n    image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0508-tolerating-multiple-effects-for-the-same-key","title":"Step 05.08 - Tolerating Multiple Effects for the Same Key","text":"<pre><code># Tolerate both NoSchedule and NoExecute for the same key\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dual-effect-pod\nspec:\n  tolerations:\n  - key: \"node-role\"\n    operator: \"Equal\"\n    value: \"edge\"\n    effect: \"NoSchedule\"\n  - key: \"node-role\"\n    operator: \"Equal\"\n    value: \"edge\"\n    effect: \"NoExecute\"\n  # Shortcut: omit 'effect' to match ALL effects for this key/value pair\n  # - key: \"node-role\"\n  #   operator: \"Equal\"\n  #   value: \"edge\"\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0509-built-in-kubernetes-taints","title":"Step 05.09 - Built-in Kubernetes Taints","text":"<p>Kubernetes automatically adds these taints to Nodes in various states:</p> Taint Effect Added when <code>node.kubernetes.io/not-ready</code> <code>NoExecute</code> Node <code>Ready</code> condition is False <code>node.kubernetes.io/unreachable</code> <code>NoExecute</code> Node <code>Ready</code> condition is Unknown <code>node.kubernetes.io/memory-pressure</code> <code>NoSchedule</code> Node has memory pressure <code>node.kubernetes.io/disk-pressure</code> <code>NoSchedule</code> Node has disk pressure <code>node.kubernetes.io/pid-pressure</code> <code>NoSchedule</code> Node has PID pressure <code>node.kubernetes.io/unschedulable</code> <code>NoSchedule</code> Node is cordoned <code>node.kubernetes.io/network-unavailable</code> <code>NoSchedule</code> Node network not configured <code>node.cloudprovider.kubernetes.io/uninitialized</code> <code>NoSchedule</code> Cloud provider not finished initializing <pre><code># Application pods should tolerate node.kubernetes.io/not-ready briefly\n# to avoid unnecessary rescheduling during transient node issues\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: resilient-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: resilient-app\n  template:\n    metadata:\n      labels:\n        app: resilient-app\n    spec:\n      tolerations:\n      # Kubernetes default tolerations - pods wait 5 minutes before rescheduling\n      - key: \"node.kubernetes.io/not-ready\"\n        operator: \"Exists\"\n        effect: \"NoExecute\"\n        tolerationSeconds: 300\n      - key: \"node.kubernetes.io/unreachable\"\n        operator: \"Exists\"\n        effect: \"NoExecute\"\n        tolerationSeconds: 300\n      containers:\n      - name: app\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0510-taint-based-node-isolation-for-control-plane","title":"Step 05.10 - Taint-based Node Isolation for Control Plane","text":"<pre><code># Control-plane nodes are automatically tainted:\nkubectl describe node &lt;control-plane&gt; | grep Taints\n# Taints: node-role.kubernetes.io/control-plane:NoSchedule\n\n# Allow a pod to run on control-plane nodes (e.g., for monitoring)\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: control-plane-monitor\nspec:\n  tolerations:\n  - key: \"node-role.kubernetes.io/control-plane\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n  nodeSelector:\n    node-role.kubernetes.io/control-plane: \"\"\n  containers:\n  - name: monitor\n    image: busybox\n    command: [\"sleep\", \"infinity\"]\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#part-06-combining-node-affinity-taints-and-tolerations","title":"Part 06 - Combining Node Affinity, Taints, and Tolerations","text":"<ul> <li>Using <code>Node Affinity</code> alone means Pods from other teams can also go to those nodes.</li> <li>Using <code>Taints</code> alone ensures no uninvited Pods land there, but your own Pods still won\u2019t be attracted to the nodes.</li> <li>The pattern is: Taint the node (repel others) + Use Affinity (attract your pods).</li> </ul> <pre><code>flowchart TD\n    subgraph \"Dedicated GPU Node Pool\"\n        N[\"gpu-node-1\\nTaint: dedicated=gpu:NoSchedule\\nLabel: hardware=gpu\"]\n    end\n    P1[\"Random Pod (no toleration)\"] --&gt; X[\"\u274c Blocked by Taint\"]\n    P2[\"GPU Workload\\n(toleration + node affinity)\"] --&gt; N\n    style N fill:#2d6a2d,color:#fff</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0601-dedicated-node-pool-taint-node-affinity","title":"Step 06.01 - Dedicated Node Pool (Taint + Node Affinity)","text":"<pre><code># Setup dedicated GPU nodes\nkubectl label nodes gpu-node-1 hardware=gpu accelerator=nvidia\nkubectl label nodes gpu-node-2 hardware=gpu accelerator=nvidia\n\n# Taint them to repel non-GPU workloads\nkubectl taint nodes gpu-node-1 dedicated=gpu:NoSchedule\nkubectl taint nodes gpu-node-2 dedicated=gpu:NoSchedule\n</code></pre> <pre><code># gpu-workload.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ml-training-job\nspec:\n  tolerations:\n  - key: \"dedicated\"\n    operator: \"Equal\"\n    value: \"gpu\"\n    effect: \"NoSchedule\"\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: hardware\n            operator: In\n            values: [gpu]\n          - key: accelerator\n            operator: In\n            values: [nvidia]\n  containers:\n  - name: train\n    image: nvidia/cuda:12.0-base\n    resources:\n      limits:\n        nvidia.com/gpu: \"1\"\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0602-multi-tenant-cluster-team-isolation","title":"Step 06.02 - Multi-Tenant Cluster Team Isolation","text":"<pre><code># Allocate nodes to team-alpha\nkubectl label nodes node-1 node-2 team=alpha\nkubectl taint nodes node-1 node-2 team=alpha:NoSchedule\n\n# Allocate nodes to team-beta\nkubectl label nodes node-3 node-4 team=beta\nkubectl taint nodes node-3 node-4 team=beta:NoSchedule\n</code></pre> <pre><code># team-alpha deployment - only lands on alpha nodes\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: alpha-service\n  namespace: team-alpha\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: alpha-service\n  template:\n    metadata:\n      labels:\n        app: alpha-service\n    spec:\n      tolerations:\n      - key: \"team\"\n        operator: \"Equal\"\n        value: \"alpha\"\n        effect: \"NoSchedule\"\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: team\n                operator: In\n                values: [alpha]\n      containers:\n      - name: service\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0603-spot-instance-nodes-with-graceful-fallback","title":"Step 06.03 - Spot Instance Nodes with Graceful Fallback","text":"<pre><code># Label and taint spot instances\nkubectl label nodes spot-1 spot-2 node-lifecycle=spot\nkubectl taint nodes spot-1 spot-2 spot-instance=true:PreferNoSchedule\n</code></pre> <pre><code># batch-job.yaml - prefer spot, but fall back to on-demand\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: batch-data-processor\nspec:\n  template:\n    spec:\n      tolerations:\n      - key: \"spot-instance\"\n        operator: \"Equal\"\n        value: \"true\"\n        effect: \"PreferNoSchedule\"\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            preference:\n              matchExpressions:\n              - key: node-lifecycle\n                operator: In\n                values: [spot]\n      restartPolicy: OnFailure\n      containers:\n      - name: processor\n        image: busybox\n        command: [\"sh\", \"-c\", \"echo processing &amp;&amp; sleep 60\"]\n</code></pre> <pre><code># critical-service.yaml - explicitly AVOID spot instances\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: critical-api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: critical-api\n  template:\n    metadata:\n      labels:\n        app: critical-api\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-lifecycle\n                operator: NotIn\n                values: [spot, preemptible]\n      containers:\n      - name: api\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#part-07-topology-spread-constraints","title":"Part 07 - Topology Spread Constraints","text":"<ul> <li><code>Topology Spread Constraints</code> give you explicit control over how Pods are distributed across topology domains.</li> <li>Unlike Anti-Affinity (which blocks placement), Topology Spread Constraints let you specify the maximum imbalance allowed.</li> <li>They are more predictable and flexible than Anti-Affinity for distribution scenarios.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#key-fields","title":"Key Fields","text":"Field Description <code>maxSkew</code> Maximum difference in Pod count between the most and least loaded domains. <code>maxSkew: 1</code> means no domain can have more than 1 extra Pod <code>topologyKey</code> The node label that defines the topology domains (e.g., zone, hostname) <code>whenUnsatisfiable</code> <code>DoNotSchedule</code> (hard) or <code>ScheduleAnyway</code> (soft - best effort) <code>labelSelector</code> Which Pods to count when computing skew <code>minDomains</code> Minimum number of topology domains that must be available (requires at least this many domains to exist) <code>nodeAffinityPolicy</code> Whether to honor <code>nodeAffinity</code>/<code>nodeSelector</code> when counting pods: <code>Honor</code> (default) or <code>Ignore</code> <code>nodeTaintsPolicy</code> Whether to honor taints when counting: <code>Honor</code> or <code>Ignore</code> (default)"},{"location":"16-Affinity-Taint-Tolleration/#step-0701-basic-zone-spreading","title":"Step 07.01 - Basic Zone Spreading","text":"<pre><code># Verify zone labels on nodes\nkubectl get nodes -L topology.kubernetes.io/zone\n</code></pre> <pre><code># deployment-zone-spread.yaml\n# Spread pods evenly across zones, allowing max 1 extra pod in any zone\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zone-spread-app\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: zone-spread-app\n  template:\n    metadata:\n      labels:\n        app: zone-spread-app\n    spec:\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: topology.kubernetes.io/zone\n        whenUnsatisfiable: DoNotSchedule        # Hard constraint\n        labelSelector:\n          matchLabels:\n            app: zone-spread-app\n      containers:\n      - name: app\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0702-node-level-spreading","title":"Step 07.02 - Node-Level Spreading","text":"<pre><code># deployment-node-spread.yaml\n# Spread pods evenly across nodes (max 1 extra pod per node)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: node-spread-app\nspec:\n  replicas: 9\n  selector:\n    matchLabels:\n      app: node-spread-app\n  template:\n    metadata:\n      labels:\n        app: node-spread-app\n    spec:\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: kubernetes.io/hostname     # Each node is a domain\n        whenUnsatisfiable: ScheduleAnyway       # Soft - still schedules if can't spread\n        labelSelector:\n          matchLabels:\n            app: node-spread-app\n      containers:\n      - name: app\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0703-multi-level-constraints-zone-and-node","title":"Step 07.03 - Multi-Level Constraints (Zone AND Node)","text":"<pre><code># deployment-multi-spread.yaml\n# Spread evenly across zones first, then evenly across nodes within zones\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: multi-level-spread\nspec:\n  replicas: 12\n  selector:\n    matchLabels:\n      app: multi-level-spread\n  template:\n    metadata:\n      labels:\n        app: multi-level-spread\n    spec:\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: topology.kubernetes.io/zone      # Zone-level spread\n        whenUnsatisfiable: DoNotSchedule\n        labelSelector:\n          matchLabels:\n            app: multi-level-spread\n      - maxSkew: 1\n        topologyKey: kubernetes.io/hostname           # Node-level spread\n        whenUnsatisfiable: ScheduleAnyway             # Best effort for nodes\n        labelSelector:\n          matchLabels:\n            app: multi-level-spread\n      containers:\n      - name: app\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0704-mindomains-ensure-minimum-domain-count","title":"Step 07.04 - <code>minDomains</code> (Ensure Minimum Domain Count)","text":"<pre><code># deployment-min-domains.yaml\n# Only schedule if at least 3 zones are available\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: min-domain-app\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: min-domain-app\n  template:\n    metadata:\n      labels:\n        app: min-domain-app\n    spec:\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: topology.kubernetes.io/zone\n        whenUnsatisfiable: DoNotSchedule\n        minDomains: 3                           # Must have at least 3 zones available\n        labelSelector:\n          matchLabels:\n            app: min-domain-app\n      containers:\n      - name: app\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0705-topology-spread-constraints-vs-anti-affinity","title":"Step 07.05 - Topology Spread Constraints vs Anti-Affinity","text":"Aspect Anti-Affinity (required) Topology Spread Constraint Semantics One pod per domain (binary) Balance up to maxSkew Flexibility Rigid - pod Pending if &gt;1 per domain Flexible - maxSkew allows limited stacking Multiple replicas per domain Not possible (required) Yes, balanced by maxSkew Partial failure handling Pod stays pending <code>ScheduleAnyway</code> for soft behavior Multiple topology levels Requires chaining Native multi-constraint support"},{"location":"16-Affinity-Taint-Tolleration/#part-08-real-world-scenarios","title":"Part 08 - Real-World Scenarios","text":""},{"location":"16-Affinity-Taint-Tolleration/#scenario-0801-database-stateful-service-co-location","title":"Scenario 08.01 - Database + Stateful Service Co-Location","text":"<pre><code># Deploy a Redis cache that MUST be co-located with application pods\n# Application pods get ~1ms latency to Redis when on the same node\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: redis-local\nspec:\n  selector:\n    matchLabels:\n      app: redis-local\n  template:\n    metadata:\n      labels:\n        app: redis-local\n        tier: cache\n    spec:\n      tolerations:\n      - operator: \"Exists\"         # Run on all nodes including tainted ones\n      containers:\n      - name: redis\n        image: redis:7-alpine\n        ports:\n        - containerPort: 6379\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: web-app\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: redis-local     # MUST be on same node as Redis\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: web-app       # Prefer different nodes for web app replicas\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: web\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#scenario-0802-high-availability-web-service-3-zones","title":"Scenario 08.02 - High-Availability Web Service (3 Zones)","text":"<pre><code># ha-web-service.yaml\n# Web: must spread across 3 zones, one replica per zone minimum\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ha-web\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ha-web\n  template:\n    metadata:\n      labels:\n        app: ha-web\n        tier: web\n    spec:\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: topology.kubernetes.io/zone\n        whenUnsatisfiable: DoNotSchedule\n        labelSelector:\n          matchLabels:\n            app: ha-web\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-type\n                operator: In\n                values: [web, general]\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  tier: web\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: web\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#scenario-0803-node-maintenance-drain-workflow","title":"Scenario 08.03 - Node Maintenance (Drain Workflow)","text":"<pre><code># 1. Cordon the node (adds node.kubernetes.io/unschedulable taint)\nkubectl cordon node-1\n\n# 2. Drain the node (evicts all pods gracefully)\nkubectl drain node-1 --ignore-daemonsets --delete-emptydir-data --grace-period=60\n\n# 3. Perform maintenance...\n\n# 4. Uncordon the node to allow scheduling again\nkubectl uncordon node-1\n\n# Check scheduling status\nkubectl get nodes\n# STATUS: Ready (not SchedulingDisabled) = uncordoned\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#scenario-0804-burstable-workloads-on-spot-instances","title":"Scenario 08.04 - Burstable Workloads on Spot Instances","text":"<pre><code># production-baseline.yaml - always on on-demand nodes\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-baseline\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: api\n      tier: baseline\n  template:\n    metadata:\n      labels:\n        app: api\n        tier: baseline\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-lifecycle\n                operator: In\n                values: [on-demand]\n      containers:\n      - name: api\n        image: nginx:1.25\n---\n# burst-capacity.yaml - scale-out pods go to cheaper spot instances\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-burst\nspec:\n  replicas: 0             # HPA will scale this up during bursts\n  selector:\n    matchLabels:\n      app: api\n      tier: burst\n  template:\n    metadata:\n      labels:\n        app: api\n        tier: burst\n    spec:\n      tolerations:\n      - key: \"spot-instance\"\n        operator: \"Exists\"\n        effect: \"NoSchedule\"\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            preference:\n              matchExpressions:\n              - key: node-lifecycle\n                operator: In\n                values: [spot]\n      containers:\n      - name: api\n        image: nginx:1.25\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#scenario-0805-dedicated-nodes-for-monitoring-stack","title":"Scenario 08.05 - Dedicated Nodes for Monitoring Stack","text":"<pre><code># Create a dedicated monitoring node pool\nkubectl label nodes monitoring-1 monitoring-2 role=monitoring\nkubectl taint nodes monitoring-1 monitoring-2 role=monitoring:NoSchedule\n</code></pre> <pre><code># prometheus-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n    spec:\n      tolerations:\n      - key: \"role\"\n        operator: \"Equal\"\n        value: \"monitoring\"\n        effect: \"NoSchedule\"\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: role\n                operator: In\n                values: [monitoring]\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: prometheus\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: prometheus\n        image: prom/prometheus:latest\n        ports:\n        - containerPort: 9090\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#part-09-daemonsets-and-system-pods","title":"Part 09 - DaemonSets and System Pods","text":"<ul> <li><code>DaemonSets</code> run one Pod per Node.</li> <li>System DaemonSets (CNI, kube-proxy, log agents) need to run even on tainted Nodes.</li> <li>They use the wildcard toleration or specific tolerations for known system taints.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#step-0901-daemonset-on-all-nodes-including-tainted","title":"Step 09.01 - DaemonSet on All Nodes Including Tainted","text":"<pre><code># node-log-agent.yaml\n# Log agent that runs on ALL nodes, including control-plane and tainted nodes\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: log-agent\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: log-agent\n  template:\n    metadata:\n      labels:\n        app: log-agent\n    spec:\n      tolerations:\n      # Standard system node taints\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n        effect: NoSchedule\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n        effect: NoSchedule\n      # Node lifecycle taints\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n        effect: NoExecute\n      - key: node.kubernetes.io/unreachable\n        operator: Exists\n        effect: NoExecute\n      - key: node.kubernetes.io/disk-pressure\n        operator: Exists\n        effect: NoSchedule\n      - key: node.kubernetes.io/memory-pressure\n        operator: Exists\n        effect: NoSchedule\n      # Custom workload taints - agents still need to run on GPU/special nodes\n      - key: dedicated\n        operator: Exists\n        effect: NoSchedule\n      containers:\n      - name: log-agent\n        image: fluent/fluent-bit:latest\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-0902-daemonset-on-a-subset-of-nodes-node-affinity","title":"Step 09.02 - DaemonSet on a Subset of Nodes (Node Affinity)","text":"<pre><code># gpu-node-daemonset.yaml\n# Driver installer that runs only on GPU nodes\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidia-driver-installer\n  namespace: gpu-system\nspec:\n  selector:\n    matchLabels:\n      app: nvidia-driver-installer\n  template:\n    metadata:\n      labels:\n        app: nvidia-driver-installer\n    spec:\n      tolerations:\n      - key: \"dedicated\"\n        operator: \"Equal\"\n        value: \"gpu\"\n        effect: \"NoSchedule\"\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: hardware\n                operator: In\n                values: [gpu]\n      containers:\n      - name: installer\n        image: nvidia/gpu-operator:latest\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#part-10-observability-and-debugging-scheduling-issues","title":"Part 10 - Observability and Debugging Scheduling Issues","text":""},{"location":"16-Affinity-Taint-Tolleration/#step-1001-check-why-a-pod-is-pending","title":"Step 10.01 - Check Why a Pod is Pending","text":"<pre><code># Describe the pod for scheduling events\nkubectl describe pod &lt;pod-name&gt;\n# Look for the \"Events\" section - FailedScheduling explains why\n\n# Common messages:\n# \"0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector\"\n# \"0/3 nodes are available: 3 node(s) had untolerated taint {dedicated: gpu}\"\n# \"0/3 nodes are available: 3 node(s) didn't match pod anti-affinity rules\"\n# \"0/3 nodes are available: 1 Insufficient cpu, 2 Insufficient memory\"\n</code></pre> <pre><code># Get all scheduler events\nkubectl get events -n &lt;namespace&gt; --field-selector reason=FailedScheduling\n\n# Sort by time\nkubectl get events --sort-by='.lastTimestamp' | grep Failed\n\n# Watch scheduling events in real time\nkubectl get events -w | grep -E \"FailedScheduling|Scheduled\"\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-1002-verify-node-labels-match-affinity-rules","title":"Step 10.02 - Verify Node Labels Match Affinity Rules","text":"<pre><code># Check if a node has the required label\nkubectl get node node-1 -o jsonpath='{.metadata.labels}' | jq\n\n# Check with a specific label key\nkubectl get nodes -l environment=production\n\n# Show which nodes match a specific label selector\nkubectl get nodes --selector='environment in (production,staging)'\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-1003-verify-taints-on-nodes","title":"Step 10.03 - Verify Taints on Nodes","text":"<pre><code># Show taints on all nodes in a table\nkubectl get nodes -o custom-columns=\\\nNAME:.metadata.name,\\\nTAINTS:.spec.taints\n\n# Check if a pod's tolerations match node taints\nkubectl get pod &lt;pod-name&gt; -o jsonpath='{.spec.tolerations}' | jq\nkubectl get node &lt;node-name&gt; -o jsonpath='{.spec.taints}' | jq\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-1004-simulate-scheduling-dry-run","title":"Step 10.04 - Simulate Scheduling (Dry Run)","text":"<pre><code># Try applying a pod definition to see if it would schedule\nkubectl apply -f pod.yaml --dry-run=server\n\n# Use kubectl describe to see what nodes are eligible after a failed schedule\nkubectl describe pod &lt;pending-pod&gt; | grep -A 20 \"Events:\"\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#part-11-complete-scenario-production-grade-multi-tier-application","title":"Part 11 - Complete Scenario: Production-Grade Multi-Tier Application","text":"<p>This scenario combines everything to deploy a 3-tier application (frontend, backend, database) with: - Zone-spread frontend pods - Backend co-located in the same zones as frontend - Database on dedicated storage nodes - Monitoring agents on every node - Spot instance Nodes for batch jobs</p> <pre><code># ===== Setup: Label and taint cluster nodes =====\n\n# Zone assignment (cloud providers set these automatically)\nkubectl label nodes node-1 node-2 topology.kubernetes.io/zone=us-east-1a\nkubectl label nodes node-3 node-4 topology.kubernetes.io/zone=us-east-1b\nkubectl label nodes node-5 node-6 topology.kubernetes.io/zone=us-east-1c\n\n# Node types\nkubectl label nodes node-1 node-3 node-5 node-type=web\nkubectl label nodes node-2 node-4 node-6 node-type=storage disk-type=ssd\n\n# Storage nodes are dedicated\nkubectl taint nodes node-2 node-4 node-6 dedicated=storage:NoSchedule\n</code></pre> <pre><code># frontend-deployment.yaml - spread across zones, one per node\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n      tier: web\n  template:\n    metadata:\n      labels:\n        app: frontend\n        tier: web\n    spec:\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: topology.kubernetes.io/zone\n        whenUnsatisfiable: DoNotSchedule\n        labelSelector:\n          matchLabels:\n            app: frontend\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-type\n                operator: In\n                values: [web]\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: frontend\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: frontend\n        image: nginx:1.25\n        ports:\n        - containerPort: 80\n</code></pre> <pre><code># backend-deployment.yaml - co-located in same zones as frontend\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: backend\n      tier: api\n  template:\n    metadata:\n      labels:\n        app: backend\n        tier: api\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-type\n                operator: In\n                values: [web]\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                tier: web              # Be in same zone as frontend\n            topologyKey: topology.kubernetes.io/zone\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: backend\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: backend\n        image: nginx:1.25\n        ports:\n        - containerPort: 8080\n</code></pre> <pre><code># database-statefulset.yaml - dedicated storage nodes\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: database\nspec:\n  serviceName: database\n  replicas: 3\n  selector:\n    matchLabels:\n      app: database\n      tier: data\n  template:\n    metadata:\n      labels:\n        app: database\n        tier: data\n    spec:\n      tolerations:\n      - key: \"dedicated\"\n        operator: \"Equal\"\n        value: \"storage\"\n        effect: \"NoSchedule\"\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-type\n                operator: In\n                values: [storage]\n              - key: disk-type\n                operator: In\n                values: [ssd]\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: database\n            topologyKey: topology.kubernetes.io/zone   # One DB replica per zone\n      containers:\n      - name: db\n        image: postgres:16\n        env:\n        - name: POSTGRES_PASSWORD\n          value: \"changeme\"\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#exercises","title":"Exercises","text":"Exercise 1: Schedule a Pod only on nodes with \u2265 16 CPU cores labeled as <code>cpu-cores</code>  **Solution:**  <pre><code># Label nodes with cpu-cores count\nkubectl label nodes node-1 cpu-cores=8\nkubectl label nodes node-2 cpu-cores=32\nkubectl label nodes node-3 cpu-cores=64\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: compute-intensive\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: cpu-cores\n            operator: Gt\n            values: [\"16\"]\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre> Exercise 2: Deploy 4 replicas of an app with HARD anti-affinity across nodes, then observe Pending pods  **Solution:**  <pre><code># First, deploy with 4 replicas - if fewer than 4 nodes, some will Pending\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: anti-affinity-test\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: anti-affinity-test\n  template:\n    metadata:\n      labels:\n        app: anti-affinity-test\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: anti-affinity-test\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: app\n        image: nginx:1.25\n</code></pre> <pre><code>kubectl apply -f anti-affinity-test.yaml\nkubectl get pods -o wide     # Check distribution\n# If only 3 nodes: 3 pods Running, 1 Pending\nkubectl describe pod &lt;pending-pod&gt; | grep -A5 Events\n</code></pre>  Now change `requiredDuringScheduling` to `preferredDuringScheduling` and observe the difference.  Exercise 3: Taint a node with <code>NoExecute</code> and observe pod eviction, then add a toleration with <code>tolerationSeconds: 120</code>  **Solution:**  <pre><code># Deploy some pods on all nodes\nkubectl run test-evict --image=nginx --replicas=3\n\n# Find which node one of the pods is on\nkubectl get pods -o wide\n\n# Taint that node with NoExecute - all pods without toleration will be evicted\nkubectl taint nodes &lt;node-name&gt; eviction-test=true:NoExecute\n\n# Watch the pods being evicted and rescheduled\nkubectl get pods -o wide --watch\n</code></pre>  To delay eviction, deploy a pod with `tolerationSeconds`:  <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: delayed-eviction\nspec:\n  tolerations:\n  - key: \"eviction-test\"\n    operator: \"Equal\"\n    value: \"true\"\n    effect: \"NoExecute\"\n    tolerationSeconds: 120      # Survives on the tainted node for 2 minutes\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre> <pre><code>kubectl apply -f delayed-eviction.yaml\nkubectl get pods --watch     # delayed-eviction stays ~120s, then evicted\n</code></pre> Exercise 4: Deploy 9 pods spread evenly across 3 zones with maxSkew=1, then scale to 12 pods  **Solution:**  <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: spread-exercise\nspec:\n  replicas: 9\n  selector:\n    matchLabels:\n      app: spread-exercise\n  template:\n    metadata:\n      labels:\n        app: spread-exercise\n    spec:\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: topology.kubernetes.io/zone\n        whenUnsatisfiable: DoNotSchedule\n        labelSelector:\n          matchLabels:\n            app: spread-exercise\n      containers:\n      - name: app\n        image: nginx:1.25\n</code></pre> <pre><code>kubectl apply -f spread-exercise.yaml\nkubectl get pods -o wide     # Should see 3 pods per zone\n\n# Scale to 12\nkubectl scale deployment spread-exercise --replicas=12\nkubectl get pods -o wide     # Should see 4 pods per zone\n</code></pre> Exercise 5: Create a \"data locality pattern\" - Redis DaemonSet + application pods that MUST be on the same node as Redis  **Solution:**  <pre><code># Step 1: DaemonSet ensures Redis runs on every node\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: redis-local\nspec:\n  selector:\n    matchLabels:\n      type: redis-local\n  template:\n    metadata:\n      labels:\n        type: redis-local\n    spec:\n      containers:\n      - name: redis\n        image: redis:7-alpine\n        ports:\n        - containerPort: 6379\n---\n# Step 2: App pods MUST be co-located with Redis (same node)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: latency-sensitive-app\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: latency-sensitive-app\n  template:\n    metadata:\n      labels:\n        app: latency-sensitive-app\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                type: redis-local      # Must be on same node as Redis DaemonSet pod\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: app\n        image: nginx:1.25\n        env:\n        - name: REDIS_HOST\n          value: \"localhost\"           # Redis is on the same node\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#cleanup","title":"Cleanup","text":"<pre><code># Remove all test pods\nkubectl delete pod --all -n default\n\n# Remove test deployments\nkubectl delete deployment ha-nginx web-spread zone-ha-app --ignore-not-found\nkubectl delete deployment gpu-app multi-level-spread spread-exercise --ignore-not-found\nkubectl delete deployment frontend backend ha-web critical-api api-baseline --ignore-not-found\nkubectl delete statefulset database --ignore-not-found\nkubectl delete daemonset redis-local log-agent --ignore-not-found\n\n# Remove labels from nodes (replace node-1, node-2 with your actual node names)\nkubectl label nodes --all environment- zone- disk-type- hardware- --overwrite 2&gt;/dev/null || true\nkubectl label nodes --all accelerator- team- node-lifecycle- role- --overwrite 2&gt;/dev/null || true\nkubectl label nodes --all node-type- memory-gb- cpu-cores- --overwrite 2&gt;/dev/null || true\n\n# Remove taints from nodes\nkubectl taint nodes --all dedicated- 2&gt;/dev/null || true\nkubectl taint nodes --all maintenance- 2&gt;/dev/null || true\nkubectl taint nodes --all spot-instance- 2&gt;/dev/null || true\nkubectl taint nodes --all team- role- eviction-test- 2&gt;/dev/null || true\n\n# Verify nodes are clean\nkubectl describe nodes | grep -A3 \"Taints:\"\nkubectl get nodes --show-labels\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#summary","title":"Summary","text":"<p>In this lab you learned:</p> Topic Covered <code>nodeSelector</code> Simple exact-match node label filtering Node Affinity Rich operators (<code>In</code>, <code>NotIn</code>, <code>Exists</code>, <code>DoesNotExist</code>, <code>Gt</code>, <code>Lt</code>), required + preferred, weight-based scoring, AND/OR logic Pod Affinity Co-locate Pods on same node or same topology domain, namespace scoping Pod Anti-Affinity Spread Pods across nodes and zones, hard + soft variants Taints Node-side repulsion with <code>NoSchedule</code>, <code>PreferNoSchedule</code>, <code>NoExecute</code> effects Tolerations Pod-side opt-in with <code>Equal</code>/<code>Exists</code> operators, <code>tolerationSeconds</code> Built-in Taints System lifecycle taints (not-ready, unreachable, disk-pressure, etc.) Topology Spread Balance Pod counts across topology domains with <code>maxSkew</code>, <code>minDomains</code> Patterns GPU pools, zone-HA, spot bursting, multi-tenant isolation, data locality Debugging FailedScheduling events, label/taint inspection, dry-run"},{"location":"16-Affinity-Taint-Tolleration/#decision-guide","title":"Decision Guide","text":"<pre><code>flowchart TD\n    Q1{\"Do you need to\\ncontrol which NODES\\nPods go to?\"}\n    Q1 -- Yes --&gt; Q2{\"Simple exact\\nlabel match?\"}\n    Q2 -- Yes --&gt; A1[\"Use nodeSelector\"]\n    Q2 -- No --&gt; A2[\"Use Node Affinity\\n(required or preferred)\"]\n    Q1 -- No --&gt; Q3{\"Do you need to\\nplace Pods relative\\nto OTHER Pods?\"}\n    Q3 -- \"Near each other\" --&gt; A3[\"Use Pod Affinity\"]\n    Q3 -- \"Away from each other\" --&gt; A4[\"Use Pod Anti-Affinity\"]\n    Q3 -- \"Balanced spread\" --&gt; A5[\"Use Topology Spread\\nConstraints\"]\n    Q3 -- No --&gt; Q4{\"Do you need to\\nREPEL pods from\\na node?\"}\n    Q4 -- Yes --&gt; A6[\"Use Taints on Node\\n+ Tolerations on Pod\"]\n    Q4 -- No --&gt; A7[\"Default scheduler\\nbehavior is fine\"]\n\n    style A1 fill:#1a73e8,color:#fff\n    style A2 fill:#1a73e8,color:#fff\n    style A3 fill:#34a853,color:#fff\n    style A4 fill:#34a853,color:#fff\n    style A5 fill:#fbbc04,color:#000\n    style A6 fill:#ea4335,color:#fff</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Pod stuck in Pending:</li> </ul> <p>Check the pod events for scheduling failures - the event message tells you exactly which constraint was violated:</p> <pre><code>kubectl describe pod &lt;pod-name&gt; | grep -A10 \"Events:\"\n# Common messages:\n# \"0/N nodes are available: N node(s) didn't match Pod's node affinity/selector\"\n# \"0/N nodes are available: N node(s) had untolerated taint {key: value}\"\n# \"0/N nodes are available: N node(s) didn't match pod anti-affinity rules\"\n</code></pre> <p></p> <ul> <li>Anti-affinity blocking new pods:</li> </ul> <p>If you have more replicas than nodes and use <code>requiredDuringScheduling</code> anti-affinity, excess pods stay Pending. Switch to <code>preferredDuringScheduling</code>:</p> <pre><code>kubectl get pods -o wide\nkubectl get events --sort-by='.lastTimestamp' | grep FailedScheduling\n</code></pre> <p></p> <ul> <li>Taint applied but pods still running on the node:</li> </ul> <p><code>NoSchedule</code> only blocks new pods. Use <code>NoExecute</code> to evict existing pods:</p> <pre><code>kubectl describe nodes | grep -A3 \"Taints:\"\n# Change NoSchedule to NoExecute if you want to evict running pods\nkubectl taint nodes &lt;node&gt; key=value:NoExecute\n</code></pre> <p></p> <ul> <li>Labels not matching affinity rules:</li> </ul> <p>Verify that node labels actually exist and match your affinity selectors:</p> <pre><code>kubectl get nodes --show-labels | grep &lt;expected-label&gt;\nkubectl get node &lt;node-name&gt; -o jsonpath='{.metadata.labels}' | jq\n</code></pre> <p></p> <ul> <li>Topology spread constraints not balancing evenly:</li> </ul> <p>Ensure your nodes have the correct topology labels (<code>topology.kubernetes.io/zone</code>, <code>kubernetes.io/hostname</code>):</p> <pre><code>kubectl get nodes -L topology.kubernetes.io/zone\nkubectl get pods -o wide -l &lt;label-selector&gt;\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Pod Priority and Preemption for workload prioritization.</li> <li>Learn about Descheduler for rebalancing pods after scheduling decisions.</li> <li>Try combining scheduling constraints with PodDisruptionBudgets (Lab 17) for production-grade high availability.</li> <li>Explore Custom Schedulers (Lab 19) for when built-in scheduling constraints aren\u2019t enough.</li> <li>Practice scheduling tasks in the Kubernetes Scheduling Tasks section.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#additional-resources","title":"Additional Resources","text":"<ul> <li>Kubernetes Node Affinity Documentation</li> <li>Taints and Tolerations Documentation</li> <li>Topology Spread Constraints</li> <li>Well-Known Node Labels, Annotations, and Taints</li> <li>kube-scheduler Configuration</li> <li>Pod Priority and Preemption</li> </ul>"},{"location":"17-PodDisruptionBudgets-PDB/","title":"Pod Disruption Budgets (PDB)","text":"<ul> <li>In this lab, we will learn about <code>Pod Disruption Budgets (PDB)</code> in Kubernetes.</li> <li>We will explore how to define and implement PDBs to ensure application availability during voluntary disruptions, such as node maintenance or cluster upgrades.</li> <li>By the end of this lab, you will understand how to create and manage Pod Disruption Budgets to maintain the desired level of service availability in your Kubernetes cluster.</li> </ul>"},{"location":"17-PodDisruptionBudgets-PDB/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What Pod Disruption Budgets are and why they are important</li> <li>How PDBs protect applications during voluntary disruptions</li> <li>How to define PDBs using <code>minAvailable</code> or <code>maxUnavailable</code></li> <li>How Kubernetes eviction policies interact with PDBs</li> </ul>"},{"location":"17-PodDisruptionBudgets-PDB/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> <li>Minikube (for feature gates configuration)</li> </ul>"},{"location":"17-PodDisruptionBudgets-PDB/#introduction","title":"Introduction","text":"<ul> <li> <p>A <code>pod disruption budget</code> is an indicator of the number of disruptions that can be tolerated at a given time for a class of pods (a budget of faults).</p> </li> <li> <p>Disruptions may be caused by deliberate or accidental Pod deletion.</p> </li> <li> <p>Whenever a disruption to the pods in a service is calculated to cause the service to drop below the budget, the operation is paused until it can maintain the budget. This means that the <code>drain event</code> could be temporarily halted while it waits for more pods to become available such that the budget isn\u2019t crossed by evicting the pods.</p> </li> <li> <p>You can specify Pod Disruption Budgets for Pods managed by these built-in Kubernetes controllers:</p> <ul> <li><code>Deployment</code></li> <li><code>ReplicationController</code></li> <li><code>ReplicaSet</code></li> <li><code>StatefulSet</code></li> </ul> </li> <li> <p>For this tutorial you should get familier with Kubernetes Eviction Policies, as it demonstrates how <code>Pod Disruption Budgets</code> handle evictions.</p> </li> <li> <p>As in the <code>Kubernetes Eviction Policies</code> tutorial, we start with <pre><code>eviction-hard=\"memory.available&lt;480M\"\n</code></pre></p> </li> </ul>"},{"location":"17-PodDisruptionBudgets-PDB/#pdb-example","title":"PDB Example","text":"<ul> <li> <p>In the below sample we will configure a <code>Pod Disruption Budget</code> which insure that we will always have at least 1 Nginx instance.</p> </li> <li> <p>First we need an Nginx Deployment:</p> </li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: codewizard\n  labels:\n    app: nginx # &lt;- We will use this name below\n...\n</code></pre> <ul> <li>Now we can create the <code>Pod Disruption Budget</code>:</li> </ul> <pre><code>apiVersion: policy/v1beta1\nkind: PodDisruptionBudget\nmetadata:\n  name: nginx-pdb\nspec:\n  minAvailable: 1 # &lt;--- This will insure that we will have at least 1\n  selector:\n    matchLabels:\n      app: nginx # &lt;- The deployment app label\n</code></pre>"},{"location":"17-PodDisruptionBudgets-PDB/#lab","title":"Lab","text":"<p>01. start minikube with Feature Gates</p> <p>02. Check Node Pressure(s)</p>"},{"location":"17-PodDisruptionBudgets-PDB/#step-01-start-minikube-with-feature-gates","title":"Step 01 - Start Minikube with Feature Gates","text":"<ul> <li>Run thwe following command to start minikube with the required <code>Feature Gates</code> and <code>Eviction Signals</code>:</li> </ul> <pre><code>minikube start \\\n    --extra-config=kubelet.eviction-hard=\"memory.available&lt;480M\" \\\n    --extra-config=kubelet.eviction-pressure-transition-period=\"30s\" \\\n    --extra-config=kubelet.feature-gates=\"ExperimentalCriticalPodAnnotation=true\"\n</code></pre> <ul> <li> <p>For more details about <code>Feature Gates</code>, read here.</p> </li> <li> <p>For more details about <code>eviction-signals</code>, read here.</p> </li> </ul>"},{"location":"17-PodDisruptionBudgets-PDB/#step-02-check-node-pressures","title":"Step 02 - Check Node Pressure(s)","text":"<ul> <li>Check to see the Node conditions, if we have any kind of \u201cPressure\u201d, by running the following:</li> </ul> <pre><code>kubectl describe node minikube | grep MemoryPressure\n\n# Output should be similar to :\nConditions:\n  Type             Status  Reason                       Message\n  ----             ------  ------                       -------\n  MemoryPressure   False   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    KubeletReady                 kubelet is posting ready status. AppArmor enabled\n  ...\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                750m (37%)  0 (0%)\n  memory             140Mi (6%)  340Mi (16%)\n  ephemeral-storage  0 (0%)      0 (0%)\n</code></pre>"},{"location":"17-PodDisruptionBudgets-PDB/#step-03-create-3-pods-using-50-mb-each","title":"Step 03 - Create 3 Pods Using 50 MB Each","text":"<ul> <li>Create a file named <code>50MB-ram.yaml</code> with the following content:</li> </ul> <pre><code># ./resources/50MB-ram.yaml\n...\n\n# 3 replicas\nspec:\n  replicas: 3\n\n# resources request and limits\nresources:\n  requests:\n    memory: \"50Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"128Mi\"\n    cpu: \"500m\"\n</code></pre> <ul> <li>Create the pods with the following command:</li> </ul> <pre><code>kubectl apply -f resources/50MB-ram.yaml\n</code></pre>"},{"location":"17-PodDisruptionBudgets-PDB/#step-04-check-memory-pressure","title":"Step 04 - Check Memory Pressure","text":"<ul> <li>Now let\u2019s check the Node conditions again to see if we have <code>MemoryPressure</code>:</li> </ul> <p><pre><code>kubectl describe node minikube | grep MemoryPressure\n\n# Output should be similar to\nMemoryPressure   False   ...   KubeletHasSufficientMemory   kubelet has sufficient memory available\n</code></pre> - As we can see, we still have <code>sufficient memory available</code>.</p>"},{"location":"17-PodDisruptionBudgets-PDB/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete -f resources/50MB-ram.yaml\nkubectl delete pdb nginx-pdb\n</code></pre>"},{"location":"18-ArgoCD/","title":"ArgoCD","text":"<ul> <li><code>ArgoCD</code> is a declarative, GitOps continuous delivery tool for Kubernetes.</li> <li>It follows the GitOps pattern where a Git repository is the single source of truth for the desired application state.</li> <li><code>ArgoCD</code> automates the deployment and reconciliation of applications against one or more Kubernetes clusters.</li> </ul>"},{"location":"18-ArgoCD/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What <code>ArgoCD</code> is and why it is useful for GitOps</li> <li>How to install and configure <code>ArgoCD</code> on Kubernetes using Helm</li> <li><code>ArgoCD</code> core concepts: Applications, Projects, Sync, and Health</li> <li>How to expose ArgoCD via a Nginx Ingress</li> <li>How to deploy applications from Git repositories</li> <li>Application health and sync status monitoring</li> <li>Rollback and sync strategies</li> <li>The App of Apps pattern to manage multiple applications declaratively</li> <li>How to deploy the EFK stack (Lab 29) via ArgoCD</li> </ul>"},{"location":"18-ArgoCD/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link ArgoCD Official Documentation argo-cd.readthedocs.io ArgoCD CLI Reference argo-cd.readthedocs.io/cli ArgoCD Helm Chart (Argo Helm Repo) artifacthub.io App of Apps Pattern argo-cd.readthedocs.io/bootstrapping Sync Waves &amp; Phases argo-cd.readthedocs.io/sync-waves ApplicationSet Controller argo-cd.readthedocs.io/applicationset GitOps Principles gitops.tech ArgoCD GitHub Repository github.com/argoproj/argo-cd"},{"location":"18-ArgoCD/#what-is-argocd","title":"What is ArgoCD?","text":"<ul> <li><code>ArgoCD</code> is a pull-based GitOps operator: it watches Git and continuously reconciles the cluster state to match what is defined in Git.</li> <li>It consists of a control plane (running in the cluster) and optionally a CLI for interacting with it.</li> <li>Unlike push-based CI/CD (Jenkins, GitHub Actions), no pipeline ever needs direct <code>kubectl</code> access to the cluster.</li> </ul>"},{"location":"18-ArgoCD/#terminology","title":"Terminology","text":"Term Description Application An ArgoCD resource linking a Git path to a target Kubernetes cluster+namespace Project A logical grouping of applications; controls permitted repos/clusters/namespaces Sync The process of applying Git state to the live cluster Sync Status <code>Synced</code> - live matches Git; <code>OutOfSync</code> - live differs from Git Health Status <code>Healthy</code>, <code>Progressing</code>, <code>Degraded</code>, <code>Suspended</code>, or <code>Missing</code> App of Apps A pattern where one ArgoCD Application manages a directory of child Application manifests"},{"location":"18-ArgoCD/#argocd-components","title":"ArgoCD Components","text":"Component Description API Server Exposes REST/gRPC API consumed by Web UI, CLI, and CI/CD systems Repository Server Local cache of Git repositories; renders Helm, Kustomize, plain YAML Application Controller Monitors live cluster state and compares against desired state from Git Dex Identity service for integrating with external SSO providers Redis Caching layer; short-lived state between components"},{"location":"18-ArgoCD/#common-argocd-cli-commands","title":"Common ArgoCD CLI Commands","text":"<p>Below are the most common <code>argocd</code> CLI commands. Each command includes syntax, description, and detailed usage examples.</p> <code>argocd login</code> - Authenticate to ArgoCD server <p>Syntax: <code>argocd login &lt;server&gt;</code></p> <p>Description: Authenticates to an ArgoCD server and creates a local session used for subsequent CLI commands.</p> <ul> <li>Supports insecure mode (no TLS verification) for local/dev environments</li> <li>Can use username/password or SSO token</li> <li> <p>Session is stored in <code>~/.config/argocd/config</code></p> <pre><code># Login via Ingress\nargocd login argocd.local --insecure\n\n# Login via port-forward\nargocd login localhost:8080 --insecure\n\n# Login with explicit credentials\nargocd login argocd.local \\\n    --username admin \\\n    --password &lt;password&gt; \\\n    --insecure\n\n# Login to a remote secured server\nargocd login argocd.example.com --grpc-web\n\n# Change the admin password after login\nargocd account update-password\n\n# Show current login/user info\nargocd account get-user-info\n</code></pre> </li> </ul> <code>argocd app create</code> - Create a new application <p>Syntax: <code>argocd app create &lt;name&gt;</code></p> <p>Description: Creates a new ArgoCD Application resource that links a Git repository path to a target Kubernetes cluster and namespace.</p> <ul> <li>Links a Git source to a Kubernetes destination</li> <li>Supports Helm charts, Kustomize, and plain YAML</li> <li> <p>Can enable automated sync policies at creation time</p> <pre><code># Create from a plain YAML path in Git\nargocd app create guestbook \\\n  --repo https://github.com/argoproj/argocd-example-apps.git \\\n  --path guestbook \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace guestbook\n\n# Create with automated sync, self-heal and auto-prune\nargocd app create guestbook \\\n  --repo https://github.com/argoproj/argocd-example-apps.git \\\n  --path guestbook \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace guestbook \\\n  --sync-policy automated \\\n  --auto-prune \\\n  --self-heal\n\n# Create from a Helm chart in a Git repo\nargocd app create my-helm-app \\\n  --repo https://github.com/my-org/my-charts.git \\\n  --path charts/my-app \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace my-app \\\n  --helm-set replicaCount=2\n\n# Create from an OCI/Helm chart registry\nargocd app create nginx-helm \\\n  --repo https://charts.bitnami.com/bitnami \\\n  --helm-chart nginx \\\n  --revision 15.1.0 \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace default\n\n# Create and auto-create the namespace\nargocd app create my-app \\\n  --repo https://github.com/my-org/my-repo.git \\\n  --path manifests \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace production \\\n  --sync-option CreateNamespace=true\n</code></pre> </li> </ul> <code>argocd app list</code> - List all applications <p>Syntax: <code>argocd app list</code></p> <p>Description: Lists all ArgoCD applications and their current sync and health status.</p> <ul> <li>Shows application name, cluster, namespace, sync status, and health</li> <li>Supports filtering by project, sync status, and health status</li> <li> <p>Useful for a quick overview of all managed applications</p> <pre><code># List all applications\nargocd app list\n\n# List applications in a specific project\nargocd app list -p my-project\n\n# Filter by sync status\nargocd app list --sync-status OutOfSync\n\n# Filter by health status\nargocd app list --health-status Degraded\n\n# Output as JSON\nargocd app list -o json\n\n# Output as YAML\nargocd app list -o yaml\n\n# Show only app names\nargocd app list -o name\n\n# Wide output with extra columns\nargocd app list -o wide\n</code></pre> </li> </ul> <code>argocd app get</code> - Get details of an application <p>Syntax: <code>argocd app get &lt;name&gt;</code></p> <p>Description: Shows detailed information about a specific ArgoCD application including sync status, health, and the managed resource tree.</p> <ul> <li>Displays sync and health status</li> <li>Shows all Kubernetes resources managed by the application</li> <li> <p>Use <code>--refresh</code> to force-fetch the latest state from Git</p> <pre><code># Get application details\nargocd app get guestbook\n\n# Force refresh from Git before displaying\nargocd app get guestbook --refresh\n\n# Show resource tree\nargocd app get guestbook --output tree\n\n# Output as JSON\nargocd app get guestbook -o json\n\n# Output as YAML\nargocd app get guestbook -o yaml\n\n# Watch live status updates\nwatch argocd app get guestbook\n</code></pre> </li> </ul> <code>argocd app sync</code> - Sync (deploy) an application <p>Syntax: <code>argocd app sync &lt;name&gt;</code></p> <p>Description: Triggers a sync operation that applies the desired Git state to the live cluster.</p> <ul> <li>Applies the desired state from Git to the cluster</li> <li>Supports dry-run mode to preview changes without applying them</li> <li> <p>Can force sync to replace resources or selectively sync specific resources</p> <pre><code># Sync an application\nargocd app sync guestbook\n\n# Sync and wait for completion\nargocd app sync guestbook --timeout 120\n\n# Dry-run - preview changes without applying\nargocd app sync guestbook --dry-run\n\n# Force sync (replace resources even if spec is unchanged)\nargocd app sync guestbook --force\n\n# Sync a specific resource only\nargocd app sync guestbook \\\n  --resource apps:Deployment:guestbook-ui\n\n# Sync with prune (delete resources removed from Git)\nargocd app sync guestbook --prune\n\n# Sync multiple applications at once\nargocd app sync guestbook efk-stack app-of-apps\n</code></pre> </li> </ul> <code>argocd app diff</code> - Show diff between Git and live state <p>Syntax: <code>argocd app diff &lt;name&gt;</code></p> <p>Description: Shows the difference between the desired state in Git and the live state in the cluster. Useful for diagnosing configuration drift.</p> <ul> <li>Outputs a unified diff of desired vs live state</li> <li>Helps diagnose <code>OutOfSync</code> applications before syncing</li> <li> <p>Can compare against a specific Git revision</p> <pre><code># Show diff for an application\nargocd app diff guestbook\n\n# Show diff and compare against a specific revision\nargocd app diff guestbook --revision HEAD~1\n\n# Show diff for a specific resource type\nargocd app diff guestbook \\\n  --resource apps:Deployment:guestbook-ui\n\n# Use in CI - exit non-zero if drift detected\nargocd app diff guestbook; echo \"Exit code: $?\"\n</code></pre> </li> </ul> <code>argocd app set</code> - Update application settings <p>Syntax: <code>argocd app set &lt;name&gt; [flags]</code></p> <p>Description: Updates the configuration of an existing ArgoCD application without deleting and recreating it.</p> <ul> <li>Modify source repository, path, revision, destination, or sync policy</li> <li>Enable or disable auto-sync, self-heal, and auto-prune</li> <li> <p>Override Helm values or add/remove sync options</p> <pre><code># Enable automated sync\nargocd app set guestbook --sync-policy automated\n\n# Enable self-heal and auto-prune\nargocd app set guestbook --self-heal --auto-prune\n\n# Disable automated sync (switch to manual)\nargocd app set guestbook --sync-policy none\n\n# Change the target revision (branch, tag, or commit SHA)\nargocd app set guestbook --revision v1.2.0\n\n# Override a Helm value\nargocd app set my-helm-app --helm-set replicaCount=3\n\n# Add a sync option\nargocd app set guestbook --sync-option CreateNamespace=true\n\n# Change the target namespace\nargocd app set guestbook --dest-namespace new-namespace\n</code></pre> </li> </ul> <code>argocd app history</code> - Show deployment history <p>Syntax: <code>argocd app history &lt;name&gt;</code></p> <p>Description: Prints the deployment history for an application, listing every revision that has been deployed to the cluster.</p> <ul> <li>Shows revision ID, timestamp, Git commit SHA, and deploy status</li> <li>Use revision IDs from history to target a specific rollback</li> <li> <p>History is maintained by ArgoCD in its internal state store</p> <pre><code># Show full deployment history\nargocd app history guestbook\n\n# Output as JSON for scripting\nargocd app history guestbook -o json\n\n# Pretty-print with jq\nargocd app history guestbook -o json | \\\n  jq '.[] | {id: .id, date: .deployedAt, revision: .revision}'\n</code></pre> </li> </ul> <code>argocd app rollback</code> - Rollback to a previous revision <p>Syntax: <code>argocd app rollback &lt;name&gt; &lt;revision-id&gt;</code></p> <p>Description: Rolls back an application to a previously deployed revision. The revision ID is obtained from <code>argocd app history</code>.</p> <ul> <li>Reverts the live cluster to a previously deployed state</li> <li>Disables automated sync on the app to prevent re-syncing forward</li> <li> <p>The rollback operation is recorded as a new entry in history</p> <pre><code># Check history to find the target revision ID\nargocd app history guestbook\n\n# Rollback to a specific revision\nargocd app rollback guestbook 3\n\n# Rollback and wait for completion\nargocd app rollback guestbook 3 --timeout 120\n\n# Re-enable auto-sync after rollback\nargocd app set guestbook --sync-policy automated\n\n# Verify the rollback succeeded\nargocd app get guestbook\nkubectl get all -n guestbook\n</code></pre> </li> </ul> <code>argocd app delete</code> - Delete an application <p>Syntax: <code>argocd app delete &lt;name&gt;</code></p> <p>Description: Deletes an ArgoCD application. By default performs a cascade delete, removing all managed Kubernetes resources along with the Application resource.</p> <ul> <li>Cascade delete removes all Kubernetes resources managed by the app</li> <li>Non-cascade delete removes only the ArgoCD Application resource itself</li> <li> <p>Use <code>--yes</code> to skip the confirmation prompt in scripts</p> <pre><code># Delete an application (cascade - removes all K8s resources)\nargocd app delete guestbook\n\n# Delete without removing Kubernetes resources (non-cascade)\nargocd app delete guestbook --cascade=false\n\n# Skip confirmation prompt (useful in scripts)\nargocd app delete guestbook --yes\n\n# Delete multiple applications\nargocd app delete guestbook efk-stack --yes\n</code></pre> </li> </ul> <code>argocd repo</code> - Manage repositories <p>Syntax: <code>argocd repo add &lt;url&gt;</code> / <code>argocd repo list</code></p> <p>Description: Manages Git and Helm chart repositories connected to ArgoCD. ArgoCD must have access to a repository before it can deploy from it.</p> <ul> <li>Add public or private repositories</li> <li>Supports HTTPS tokens, SSH keys, and TLS certificates for authentication</li> <li> <p>List and remove existing repository connections</p> <pre><code># List all connected repositories\nargocd repo list\n\n# Add a public HTTPS repository\nargocd repo add https://github.com/argoproj/argocd-example-apps.git\n\n# Add a private repository with an HTTPS token\nargocd repo add https://github.com/my-org/private-repo.git \\\n  --username git \\\n  --password &lt;token&gt;\n\n# Add a repository using an SSH key\nargocd repo add git@github.com:my-org/private-repo.git \\\n  --ssh-private-key-path ~/.ssh/id_rsa\n\n# Add a Helm chart repository\nargocd repo add https://charts.bitnami.com/bitnami \\\n  --type helm \\\n  --name bitnami\n\n# Remove a repository\nargocd repo rm https://github.com/my-org/private-repo.git\n</code></pre> </li> </ul> <code>argocd context</code> - Manage server contexts <p>Syntax: <code>argocd context [context-name]</code></p> <p>Description: Manages multiple ArgoCD server connections (contexts), similar to <code>kubectl config use-context</code>. Useful when managing applications across multiple clusters or environments.</p> <ul> <li>Switch between multiple ArgoCD server connections</li> <li>Contexts are stored in <code>~/.config/argocd/config</code></li> <li> <p>Each context holds a server address and authentication token</p> <pre><code># List all saved contexts\nargocd context\n\n# Switch to a different context\nargocd context my-prod-server\n\n# Show the currently active context\nargocd context --current\n\n# Delete a context\nargocd context --delete my-old-server\n</code></pre> </li> </ul>"},{"location":"18-ArgoCD/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    dev[\"Developer\\npushes to Git\"] --&gt; git[\"Git Repository\\n(Source of Truth)\"]\n\n    subgraph cluster[\"Kubernetes Cluster\"]\n        subgraph argocd[\"argocd namespace\"]\n            api[\"ArgoCD API Server\\n(argocd.local via Ingress)\"]\n            ctrl[\"Application Controller\"]\n            repo[\"Repository Server\"]\n            dex[\"Dex (SSO)\"]\n            redis[\"Redis (cache)\"]\n            ingress[\"Nginx Ingress\\nargocd.local\"]\n        end\n\n        subgraph apps[\"Managed Namespaces\"]\n            guestbook[\"guestbook namespace\\nGuestbook App\"]\n            efk[\"efk namespace\\nEFK Stack\"]\n        end\n    end\n\n    browser[\"Browser / argocd CLI\"] --&gt; ingress\n    ingress --&gt; api\n    git --&gt; repo\n    repo --&gt; ctrl\n    ctrl --&gt; guestbook\n    ctrl --&gt; efk\n    api --&gt; ctrl\n    api --&gt; dex\n    ctrl --&gt; redis</code></pre>"},{"location":"18-ArgoCD/#app-of-apps-pattern","title":"App of Apps Pattern","text":"<pre><code>graph TD\n    root[\"app-of-apps\\nwatches: Labs/18-ArgoCD/apps/\"]\n\n    root --&gt; guestbook[\"guestbook\\nrepo: argocd-example-apps\\nnamespace: guestbook\"]\n    root --&gt; efk[\"efk-stack\\nwatches: Labs/33-EFK/argocd-apps/\"]\n\n    efk --&gt; es[\"efk-elasticsearch\\nHelm chart\\nwave: 0\"]\n    efk --&gt; fb[\"efk-filebeat\\nHelm chart\\nwave: 1\"]\n    efk --&gt; kb[\"efk-kibana\\nHelm chart\\nwave: 1\"]\n    efk --&gt; lg[\"efk-log-generator\\nHelm chart\\nwave: 2\"]\n    efk --&gt; lp[\"efk-log-processor\\nHelm chart\\nwave: 2\"]</code></pre>"},{"location":"18-ArgoCD/#directory-structure","title":"Directory Structure","text":"<pre><code>18-ArgoCD/\n\u251c\u2500\u2500 README.md                 # This file\n\u251c\u2500\u2500 demo.sh                   # Full automated demo script\n\u251c\u2500\u2500 ArgoCD.sh                 # Legacy install script (manual)\n\u251c\u2500\u2500 install-argocd.sh         # Install ArgoCD via kustomize (legacy)\n\u251c\u2500\u2500 install.sh                # Print admin password\n\u251c\u2500\u2500 run-demo.sh               # Run guestbook demo (legacy)\n\u2502\n\u251c\u2500\u2500 manifests/\n\u2502   \u2514\u2500\u2500 argocd-ingress.yaml   # Nginx Ingress for ArgoCD UI\n\u2502\n\u251c\u2500\u2500 apps/                     # App of Apps - all YAML files here are managed\n\u2502   \u251c\u2500\u2500 app-of-apps.yaml      # Root application - points to this apps/ folder\n\u2502   \u251c\u2500\u2500 guestbook.yaml        # Guestbook demo application\n\u2502   \u2514\u2500\u2500 efk-stack.yaml        # EFK stack App of Apps (points to Lab 29)\n\u2502\n\u251c\u2500\u2500 guestbook-app.yaml        # Standalone guestbook application manifest\n\u251c\u2500\u2500 kustomization.yaml        # Kustomize patch for argocd-server --insecure\n\u2514\u2500\u2500 patch-replace.yaml        # Kustomize strategic merge patch\n</code></pre>"},{"location":"18-ArgoCD/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (v1.24+)</li> <li><code>kubectl</code> configured to access your cluster</li> <li><code>Helm 3.x</code> installed</li> <li>Nginx Ingress Controller installed on the cluster</li> <li>(Optional) <code>argocd</code> CLI</li> </ul> <pre><code># Install kubectl (macOS)\nbrew install kubectl\n\n# Install Helm\nbrew install helm\n\n# Install argocd CLI (optional)\nbrew install argocd\n\n# Install Nginx Ingress Controller (if not present)\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \\\n    --namespace ingress-nginx --create-namespace\n</code></pre>"},{"location":"18-ArgoCD/#lab","title":"Lab","text":""},{"location":"18-ArgoCD/#part-01-full-automated-demo","title":"Part 01 - Full Automated Demo","text":"<p>The <code>demo.sh</code> script handles the complete lifecycle in one command: ArgoCD installation via Helm, Ingress setup, Guestbook deployment, and App of Apps deployment.</p>"},{"location":"18-ArgoCD/#01-run-the-demo","title":"01. Run the Demo","text":"<pre><code>chmod +x demo.sh\n./demo.sh deploy\n</code></pre> <p>The script will:</p> <ul> <li>Install ArgoCD via Helm (<code>argo/argo-cd</code> chart) with <code>--insecure</code> mode</li> <li>Apply the Nginx Ingress pointing <code>argocd.local</code> to the ArgoCD server</li> <li>Print admin credentials</li> <li>Deploy the Guestbook demo application and wait for it to sync</li> <li>Deploy the App of Apps, which triggers the EFK stack deployment from Lab 29</li> </ul>"},{"location":"18-ArgoCD/#02-other-commands","title":"02. Other Commands","text":"<pre><code># Show current status of all applications\n./demo.sh status\n\n# Print admin username and password\n./demo.sh credentials\n\n# Remove all ArgoCD resources and managed apps\n./demo.sh cleanup\n</code></pre>"},{"location":"18-ArgoCD/#part-02-manual-argocd-installation","title":"Part 02 - Manual ArgoCD Installation","text":""},{"location":"18-ArgoCD/#01-install-argocd-via-helm","title":"01. Install ArgoCD via Helm","text":"<pre><code># Add Argo Helm repository\nhelm repo add argo https://argoproj.github.io/argo-helm\nhelm repo update argo\n\n# Install ArgoCD (insecure mode - TLS handled by Ingress)\nhelm upgrade --install argocd argo/argo-cd \\\n    --namespace argocd \\\n    --create-namespace \\\n    --set server.insecure=true \\\n    --wait\n</code></pre>"},{"location":"18-ArgoCD/#02-verify-installation","title":"02. Verify Installation","text":"<pre><code>kubectl get pods -n argocd\n</code></pre> <p>Expected output (all pods Running):</p> <pre><code>NAME                                                READY   STATUS\nargocd-application-controller-0                    1/1     Running\nargocd-dex-server-xxxx                             1/1     Running\nargocd-redis-xxxx                                  1/1     Running\nargocd-repo-server-xxxx                            1/1     Running\nargocd-server-xxxx                                 1/1     Running\n</code></pre>"},{"location":"18-ArgoCD/#03-get-admin-password","title":"03. Get Admin Password","text":"<pre><code>kubectl -n argocd get secret argocd-initial-admin-secret \\\n    -o jsonpath=\"{.data.password}\" | base64 -d; echo\n</code></pre> <p>Note</p> <p>Save this password - you\u2019ll need it to log in to the ArgoCD UI and CLI.</p>"},{"location":"18-ArgoCD/#part-03-expose-argocd-via-ingress","title":"Part 03 - Expose ArgoCD via Ingress","text":"<p>An Nginx Ingress allows you to access the ArgoCD UI at <code>http://argocd.local</code> instead of requiring port-forwarding.</p> <p>Prerequisite</p> <p>Nginx Ingress Controller must be installed in the cluster.</p>"},{"location":"18-ArgoCD/#01-apply-the-ingress","title":"01. Apply the Ingress","text":"<pre><code>kubectl apply -f manifests/argocd-ingress.yaml\n</code></pre> <p>The Ingress forwards HTTP traffic to the ArgoCD server which runs in <code>--insecure</code> mode (no TLS at the pod level):</p> <pre><code># manifests/argocd-ingress.yaml (summary)\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTP\"\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: argocd.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: argocd-server\n                port:\n                  number: 80\n</code></pre>"},{"location":"18-ArgoCD/#02-add-to-etchosts","title":"02. Add to /etc/hosts","text":"<pre><code># Get the node IP\nINGRESS_IP=$(kubectl get nodes \\\n    -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n\n# Add entry\necho \"${INGRESS_IP}  argocd.local\" | sudo tee -a /etc/hosts\n\n# Open the UI\nopen http://argocd.local\n</code></pre>"},{"location":"18-ArgoCD/#03-port-forward-fallback","title":"03. Port-Forward Fallback","text":"<p>If Ingress is not available:</p> <pre><code>kubectl port-forward svc/argocd-server -n argocd 8080:80 &amp;\nopen http://localhost:8080\n</code></pre>"},{"location":"18-ArgoCD/#part-04-argocd-cli","title":"Part 04 - ArgoCD CLI","text":""},{"location":"18-ArgoCD/#01-install-the-cli","title":"01. Install the CLI","text":"<pre><code># macOS\nbrew install argocd\n\n# Linux\ncurl -sSL -o argocd-linux-amd64 \\\n    https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\n</code></pre>"},{"location":"18-ArgoCD/#02-login","title":"02. Login","text":"<pre><code># Via Ingress\nargocd login argocd.local --insecure\n\n# Via port-forward\nargocd login localhost:8080 --insecure\n\n# Change admin password (recommended)\nargocd account update-password\n</code></pre>"},{"location":"18-ArgoCD/#part-05-deploying-the-guestbook-application","title":"Part 05 - Deploying the Guestbook Application","text":""},{"location":"18-ArgoCD/#01-create-via-manifest","title":"01. Create via Manifest","text":"<pre><code>kubectl apply -f apps/guestbook.yaml\n</code></pre>"},{"location":"18-ArgoCD/#02-create-via-cli","title":"02. Create via CLI","text":"<pre><code>argocd app create guestbook \\\n  --repo https://github.com/argoproj/argocd-example-apps.git \\\n  --path guestbook \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace guestbook \\\n  --sync-policy automated \\\n  --auto-prune \\\n  --self-heal\n</code></pre>"},{"location":"18-ArgoCD/#03-monitor-sync","title":"03. Monitor Sync","text":"<pre><code># Via CLI\nargocd app get guestbook\n\n# Via kubectl\nkubectl get application guestbook -n argocd -w\n</code></pre>"},{"location":"18-ArgoCD/#04-access-the-guestbook","title":"04. Access the Guestbook","text":"<pre><code>kubectl port-forward svc/guestbook-ui -n guestbook 8081:80 &amp;\nopen http://localhost:8081\n</code></pre>"},{"location":"18-ArgoCD/#part-06-app-of-apps-pattern","title":"Part 06 - App of Apps Pattern","text":"<p>The App of Apps pattern uses a single root ArgoCD Application as a controller that watches a Git directory for Application manifests. Adding an app is as simple as committing a new YAML file.</p>"},{"location":"18-ArgoCD/#gitops-flow","title":"GitOps Flow","text":"<pre><code>sequenceDiagram\n    participant Dev as Developer\n    participant Git as Git Repository\n    participant ArgoCD as ArgoCD\n    participant K8s as Kubernetes\n\n    Dev-&gt;&gt;Git: git push (new app YAML in apps/)\n    ArgoCD-&gt;&gt;Git: Poll for changes (every 3 min)\n    Git--&gt;&gt;ArgoCD: Detects new Application manifest\n    ArgoCD-&gt;&gt;K8s: Creates child Application resource\n    ArgoCD-&gt;&gt;Git: Fetches manifests from child app's source\n    Git--&gt;&gt;ArgoCD: Returns Helm/Kustomize/YAML manifests\n    ArgoCD-&gt;&gt;K8s: Deploys resources to target namespace\n    K8s--&gt;&gt;ArgoCD: Reports health status</code></pre>"},{"location":"18-ArgoCD/#01-deploy-the-app-of-apps","title":"01. Deploy the App of Apps","text":"<pre><code>kubectl apply -f apps/app-of-apps.yaml\n</code></pre> <p>ArgoCD will:</p> <ol> <li>Detect the <code>apps/</code> directory in the repo</li> <li>Create a child Application for each <code>.yaml</code> file found there</li> <li>Each child Application then deploys its own resources</li> </ol>"},{"location":"18-ArgoCD/#02-verify-all-applications","title":"02. Verify All Applications","text":"<pre><code>argocd app list\n</code></pre> <p>Expected output:</p> <pre><code>NAME               CLUSTER     NAMESPACE  STATUS  HEALTH   SYNCPOLICY\napp-of-apps        in-cluster  argocd     Synced  Healthy  Auto-Prune\nefk-stack          in-cluster  argocd     Synced  Healthy  Auto-Prune\nefk-elasticsearch  in-cluster  efk        Synced  Healthy  Auto-Prune\nefk-filebeat       in-cluster  efk        Synced  Healthy  Auto-Prune\nefk-kibana         in-cluster  efk        Synced  Healthy  Auto-Prune\nefk-log-generator  in-cluster  efk        Synced  Healthy  Auto-Prune\nefk-log-processor  in-cluster  efk        Synced  Healthy  Auto-Prune\nguestbook          in-cluster  guestbook  Synced  Healthy  Auto-Prune\n</code></pre>"},{"location":"18-ArgoCD/#03-add-a-new-application","title":"03. Add a New Application","text":"<p>To add a new application, commit a new manifest to <code>apps/</code>:</p> <pre><code>cat &gt; apps/my-new-app.yaml &lt;&lt; 'EOF'\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-new-app\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/my-org/my-repo.git\n    targetRevision: HEAD\n    path: manifests\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: my-app\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\nEOF\n\ngit add apps/my-new-app.yaml\ngit commit -m \"feat: add my-new-app to App of Apps\"\ngit push\n# ArgoCD automatically detects and deploys the new application\n</code></pre>"},{"location":"18-ArgoCD/#part-07-auto-sync-and-self-healing","title":"Part 07 - Auto-Sync and Self-Healing","text":""},{"location":"18-ArgoCD/#enable-auto-sync","title":"Enable Auto-Sync","text":"<pre><code>argocd app set guestbook \\\n  --sync-policy automated \\\n  --self-heal \\\n  --auto-prune\n</code></pre>"},{"location":"18-ArgoCD/#test-self-healing","title":"Test Self-Healing","text":"<pre><code># Manually break the desired state\nkubectl scale deployment guestbook-ui --replicas=5 -n guestbook\n\n# ArgoCD detects the drift and restores the desired replica count from Git\nwatch argocd app get guestbook\n</code></pre>"},{"location":"18-ArgoCD/#part-08-rollback","title":"Part 08 - Rollback","text":"<pre><code># View deployment history\nargocd app history guestbook\n\n# Rollback to a specific revision\nargocd app rollback guestbook &lt;revision-id&gt;\n\n# Verify the rollback\nargocd app get guestbook\nkubectl get all -n guestbook\n</code></pre>"},{"location":"18-ArgoCD/#part-09-sync-waves-deployment-ordering","title":"Part 09 - Sync Waves (Deployment Ordering)","text":"<p>Sync waves control the order in which resources are applied during a sync. Resources in wave <code>N</code> are applied only after all resources in wave <code>N-1</code> are healthy. The EFK App of Apps uses waves to ensure Elasticsearch is ready before Filebeat and Kibana start.</p> <pre><code># wave 0: Elasticsearch (must be ready first)\nmetadata:\n  annotations:\n    argocd.argoproj.io/sync-wave: \"0\"\n\n# wave 1: Filebeat and Kibana (require Elasticsearch)\nmetadata:\n  annotations:\n    argocd.argoproj.io/sync-wave: \"1\"\n\n# wave 2: Log Generator and Processor (require everything else)\nmetadata:\n  annotations:\n    argocd.argoproj.io/sync-wave: \"2\"\n</code></pre>"},{"location":"18-ArgoCD/#part-10-working-with-helm-charts","title":"Part 10 - Working with Helm Charts","text":"<pre><code># Deploy a Helm chart from a registry\nargocd app create nginx-helm \\\n  --repo https://charts.bitnami.com/bitnami \\\n  --helm-chart nginx \\\n  --revision 15.1.0 \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace default \\\n  --helm-set service.type=NodePort \\\n  --helm-set replicaCount=3\n\nargocd app sync nginx-helm\n</code></pre>"},{"location":"18-ArgoCD/#part-11-troubleshooting","title":"Part 11 - Troubleshooting","text":""},{"location":"18-ArgoCD/#argocd-server-not-accessible","title":"ArgoCD Server Not Accessible","text":"<pre><code># Check ArgoCD pods\nkubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server\n\n# Check Ingress\nkubectl get ingress -n argocd\nkubectl describe ingress argocd-server-ingress -n argocd\n</code></pre>"},{"location":"18-ArgoCD/#application-stuck-in-progressing","title":"Application Stuck in Progressing","text":"<pre><code>argocd app get &lt;app-name&gt;\nkubectl describe application &lt;app-name&gt; -n argocd\nkubectl logs -n argocd -l app.kubernetes.io/name=argocd-application-controller --tail=50\n</code></pre>"},{"location":"18-ArgoCD/#out-of-sync-repository-error","title":"Out of Sync / Repository Error","text":"<pre><code># Show the diff\nargocd app diff &lt;app-name&gt;\n\n# Force refresh from Git\nargocd app get &lt;app-name&gt; --refresh\n\n# Force sync\nargocd app sync &lt;app-name&gt; --force\n</code></pre>"},{"location":"18-ArgoCD/#app-of-apps-not-creating-child-apps","title":"App of Apps Not Creating Child Apps","text":"<pre><code># Check root app is synced\nargocd app get app-of-apps\n\n# Check ArgoCD can reach the repo\nargocd repo list\n\n# Check the apps/ directory exists in the configured path\nargocd app manifests app-of-apps\n</code></pre>"},{"location":"18-ArgoCD/#cleanup","title":"Cleanup","text":"<pre><code># Full cleanup via demo.sh\n./demo.sh cleanup\n\n# Or manually\nkubectl delete applications --all -n argocd\nhelm uninstall argocd --namespace argocd\nkubectl delete namespace argocd guestbook efk\n</code></pre>"},{"location":"18-ArgoCD/#part-12-end-to-end-from-helm-chart-to-gitops-with-argocd","title":"Part 12 - End-to-End: From Helm Chart to GitOps with ArgoCD","text":"<p>This part walks through the complete lifecycle in baby steps - starting from zero, building a Helm chart from scratch, installing ArgoCD, and then managing the chart purely through GitOps. By the end you will have a working ArgoCD setup where every <code>git push</code> automatically deploys your Helm chart.</p> <pre><code>flowchart LR\n    A[\"Step 1-3\\nCreate Helm Chart\"] --&gt; B[\"Step 4-5\\nValidate &amp; Commit to Git\"]\n    B --&gt; C[\"Step 6-7\\nInstall ArgoCD\"]\n    C --&gt; D[\"Step 8-9\\nConfigure &amp; Login\"]\n    D --&gt; E[\"Step 10-11\\nCreate ArgoCD App\"]\n    E --&gt; F[\"Step 12-13\\nGitOps in Action\"]\n    F --&gt; G[\"Step 14\\nApp of Apps\"]</code></pre>"},{"location":"18-ArgoCD/#step-01-install-prerequisites","title":"Step 01 - Install Prerequisites","text":"<p>Install the required tools before starting.</p> macOSLinux <pre><code>brew install helm kubectl argocd\n</code></pre> <pre><code># Helm\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n\n# ArgoCD CLI\ncurl -sSL -o argocd-linux-amd64 \\\n    https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\nrm argocd-linux-amd64\n</code></pre>"},{"location":"18-ArgoCD/#verify-tools-are-ready","title":"Verify tools are ready","text":"<pre><code>kubectl version --client --short\nhelm version --short\nargocd version --client --short\n</code></pre> <p>Expected output (versions may differ):</p> <pre><code>Client Version: v1.28.x\nHelm: v3.x.x\nargocd: v2.x.x\n</code></pre>"},{"location":"18-ArgoCD/#step-02-create-a-helm-chart-from-scratch","title":"Step 02 - Create a Helm Chart from Scratch","text":"<p>We will build the <code>my-webserver</code> chart - a simple nginx-based web server.</p>"},{"location":"18-ArgoCD/#0201-scaffold-the-chart","title":"02.01 Scaffold the chart","text":"<pre><code># Create a fresh chart skeleton\nhelm create my-webserver\n\n# Inspect the generated structure\nfind my-webserver -type f | sort\n</code></pre> <p>You will see:</p> <pre><code>my-webserver/\n\u251c\u2500\u2500 Chart.yaml           # Chart metadata (name, version, appVersion)\n\u251c\u2500\u2500 values.yaml          # Default configuration values\n\u251c\u2500\u2500 charts/              # Subchart dependencies (empty for now)\n\u2514\u2500\u2500 templates/\n    \u251c\u2500\u2500 _helpers.tpl     # Reusable named templates\n    \u251c\u2500\u2500 deployment.yaml  # Deployment resource template\n    \u251c\u2500\u2500 hpa.yaml         # HorizontalPodAutoscaler (optional)\n    \u251c\u2500\u2500 ingress.yaml     # Ingress resource (optional)\n    \u251c\u2500\u2500 NOTES.txt        # Post-install message shown to user\n    \u251c\u2500\u2500 service.yaml     # Service resource template\n    \u251c\u2500\u2500 serviceaccount.yaml\n    \u2514\u2500\u2500 tests/\n        \u2514\u2500\u2500 test-connection.yaml\n</code></pre>"},{"location":"18-ArgoCD/#0202-clean-up-the-scaffold-keep-only-what-we-need","title":"02.02 Clean up the scaffold (keep only what we need)","text":"<pre><code># Remove files we will not use in this demo\nrm my-webserver/templates/hpa.yaml\nrm my-webserver/templates/serviceaccount.yaml\nrm my-webserver/templates/ingress.yaml\n</code></pre>"},{"location":"18-ArgoCD/#0203-rewrite-chartyaml","title":"02.03 Rewrite <code>Chart.yaml</code>","text":"<p>Replace the contents of <code>my-webserver/Chart.yaml</code> with:</p> <pre><code>apiVersion: v2\nname: my-webserver\ndescription: A simple nginx web server - Helm + ArgoCD demo\ntype: application\nversion: 1.0.0\nappVersion: \"1.25.3\"\n</code></pre> <p>version vs appVersion</p> <ul> <li><code>version</code> is the chart version - bump it every time you release a new chart.</li> <li><code>appVersion</code> is the version of the application (nginx image tag) shipped inside the chart.</li> </ul>"},{"location":"18-ArgoCD/#0204-rewrite-valuesyaml","title":"02.04 Rewrite <code>values.yaml</code>","text":"<p>Replace the contents of <code>my-webserver/values.yaml</code> with a minimal, well-documented set of values:</p> <pre><code># Number of nginx pods to run\nreplicaCount: 1\n\nimage:\n  repository: nginx\n  tag: \"1.25.3\"\n  pullPolicy: IfNotPresent\n\n# The HTML content served by nginx\ngreeting: \"Hello from my Helm chart + ArgoCD!\"\n\nservice:\n  type: ClusterIP\n  port: 80\n\n# Resource limits/requests\nresources:\n  requests:\n    cpu: \"50m\"\n    memory: \"64Mi\"\n  limits:\n    cpu: \"200m\"\n    memory: \"128Mi\"\n</code></pre>"},{"location":"18-ArgoCD/#0205-rewrite-the-deployment-template","title":"02.05 Rewrite the Deployment template","text":"<p>Replace the contents of <code>my-webserver/templates/deployment.yaml</code>:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"my-webserver.fullname\" . }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"my-webserver.labels\" . | nindent 4 }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  selector:\n    matchLabels:\n      {{- include \"my-webserver.selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      labels:\n        {{- include \"my-webserver.selectorLabels\" . | nindent 8 }}\n    spec:\n      containers:\n        - name: nginx\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n          imagePullPolicy: {{ .Values.image.pullPolicy }}\n          ports:\n            - containerPort: 80\n          # Inject a simple HTML index from the ConfigMap\n          volumeMounts:\n            - name: html\n              mountPath: /usr/share/nginx/html\n          resources:\n            {{- toYaml .Values.resources | nindent 12 }}\n      volumes:\n        - name: html\n          configMap:\n            name: {{ include \"my-webserver.fullname\" . }}-html\n</code></pre>"},{"location":"18-ArgoCD/#0206-add-a-configmap-template","title":"02.06 Add a ConfigMap template","text":"<p>Create <code>my-webserver/templates/configmap.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"my-webserver.fullname\" . }}-html\n  namespace: {{ .Release.Namespace }}\n  labels:\n    {{- include \"my-webserver.labels\" . | nindent 4 }}\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;&lt;title&gt;{{ .Values.greeting }}&lt;/title&gt;&lt;/head&gt;\n    &lt;body&gt;\n      &lt;h1&gt;{{ .Values.greeting }}&lt;/h1&gt;\n      &lt;p&gt;Release: &lt;strong&gt;{{ .Release.Name }}&lt;/strong&gt; &amp;nbsp;|&amp;nbsp;\n         Revision: &lt;strong&gt;{{ .Release.Revision }}&lt;/strong&gt; &amp;nbsp;|&amp;nbsp;\n         Namespace: &lt;strong&gt;{{ .Release.Namespace }}&lt;/strong&gt;&lt;/p&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n</code></pre>"},{"location":"18-ArgoCD/#0207-update-notestxt","title":"02.07 Update NOTES.txt","text":"<p>Replace <code>my-webserver/templates/NOTES.txt</code>:</p> <pre><code>\ud83c\udf89 {{ .Chart.Name }} v{{ .Chart.Version }} installed as release \"{{ .Release.Name }}\"\n\nQuick access:\n  kubectl port-forward svc/{{ include \"my-webserver.fullname\" . }} 8080:80 -n {{ .Release.Namespace }}\n  Open http://localhost:8080 in your browser\n\nTo check pod status:\n  kubectl get pods -n {{ .Release.Namespace }} -l app.kubernetes.io/instance={{ .Release.Name }}\n</code></pre>"},{"location":"18-ArgoCD/#step-03-validate-the-chart","title":"Step 03 - Validate the Chart","text":"<p>Always validate a chart before committing or installing it.</p>"},{"location":"18-ArgoCD/#0301-lint","title":"03.01 Lint","text":"<pre><code>helm lint my-webserver\n</code></pre> <p>Expected: <code>1 chart(s) linted, 0 chart(s) failed</code></p>"},{"location":"18-ArgoCD/#0302-render-templates-locally-dry-run-without-a-cluster","title":"03.02 Render templates locally (dry-run without a cluster)","text":"<pre><code># Render to stdout and inspect what will be applied\nhelm template my-release my-webserver --namespace my-ns\n\n# Render to a file for closer inspection\nhelm template my-release my-webserver \\\n    --namespace my-ns \\\n    --output-dir /tmp/rendered-my-webserver\n\nls /tmp/rendered-my-webserver/my-webserver/templates/\n</code></pre>"},{"location":"18-ArgoCD/#0303-test-override-values-locally","title":"03.03 Test override values locally","text":"<pre><code># Preview with 3 replicas and a custom greeting\nhelm template my-release my-webserver \\\n    --set replicaCount=3 \\\n    --set greeting=\"GitOps is awesome!\"\n</code></pre> <p>Look for <code>replicas: 3</code> and your custom greeting in the output.</p>"},{"location":"18-ArgoCD/#step-04-install-the-chart-locally-optional-smoke-test","title":"Step 04 - Install the Chart Locally (Optional Smoke Test)","text":"<p>Before hooking things up to ArgoCD, confirm the chart deploys correctly.</p> <pre><code># Install into a dedicated namespace\nhelm upgrade --install my-webserver my-webserver \\\n    --namespace my-webserver \\\n    --create-namespace \\\n    --wait\n\n# Verify pods are running\nkubectl get all -n my-webserver\n\n# Quick curl test via port-forward\nkubectl port-forward svc/my-webserver 8080:80 -n my-webserver &amp;\nsleep 2\ncurl -s http://localhost:8080 | grep 'Hello'\n\n# Stop the port-forward\nkill %1\n</code></pre> <p>If the page shows your greeting, the chart is working. Uninstall before handing over to ArgoCD:</p> <pre><code>helm uninstall my-webserver --namespace my-webserver\nkubectl delete namespace my-webserver\n</code></pre>"},{"location":"18-ArgoCD/#step-05-commit-the-chart-to-git","title":"Step 05 - Commit the Chart to Git","text":"<p>ArgoCD is a pull-based GitOps tool - it watches a Git repository and deploys whatever is there. Your chart must live in a Git repository that ArgoCD can reach.</p>"},{"location":"18-ArgoCD/#0501-recommended-directory-layout-inside-the-repo","title":"05.01 Recommended directory layout inside the repo","text":"<pre><code>my-repo/\n\u251c\u2500\u2500 charts/\n\u2502   \u2514\u2500\u2500 my-webserver/       \u2190 the Helm chart we just created\n\u2502       \u251c\u2500\u2500 Chart.yaml\n\u2502       \u251c\u2500\u2500 values.yaml\n\u2502       \u2514\u2500\u2500 templates/\n\u2514\u2500\u2500 argocd/\n    \u2514\u2500\u2500 my-webserver-app.yaml  \u2190 ArgoCD Application manifest (added in Step 10)\n</code></pre>"},{"location":"18-ArgoCD/#0502-commit-and-push","title":"05.02 Commit and push","text":"<pre><code># From the root of your Git repository\nmkdir -p charts\ncp -r my-webserver charts/\n\ngit add charts/my-webserver/\ngit commit -m \"feat: add my-webserver Helm chart v1.0.0\"\ngit push\n</code></pre> <p>Using this KubernetesLabs repo</p> <p>If you are working inside the <code>KubernetesLabs</code> repository, place your chart under <code>Labs/18-ArgoCD/charts/my-webserver/</code> so it is already reachable via <code>https://github.com/nirgeier/KubernetesLabs</code>.</p>"},{"location":"18-ArgoCD/#step-06-install-argocd-via-helm","title":"Step 06 - Install ArgoCD via Helm","text":"<p>Now we install ArgoCD itself into the cluster.</p>"},{"location":"18-ArgoCD/#0601-add-the-argo-helm-repository","title":"06.01 Add the Argo Helm repository","text":"<pre><code>helm repo add argo https://argoproj.github.io/argo-helm\nhelm repo update argo\n\n# Confirm the chart is available\nhelm search repo argo/argo-cd\n</code></pre>"},{"location":"18-ArgoCD/#0602-install-argocd","title":"06.02 Install ArgoCD","text":"<pre><code>helm upgrade --install argocd argo/argo-cd \\\n    --namespace argocd \\\n    --create-namespace \\\n    --set server.insecure=true \\\n    --wait\n</code></pre> <p><code>--set server.insecure=true</code></p> <p>This disables TLS at the ArgoCD server pod so that a plain HTTP Ingress works without certificate configuration. In production you should terminate TLS at the Ingress instead.</p>"},{"location":"18-ArgoCD/#0603-verify-all-pods-are-running","title":"06.03 Verify all pods are Running","text":"<pre><code>kubectl get pods -n argocd\n</code></pre> <p>Expected (all <code>1/1 Running</code>):</p> <pre><code>NAME                                                READY   STATUS\nargocd-application-controller-0                    1/1     Running\nargocd-applicationset-controller-xxxx              1/1     Running\nargocd-dex-server-xxxx                             1/1     Running\nargocd-notifications-controller-xxxx               1/1     Running\nargocd-redis-xxxx                                  1/1     Running\nargocd-repo-server-xxxx                            1/1     Running\nargocd-server-xxxx                                 1/1     Running\n</code></pre>"},{"location":"18-ArgoCD/#step-07-expose-the-argocd-ui","title":"Step 07 - Expose the ArgoCD UI","text":"<p>Choose one of the methods below depending on your environment.</p>"},{"location":"18-ArgoCD/#option-a-port-forward-simplest-no-ingress-needed","title":"Option A - Port-Forward (simplest, no Ingress needed)","text":"<pre><code>kubectl port-forward svc/argocd-server -n argocd 8080:80 &amp;\necho \"ArgoCD UI \u2192 http://localhost:8080\"\n</code></pre>"},{"location":"18-ArgoCD/#option-b-nginx-ingress-persistent-url","title":"Option B - Nginx Ingress (persistent URL)","text":"<p>Requires Nginx Ingress Controller</p> <p>Install it first if not present: <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \\\n    --namespace ingress-nginx --create-namespace\n</code></pre></p> <p>Apply the Ingress manifest already in this lab:</p> <pre><code>kubectl apply -f manifests/argocd-ingress.yaml\n</code></pre> <p>Add the cluster IP to <code>/etc/hosts</code>:</p> <pre><code>INGRESS_IP=$(kubectl get nodes \\\n    -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n\necho \"${INGRESS_IP}  argocd.local\" | sudo tee -a /etc/hosts\n\necho \"ArgoCD UI \u2192 http://argocd.local\"\n</code></pre>"},{"location":"18-ArgoCD/#step-08-retrieve-the-admin-password","title":"Step 08 - Retrieve the Admin Password","text":"<p>ArgoCD generates a random admin password on first install. Retrieve it with:</p> <pre><code># Decode and print on a single line\nkubectl -n argocd get secret argocd-initial-admin-secret \\\n    -o jsonpath=\"{.data.password}\" | base64 -d; echo\n</code></pre> <p>Save this password</p> <p>Store it somewhere safe before proceeding. You will use it to log in to the Web UI and the CLI.</p>"},{"location":"18-ArgoCD/#step-09-log-in-to-argocd","title":"Step 09 - Log in to ArgoCD","text":""},{"location":"18-ArgoCD/#0901-web-ui","title":"09.01 Web UI","text":"<p>Open the URL from Step 07 in your browser. Username: <code>admin</code> Password: (the password from Step 08)</p>"},{"location":"18-ArgoCD/#0902-argocd-cli","title":"09.02 ArgoCD CLI","text":"<pre><code># Via port-forward (Option A)\nargocd login localhost:8080 \\\n    --username admin \\\n    --password \"$(kubectl -n argocd get secret argocd-initial-admin-secret \\\n        -o jsonpath='{.data.password}' | base64 -d)\" \\\n    --insecure\n\n# Via Ingress (Option B)\nargocd login argocd.local \\\n    --username admin \\\n    --password \"$(kubectl -n argocd get secret argocd-initial-admin-secret \\\n        -o jsonpath='{.data.password}' | base64 -d)\" \\\n    --insecure\n</code></pre>"},{"location":"18-ArgoCD/#0903-recommended-change-the-admin-password","title":"09.03 (Recommended) Change the admin password","text":"<pre><code>argocd account update-password\n</code></pre>"},{"location":"18-ArgoCD/#0904-verify-login","title":"09.04 Verify login","text":"<pre><code>argocd account get-user-info\n# Should show: Logged In: true\n</code></pre>"},{"location":"18-ArgoCD/#step-10-create-an-argocd-application-for-the-helm-chart","title":"Step 10 - Create an ArgoCD Application for the Helm Chart","text":"<p>There are three equivalent ways to create an ArgoCD Application. We will cover all three so you understand what each does.</p>"},{"location":"18-ArgoCD/#method-a-argocd-cli","title":"Method A - ArgoCD CLI","text":"<pre><code>argocd app create my-webserver \\\n    --repo https://github.com/nirgeier/KubernetesLabs.git \\\n    --path Labs/18-ArgoCD/charts/my-webserver \\\n    --dest-server https://kubernetes.default.svc \\\n    --dest-namespace my-webserver \\\n    --helm-set replicaCount=2 \\\n    --sync-policy automated \\\n    --auto-prune \\\n    --self-heal \\\n    --sync-option CreateNamespace=true\n</code></pre> Flag What it does <code>--repo</code> Git repository URL <code>--path</code> Path inside the repo where the chart lives <code>--dest-namespace</code> The Kubernetes namespace to deploy into <code>--helm-set</code> Override a chart value (same as <code>--set</code> in <code>helm install</code>) <code>--sync-policy automated</code> ArgoCD will automatically apply every Git change <code>--auto-prune</code> Delete resources removed from Git <code>--self-heal</code> Restore any manual cluster changes back to Git state <code>--sync-option CreateNamespace=true</code> Create the namespace if it does not exist"},{"location":"18-ArgoCD/#method-b-kubernetes-manifest","title":"Method B - Kubernetes Manifest","text":"<p>Prefer this method</p> <p>Manifests are version-controlled, repeatable, and fit perfectly into the App of Apps pattern.</p> <p>Create <code>argocd/my-webserver-app.yaml</code> in your repo with:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-webserver\n  namespace: argocd\n  # Ensures child resources are deleted when this Application is deleted\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/nirgeier/KubernetesLabs.git\n    targetRevision: HEAD          # Track the default branch\n    path: Labs/18-ArgoCD/charts/my-webserver\n    helm:\n      # Override chart values directly in the Application manifest\n      values: |\n        replicaCount: 2\n        greeting: \"Deployed by ArgoCD!\"\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: my-webserver\n  syncPolicy:\n    automated:\n      prune: true       # Remove resources deleted from Git\n      selfHeal: true    # Revert manual cluster changes\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <p>Apply it:</p> <pre><code>kubectl apply -f argocd/my-webserver-app.yaml\n</code></pre>"},{"location":"18-ArgoCD/#method-c-argocd-web-ui","title":"Method C - ArgoCD Web UI","text":"<ol> <li>Click \u201d+ NEW APP\u201d in the top-left of the UI.</li> <li>Fill in:</li> <li>Application Name: <code>my-webserver</code></li> <li>Project: <code>default</code></li> <li>Sync Policy: <code>Automatic</code>  \u2611 Prune Resources  \u2611 Self Heal</li> <li>Source:</li> <li>Repository URL: <code>https://github.com/nirgeier/KubernetesLabs.git</code></li> <li>Revision: <code>HEAD</code></li> <li>Path: <code>Labs/18-ArgoCD/charts/my-webserver</code></li> <li>Destination:</li> <li>Cluster URL: <code>https://kubernetes.default.svc</code></li> <li>Namespace: <code>my-webserver</code></li> <li>Helm section expands automatically since ArgoCD detects the chart.    Add an override: <code>replicaCount</code> = <code>2</code></li> <li>Click CREATE.</li> </ol>"},{"location":"18-ArgoCD/#step-11-watch-the-sync-and-verify-the-deployment","title":"Step 11 - Watch the Sync and Verify the Deployment","text":""},{"location":"18-ArgoCD/#1101-monitor-sync-via-cli","title":"11.01 Monitor sync via CLI","text":"<pre><code># Watch until status shows Synced + Healthy\nwatch argocd app get my-webserver\n</code></pre> <p>Expected output:</p> <pre><code>Name:               argocd/my-webserver\nProject:            default\nSync Status:        Synced\nHealth Status:      Healthy\n\nGROUP  KIND        NAMESPACE     NAME                    STATUS   HEALTH\n       Namespace   my-webserver  my-webserver            Synced   Healthy\n       ConfigMap   my-webserver  my-webserver-html       Synced   Healthy\napps   Deployment  my-webserver  my-webserver            Synced   Healthy\n       Service     my-webserver  my-webserver            Synced   Healthy\n</code></pre>"},{"location":"18-ArgoCD/#1102-verify-kubernetes-resources","title":"11.02 Verify Kubernetes resources","text":"<pre><code>kubectl get all -n my-webserver\n</code></pre> <p>Expected:</p> <pre><code>NAME                               READY   STATUS    RESTARTS\npod/my-webserver-xxxx              1/1     Running   0\npod/my-webserver-xxxx              1/1     Running   0\n\nNAME                   TYPE        CLUSTER-IP   PORT(S)\nservice/my-webserver   ClusterIP   10.x.x.x     80/TCP\n\nNAME                           READY   UP-TO-DATE   AVAILABLE\ndeployment.apps/my-webserver   2/2     2            2\n</code></pre>"},{"location":"18-ArgoCD/#1103-test-the-application","title":"11.03 Test the application","text":"<pre><code>kubectl port-forward svc/my-webserver 8080:80 -n my-webserver &amp;\nsleep 2\ncurl -s http://localhost:8080 | grep 'Deployed by ArgoCD'\n\n# Stop port-forward\nkill %1\n</code></pre>"},{"location":"18-ArgoCD/#step-12-gitops-in-action-make-a-change-via-git","title":"Step 12 - GitOps in Action: Make a Change via Git","text":"<p>This is the key GitOps moment - you never run <code>kubectl</code> or <code>helm upgrade</code>. Instead, you push a change to Git and ArgoCD applies it automatically.</p>"},{"location":"18-ArgoCD/#1201-update-the-chart-values-in-git","title":"12.01 Update the chart values in Git","text":"<p>Open <code>charts/my-webserver/values.yaml</code> (in your repo) and change:</p> <pre><code># Before\nreplicaCount: 1\ngreeting: \"Hello from my Helm chart + ArgoCD!\"\n\n# After\nreplicaCount: 3\ngreeting: \"Updated via GitOps - no kubectl needed!\"\n</code></pre> <p>Commit and push:</p> <pre><code>git add charts/my-webserver/values.yaml\ngit commit -m \"feat: scale to 3 replicas and update greeting\"\ngit push\n</code></pre>"},{"location":"18-ArgoCD/#1202-watch-argocd-detect-and-apply-the-change","title":"12.02 Watch ArgoCD detect and apply the change","text":"<pre><code># ArgoCD polls Git every 3 minutes by default.\n# You can trigger an immediate refresh:\nargocd app get my-webserver --refresh\n\n# Then watch the sync happen:\nwatch argocd app get my-webserver\n</code></pre> <p>Within seconds of the refresh ArgoCD will: 1. Detect the diff between Git (3 replicas) and the cluster (2 replicas) 2. Apply the updated Deployment 3. Report <code>Synced</code> + <code>Healthy</code> once the 3rd pod is running</p>"},{"location":"18-ArgoCD/#1203-verify-the-change","title":"12.03 Verify the change","text":"<pre><code># Should show 3/3 ready\nkubectl get deployment my-webserver -n my-webserver\n\n# Test the new greeting\nkubectl port-forward svc/my-webserver 8080:80 -n my-webserver &amp;\nsleep 2\ncurl -s http://localhost:8080 | grep 'GitOps'\nkill %1\n</code></pre>"},{"location":"18-ArgoCD/#1204-test-self-healing","title":"12.04 Test Self-Healing","text":"<p>ArgoCD\u2019s self-heal feature will restore any manual change that diverges from Git state.</p> <pre><code># Manually scale to 1 replica (simulating an accidental change)\nkubectl scale deployment my-webserver --replicas=1 -n my-webserver\n\n# ArgoCD immediately detects the drift\nargocd app get my-webserver --refresh\n\n# Within ~15-30 seconds ArgoCD restores 3 replicas\nwatch kubectl get pods -n my-webserver\n</code></pre> <p>You should see the replicas jump back from 1 \u2192 3 automatically.</p>"},{"location":"18-ArgoCD/#step-13-bump-the-chart-version-and-upgrade","title":"Step 13 - Bump the Chart Version and Upgrade","text":"<p>When you change the chart templates themselves (not just values), bump the chart version.</p>"},{"location":"18-ArgoCD/#1301-add-a-new-label-to-all-resources","title":"13.01 Add a new label to all resources","text":"<p>Open <code>my-webserver/templates/_helpers.tpl</code> and add a <code>environment</code> label to the <code>my-webserver.labels</code> template:</p> <pre><code>{{/*\nCommon labels\n*/}}\n{{- define \"my-webserver.labels\" -}}\nhelm.sh/chart: {{ include \"my-webserver.chart\" . }}\n{{ include \"my-webserver.selectorLabels\" . }}\napp.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\napp.kubernetes.io/managed-by: {{ .Release.Service }}\nenvironment: {{ .Values.environment | default \"dev\" }}\n{{- end }}\n</code></pre>"},{"location":"18-ArgoCD/#1302-add-environment-to-valuesyaml","title":"13.02 Add <code>environment</code> to <code>values.yaml</code>","text":"<pre><code>environment: \"production\"\n</code></pre>"},{"location":"18-ArgoCD/#1303-bump-the-chart-version-in-chartyaml","title":"13.03 Bump the chart version in <code>Chart.yaml</code>","text":"<pre><code>version: 1.1.0\n</code></pre>"},{"location":"18-ArgoCD/#1304-commit-push-and-let-argocd-upgrade","title":"13.04 Commit, push, and let ArgoCD upgrade","text":"<pre><code>git add charts/my-webserver/\ngit commit -m \"feat: add environment label, bump chart to v1.1.0\"\ngit push\n</code></pre> <p>ArgoCD detects the new chart version and runs a rolling upgrade.</p>"},{"location":"18-ArgoCD/#1305-check-the-helm-history-argocd-tracks","title":"13.05 Check the Helm history ArgoCD tracks","text":"<pre><code># ArgoCD tracks revisions in its own history\nargocd app history my-webserver\n</code></pre>"},{"location":"18-ArgoCD/#step-14-rollback-via-argocd","title":"Step 14 - Rollback via ArgoCD","text":"<p>If a deployment breaks production you can roll back to any previous revision in seconds.</p> <pre><code># 1. See what revisions are available\nargocd app history my-webserver\n\n# Output example:\n# ID  DATE        REVISION  INITIATOR\n# 1   2026-02-01  abc1234   automated\n# 2   2026-02-10  def5678   automated\n# 3   2026-02-22  fed9876   automated\n\n# 2. Roll back to revision 2\nargocd app rollback my-webserver 2\n\n# 3. Verify the rollback\nargocd app get my-webserver\nkubectl get deployment my-webserver -n my-webserver -o wide\n</code></pre> <p>Rollback and Auto-Sync</p> <p>Rollback disables automated sync to prevent ArgoCD from immediately re-applying the newer Git state. Re-enable it when you are ready: <pre><code>argocd app set my-webserver --sync-policy automated --self-heal\n</code></pre></p>"},{"location":"18-ArgoCD/#step-15-app-of-apps-managing-multiple-helm-charts-declaratively","title":"Step 15 - App of Apps: Managing Multiple Helm Charts Declaratively","text":"<p>Once you have more than one application, use the App of Apps pattern so every child application is itself version-controlled and managed by ArgoCD.</p>"},{"location":"18-ArgoCD/#1501-create-a-second-chart-optional","title":"15.01 Create a second chart (optional)","text":"<pre><code># Clone or duplicate my-webserver as my-api\ncp -r charts/my-webserver charts/my-api\nsed -i '' 's/my-webserver/my-api/g' charts/my-api/Chart.yaml\nsed -i '' 's/my-webserver/my-api/g' charts/my-api/values.yaml\n</code></pre>"},{"location":"18-ArgoCD/#1502-create-application-manifests-for-each-service","title":"15.02 Create Application manifests for each service","text":"<p>Create <code>argocd/apps/my-webserver-app.yaml</code>:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-webserver\n  namespace: argocd\n  annotations:\n    # Deploy only after a wave-0 infrastructure app is healthy (if needed)\n    argocd.argoproj.io/sync-wave: \"1\"\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/nirgeier/KubernetesLabs.git\n    targetRevision: HEAD\n    path: Labs/18-ArgoCD/charts/my-webserver\n    helm:\n      values: |\n        replicaCount: 2\n        greeting: \"Front-end service\"\n        environment: \"production\"\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: my-webserver\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <p>Create <code>argocd/apps/my-api-app.yaml</code>:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-api\n  namespace: argocd\n  annotations:\n    argocd.argoproj.io/sync-wave: \"1\"\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/nirgeier/KubernetesLabs.git\n    targetRevision: HEAD\n    path: Labs/18-ArgoCD/charts/my-api\n    helm:\n      values: |\n        replicaCount: 1\n        greeting: \"API service\"\n        environment: \"production\"\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: my-api\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n</code></pre>"},{"location":"18-ArgoCD/#1503-create-the-root-app-of-apps-manifest","title":"15.03 Create the root App of Apps manifest","text":"<p>Create <code>argocd/app-of-apps.yaml</code>:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: app-of-apps\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/nirgeier/KubernetesLabs.git\n    targetRevision: HEAD\n    # ArgoCD will read EVERY .yaml file in this directory\n    # and create an Application resource for each one\n    path: Labs/18-ArgoCD/argocd/apps\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: argocd          # child Application CRs live in argocd ns\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n</code></pre>"},{"location":"18-ArgoCD/#1504-commit-everything-and-bootstrap","title":"15.04 Commit everything and bootstrap","text":"<pre><code>git add argocd/\ngit commit -m \"feat: add App of Apps with my-webserver and my-api\"\ngit push\n</code></pre> <p>Apply only the root application - ArgoCD takes care of the rest:</p> <pre><code>kubectl apply -f argocd/app-of-apps.yaml\n</code></pre>"},{"location":"18-ArgoCD/#1505-watch-all-applications-appear","title":"15.05 Watch all applications appear","text":"<pre><code>watch argocd app list\n</code></pre> <p>Expected:</p> <pre><code>NAME           CLUSTER     NAMESPACE     STATUS  HEALTH   SYNCPOLICY\napp-of-apps    in-cluster  argocd        Synced  Healthy  Auto-Prune\nmy-api         in-cluster  my-api        Synced  Healthy  Auto-Prune\nmy-webserver   in-cluster  my-webserver  Synced  Healthy  Auto-Prune\n</code></pre>"},{"location":"18-ArgoCD/#1506-add-a-new-application-zero-extra-operators-needed","title":"15.06 Add a new application - zero extra operators needed","text":"<p>From now on, adding any new Helm chart is just:</p> <ol> <li>Add the chart under <code>charts/</code></li> <li>Add an <code>Application</code> manifest under <code>argocd/apps/</code></li> <li><code>git push</code></li> </ol> <p>ArgoCD automatically detects the new file and deploys it. No <code>helm install</code>, no <code>kubectl apply</code> needed.</p>"},{"location":"18-ArgoCD/#quick-reference-helm-argocd-workflow-cheatsheet","title":"Quick Reference: Helm + ArgoCD Workflow Cheatsheet","text":"Goal Command Create chart skeleton <code>helm create &lt;name&gt;</code> Render templates locally <code>helm template &lt;release&gt; &lt;chart&gt;</code> Validate chart <code>helm lint &lt;chart&gt;</code> Preview upgrade diff <code>argocd app diff &lt;app&gt;</code> Trigger immediate sync <code>argocd app sync &lt;app&gt;</code> Watch sync status <code>watch argocd app get &lt;app&gt;</code> List all apps <code>argocd app list</code> Roll back to revision N <code>argocd app rollback &lt;app&gt; &lt;N&gt;</code> Show deployment history <code>argocd app history &lt;app&gt;</code> Force refresh from Git <code>argocd app get &lt;app&gt; --refresh</code> Pause auto-sync <code>argocd app set &lt;app&gt; --sync-policy none</code> Resume auto-sync <code>argocd app set &lt;app&gt; --sync-policy automated --self-heal</code> Delete app + resources <code>argocd app delete &lt;app&gt;</code>"},{"location":"18-ArgoCD/#part-12-cleanup","title":"Part 12 Cleanup","text":"<pre><code># Delete all managed applications\nargocd app delete app-of-apps --cascade         # removes all child apps too\nargocd app delete my-webserver --cascade\nargocd app delete my-api --cascade\n\n# Remove the namespaces\nkubectl delete namespace my-webserver my-api\n\n# Uninstall ArgoCD itself\nhelm uninstall argocd --namespace argocd\nkubectl delete namespace argocd\n</code></pre>"},{"location":"19-CustomScheduler/","title":"Writing a Custom Scheduler","text":"<ul> <li><code>Scheduling</code> is the process of selecting a node for a pod to run on.</li> <li>In this lab we will write our own pods <code>scheduler</code>.</li> <li>It is probably not something that you will ever need to do, but still it\u2019s a good practice to understand how scheduling works in K8S and how you can extend it.</li> </ul>"},{"location":"19-CustomScheduler/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>How scheduling works in Kubernetes</li> <li>How to write a custom scheduler</li> <li>How to assign a pod to a specific scheduler using <code>.spec.schedulerName</code></li> <li>How scheduling profiles and extension points work</li> </ul>"},{"location":"19-CustomScheduler/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> <li>Docker installed (for building custom images)</li> </ul>"},{"location":"19-CustomScheduler/#introduction","title":"Introduction","text":"<ul> <li>See further information in the official documentation: Scheduler Configuration</li> <li>To schedule a given pod using a specific scheduler, specify the name of the scheduler in that specification <code>.spec.schedulerName</code>.</li> <li>Scheduling happens in a series of stages that are exposed through extension points.</li> <li>We can define several scheduling Profile. A scheduling Profile allows you to configure the different stages of scheduling in the <code>kube-scheduler</code></li> </ul>"},{"location":"19-CustomScheduler/#sample-kubeschedulerconfiguration","title":"Sample <code>KubeSchedulerConfiguration</code>","text":"<pre><code>###\n# Sample KubeSchedulerConfiguration\n###\n#\n# You can configure `kube-scheduler` to run more than one profile.\n# Each profile has an associated scheduler name and can have a different\n# set of plugins configured in its extension points.\n\n# With the following sample configuration,\n# the scheduler will run with two profiles:\n# - default plugins\n# - all scoring plugins disabled\n\napiVersion: kubescheduler.config.k8s.io/v1beta1\nkind: KubeSchedulerConfiguration\nprofiles:\n  - schedulerName: default-scheduler\n  - schedulerName: no-scoring-scheduler\n    plugins:\n      preScore:\n        disabled:\n        - name: '*'\n      score:\n        disabled:\n        - name: '*'\n</code></pre> <ul> <li>Once you have your scheduler code, you can use it in your pod scheduler:</li> </ul> <pre><code># In this sample we use deployment but it will apply to any pod\n...\napiVersion: apps/v1\nkind: Deployment\nspec:\n    spec:\n      # This is the import part of this file.\n      # Here we define our custom scheduler\n      schedulerName: CodeWizardScheduler # &lt;------\n      containers:\n      - name: nginx\n        image: nginx\n</code></pre>"},{"location":"19-CustomScheduler/#sample-bash-scheduler","title":"Sample Bash Scheduler","text":"<ul> <li>The \u201ctrick\u201d is loop over all the waiting pods and search for the custom scheduler match in <code>spec.schedulerName</code></li> </ul> <pre><code>...\n  # Get a list of all our pods in pending state\n  for POD in $(kubectl  get pods \\\n                        --server ${CLUSTER_URL} \\\n                        --all-namespaces \\\n                        --output jsonpath='{.items..metadata.name}' \\\n                        --field-selector=status.phase==Pending);\n    do\n\n    # Get the desired schedulerName if th epod has defined any schedulerName\n    CUSTOM_SCHEDULER_NAME=$(kubectl get pod ${POD} \\\n                                    --output jsonpath='{.spec.schedulerName}')\n\n    # Check if the desired schedulerName is our custome one\n    # If its a match this is where our custom scheduler will \"jump in\"\n    if [ \"${CUSTOM_SCHEDULER_NAME}\" == \"${CUSTOM_SCHEDULER}\" ];\n      then\n        # Do your magic here ......\n        # Schedule the PODS as you wish\n    fi\n    ...\n</code></pre>"},{"location":"20-CronJob/","title":"CronJobs","text":"<ul> <li>In this lab, we will learn how to create and manage <code>CronJobs</code> in Kubernetes.</li> <li>A <code>CronJob</code> creates <code>Jobs</code> on a time-based schedule. It is useful for running periodic and recurring tasks, such as backups or report generation.</li> </ul>"},{"location":"20-CronJob/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What CronJobs are and how they work in Kubernetes</li> <li>How to create, monitor, and manage CronJobs</li> <li>How to view Job and Pod outputs from scheduled tasks</li> </ul>"},{"location":"20-CronJob/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> </ul>"},{"location":"20-CronJob/#introduction","title":"Introduction","text":"<ul> <li>A <code>CronJob</code> in Kubernetes runs Jobs on a time-based schedule, similar to Linux cron.</li> <li>Useful for periodic tasks like backups, reports, or cleanup.</li> </ul>"},{"location":"20-CronJob/#step-01-create-a-cronjob-yaml","title":"Step 01 - Create a CronJob YAML","text":"<ul> <li>Create a file named <code>hello-cronjob.yaml</code> with the following content:</li> </ul> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\n  namespace: default\nspec:\n  schedule: \"*/1 * * * *\" # Every 1 minute\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox\n            args:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes CronJob!\n          restartPolicy: OnFailure\n</code></pre>"},{"location":"20-CronJob/#step-02-apply-the-cronjob","title":"Step 02 - Apply the CronJob","text":"<pre><code>kubectl apply -f hello-cronjob.yaml\n</code></pre>"},{"location":"20-CronJob/#step-03-verify-cronjob-creation","title":"Step 03 - Verify CronJob Creation","text":"<pre><code>kubectl get cronjob hello\n</code></pre>"},{"location":"20-CronJob/#step-04-check-cronjob-and-jobs","title":"Step 04 - Check CronJob and Jobs","text":"<ul> <li>List CronJobs:</li> </ul> <pre><code>kubectl get cronjobs\n</code></pre> <ul> <li>List Jobs created by the CronJob:</li> </ul> <pre><code>kubectl get jobs\n</code></pre> <ul> <li>List Pods created by Jobs:</li> </ul> <pre><code>kubectl get pods\n</code></pre>"},{"location":"20-CronJob/#step-05-view-job-output","title":"Step 05 - View Job Output","text":"<ul> <li>Get the name of a pod created by the CronJob, then view its logs:</li> </ul> <pre><code>kubectl logs &lt;pod-name&gt;\n</code></pre> <p>Example output:</p> <pre><code>Mon Nov 10 12:00:00 UTC 2025\nHello from the Kubernetes CronJob!\n</code></pre>"},{"location":"20-CronJob/#cleanup","title":"Cleanup","text":"<pre><code>kubectl delete cronjob hello\nkubectl delete jobs --all\n</code></pre>"},{"location":"20-CronJob/#questions","title":"Questions","text":"<ul> <li>What happens if the job takes longer than the schedule interval?</li> <li>How would you change the schedule to run every 5 minutes?</li> <li>How can you limit the number of successful or failed jobs to keep?</li> </ul>"},{"location":"21-KubeAPI/","title":"Kube API Access from Pod","text":"<ul> <li>In this lab, we will learn how to access the Kubernetes API from within a Pod.</li> <li>We will create a simple Pod that runs a script to query the Kubernetes API server and retrieve information about the cluster.</li> </ul>"},{"location":"21-KubeAPI/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>How to access the Kubernetes API from within a Pod</li> <li>How Kubernetes Service Account tokens are mounted inside pods</li> <li>How to build a custom Docker image for API access</li> <li>How to deploy and test the API query using Kustomize</li> </ul>"},{"location":"21-KubeAPI/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> <li>Docker installed (for building custom images)</li> </ul>"},{"location":"21-KubeAPI/#part-01-build-the-docker-image","title":"Part 01 - Build the Docker Image","text":"<ul> <li>In order to demonstrate the API query we will build a custom docker image.</li> <li>It is optional to use the pre-build image and skip this step.</li> </ul>"},{"location":"21-KubeAPI/#step-01-the-script-for-querying-k8s-api","title":"Step 01 - The Script for Querying K8S API","text":"<ul> <li>In order to be able to access K8S API from within a pod, we will be using the following script:</li> </ul> <pre><code># `api_query.sh`\n\n#!/bin/sh\n\n#################################\n## Access the internal K8S API ##\n#################################\n# Point to the internal API server hostname\nAPI_SERVER_URL=https://kubernetes.default.svc\n\n# Path to ServiceAccount token\n# The service account is mapped by the K8S Api server in the pods\nSERVICE_ACCOUNT_FOLDER=/var/run/secrets/kubernetes.io/serviceaccount\n\n# Read this Pod's namespace if required\n# NAMESPACE=$(cat ${SERVICE_ACCOUNT_FOLDER}/namespace)\n\n# Read the ServiceAccount bearer token\nTOKEN=$(cat ${SERVICE_ACCOUNT_FOLDER}/token)\n\n# Reference the internal certificate authority (CA)\nCACERT=${SERVICE_ACCOUNT_FOLDER}/ca.crt\n\n# Explore the API with TOKEN and the Certificate\ncurl --cacert ${CACERT} --header \"Authorization: Bearer ${TOKEN}\" -X GET ${API_SERVER_URL}/api\n</code></pre>"},{"location":"21-KubeAPI/#step-02-build-the-docker-image","title":"Step 02 - Build the Docker Image","text":"<ul> <li>For the pod image we will use the following Dockerfile:</li> </ul> <pre><code># `Dockerfile`\n\nFROM    alpine\n\n# Update and install dependencies\nRUN     apk add --update nodejs npm curl\n\n# Copy the endpoint script\nCOPY    api_query.sh .\n\n# Set the execution bit\nRUN     chmod +x api_query.sh .\n</code></pre>"},{"location":"21-KubeAPI/#part-02-deploy-the-pod-to-k8s","title":"Part 02 - Deploy the Pod to K8S","text":"<ul> <li>Once the image is ready, we can deploy it as a pod to the cluster.</li> <li>The required resources are under the k8s folder.</li> </ul>"},{"location":"21-KubeAPI/#step-01-run-kustomization-to-deploy","title":"Step 01 - Run Kustomization to Deploy","text":"<ul> <li>Deploy to the cluster</li> </ul> <pre><code># Remove old content if any\nkubectl kustomize k8s | kubectl delete -f -\n\n# Deploy the content\nkubectl kustomize k8s | kubectl apply -f -\n</code></pre>"},{"location":"21-KubeAPI/#step-02-query-the-k8s-api","title":"Step 02 - Query the K8S API","text":"<ul> <li>Run the following script to verify that the connection to the API is working:</li> </ul> <pre><code># Get the deployment pod name\nPOD_NAME=$(kubectl get pod -A -l app=monitor-app -o jsonpath=\"{.items[0].metadata.name}\")\n\n# Print out the logs to verify that the pods is connected to the API\nkubectl exec -it -n codewizard $POD_NAME sh ./api_query.sh\n</code></pre>"},{"location":"23-HelmOperator/","title":"Helm Operator","text":"<ul> <li>An in-depth Helm-based operator tutorial.</li> <li>The <code>Helm Operator</code> is a Kubernetes operator, allowing one to declaratively manage Helm chart releases.</li> </ul>"},{"location":"23-HelmOperator/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What the Helm Operator is and how it works</li> <li>How to create a Helm-based operator using <code>operator-sdk</code></li> <li>How to customize the operator logic and deploy it to a cluster</li> <li>How to manage Helm chart releases declaratively through Custom Resources</li> </ul>"},{"location":"23-HelmOperator/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> <li>Docker installed (for building operator images)</li> <li><code>operator-sdk</code> CLI installed (steps below)</li> </ul>"},{"location":"23-HelmOperator/#install-operator-sdk","title":"Install <code>operator-sdk</code>","text":"<pre><code># Grab the ARCH and OS\nexport ARCH=$(case $(uname -m) in x86_64) echo -n amd64 ;; aarch64) echo -n arm64 ;; *) echo -n $(uname -m) ;; esac)\nexport OS=$(uname | awk '{print tolower($0)}')\n\n# Get the desired download URL\nexport OPERATOR_SDK_DL_URL=https://github.com/operator-framework/operator-sdk/releases/download/v1.23.0\n\n# Download the Operator binaries\ncurl -LO ${OPERATOR_SDK_DL_URL}/operator-sdk_${OS}_${ARCH}\n\n# Install the release binary in your PATH\nchmod +x operator-sdk_${OS}_${ARCH} &amp;&amp; sudo mv operator-sdk_${OS}_${ARCH} /usr/local/bin/operator-sdk\n</code></pre>"},{"location":"23-HelmOperator/#step-01-create-a-new-project","title":"Step 01 - Create a new project","text":"<ul> <li>Use the CLI to create a new Helm-based nginx-operator project:</li> </ul> <pre><code># Create the desired folder\nmkdir nginx-operator\n\n# Switch to the desired folder\ncd nginx-operator\n\n# Create the helm operator\noperator-sdk                \\\n        init                \\\n        --kind    Nginx     \\\n        --group   demo      \\\n        --plugins helm      \\\n        --version v1alpha1  \\\n        --domain  codewizard.co.il\n</code></pre> <ul> <li>This creates the <code>nginx-operator</code> project specifically for watching the <code>Nginx</code> resource with APIVersion <code>demo.codewizard.co.il/v1alpha1</code> and Kind <code>Nginx</code>.</li> </ul>"},{"location":"23-HelmOperator/#operator-sdk-project-layout","title":"Operator SDK Project Layout","text":"<ul> <li>The command will generate the following structure:</li> </ul> <pre><code>.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 PROJECT\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 crd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bases\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 demo.codewizard.co.il_nginxes.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 default\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 manager_auth_proxy_patch.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 manager_config_patch.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 manager\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 controller_manager_config.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 manager.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 prometheus\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 monitor.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 rbac\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 auth_proxy_client_clusterrole.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 auth_proxy_role.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 auth_proxy_role_binding.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 auth_proxy_service.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 leader_election_role.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 leader_election_role_binding.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 nginx_editor_role.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 nginx_viewer_role.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 role.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 role_binding.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 service_account.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samples\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 demo_v1alpha1_nginx.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 scorecard\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 bases\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 config.yaml\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 patches\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 basic.config.yaml\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 olm.config.yaml\n\u251c\u2500\u2500 helm-charts\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 nginx\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 Chart.yaml\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 NOTES.txt\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 _helpers.tpl\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 deployment.yaml\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hpa.yaml\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 ingress.yaml\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 service.yaml\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 serviceaccount.yaml\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 test-connection.yaml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 values.yaml\n\u251c\u2500\u2500 tree.txt\n\u2514\u2500\u2500 watches.yaml\n\n16 directories, 44 files\n</code></pre>"},{"location":"23-HelmOperator/#step-02-customize-the-operator-logic","title":"Step 02 - Customize the operator logic","text":"<ul> <li>For this example the nginx-operator will execute the following reconciliation logic for each Nginx Custom Resource (CR):</li> <li>Create an nginx Deployment, if it doesn\u2019t exist.</li> <li>Create an nginx Service, if it doesn\u2019t exist.</li> <li>Create an nginx Ingress, if it is enabled and doesn\u2019t exist.</li> <li>Update the Deployment, Service, and Ingress, if they already exist but don\u2019t match the desired configuration as specified by the Nginx CR.</li> <li>Ensure that the Deployment, Service, and optional Ingress all match the desired configuration (e.g. replica count, image, service type, etc) as specified by the Nginx CR.</li> </ul>"},{"location":"23-HelmOperator/#watch-the-nginx-cr","title":"Watch the Nginx CR","text":"<ul> <li>By default, the Nginx-operator watches Nginx resource events as shown in <code>watches.yaml</code> and executes Helm releases using the specified chart:</li> </ul> <pre><code># Use the 'create api' subcommand to add watches to this file.\n- group: demo\n  version: v1alpha1\n  kind: Nginx\n  chart: helm-charts/nginx\n</code></pre>"},{"location":"23-HelmOperator/#reviewing-the-nginx-helm-chart","title":"Reviewing the Nginx Helm Chart","text":"<ul> <li> <p>When a Helm operator project is created, the SDK creates an example Helm chart that contains a set of templates for a simple Nginx release.</p> </li> <li> <p>For this example, we have templates for deployment, service, and ingress resources, along with a <code>NOTES.txt</code> template, which Helm chart developers use to convey helpful information about a release.</p> </li> </ul> <p></p>"},{"location":"23-HelmOperator/#understanding-the-nginx-cr-spec","title":"Understanding the Nginx CR spec","text":"<ul> <li> <p>Helm uses a concept called <code>values</code> to provide customizations to a Helm chart\u2019s defaults, which are defined in the Helm chart\u2019s <code>values.yaml</code> file.</p> </li> <li> <p>Overriding these defaults is as simple as setting the desired values in the CR spec.</p> </li> <li> <p>Let\u2019s use the number of replicas value as an example.</p> </li> <li> <p>First, inspecting <code>helm-charts/nginx/values.yaml</code>, we can see that the chart has a value called <code>replicaCount</code> and it is set to <code>1</code> by default.</p> </li> <li> <p>Let\u2019s update the value to 3 - <code>replicaCount: 3</code>.</p> </li> </ul> <pre><code># Update `config/samples/demo_v1alpha1_nginx.yaml` to look like the following:\napiVersion: demo.codewizard.co.il/v1alpha1\nkind: Nginx\nmetadata:\n  name: nginx-sample\nspec:\n  #... (Around line 33)\n  replicaCount: 3 # &lt;------- Adding our replicas count\n</code></pre> <ul> <li>Similarly, we see that the default service port is set to <code>80</code>, but we would like to use <code>8888</code>, so we will again update config/samples/demo_v1alpha1_nginx.yaml by adding the service port override.</li> </ul> <pre><code># Update `config/samples/demo_v1alpha1_nginx.yaml` to look like the following:\napiVersion: demo.codewizard.co.il/v1alpha1\nkind: Nginx\nmetadata:\n  name: nginx-sample\nspec:\n  #... (Around line 36)\n  service:\n    port: 8888 # &lt;------- Updating our service port\n</code></pre>"},{"location":"23-HelmOperator/#step-03-build-the-operators-image","title":"Step 03 - Build the operator\u2019s image","text":"<pre><code># Login to your DockerHub / acr / ecr or any other registry account\n\n# Set the desired image name and tag\n\n# In the Makefile update the following line\n# Image URL to use all building/pushing image targets\nIMG ?= controller:latest\n\n# change it to your registry account\nIMG ?= nirgeier/helm_operator:latest\n</code></pre> <ul> <li>Now let\u2019s build and push the image:</li> </ul> <pre><code>make docker-build docker-push\n</code></pre>"},{"location":"23-HelmOperator/#step-04-deploy-the-operator-to-the-cluster","title":"Step 04 - Deploy the operator to the cluster","text":"<pre><code>make deploy\n\n# Verify that the operator is deployed\nkubectl get deployment -n nginx-operator-system\n</code></pre>"},{"location":"23-HelmOperator/#step-05-create-the-custom-nginx","title":"Step 05 - Create the custom Nginx","text":"<pre><code># Deploy the custom nginx we created earlier\nkubectl apply -f config/samples/demo_v1alpha1_nginx.yaml\n\n# Ensure that the nginx-operator created\nkubectl get deployment | grep nginx-sample\n\n# Check that we have 3 replicas as defined earlier\nkubectl get pods | grep nginx-sample\n\n# Check that the port is set to 8888\nkubectl get svc | grep nginx-sample\n</code></pre>"},{"location":"23-HelmOperator/#step-06-check-the-operator-logic","title":"Step 06 - Check the operator logic","text":"<pre><code># Update the replicaCount and remove the port\n# Once we update the yaml we will check that the operator is working\n# and updating the desired values\n\n# Update the replicaCount in `config/samples/demo_v1alpha1_nginx.yaml`\nreplicaCount: 5\n\n# Remark the service section in the yaml file\n# We wish to see that the operator will use the default values\n36   #service:\n37   #  port: 8888\n38   #  type: ClusterIP\n</code></pre> <ul> <li>Apply the changes:</li> </ul> <pre><code># Apply the changes\nkubectl apply -f config/samples/demo_v1alpha1_nginx.yaml\n</code></pre> <ul> <li>Check to see that the operator is working as expected:</li> </ul> <pre><code># Ensure that the nginx-operator still running\nkubectl get deployment | grep nginx-sample\n\n# Deploy the custom nginx we created earlier\nkubectl apply -f config/samples/demo_v1alpha1_nginx.yaml\n\n# Check that we have 5 replicas as defined earlier\nkubectl get pods | grep nginx-sample\n\n# Check that the port is set back to its default (80)\nkubectl get svc | grep nginx-sample\n</code></pre>"},{"location":"23-HelmOperator/#step-07-logging-debugging","title":"Step 07 - Logging / Debugging","text":"<ul> <li>We can view the operator\u2019s logs using the following command:</li> </ul> <pre><code># View the operator logs\nkubectl logs deployment.apps/nginx-operator-controller-manager  -n nginx-operator-system -c manager\n</code></pre> <ul> <li>Review the CR status and events:</li> </ul> <pre><code>kubectl describe nginxes.demo.codewizard.co.il\n</code></pre>"},{"location":"24-kubebuilder/","title":"Kubebuilder - Building Kubernetes Operators","text":"<ul> <li><code>Kubebuilder</code> is an SDK for building production-grade Kubernetes APIs and controllers (Operators) using Go and the <code>controller-runtime</code> library.</li> <li>Instead of writing low-level machinery by hand, <code>Kubebuilder</code> scaffolds everything - CRD types, RBAC manifests, Makefile targets, and the reconcile loop - so you can focus on business logic.</li> <li>In this lab we build a real WebApp Operator that manages a <code>WebApp</code> custom resource and automatically provisions the correct <code>Deployment</code>, <code>Service</code>, and <code>ConfigMap</code> in the cluster.</li> </ul>"},{"location":"24-kubebuilder/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What the Operator pattern is and why it exists</li> <li>How <code>Kubebuilder</code> scaffolds a complete operator project</li> <li>How to define a Custom Resource Definition (CRD) with validation markers</li> <li>How to write a Reconciliation Loop using <code>controller-runtime</code></li> <li>How to manage child resources (<code>Deployment</code>, <code>Service</code>, <code>ConfigMap</code>) and their ownership</li> <li>How to update Status subresources and surface conditions</li> <li>How to run the operator locally and in-cluster</li> <li>How to write admission webhooks for defaulting and validation</li> <li>How to write controller tests with <code>envtest</code></li> <li>How to build and push the operator Docker image and deploy via Kustomize</li> </ul>"},{"location":"24-kubebuilder/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link Kubebuilder Book (official) book.kubebuilder.io Kubebuilder GitHub github.com/kubernetes-sigs/kubebuilder controller-runtime docs pkg.go.dev/sigs.k8s.io/controller-runtime Kubernetes API Conventions github.com/kubernetes/community/contributors/devel/sig-architecture/api-conventions.md CRD Validation Markers book.kubebuilder.io/reference/markers/crd-validation RBAC Markers book.kubebuilder.io/reference/markers/rbac Operator SDK (alternative) sdk.operatorframework.io OperatorHub.io operatorhub.io envtest (controller tests) book.kubebuilder.io/cronjob-tutorial/writing-tests Go Modules go.dev/ref/mod"},{"location":"24-kubebuilder/#introduction","title":"Introduction","text":""},{"location":"24-kubebuilder/#what-is-the-operator-pattern","title":"What is the Operator Pattern?","text":"<ul> <li>Kubernetes manages built-in resources (Pods, Deployments, Services) with built-in controllers that run a reconciliation loop.</li> <li>A Kubernetes Operator extends this pattern to your own domain-specific resources.</li> <li>An operator is a combination of:</li> <li>A Custom Resource Definition (CRD) - defines the new resource type and its schema in the Kubernetes API</li> <li>A Controller - watches for changes to the custom resource and reconciles the cluster state towards the desired state</li> </ul> <pre><code>flowchart LR\n    user[\"Developer\\nkubectl apply -f webapp.yaml\"] --&gt; api[\"Kubernetes API Server\"]\n    api --&gt; etcd[\"etcd\\n(stores WebApp object)\"]\n    api --&gt; ctrl[\"WebApp Controller\\n(our operator)\"]\n    ctrl --&gt;|\"Reconcile Loop\\nCreate/Update/Delete\"| deploy[\"Deployment\"]\n    ctrl --&gt; svc[\"Service\"]\n    ctrl --&gt; cm[\"ConfigMap\"]\n    ctrl --&gt;|\"Status update\"| api</code></pre>"},{"location":"24-kubebuilder/#when-should-you-write-an-operator","title":"When should you write an Operator?","text":"Use case Example Manage a stateful application lifecycle Database cluster (create, backup, restore, scale, upgrade) Encode operational runbook as code Auto-healing, canary rollouts Extend Kubernetes with domain knowledge CI/CD pipelines, ML training jobs Complex multi-resource coordination Provision <code>Deployment</code> + <code>Service</code> + <code>Certificate</code> as a single object"},{"location":"24-kubebuilder/#kubebuilder-vs-raw-client-go","title":"Kubebuilder vs Raw client-go","text":"Raw client-go Kubebuilder Code scaffolding Manual Automated CRD schema generation Manual YAML Auto-generated from Go struct + markers RBAC generation Manual Auto-generated from <code>//+kubebuilder:rbac</code> markers Controller boilerplate Manual Scaffolded Testing framework DIY <code>envtest</code> built in Webhook scaffolding Manual Scaffolded"},{"location":"24-kubebuilder/#terminology","title":"Terminology","text":"Term Description CRD Custom Resource Definition - registers a new resource type with the Kubernetes API CR Custom Resource - an instance of a CRD (like a Pod is an instance of the Pod resource) Operator A controller that implements domain-specific logic for a custom resource Reconciler The Go struct that implements the <code>Reconcile(ctx, req)</code> method Reconcile Loop Watch \u2192 Diff \u2192 Act cycle that continuously drives the cluster toward desired state Desired State What the user declared in the CR spec Observed State What is actually running in the cluster Finalizer A string added to <code>.metadata.finalizers</code>; prevents deletion until cleanup logic finished Owner Reference A pointer from a child resource (e.g. Deployment) back to its parent (WebApp CR) Status Subresource A separate sub-API for writing <code>.status</code> without triggering watches on <code>.spec</code> Marker A Go comment like <code>//+kubebuilder:...</code> that drives code/manifest generation envtest A test environment that starts a real <code>kube-apiserver</code> + <code>etcd</code> binary for integration tests Webhook HTTP server ArgoCD calls before creating/updating resources; used for defaulting and validation"},{"location":"24-kubebuilder/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    subgraph dev[\"Developer Workflow\"]\n        code[\"Write Go types + reconciler\"]\n        gen[\"make generate\\n(deepcopy funcs)\"]\n        manifest[\"make manifests\\n(CRD YAML, RBAC YAML)\"]\n        test[\"make test\\n(envtest suite)\"]\n        docker[\"make docker-build docker-push\\n(build operator image)\"]\n        deploy[\"make deploy\\n(kustomize | kubectl apply)\"]\n    end\n\n    subgraph cluster[\"Kubernetes Cluster\"]\n        crd[\"CRD: webapps.apps.codewizard.io\"]\n        ns[\"Namespace: webapp-system\"]\n        ctrl_pod[\"Controller Pod\\n(our operator binary)\"]\n        webhook_svc[\"Webhook Service\"]\n\n        subgraph managed[\"User Namespace\"]\n            webapp_cr[\"WebApp CR\\n(desired state)\"]\n            cm[\"ConfigMap\\n(HTML content)\"]\n            dep[\"Deployment\\n(nginx pods)\"]\n            svc[\"Service\\n(ClusterIP)\"]\n        end\n    end\n\n    code --&gt; gen --&gt; manifest --&gt; test --&gt; docker --&gt; deploy\n    deploy --&gt; crd\n    deploy --&gt; ns\n    deploy --&gt; ctrl_pod\n    ctrl_pod --&gt;|\"watch + reconcile\"| webapp_cr\n    ctrl_pod --&gt;|\"owns\"| cm\n    ctrl_pod --&gt;|\"owns\"| dep\n    ctrl_pod --&gt;|\"owns\"| svc\n    ctrl_pod --&gt;|\"updates\"| webapp_cr</code></pre>"},{"location":"24-kubebuilder/#project-structure","title":"Project Structure","text":"<p>After running <code>kubebuilder init</code> and <code>kubebuilder create api</code>, the project looks like:</p> <pre><code>webapp-operator/\n\u251c\u2500\u2500 api/\n\u2502   \u2514\u2500\u2500 v1/\n\u2502       \u251c\u2500\u2500 groupversion_info.go    # Group/Version registration\n\u2502       \u251c\u2500\u2500 webapp_types.go         # CRD Go types (Spec, Status, markers)\n\u2502       \u2514\u2500\u2500 zz_generated.deepcopy.go  # Auto-generated (make generate)\n\u2502\n\u251c\u2500\u2500 internal/\n\u2502   \u2514\u2500\u2500 controller/\n\u2502       \u251c\u2500\u2500 webapp_controller.go    # Reconcile() implementation\n\u2502       \u2514\u2500\u2500 webapp_controller_test.go  # envtest-based integration tests\n\u2502\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 crd/                        # Generated CRD YAML manifests\n\u2502   \u251c\u2500\u2500 rbac/                       # Generated RBAC manifests\n\u2502   \u251c\u2500\u2500 manager/                    # Controller Deployment manifests\n\u2502   \u251c\u2500\u2500 default/                    # Kustomize base that wires everything together\n\u2502   \u251c\u2500\u2500 webhook/                    # Webhook certificates and Service\n\u2502   \u2514\u2500\u2500 samples/\n\u2502       \u2514\u2500\u2500 apps_v1_webapp.yaml     # Example CR for testing\n\u2502\n\u251c\u2500\u2500 cmd/\n\u2502   \u2514\u2500\u2500 main.go                     # Entry point: registers scheme, starts manager\n\u2502\n\u251c\u2500\u2500 Dockerfile                      # Multi-stage build for the operator image\n\u251c\u2500\u2500 Makefile                        # All development targets\n\u251c\u2500\u2500 go.mod                          # Go module definition\n\u2514\u2500\u2500 go.sum                          # Dependency checksums\n</code></pre>"},{"location":"24-kubebuilder/#common-kubebuilder-commands","title":"Common Kubebuilder Commands","text":"<code>kubebuilder init</code> - Initialize a new operator project <p>Syntax: <code>kubebuilder init --domain &lt;domain&gt; --repo &lt;module&gt;</code></p> <p>Description: Scaffolds the complete project skeleton: <code>cmd/main.go</code>, <code>Makefile</code>, <code>go.mod</code>, base Kustomize configs, and <code>.gitignore</code>.</p> <ul> <li><code>--domain</code> sets the API group suffix (e.g., resources will be <code>&lt;group&gt;.&lt;domain&gt;</code>)</li> <li><code>--repo</code> sets the Go module path</li> <li> <p><code>--plugins=go/v4</code> (default) uses the latest stable Go plugin</p> <pre><code># Initialize with domain codewizard.io\nkubebuilder init \\\n    --domain codewizard.io \\\n    --repo codewizard.io/webapp-operator\n\n# Initialize with an older plugin (kustomize only, no controller)\nkubebuilder init \\\n    --domain my.domain \\\n    --repo my.domain/guestbook \\\n    --plugins=kustomize/v2-alpha\n\n# Verify the scaffolded structure\nls -la\ncat go.mod\ncat Makefile\n</code></pre> </li> </ul> <code>kubebuilder create api</code> - Create a new CRD + Controller <p>Syntax: <code>kubebuilder create api --group &lt;group&gt; --version &lt;version&gt; --kind &lt;Kind&gt;</code></p> <p>Description: Scaffolds a new API type (CRD struct in <code>api/&lt;version&gt;/&lt;kind&gt;_types.go</code>) and a controller stub (<code>internal/controller/&lt;kind&gt;_controller.go</code>).</p> <ul> <li>Prompts whether to create the Resource (CRD type) and the Controller</li> <li>Adds the type to the scheme and wires the controller into <code>cmd/main.go</code></li> <li> <p>Can be run multiple times to add more API kinds to the same project</p> <pre><code># Create WebApp API and controller\nkubebuilder create api \\\n    --group apps \\\n    --version v1 \\\n    --kind WebApp\n\n# Create a second kind in the same project\nkubebuilder create api \\\n    --group apps \\\n    --version v1 \\\n    --kind WebAppPolicy\n\n# Scaffolds CRD only, no controller\nkubebuilder create api \\\n    --group apps \\\n    --version v1 \\\n    --kind Database \\\n    --controller=false\n\n# View generated type\ncat api/v1/webapp_types.go\n</code></pre> </li> </ul> <code>kubebuilder create webhook</code> - Add defaulting or validation webhook <p>Syntax: <code>kubebuilder create webhook --group &lt;group&gt; --version &lt;version&gt; --kind &lt;Kind&gt;</code></p> <p>Description: Scaffolds a webhook server for the given kind, supporting defaulting (mutating) and validation (validating) webhooks.</p> <ul> <li><code>--defaulting</code> generates a <code>Default()</code> method (MutatingAdmissionWebhook)</li> <li><code>--programmatic-validation</code> generates a <code>ValidateCreate/Update/Delete()</code> method (ValidatingAdmissionWebhook)</li> <li> <p>Also generates certificate management setup in <code>config/webhook/</code></p> <pre><code># Add both defaulting and validation webhooks\nkubebuilder create webhook \\\n    --group apps \\\n    --version v1 \\\n    --kind WebApp \\\n    --defaulting \\\n    --programmatic-validation\n\n# View scaffolded webhook file\ncat api/v1/webapp_webhook.go\n</code></pre> </li> </ul> <code>make generate</code> - Generate DeepCopy functions <p>Syntax: <code>make generate</code></p> <p>Description: Runs <code>controller-gen object</code> to auto-generate <code>DeepCopyObject()</code> methods for all types. These are required by the Kubernetes runtime and must be regenerated after every change to <code>*_types.go</code>.</p> <pre><code>```bash\n# Regenerate after changing types\nmake generate\n\n# View generated file\ncat api/v1/zz_generated.deepcopy.go\n\n# What it runs under the hood:\n# controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\"\n```\n</code></pre> <code>make manifests</code> - Generate CRD and RBAC manifests <p>Syntax: <code>make manifests</code></p> <p>Description: Runs <code>controller-gen</code> to generate CRD YAML, RBAC ClusterRole, and webhook manifests from Go markers. Must be run after every change to markers in <code>*_types.go</code> or <code>*_controller.go</code>.</p> <pre><code>```bash\n# Generate all manifests\nmake manifests\n\n# View generated CRD YAML\ncat config/crd/bases/apps.codewizard.io_webapps.yaml\n\n# View generated RBAC\ncat config/rbac/role.yaml\n\n# What it runs:\n# controller-gen rbac:roleName=manager-role crd webhook paths=\"./...\" \\\n#   output:crd:artifacts:config=config/crd/bases\n```\n</code></pre> <code>make install</code> - Install CRDs into the cluster <p>Syntax: <code>make install</code></p> <p>Description: Applies the generated CRD manifests to the currently active cluster using <code>kubectl apply</code>. After this, <code>kubectl get webapps</code> will work.</p> <pre><code>```bash\n# Install CRDs\nmake install\n\n# Verify CRD is registered\nkubectl get crds | grep codewizard\n\n# Describe the CRD schema\nkubectl describe crd webapps.apps.codewizard.io\n\n# What it runs:\n# kubectl apply -k config/crd\n```\n</code></pre> <code>make run</code> - Run the controller locally <p>Syntax: <code>make run</code></p> <p>Description: Runs the controller binary on your local machine using the kubeconfig in <code>~/.kube/config</code>. The controller connects to the cluster and reconciles resources but runs outside the cluster (useful for development).</p> <pre><code>```bash\n# Run controller locally (uses current kubeconfig)\nmake run\n\n# Run with extra verbosity\nmake run ARGS=\"--zap-log-level=debug\"\n\n# Run with leader election disabled (single instance mode)\nmake run ARGS=\"--leader-elect=false\"\n```\n</code></pre> <code>make test</code> - Run controller tests with envtest <p>Syntax: <code>make test</code></p> <p>Description: Runs the full test suite using <code>envtest</code>, which starts a real <code>kube-apiserver</code> and <code>etcd</code> binary locally - no cluster required.</p> <pre><code>```bash\n# Run all tests\nmake test\n\n# Run with verbose output\nmake test ARGS=\"-v\"\n\n# Run only specific tests\nmake test ARGS=\"-run TestWebAppReconciler\"\n\n# Run with coverage report\nmake test-coverage\n```\n</code></pre> <code>make docker-build</code> - Build the operator image <p>Syntax: <code>make docker-build IMG=&lt;image:tag&gt;</code></p> <p>Description: Builds a multi-stage Docker image containing the compiled operator binary.</p> <pre><code>```bash\n# Build with a custom image name\nmake docker-build IMG=ghcr.io/myorg/webapp-operator:v0.1.0\n\n# Build and push in one step\nmake docker-build docker-push IMG=ghcr.io/myorg/webapp-operator:v0.1.0\n\n# Build for multiple platforms (requires buildx)\nmake docker-buildx IMG=ghcr.io/myorg/webapp-operator:v0.1.0\n```\n</code></pre> <code>make deploy</code> - Deploy the operator to the cluster <p>Syntax: <code>make deploy IMG=&lt;image:tag&gt;</code></p> <p>Description: Uses Kustomize to render all manifests (CRD, RBAC, Deployment, webhook certs) and applies them to the cluster.</p> <pre><code>```bash\n# Deploy with a specific image\nmake deploy IMG=ghcr.io/myorg/webapp-operator:v0.1.0\n\n# Verify the operator pod is running\nkubectl get pods -n webapp-system\n\n# Check operator logs\nkubectl logs -n webapp-system -l control-plane=controller-manager -f\n\n# Undeploy\nmake undeploy\n```\n</code></pre>"},{"location":"24-kubebuilder/#lab","title":"Lab","text":""},{"location":"24-kubebuilder/#part-01-prerequisites","title":"Part 01 - Prerequisites","text":""},{"location":"24-kubebuilder/#0101-install-go","title":"01.01 Install Go","text":"<p>Kubebuilder requires Go 1.21+.</p> macOSLinux <pre><code>brew install go\n\n# Or download directly\n# https://go.dev/dl/\n</code></pre> <pre><code># Download and install Go 1.22\ncurl -LO https://go.dev/dl/go1.22.4.linux-amd64.tar.gz\nsudo tar -C /usr/local -xzf go1.22.4.linux-amd64.tar.gz\necho 'export PATH=$PATH:/usr/local/go/bin' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"24-kubebuilder/#verify","title":"Verify","text":"<pre><code>go version\n# go version go1.22.x linux/amd64  (or darwin/arm64 etc.)\n</code></pre>"},{"location":"24-kubebuilder/#0102-install-kubebuilder","title":"01.02 Install Kubebuilder","text":"<pre><code># Detect OS and architecture\nOS=$(go env GOOS)\nARCH=$(go env GOARCH)\n\n# Download the latest kubebuilder binary\ncurl -L \"https://go.kubebuilder.io/dl/latest/${OS}/${ARCH}\" \\\n    -o /tmp/kubebuilder\n\nsudo mv /tmp/kubebuilder /usr/local/bin/kubebuilder\nsudo chmod +x /usr/local/bin/kubebuilder\n</code></pre>"},{"location":"24-kubebuilder/#verify_1","title":"Verify","text":"<pre><code>kubebuilder version\n# Version: main.version{...KubeBuilderVersion:\"4.x.x\",...}\n</code></pre>"},{"location":"24-kubebuilder/#0103-install-controller-gen-and-other-tools","title":"01.03 Install controller-gen and other tools","text":"<p>Kubebuilder uses several helper binaries installed by <code>make</code>. They are downloaded automatically on first use:</p> <pre><code># These are auto-downloaded by the Makefile when needed:\n# - controller-gen   (code + manifest generation)\n# - envtest          (testing framework binaries)\n# - kustomize        (manifest composition)\n# - golangci-lint    (linter)\n\n# You can pre-download them:\nmake controller-gen\nmake kustomize\n</code></pre>"},{"location":"24-kubebuilder/#0104-verify-cluster-access","title":"01.04 Verify cluster access","text":"<pre><code>kubectl cluster-info\nkubectl get nodes\n</code></pre>"},{"location":"24-kubebuilder/#part-02-initialize-the-project","title":"Part 02 - Initialize the Project","text":"<p>We will build a WebApp Operator that manages a <code>WebApp</code> custom resource. A <code>WebApp</code> CR creates a <code>Deployment</code> (nginx), a <code>Service</code> (ClusterIP), and a <code>ConfigMap</code> (HTML content) - all owned and reconciled by our controller.</p>"},{"location":"24-kubebuilder/#0201-create-and-enter-the-project-directory","title":"02.01 Create and enter the project directory","text":"<pre><code>mkdir webapp-operator\ncd webapp-operator\n</code></pre>"},{"location":"24-kubebuilder/#0202-initialize-the-kubebuilder-project","title":"02.02 Initialize the Kubebuilder project","text":"<pre><code>kubebuilder init \\\n    --domain codewizard.io \\\n    --repo   codewizard.io/webapp-operator\n</code></pre> <p>Output:</p> <pre><code>Writing kustomize manifests for you to edit...\nWriting scaffold for you to edit...\nGet controller runtime:\n$ go get sigs.k8s.io/controller-runtime@v0.18.x\ngo: downloading sigs.k8s.io/controller-runtime v0.18.x\n...\nNext: define a resource with:\n$ kubebuilder create api\n</code></pre>"},{"location":"24-kubebuilder/#0203-inspect-the-scaffolded-files","title":"02.03 Inspect the scaffolded files","text":"<pre><code># Project layout\nfind . -type f | grep -v '.git\\|vendor\\|_test' | sort\n\n# Go module\ncat go.mod\n\n# Entrypoint\ncat cmd/main.go\n\n# Makefile targets\nmake help\n</code></pre> <p>The <code>cmd/main.go</code> sets up the manager - it starts the controller, serves metrics, and manages leader election. You rarely need to edit this file by hand.</p>"},{"location":"24-kubebuilder/#part-03-create-the-api-crd-controller-scaffold","title":"Part 03 - Create the API (CRD + Controller Scaffold)","text":""},{"location":"24-kubebuilder/#0301-scaffold-the-webapp-api","title":"03.01 Scaffold the WebApp API","text":"<pre><code>kubebuilder create api \\\n    --group   apps \\\n    --version v1 \\\n    --kind    WebApp\n</code></pre> <p>When prompted: <pre><code>Create Resource [y/n]   \u2192 y\nCreate Controller [y/n] \u2192 y\n</code></pre></p>"},{"location":"24-kubebuilder/#0302-inspect-the-generated-files","title":"03.02 Inspect the generated files","text":"<pre><code># Type definition (we will fill this in next)\ncat api/v1/webapp_types.go\n\n# Reconciler stub (we will fill this in next)\ncat internal/controller/webapp_controller.go\n\n# main.go is updated to register WebApp\ngrep WebApp cmd/main.go\n</code></pre>"},{"location":"24-kubebuilder/#part-04-define-the-crd-types","title":"Part 04 - Define the CRD Types","text":"<p>This is the heart of the API definition. Open <code>api/v1/webapp_types.go</code> and replace its contents with the following:</p> <pre><code>// api/v1/webapp_types.go\npackage v1\n\nimport (\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n)\n\n// WebAppSpec defines the desired state of WebApp.\ntype WebAppSpec struct {\n    // Replicas is the desired number of nginx Pods.\n    // +kubebuilder:validation:Minimum=1\n    // +kubebuilder:validation:Maximum=10\n    // +kubebuilder:default=1\n    Replicas int32 `json:\"replicas,omitempty\"`\n\n    // Image is the nginx container image (repository:tag).\n    // +kubebuilder:default=\"nginx:1.25.3\"\n    // +kubebuilder:validation:MinLength=1\n    Image string `json:\"image,omitempty\"`\n\n    // Message is the HTML body text served by nginx.\n    // +kubebuilder:validation:MinLength=1\n    // +kubebuilder:validation:MaxLength=500\n    Message string `json:\"message\"`\n\n    // Port is the container port nginx listens on.\n    // +kubebuilder:validation:Minimum=1\n    // +kubebuilder:validation:Maximum=65535\n    // +kubebuilder:default=80\n    Port int32 `json:\"port,omitempty\"`\n\n    // ServiceType controls how the Service is exposed.\n    // +kubebuilder:validation:Enum=ClusterIP;NodePort;LoadBalancer\n    // +kubebuilder:default=ClusterIP\n    ServiceType string `json:\"serviceType,omitempty\"`\n}\n\n// WebAppPhase is a simple enum for the overall lifecycle state.\n// +kubebuilder:validation:Enum=Pending;Running;Degraded;Failed\ntype WebAppPhase string\n\nconst (\n    WebAppPhasePending  WebAppPhase = \"Pending\"\n    WebAppPhaseRunning  WebAppPhase = \"Running\"\n    WebAppPhaseDegraded WebAppPhase = \"Degraded\"\n    WebAppPhaseFailed   WebAppPhase = \"Failed\"\n)\n\n// WebAppStatus defines the observed state of WebApp.\ntype WebAppStatus struct {\n    // AvailableReplicas is the number of Pods in the Ready state.\n    AvailableReplicas int32 `json:\"availableReplicas,omitempty\"`\n\n    // ReadyReplicas is the number of Pods that have passed readiness checks.\n    ReadyReplicas int32 `json:\"readyReplicas,omitempty\"`\n\n    // Phase is a high-level summary of the WebApp lifecycle.\n    Phase WebAppPhase `json:\"phase,omitempty\"`\n\n    // DeploymentName is the name of the managed Deployment.\n    DeploymentName string `json:\"deploymentName,omitempty\"`\n\n    // ServiceName is the name of the managed Service.\n    ServiceName string `json:\"serviceName,omitempty\"`\n\n    // Conditions holds standard API conditions.\n    // +listType=map\n    // +listMapKey=type\n    Conditions []metav1.Condition `json:\"conditions,omitempty\"`\n}\n\n// Condition type constants\nconst (\n    // ConditionTypeAvailable means the WebApp has at least one ready pod.\n    ConditionTypeAvailable = \"Available\"\n    // ConditionTypeProgressing means a rollout or scale is in progress.\n    ConditionTypeProgressing = \"Progressing\"\n    // ConditionTypeDegraded means some (but not all) replicas are ready.\n    ConditionTypeDegraded = \"Degraded\"\n)\n\n//+kubebuilder:object:root=true\n//+kubebuilder:subresource:status\n//+kubebuilder:resource:shortName=wa,categories=all\n//+kubebuilder:printcolumn:name=\"Replicas\",type=integer,JSONPath=\".spec.replicas\"\n//+kubebuilder:printcolumn:name=\"Available\",type=integer,JSONPath=\".status.availableReplicas\"\n//+kubebuilder:printcolumn:name=\"Phase\",type=string,JSONPath=\".status.phase\"\n//+kubebuilder:printcolumn:name=\"Image\",type=string,JSONPath=\".spec.image\"\n//+kubebuilder:printcolumn:name=\"Age\",type=date,JSONPath=\".metadata.creationTimestamp\"\n\n// WebApp is the Schema for the webapps API.\n// It provisions a Deployment, Service, and ConfigMap that serve the configured HTML page.\ntype WebApp struct {\n    metav1.TypeMeta   `json:\",inline\"`\n    metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n    Spec   WebAppSpec   `json:\"spec,omitempty\"`\n    Status WebAppStatus `json:\"status,omitempty\"`\n}\n\n//+kubebuilder:object:root=true\n\n// WebAppList contains a list of WebApp.\ntype WebAppList struct {\n    metav1.TypeMeta `json:\",inline\"`\n    metav1.ListMeta `json:\"metadata,omitempty\"`\n    Items           []WebApp `json:\"items\"`\n}\n\nfunc init() {\n    SchemeBuilder.Register(&amp;WebApp{}, &amp;WebAppList{})\n}\n</code></pre>"},{"location":"24-kubebuilder/#marker-reference","title":"Marker Reference","text":"Marker Effect <code>//+kubebuilder:object:root=true</code> Marks this type as a root object (has its own API endpoint) <code>//+kubebuilder:subresource:status</code> Generates a <code>/status</code> sub-resource (status updates don\u2019t trigger spec watches) <code>//+kubebuilder:resource:shortName=wa</code> Allows <code>kubectl get wa</code> as a shorthand <code>//+kubebuilder:printcolumn:...</code> Extra columns shown by <code>kubectl get wa</code> <code>//+kubebuilder:validation:Minimum=1</code> Adds server-side validation to the CRD schema <code>//+kubebuilder:default=1</code> Sets a default value when the field is omitted <code>//+kubebuilder:validation:Enum=...</code> Restricts the field to a fixed set of values"},{"location":"24-kubebuilder/#0401-generate-deepcopy-functions","title":"04.01 Generate DeepCopy functions","text":"<p>After every change to <code>*_types.go</code> run:</p> <pre><code>make generate\n</code></pre> <p>This auto-generates <code>api/v1/zz_generated.deepcopy.go</code> which implements <code>DeepCopyObject()</code> - required by the Kubernetes runtime for garbage collection and caching.</p>"},{"location":"24-kubebuilder/#0402-generate-crd-manifest","title":"04.02 Generate CRD manifest","text":"<pre><code>make manifests\n</code></pre> <p>Inspect the generated CRD YAML:</p> <pre><code>cat config/crd/bases/apps.codewizard.io_webapps.yaml\n</code></pre> <p>You will see the full OpenAPI v3 schema, validation rules, printer columns, and status subresource settings - all derived from the Go markers.</p>"},{"location":"24-kubebuilder/#part-05-implement-the-reconciler","title":"Part 05 - Implement the Reconciler","text":"<p>Open <code>internal/controller/webapp_controller.go</code> and replace its contents with the full reconciler:</p> <pre><code>// internal/controller/webapp_controller.go\npackage controller\n\nimport (\n    \"context\"\n    \"fmt\"\n\n    appsv1 \"k8s.io/api/apps/v1\"\n    corev1 \"k8s.io/api/core/v1\"\n    \"k8s.io/apimachinery/pkg/api/errors\"\n    \"k8s.io/apimachinery/pkg/api/meta\"\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n    \"k8s.io/apimachinery/pkg/runtime\"\n    \"k8s.io/apimachinery/pkg/types\"\n    ctrl \"sigs.k8s.io/controller-runtime\"\n    \"sigs.k8s.io/controller-runtime/pkg/client\"\n    \"sigs.k8s.io/controller-runtime/pkg/log\"\n\n    webappv1 \"codewizard.io/webapp-operator/api/v1\"\n)\n\n// WebAppReconciler reconciles a WebApp object.\ntype WebAppReconciler struct {\n    client.Client\n    Scheme *runtime.Scheme\n}\n\n// RBAC markers - these generate config/rbac/role.yaml when `make manifests` is run.\n//\n//+kubebuilder:rbac:groups=apps.codewizard.io,resources=webapps,verbs=get;list;watch;create;update;patch;delete\n//+kubebuilder:rbac:groups=apps.codewizard.io,resources=webapps/status,verbs=get;update;patch\n//+kubebuilder:rbac:groups=apps.codewizard.io,resources=webapps/finalizers,verbs=update\n//+kubebuilder:rbac:groups=apps,resources=deployments,verbs=get;list;watch;create;update;patch;delete\n//+kubebuilder:rbac:groups=core,resources=services,verbs=get;list;watch;create;update;patch;delete\n//+kubebuilder:rbac:groups=core,resources=configmaps,verbs=get;list;watch;create;update;patch;delete\n//+kubebuilder:rbac:groups=core,resources=events,verbs=create;patch\n\n// Reconcile is the main reconciliation loop.\n// It is called whenever a WebApp CR (or a resource it owns) changes.\nfunc (r *WebAppReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n    logger := log.FromContext(ctx)\n\n    // \u2500\u2500 Step 1: Fetch the WebApp instance \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    webapp := &amp;webappv1.WebApp{}\n    if err := r.Get(ctx, req.NamespacedName, webapp); err != nil {\n        if errors.IsNotFound(err) {\n            // Object was deleted before we could reconcile - nothing to do.\n            logger.Info(\"WebApp not found, likely deleted\", \"name\", req.Name)\n            return ctrl.Result{}, nil\n        }\n        return ctrl.Result{}, fmt.Errorf(\"fetching WebApp: %w\", err)\n    }\n\n    logger.Info(\"Reconciling WebApp\",\n        \"name\", webapp.Name,\n        \"namespace\", webapp.Namespace,\n        \"replicas\", webapp.Spec.Replicas)\n\n    // \u2500\u2500 Step 2: Reconcile ConfigMap (HTML content) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if err := r.reconcileConfigMap(ctx, webapp); err != nil {\n        return ctrl.Result{}, fmt.Errorf(\"reconciling ConfigMap: %w\", err)\n    }\n\n    // \u2500\u2500 Step 3: Reconcile Deployment \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    deployment, err := r.reconcileDeployment(ctx, webapp)\n    if err != nil {\n        return ctrl.Result{}, fmt.Errorf(\"reconciling Deployment: %w\", err)\n    }\n\n    // \u2500\u2500 Step 4: Reconcile Service \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if err := r.reconcileService(ctx, webapp); err != nil {\n        return ctrl.Result{}, fmt.Errorf(\"reconciling Service: %w\", err)\n    }\n\n    // \u2500\u2500 Step 5: Update Status \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if err := r.updateStatus(ctx, webapp, deployment); err != nil {\n        return ctrl.Result{}, fmt.Errorf(\"updating status: %w\", err)\n    }\n\n    return ctrl.Result{}, nil\n}\n\n// \u2500\u2500 reconcileConfigMap ensures the HTML ConfigMap exists and is up-to-date. \u2500\u2500\n\nfunc (r *WebAppReconciler) reconcileConfigMap(ctx context.Context, webapp *webappv1.WebApp) error {\n    logger := log.FromContext(ctx)\n\n    desired := &amp;corev1.ConfigMap{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      webapp.Name + \"-html\",\n            Namespace: webapp.Namespace,\n            Labels:    labelsForWebApp(webapp.Name),\n        },\n        Data: map[string]string{\n            \"index.html\": fmt.Sprintf(`&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;%s&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;%s&lt;/h1&gt;\n  &lt;p&gt;Managed by the &lt;strong&gt;WebApp Operator&lt;/strong&gt; | Instance: %s&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;`, webapp.Spec.Message, webapp.Spec.Message, webapp.Name),\n        },\n    }\n\n    // Set the WebApp as the owner of the ConfigMap.\n    // When the WebApp CR is deleted, Kubernetes garbage-collects the ConfigMap automatically.\n    if err := ctrl.SetControllerReference(webapp, desired, r.Scheme); err != nil {\n        return err\n    }\n\n    // Fetch the existing ConfigMap\n    existing := &amp;corev1.ConfigMap{}\n    err := r.Get(ctx, types.NamespacedName{Name: desired.Name, Namespace: desired.Namespace}, existing)\n    if errors.IsNotFound(err) {\n        logger.Info(\"Creating ConfigMap\", \"name\", desired.Name)\n        return r.Create(ctx, desired)\n    }\n    if err != nil {\n        return err\n    }\n\n    // Update if the content has changed\n    if existing.Data[\"index.html\"] != desired.Data[\"index.html\"] {\n        existing.Data = desired.Data\n        logger.Info(\"Updating ConfigMap\", \"name\", existing.Name)\n        return r.Update(ctx, existing)\n    }\n\n    return nil\n}\n\n// \u2500\u2500 reconcileDeployment ensures the nginx Deployment exists and matches spec. \u2500\u2500\n\nfunc (r *WebAppReconciler) reconcileDeployment(ctx context.Context, webapp *webappv1.WebApp) (*appsv1.Deployment, error) {\n    logger := log.FromContext(ctx)\n\n    labels := labelsForWebApp(webapp.Name)\n    replicas := webapp.Spec.Replicas\n\n    desired := &amp;appsv1.Deployment{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      webapp.Name,\n            Namespace: webapp.Namespace,\n            Labels:    labels,\n        },\n        Spec: appsv1.DeploymentSpec{\n            Replicas: &amp;replicas,\n            Selector: &amp;metav1.LabelSelector{MatchLabels: labels},\n            Template: corev1.PodTemplateSpec{\n                ObjectMeta: metav1.ObjectMeta{Labels: labels},\n                Spec: corev1.PodSpec{\n                    Containers: []corev1.Container{\n                        {\n                            Name:            \"nginx\",\n                            Image:           webapp.Spec.Image,\n                            ImagePullPolicy: corev1.PullIfNotPresent,\n                            Ports: []corev1.ContainerPort{\n                                {ContainerPort: webapp.Spec.Port, Protocol: corev1.ProtocolTCP},\n                            },\n                            VolumeMounts: []corev1.VolumeMount{\n                                {\n                                    Name:      \"html\",\n                                    MountPath: \"/usr/share/nginx/html\",\n                                },\n                            },\n                            ReadinessProbe: &amp;corev1.Probe{\n                                ProbeHandler: corev1.ProbeHandler{\n                                    HTTPGet: &amp;corev1.HTTPGetAction{\n                                        Path: \"/\",\n                                        Port: intOrString(webapp.Spec.Port),\n                                    },\n                                },\n                                InitialDelaySeconds: 5,\n                                PeriodSeconds:       10,\n                            },\n                            LivenessProbe: &amp;corev1.Probe{\n                                ProbeHandler: corev1.ProbeHandler{\n                                    HTTPGet: &amp;corev1.HTTPGetAction{\n                                        Path: \"/\",\n                                        Port: intOrString(webapp.Spec.Port),\n                                    },\n                                },\n                                InitialDelaySeconds: 15,\n                                PeriodSeconds:       20,\n                            },\n                        },\n                    },\n                    Volumes: []corev1.Volume{\n                        {\n                            Name: \"html\",\n                            VolumeSource: corev1.VolumeSource{\n                                ConfigMap: &amp;corev1.ConfigMapVolumeSource{\n                                    LocalObjectReference: corev1.LocalObjectReference{\n                                        Name: webapp.Name + \"-html\",\n                                    },\n                                },\n                            },\n                        },\n                    },\n                },\n            },\n        },\n    }\n\n    if err := ctrl.SetControllerReference(webapp, desired, r.Scheme); err != nil {\n        return nil, err\n    }\n\n    existing := &amp;appsv1.Deployment{}\n    err := r.Get(ctx, types.NamespacedName{Name: desired.Name, Namespace: desired.Namespace}, existing)\n    if errors.IsNotFound(err) {\n        logger.Info(\"Creating Deployment\", \"name\", desired.Name)\n        if err := r.Create(ctx, desired); err != nil {\n            return nil, err\n        }\n        return desired, nil\n    }\n    if err != nil {\n        return nil, err\n    }\n\n    // Reconcile mutable fields: replicas and image\n    needsUpdate := false\n    if *existing.Spec.Replicas != replicas {\n        existing.Spec.Replicas = &amp;replicas\n        needsUpdate = true\n    }\n    if existing.Spec.Template.Spec.Containers[0].Image != webapp.Spec.Image {\n        existing.Spec.Template.Spec.Containers[0].Image = webapp.Spec.Image\n        needsUpdate = true\n    }\n\n    if needsUpdate {\n        logger.Info(\"Updating Deployment\", \"name\", existing.Name,\n            \"replicas\", replicas, \"image\", webapp.Spec.Image)\n        if err := r.Update(ctx, existing); err != nil {\n            return nil, err\n        }\n    }\n\n    return existing, nil\n}\n\n// \u2500\u2500 reconcileService ensures the Service exists and matches spec. \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nfunc (r *WebAppReconciler) reconcileService(ctx context.Context, webapp *webappv1.WebApp) error {\n    logger := log.FromContext(ctx)\n\n    labels := labelsForWebApp(webapp.Name)\n    svcType := corev1.ServiceType(webapp.Spec.ServiceType)\n\n    desired := &amp;corev1.Service{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      webapp.Name,\n            Namespace: webapp.Namespace,\n            Labels:    labels,\n        },\n        Spec: corev1.ServiceSpec{\n            Selector: labels,\n            Type:     svcType,\n            Ports: []corev1.ServicePort{\n                {\n                    Port:       webapp.Spec.Port,\n                    TargetPort: intOrString(webapp.Spec.Port),\n                    Protocol:   corev1.ProtocolTCP,\n                },\n            },\n        },\n    }\n\n    if err := ctrl.SetControllerReference(webapp, desired, r.Scheme); err != nil {\n        return err\n    }\n\n    existing := &amp;corev1.Service{}\n    err := r.Get(ctx, types.NamespacedName{Name: desired.Name, Namespace: desired.Namespace}, existing)\n    if errors.IsNotFound(err) {\n        logger.Info(\"Creating Service\", \"name\", desired.Name)\n        return r.Create(ctx, desired)\n    }\n    if err != nil {\n        return err\n    }\n\n    // Reconcile Service type (immutable field - recreate required)\n    if existing.Spec.Type != svcType {\n        logger.Info(\"Recreating Service due to type change\", \"old\", existing.Spec.Type, \"new\", svcType)\n        if err := r.Delete(ctx, existing); err != nil {\n            return err\n        }\n        return r.Create(ctx, desired)\n    }\n\n    return nil\n}\n\n// \u2500\u2500 updateStatus computes and persists the WebApp status. \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nfunc (r *WebAppReconciler) updateStatus(ctx context.Context, webapp *webappv1.WebApp, deployment *appsv1.Deployment) error {\n    // Work on a copy to avoid mutating the cached object\n    updated := webapp.DeepCopy()\n\n    available := deployment.Status.AvailableReplicas\n    ready := deployment.Status.ReadyReplicas\n\n    updated.Status.AvailableReplicas = available\n    updated.Status.ReadyReplicas = ready\n    updated.Status.DeploymentName = deployment.Name\n    updated.Status.ServiceName = webapp.Name\n\n    // Compute phase\n    switch {\n    case available == 0:\n        updated.Status.Phase = webappv1.WebAppPhasePending\n    case ready &lt; webapp.Spec.Replicas:\n        updated.Status.Phase = webappv1.WebAppPhaseDegraded\n    default:\n        updated.Status.Phase = webappv1.WebAppPhaseRunning\n    }\n\n    // Set the Available condition\n    availableCond := metav1.Condition{\n        Type:               webappv1.ConditionTypeAvailable,\n        ObservedGeneration: webapp.Generation,\n        LastTransitionTime: metav1.Now(),\n    }\n    if available &gt;= webapp.Spec.Replicas {\n        availableCond.Status = metav1.ConditionTrue\n        availableCond.Reason = \"DeploymentAvailable\"\n        availableCond.Message = fmt.Sprintf(\"%d/%d replicas are available\", available, webapp.Spec.Replicas)\n    } else {\n        availableCond.Status = metav1.ConditionFalse\n        availableCond.Reason = \"DeploymentUnavailable\"\n        availableCond.Message = fmt.Sprintf(\"only %d/%d replicas are available\", available, webapp.Spec.Replicas)\n    }\n    meta.SetStatusCondition(&amp;updated.Status.Conditions, availableCond)\n\n    // Only issue an update if anything changed\n    if updated.Status.Phase != webapp.Status.Phase ||\n        updated.Status.AvailableReplicas != webapp.Status.AvailableReplicas {\n        return r.Status().Update(ctx, updated)\n    }\n\n    return nil\n}\n\n// \u2500\u2500 SetupWithManager wires the controller into the manager. \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nfunc (r *WebAppReconciler) SetupWithManager(mgr ctrl.Manager) error {\n    return ctrl.NewControllerManagedBy(mgr).\n        // Primary watch: WebApp CRs\n        For(&amp;webappv1.WebApp{}).\n        // Secondary watches: owned resources - any change triggers reconciliation\n        Owns(&amp;appsv1.Deployment{}).\n        Owns(&amp;corev1.Service{}).\n        Owns(&amp;corev1.ConfigMap{}).\n        Complete(r)\n}\n\n// \u2500\u2500 Helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nfunc labelsForWebApp(name string) map[string]string {\n    return map[string]string{\n        \"app.kubernetes.io/name\":       \"webapp\",\n        \"app.kubernetes.io/instance\":   name,\n        \"app.kubernetes.io/managed-by\": \"webapp-operator\",\n    }\n}\n\nfunc intOrString(port int32) intstr.IntOrString {\n    return intstr.FromInt32(port)\n}\n</code></pre> <p>Add the missing import</p> <p>The <code>intOrString</code> helper uses <code>k8s.io/apimachinery/pkg/util/intstr</code>. Add it to the import block: <pre><code>\"k8s.io/apimachinery/pkg/util/intstr\"\n</code></pre></p>"},{"location":"24-kubebuilder/#understanding-the-reconcile-loop","title":"Understanding the Reconcile Loop","text":"<pre><code>flowchart TD\n    trigger[\"Event: WebApp changed\\nor Deployment / Service / ConfigMap changed\"]\n    --&gt;fetch[\"r.Get(ctx, req, &amp;webapp)\\nFetch current desired state\"]\n    --&gt;notfound{Not found?}\n    notfound --&gt;|yes| done[\"return - resource deleted, nothing to do\"]\n    notfound --&gt;|no| cm[\"reconcileConfigMap()\\nCreate or update HTML ConfigMap\"]\n    --&gt;dep[\"reconcileDeployment()\\nCreate or update nginx Deployment\"]\n    --&gt;svc[\"reconcileService()\\nCreate or update Service\"]\n    --&gt;status[\"updateStatus()\\nCompute phase + conditions\\nr.Status().Update()\"]\n    --&gt;requeue[\"return ctrl.Result{}\"]</code></pre>"},{"location":"24-kubebuilder/#key-controller-runtime-concepts","title":"Key controller-runtime Concepts","text":"Concept Code Explanation Fetch CR <code>r.Get(ctx, req.NamespacedName, webapp)</code> Always read fresh from API server IsNotFound <code>errors.IsNotFound(err)</code> Distinguish \u201cdoesn\u2019t exist\u201d from real errors Owner reference <code>ctrl.SetControllerReference(webapp, child, r.Scheme)</code> Garbage-collect child when parent deleted Status update <code>r.Status().Update(ctx, updated)</code> Use the status subresource, not <code>r.Update</code> Watch child <code>.Owns(&amp;appsv1.Deployment{})</code> Re-queue parent when child changes Logger <code>log.FromContext(ctx)</code> Context-scoped structured logger"},{"location":"24-kubebuilder/#part-06-install-crds-and-run-locally","title":"Part 06 - Install CRDs and Run Locally","text":""},{"location":"24-kubebuilder/#0601-install-crds-into-the-cluster","title":"06.01 Install CRDs into the cluster","text":"<pre><code># Generate manifests from markers\nmake manifests\n\n# Apply CRDs to the cluster\nmake install\n\n# Verify\nkubectl get crds | grep codewizard\nkubectl describe crd webapps.apps.codewizard.io | grep -A 20 \"COLUMNS\\|Validation\"\n</code></pre>"},{"location":"24-kubebuilder/#0602-verify-the-short-name-works","title":"06.02 Verify the short name works","text":"<pre><code># The //+kubebuilder:resource:shortName=wa marker enables this\nkubectl get wa\n# No resources found in default namespace.\n</code></pre>"},{"location":"24-kubebuilder/#0603-run-the-controller-locally","title":"06.03 Run the controller locally","text":"<pre><code>make run\n</code></pre> <p>You will see logs like:</p> <pre><code>INFO    Starting manager\nINFO    Starting Controller    {\"controller\": \"webapp\"}\nINFO    Starting workers       {\"controller\": \"webapp\", \"worker count\": 1}\n</code></pre> <p>Leave this running in one terminal and open a second terminal for the next steps.</p>"},{"location":"24-kubebuilder/#part-07-create-your-first-webapp-cr","title":"Part 07 - Create Your First WebApp CR","text":""},{"location":"24-kubebuilder/#0701-apply-the-sample-cr","title":"07.01 Apply the sample CR","text":"<p>Create <code>config/samples/apps_v1_webapp.yaml</code>:</p> <pre><code>apiVersion: apps.codewizard.io/v1\nkind: WebApp\nmetadata:\n  name: my-webapp\n  namespace: default\nspec:\n  replicas: 2\n  image: nginx:1.25.3\n  message: \"Hello from the WebApp Operator!\"\n  port: 80\n  serviceType: ClusterIP\n</code></pre> <pre><code>kubectl apply -f config/samples/apps_v1_webapp.yaml\n</code></pre>"},{"location":"24-kubebuilder/#0702-watch-the-controller-create-child-resources","title":"07.02 Watch the controller create child resources","text":"<p>In your second terminal:</p> <pre><code># Watch pods appear\nkubectl get pods -l app.kubernetes.io/name=webapp -w\n\n# Check all resources created by the operator\nkubectl get deployment,service,configmap -l app.kubernetes.io/managed-by=webapp-operator\n</code></pre> <p>Expected output:</p> <pre><code>NAME                          READY   UP-TO-DATE   AVAILABLE\ndeployment.apps/my-webapp     2/2     2            2\n\nNAME                 TYPE        CLUSTER-IP     PORT(S)\nservice/my-webapp    ClusterIP   10.96.x.x      80/TCP\n\nNAME                          DATA\nconfigmap/my-webapp-html      1\n</code></pre>"},{"location":"24-kubebuilder/#0703-inspect-the-webapp-status","title":"07.03 Inspect the WebApp status","text":"<pre><code>kubectl get wa\n\n# Output (notice the printer columns from our markers):\n# NAME        REPLICAS   AVAILABLE   PHASE     IMAGE           AGE\n# my-webapp   2          2           Running   nginx:1.25.3    30s\n\n# Full status\nkubectl get wa my-webapp -o jsonpath='{.status}' | jq .\n\n# Check conditions\nkubectl get wa my-webapp -o jsonpath='{.status.conditions}' | jq .\n</code></pre>"},{"location":"24-kubebuilder/#0704-test-the-application","title":"07.04 Test the application","text":"<pre><code>kubectl port-forward svc/my-webapp 8080:80 &amp;\nsleep 2\ncurl http://localhost:8080\n</code></pre> <p>Expected:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;Hello from the WebApp Operator!&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Hello from the WebApp Operator!&lt;/h1&gt;\n  &lt;p&gt;Managed by the &lt;strong&gt;WebApp Operator&lt;/strong&gt; | Instance: my-webapp&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <pre><code>kill %1  # stop port-forward\n</code></pre>"},{"location":"24-kubebuilder/#part-08-self-healing-the-reconciler-restores-deleted-resources","title":"Part 08 - Self-Healing: The Reconciler Restores Deleted Resources","text":"<p>This is one of the most powerful operator features: if someone manually deletes or modifies a child resource, the operator recreates it immediately.</p>"},{"location":"24-kubebuilder/#0801-delete-the-deployment-manually","title":"08.01 Delete the Deployment manually","text":"<pre><code>kubectl delete deployment my-webapp\n</code></pre>"},{"location":"24-kubebuilder/#0802-watch-the-operator-restore-it","title":"08.02 Watch the operator restore it","text":"<pre><code># In the `make run` terminal you will see:\n# INFO    Reconciling WebApp    {\"name\": \"my-webapp\"}\n# INFO    Creating Deployment   {\"name\": \"my-webapp\"}\n\nkubectl get deployment my-webapp\n# NAME        READY   UP-TO-DATE   AVAILABLE   AGE\n# my-webapp   2/2     2            2           5s\n</code></pre> <p>The deployment was recreated in seconds by the reconcile loop.</p>"},{"location":"24-kubebuilder/#0803-why-does-this-work","title":"08.03 Why does this work?","text":"<p>When <code>.Owns(&amp;appsv1.Deployment{})</code> is set in <code>SetupWithManager</code>, controller-runtime watches all Deployments that have an owner reference pointing to a <code>WebApp</code>. Any change (including deletion) enqueues the parent <code>WebApp</code> for reconciliation.</p>"},{"location":"24-kubebuilder/#part-09-update-the-cr-and-observe-reconciliation","title":"Part 09 - Update the CR and Observe Reconciliation","text":""},{"location":"24-kubebuilder/#0901-scale-up-to-4-replicas","title":"09.01 Scale up to 4 replicas","text":"<pre><code>kubectl patch wa my-webapp --type=merge -p '{\"spec\":{\"replicas\":4}}'\n</code></pre>"},{"location":"24-kubebuilder/#0902-watch-the-deployment-scale","title":"09.02 Watch the Deployment scale","text":"<pre><code>kubectl get pods -l app.kubernetes.io/name=webapp -w\n# NAME              READY   STATUS    ...\n# my-webapp-xxx     1/1     Running\n# my-webapp-yyy     1/1     Running\n# my-webapp-zzz     1/1     Running   \u2190 new\n# my-webapp-aaa     1/1     Running   \u2190 new\n\nkubectl get wa my-webapp\n# NAME        REPLICAS   AVAILABLE   PHASE     ...\n# my-webapp   4          4           Running\n</code></pre>"},{"location":"24-kubebuilder/#0903-update-the-message","title":"09.03 Update the message","text":"<pre><code>kubectl patch wa my-webapp --type=merge \\\n    -p '{\"spec\":{\"message\":\"Updated by the operator!\"}}'\n</code></pre> <pre><code>kubectl port-forward svc/my-webapp 8080:80 &amp;\nsleep 2\ncurl http://localhost:8080 | grep \"Updated\"\nkill %1\n</code></pre>"},{"location":"24-kubebuilder/#0904-update-the-image","title":"09.04 Update the image","text":"<pre><code>kubectl patch wa my-webapp --type=merge \\\n    -p '{\"spec\":{\"image\":\"nginx:1.26.0\"}}'\n\nkubectl get deployment my-webapp -o jsonpath='{.spec.template.spec.containers[0].image}'\n# nginx:1.26.0\n</code></pre>"},{"location":"24-kubebuilder/#part-10-add-a-finalizer-for-cleanup-logic","title":"Part 10 - Add a Finalizer for Cleanup Logic","text":"<p>Finalizers let you run custom cleanup code before the resource is actually deleted from etcd.</p>"},{"location":"24-kubebuilder/#1001-add-the-finalizer-constant","title":"10.01 Add the finalizer constant","text":"<p>In <code>internal/controller/webapp_controller.go</code> add:</p> <pre><code>const webappFinalizer = \"apps.codewizard.io/finalizer\"\n</code></pre>"},{"location":"24-kubebuilder/#1002-add-finalizer-handling-to-reconcile","title":"10.02 Add finalizer handling to Reconcile","text":"<p>Insert this block at the beginning of the <code>Reconcile()</code> function, after fetching the WebApp:</p> <pre><code>// \u2500\u2500 Finalizer handling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nif webapp.DeletionTimestamp.IsZero() {\n    // Not being deleted - ensure finalizer is present\n    if !controllerutil.ContainsFinalizer(webapp, webappFinalizer) {\n        controllerutil.AddFinalizer(webapp, webappFinalizer)\n        if err := r.Update(ctx, webapp); err != nil {\n            return ctrl.Result{}, err\n        }\n        return ctrl.Result{}, nil  // re-queue after update\n    }\n} else {\n    // Being deleted - run cleanup before allowing deletion\n    if controllerutil.ContainsFinalizer(webapp, webappFinalizer) {\n        logger.Info(\"Running finalizer cleanup\", \"name\", webapp.Name)\n        // (perform any external cleanup here, e.g. cloud resources, DNS records)\n\n        // Remove finalizer - Kubernetes will then delete the object\n        controllerutil.RemoveFinalizer(webapp, webappFinalizer)\n        if err := r.Update(ctx, webapp); err != nil {\n            return ctrl.Result{}, err\n        }\n    }\n    return ctrl.Result{}, nil\n}\n</code></pre> <p>Add <code>\"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil\"</code> to imports.</p>"},{"location":"24-kubebuilder/#1003-test-the-finalizer","title":"10.03 Test the finalizer","text":"<pre><code># Delete the WebApp\nkubectl delete wa my-webapp\n\n# Before the finalizer is removed, the object shows DeletionTimestamp\nkubectl get wa my-webapp -o jsonpath='{.metadata.deletionTimestamp}'\n\n# After cleanup the object and all owned resources disappear\nkubectl get deployment,service,configmap -l app.kubernetes.io/managed-by=webapp-operator\n# No resources found.\n</code></pre>"},{"location":"24-kubebuilder/#part-11-add-a-validation-webhook","title":"Part 11 - Add a Validation Webhook","text":"<p>Webhooks intercept API calls to validate or mutate resources before they are persisted to etcd.</p>"},{"location":"24-kubebuilder/#1101-scaffold-the-webhook","title":"11.01 Scaffold the webhook","text":"<pre><code>kubebuilder create webhook \\\n    --group apps \\\n    --version v1 \\\n    --kind WebApp \\\n    --defaulting \\\n    --programmatic-validation\n</code></pre> <p>This creates <code>api/v1/webapp_webhook.go</code>.</p>"},{"location":"24-kubebuilder/#1102-implement-defaulting-mutating-webhook","title":"11.02 Implement defaulting (mutating webhook)","text":"<p>In <code>api/v1/webapp_webhook.go</code>, implement the <code>Default()</code> method:</p> <pre><code>func (r *WebApp) Default() {\n    log := logf.Log.WithName(\"webapp-resource\")\n    log.Info(\"Applying defaults\", \"name\", r.Name)\n\n    // Set default image if not provided\n    if r.Spec.Image == \"\" {\n        r.Spec.Image = \"nginx:1.25.3\"\n    }\n\n    // Set default replicas\n    if r.Spec.Replicas == 0 {\n        r.Spec.Replicas = 1\n    }\n\n    // Set default port\n    if r.Spec.Port == 0 {\n        r.Spec.Port = 80\n    }\n\n    // Set default service type\n    if r.Spec.ServiceType == \"\" {\n        r.Spec.ServiceType = \"ClusterIP\"\n    }\n}\n</code></pre>"},{"location":"24-kubebuilder/#1103-implement-validation-validating-webhook","title":"11.03 Implement validation (validating webhook)","text":"<pre><code>func (r *WebApp) ValidateCreate() (admission.Warnings, error) {\n    return r.validateWebApp()\n}\n\nfunc (r *WebApp) ValidateUpdate(old runtime.Object) (admission.Warnings, error) {\n    return r.validateWebApp()\n}\n\nfunc (r *WebApp) ValidateDelete() (admission.Warnings, error) {\n    return nil, nil\n}\n\nfunc (r *WebApp) validateWebApp() (admission.Warnings, error) {\n    var errs field.ErrorList\n\n    // Replicas must be between 1 and 10\n    if r.Spec.Replicas &lt; 1 || r.Spec.Replicas &gt; 10 {\n        errs = append(errs, field.Invalid(\n            field.NewPath(\"spec\", \"replicas\"),\n            r.Spec.Replicas,\n            \"must be between 1 and 10\",\n        ))\n    }\n\n    // Message must not be empty\n    if r.Spec.Message == \"\" {\n        errs = append(errs, field.Required(\n            field.NewPath(\"spec\", \"message\"),\n            \"message is required and cannot be empty\",\n        ))\n    }\n\n    if len(errs) &gt; 0 {\n        return nil, apierrors.NewInvalid(\n            schema.GroupKind{Group: \"apps.codewizard.io\", Kind: \"WebApp\"},\n            r.Name,\n            errs,\n        )\n    }\n\n    return nil, nil\n}\n</code></pre>"},{"location":"24-kubebuilder/#1104-test-webhook-validation","title":"11.04 Test webhook validation","text":"<pre><code># This should fail - replicas = 15 exceeds maximum of 10\nkubectl apply -f - &lt;&lt;EOF\napiVersion: apps.codewizard.io/v1\nkind: WebApp\nmetadata:\n  name: invalid-webapp\nspec:\n  replicas: 15\n  message: \"test\"\n  image: nginx:1.25.3\n  port: 80\nEOF\n\n# Expected error:\n# Error from server (WebApp.apps.codewizard.io \"invalid-webapp\" is invalid):\n#   spec.replicas: Invalid value: 15: must be between 1 and 10\n</code></pre>"},{"location":"24-kubebuilder/#part-12-writing-controller-tests","title":"Part 12 - Writing Controller Tests","text":"<p>Kubebuilder sets up <code>envtest</code> which runs a real <code>kube-apiserver</code> and <code>etcd</code> - no cluster needed.</p>"},{"location":"24-kubebuilder/#1201-inspect-the-test-suite-setup","title":"12.01 Inspect the test suite setup","text":"<pre><code>cat internal/controller/suite_test.go\n</code></pre>"},{"location":"24-kubebuilder/#1202-write-a-reconciler-integration-test","title":"12.02 Write a reconciler integration test","text":"<p>Create <code>internal/controller/webapp_controller_test.go</code>:</p> <pre><code>package controller\n\nimport (\n    \"context\"\n    \"time\"\n\n    . \"github.com/onsi/ginkgo/v2\"\n    . \"github.com/onsi/gomega\"\n    appsv1 \"k8s.io/api/apps/v1\"\n    corev1 \"k8s.io/api/core/v1\"\n    \"k8s.io/apimachinery/pkg/types\"\n\n    webappv1 \"codewizard.io/webapp-operator/api/v1\"\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n)\n\nvar _ = Describe(\"WebApp Controller\", func() {\n    const (\n        WebAppName      = \"test-webapp\"\n        WebAppNamespace = \"default\"\n        timeout         = time.Second * 30\n        interval        = time.Millisecond * 250\n    )\n\n    ctx := context.Background()\n\n    Context(\"When creating a WebApp\", func() {\n        It(\"should create a Deployment, Service and ConfigMap\", func() {\n            // \u2500\u2500 Create the WebApp CR \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            webapp := &amp;webappv1.WebApp{\n                ObjectMeta: metav1.ObjectMeta{\n                    Name:      WebAppName,\n                    Namespace: WebAppNamespace,\n                },\n                Spec: webappv1.WebAppSpec{\n                    Replicas:    2,\n                    Image:       \"nginx:1.25.3\",\n                    Message:     \"Hello from test\",\n                    Port:        80,\n                    ServiceType: \"ClusterIP\",\n                },\n            }\n            Expect(k8sClient.Create(ctx, webapp)).To(Succeed())\n\n            // \u2500\u2500 Assert: Deployment is created \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            deploymentLookup := types.NamespacedName{Name: WebAppName, Namespace: WebAppNamespace}\n            createdDeployment := &amp;appsv1.Deployment{}\n\n            Eventually(func() error {\n                return k8sClient.Get(ctx, deploymentLookup, createdDeployment)\n            }, timeout, interval).Should(Succeed())\n\n            Expect(*createdDeployment.Spec.Replicas).To(Equal(int32(2)))\n            Expect(createdDeployment.Spec.Template.Spec.Containers[0].Image).To(Equal(\"nginx:1.25.3\"))\n\n            // \u2500\u2500 Assert: Service is created \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            createdService := &amp;corev1.Service{}\n            Eventually(func() error {\n                return k8sClient.Get(ctx, deploymentLookup, createdService)\n            }, timeout, interval).Should(Succeed())\n\n            Expect(createdService.Spec.Type).To(Equal(corev1.ServiceTypeClusterIP))\n\n            // \u2500\u2500 Assert: ConfigMap is created \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            cmLookup := types.NamespacedName{Name: WebAppName + \"-html\", Namespace: WebAppNamespace}\n            createdCM := &amp;corev1.ConfigMap{}\n            Eventually(func() error {\n                return k8sClient.Get(ctx, cmLookup, createdCM)\n            }, timeout, interval).Should(Succeed())\n\n            Expect(createdCM.Data[\"index.html\"]).To(ContainSubstring(\"Hello from test\"))\n        })\n\n        It(\"should update the Deployment when replicas change\", func() {\n            // Patch replicas from 2 \u2192 4\n            webapp := &amp;webappv1.WebApp{}\n            Expect(k8sClient.Get(ctx, types.NamespacedName{\n                Name:      WebAppName,\n                Namespace: WebAppNamespace,\n            }, webapp)).To(Succeed())\n\n            webapp.Spec.Replicas = 4\n            Expect(k8sClient.Update(ctx, webapp)).To(Succeed())\n\n            // Assert Deployment reflects new replica count\n            Eventually(func() int32 {\n                dep := &amp;appsv1.Deployment{}\n                _ = k8sClient.Get(ctx, types.NamespacedName{\n                    Name:      WebAppName,\n                    Namespace: WebAppNamespace,\n                }, dep)\n                return *dep.Spec.Replicas\n            }, timeout, interval).Should(Equal(int32(4)))\n        })\n\n        It(\"should update the ConfigMap when message changes\", func() {\n            webapp := &amp;webappv1.WebApp{}\n            Expect(k8sClient.Get(ctx, types.NamespacedName{\n                Name:      WebAppName,\n                Namespace: WebAppNamespace,\n            }, webapp)).To(Succeed())\n\n            webapp.Spec.Message = \"Updated message via test\"\n            Expect(k8sClient.Update(ctx, webapp)).To(Succeed())\n\n            Eventually(func() string {\n                cm := &amp;corev1.ConfigMap{}\n                _ = k8sClient.Get(ctx, types.NamespacedName{\n                    Name:      WebAppName + \"-html\",\n                    Namespace: WebAppNamespace,\n                }, cm)\n                return cm.Data[\"index.html\"]\n            }, timeout, interval).Should(ContainSubstring(\"Updated message via test\"))\n        })\n    })\n})\n</code></pre>"},{"location":"24-kubebuilder/#1203-run-the-tests","title":"12.03 Run the tests","text":"<pre><code>make test\n\n# With verbose output\nmake test ARGS=\"--v\"\n\n# Run only specific tests\ngo test ./internal/controller/... -run \"WebApp Controller\" -v\n</code></pre>"},{"location":"24-kubebuilder/#part-13-build-and-deploy-the-operator-in-cluster","title":"Part 13 - Build and Deploy the Operator In-Cluster","text":""},{"location":"24-kubebuilder/#1301-write-the-dockerfile","title":"13.01 Write the Dockerfile","text":"<p>The scaffolded <code>Dockerfile</code> uses a multi-stage build:</p> <pre><code># Build stage\nFROM golang:1.22 AS builder\nARG TARGETOS\nARG TARGETARCH\nWORKDIR /workspace\n\nCOPY go.mod go.sum ./\nRUN go mod download\n\nCOPY cmd/       cmd/\nCOPY api/       api/\nCOPY internal/  internal/\n\nRUN CGO_ENABLED=0 GOOS=${TARGETOS:-linux} GOARCH=${TARGETARCH} \\\n    go build -a -o manager cmd/main.go\n\n# Runtime stage - distroless for minimal attack surface\nFROM gcr.io/distroless/static:nonroot\nWORKDIR /\nCOPY --from=builder /workspace/manager .\nUSER 65532:65532\nENTRYPOINT [\"/manager\"]\n</code></pre>"},{"location":"24-kubebuilder/#1302-build-and-push-the-image","title":"13.02 Build and push the image","text":"<pre><code># Set your image registry\nexport IMG=ghcr.io/your-org/webapp-operator:v0.1.0\n\n# Build the image\nmake docker-build IMG=${IMG}\n\n# Push (requires docker login)\nmake docker-push IMG=${IMG}\n</code></pre>"},{"location":"24-kubebuilder/#1303-deploy-to-the-cluster","title":"13.03 Deploy to the cluster","text":"<pre><code># Deploys CRDs, RBAC, and the operator Deployment via Kustomize\nmake deploy IMG=${IMG}\n\n# Verify the operator pod is running\nkubectl get pods -n webapp-system\n\n# Watch the operator logs\nkubectl logs -n webapp-system \\\n    -l control-plane=controller-manager \\\n    -f\n</code></pre>"},{"location":"24-kubebuilder/#1304-apply-a-webapp-cr-in-the-cluster","title":"13.04 Apply a WebApp CR in the cluster","text":"<pre><code>kubectl apply -f config/samples/apps_v1_webapp.yaml\n\nkubectl get wa -A\nkubectl get deployment,service,configmap -l app.kubernetes.io/managed-by=webapp-operator\n</code></pre>"},{"location":"24-kubebuilder/#1305-undeploy","title":"13.05 Undeploy","text":"<pre><code>make undeploy\nmake uninstall   # remove CRDs\n</code></pre>"},{"location":"24-kubebuilder/#part-14-quick-reference-cheatsheet","title":"Part 14 - Quick Reference Cheatsheet","text":"Goal Command Init new project <code>kubebuilder init --domain codewizard.io --repo codewizard.io/my-op</code> Create CRD + controller <code>kubebuilder create api --group apps --version v1 --kind MyKind</code> Create webhook <code>kubebuilder create webhook --group apps --version v1 --kind MyKind --defaulting --programmatic-validation</code> Regenerate DeepCopy <code>make generate</code> Regenerate CRD/RBAC YAML <code>make manifests</code> Install CRDs to cluster <code>make install</code> Run controller locally <code>make run</code> Run tests (no cluster) <code>make test</code> Build operator image <code>make docker-build IMG=myregistry/myop:v1</code> Push operator image <code>make docker-push IMG=myregistry/myop:v1</code> Deploy to cluster <code>make deploy IMG=myregistry/myop:v1</code> Undeploy <code>make undeploy</code> Remove CRDs <code>make uninstall</code> View all API resources <code>kubectl api-resources --api-group=apps.codewizard.io</code> Short-name get <code>kubectl get wa</code> Watch reconcile logs <code>kubectl logs -n webapp-system -l control-plane=controller-manager -f</code>"},{"location":"24-kubebuilder/#exercises","title":"Exercises","text":"<p>The following exercises build on the <code>webapp-operator</code> created in this lab.</p>"},{"location":"24-kubebuilder/#exercise-01-add-a-maxunavailable-field","title":"Exercise 01 - Add a <code>maxUnavailable</code> field","text":"<p>Add a <code>MaxUnavailable</code> field to <code>WebAppSpec</code> that maps to <code>deployment.spec.strategy.rollingUpdate.maxUnavailable</code>.</p> <ul> <li>Add the field with validation <code>Minimum=0</code>, <code>Maximum=replicas</code></li> <li>Default it to <code>1</code></li> <li>Implement the mapping in <code>reconcileDeployment()</code></li> <li>Run <code>make generate &amp;&amp; make manifests</code></li> <li>Test with a patch: <code>kubectl patch wa my-webapp --type=merge -p '{\"spec\":{\"maxUnavailable\":2}}'</code></li> </ul> Hint <pre><code>// In WebAppSpec\n// +kubebuilder:validation:Minimum=0\n// +kubebuilder:default=1\nMaxUnavailable int32 `json:\"maxUnavailable,omitempty\"`\n\n// In reconcileDeployment, set:\nintMaxUnavailable := intstr.FromInt32(webapp.Spec.MaxUnavailable)\ndesired.Spec.Strategy = appsv1.DeploymentStrategy{\n    Type: appsv1.RollingUpdateDeploymentStrategyType,\n    RollingUpdate: &amp;appsv1.RollingUpdateDeployment{\n        MaxUnavailable: &amp;intMaxUnavailable,\n    },\n}\n</code></pre>"},{"location":"24-kubebuilder/#exercise-02-surface-a-url-in-the-status","title":"Exercise 02 - Surface a URL in the status","text":"<p>Add a <code>URL</code> field to <code>WebAppStatus</code> that the controller populates with <code>http://&lt;service-cluster-ip&gt;:&lt;port&gt;</code>.</p> <ul> <li>Add <code>URL string</code> to <code>WebAppStatus</code></li> <li>In <code>updateStatus()</code>, fetch the Service\u2019s <code>ClusterIP</code> and populate <code>updated.Status.URL</code></li> <li>Verify: <code>kubectl get wa my-webapp -o jsonpath='{.status.url}'</code></li> </ul> Hint <pre><code>// Fetch the service inside updateStatus\nsvc := &amp;corev1.Service{}\nif err := r.Get(ctx, types.NamespacedName{Name: webapp.Name, Namespace: webapp.Namespace}, svc); err == nil {\n    if svc.Spec.ClusterIP != \"\" {\n        updated.Status.URL = fmt.Sprintf(\"http://%s:%d\", svc.Spec.ClusterIP, webapp.Spec.Port)\n    }\n}\n</code></pre>"},{"location":"24-kubebuilder/#exercise-03-add-a-paused-field-to-skip-reconciliation","title":"Exercise 03 - Add a <code>Paused</code> field to skip reconciliation","text":"<p>Add a <code>Paused bool</code> field to <code>WebAppSpec</code>. When <code>true</code>, the controller exits the reconcile loop early with a log message, leaving all resources unchanged.</p> <ul> <li>Add <code>// +kubebuilder:default=false</code> and <code>Paused bool</code> to <code>WebAppSpec</code></li> <li>In <code>Reconcile()</code>, check early: <code>if webapp.Spec.Paused { logger.Info(\"Skipping - paused\"); return ctrl.Result{}, nil }</code></li> <li>Test: <code>kubectl patch wa my-webapp --type=merge -p '{\"spec\":{\"paused\":true}}'</code>   Then scale down manually and confirm the operator does not restore it.</li> </ul>"},{"location":"24-kubebuilder/#exercise-04-add-a-webapppolicy-crd","title":"Exercise 04 - Add a <code>WebAppPolicy</code> CRD","text":"<p>Create a second API kind <code>WebAppPolicy</code> in the same group that defines a <code>maxReplicas</code> namespace-scoped limit.</p> <pre><code>kubebuilder create api \\\n    --group apps \\\n    --version v1 \\\n    --kind WebAppPolicy\n</code></pre> <p>In the <code>WebAppReconciler</code>, after fetching the <code>WebApp</code>, look for a <code>WebAppPolicy</code> in the same namespace and enforce the <code>maxReplicas</code> limit by clamping <code>webapp.Spec.Replicas</code>.</p>"},{"location":"24-kubebuilder/#exercise-05-write-a-webhook-that-prevents-downscaling-to-0","title":"Exercise 05 - Write a webhook that prevents downscaling to 0","text":"<p>Add a validating webhook that rejects any update to a <code>WebApp</code> where <code>spec.replicas</code> was changed from &gt; 0 to 0.</p> <pre><code>func (r *WebApp) ValidateUpdate(old runtime.Object) (admission.Warnings, error) {\n    oldWebApp := old.(*WebApp)\n    if oldWebApp.Spec.Replicas &gt; 0 &amp;&amp; r.Spec.Replicas == 0 {\n        return nil, apierrors.NewForbidden(...)\n    }\n    return r.validateWebApp()\n}\n</code></pre>"},{"location":"24-kubebuilder/#cleanup","title":"Cleanup","text":"<pre><code># Delete sample CRs\nkubectl delete wa --all\n\n# Uninstall CRDs from the cluster\nmake uninstall\n\n# If deployed in-cluster\nmake undeploy\n\n# Remove the project directory\ncd ..\nrm -rf webapp-operator\n</code></pre>"},{"location":"24-kubebuilder/#troubleshooting","title":"Troubleshooting","text":"<ul> <li><code>make generate</code> fails with missing tool:</li> </ul> <p>The Makefile auto-downloads <code>controller-gen</code>. If it fails, install manually:</p> <pre><code>go install sigs.k8s.io/controller-tools/cmd/controller-gen@latest\n</code></pre> <p></p> <ul> <li>CRD not appearing after <code>make install</code>:</li> </ul> <p>Verify the CRD was generated and applied:</p> <pre><code>cat config/crd/bases/*.yaml | head -20\nkubectl get crds | grep &lt;your-domain&gt;\nkubectl describe crd &lt;crd-name&gt;\n</code></pre> <p></p> <ul> <li>Controller crashes with <code>cannot create resource</code> errors:</li> </ul> <p>The RBAC markers in your controller file may be incomplete. Ensure you have <code>//+kubebuilder:rbac</code> markers for every resource your controller touches:</p> <pre><code># Check the generated RBAC role\ncat config/rbac/role.yaml\n\n# Regenerate after adding markers\nmake manifests\nmake deploy IMG=${IMG}\n</code></pre> <p></p> <ul> <li><code>make test</code> fails with <code>envtest</code> binary not found:</li> </ul> <p>Download the envtest binaries:</p> <pre><code>make envtest\nsource &lt;(setup-envtest use -p env)\nmake test\n</code></pre> <p></p> <ul> <li>Status not updating on the CR:</li> </ul> <p>Ensure you use <code>r.Status().Update()</code> (the status subresource), not <code>r.Update()</code>:</p> <pre><code>kubectl get &lt;kind&gt; -o jsonpath='{.status}'\nkubectl describe &lt;kind&gt; &lt;name&gt; | grep -A10 \"Status:\"\n</code></pre> <p></p> <ul> <li>Owner reference / garbage collection not working:</li> </ul> <p>Verify <code>ctrl.SetControllerReference()</code> is called for every child resource and that <code>.Owns()</code> is set in <code>SetupWithManager</code>:</p> <pre><code>kubectl get deployment &lt;child&gt; -o jsonpath='{.metadata.ownerReferences}' | jq\n</code></pre>"},{"location":"24-kubebuilder/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Kubebuilder Book for advanced patterns: multi-version APIs, conversion webhooks, and external event sources.</li> <li>Try the Operator SDK which builds on Kubebuilder with Ansible and Helm-based operators.</li> <li>Browse OperatorHub.io to see real-world operators and their patterns.</li> <li>Learn about Finalizers for managing external resources during deletion.</li> <li>Combine your operator with ArgoCD (Lab 18) for GitOps-managed operator deployments.</li> <li>Practice operator tasks in the Kubernetes Kubebuilder Tasks section.</li> </ul>"},{"location":"25-krew/","title":"Krew - kubectl Plugin Manager","text":"<ul> <li>Welcome to the <code>Krew</code> hands-on lab! In this tutorial, you\u2019ll learn how to use <code>Krew</code>, the plugin manager for <code>kubectl</code>, to discover, install, and manage plugins that extend Kubernetes CLI capabilities.</li> <li>You\u2019ll install useful plugins, explore their functionality, and learn how to build your own <code>kubectl</code> plugin.</li> </ul>"},{"location":"25-krew/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What <code>Krew</code> is and why it is useful</li> <li>How to install and configure <code>Krew</code></li> <li>How to discover, install, update, and remove <code>kubectl</code> plugins</li> <li>Essential <code>kubectl</code> plugins for daily Kubernetes work</li> <li>How to create your own <code>kubectl</code> plugin</li> <li>Troubleshooting and best practices</li> </ul>"},{"location":"25-krew/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link Krew Official Site krew.sigs.k8s.io Krew User Guide krew.sigs.k8s.io/docs/user-guide Krew Plugin Index krew.sigs.k8s.io/plugins Krew GitHub Repository github.com/kubernetes-sigs/krew Writing Custom kubectl Plugins kubernetes.io/docs/tasks/extend-kubectl kubectl Plugin Discovery kubernetes.io/docs/concepts/extend-kubectl"},{"location":"25-krew/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (minikube, kind, Docker Desktop, or cloud-managed)</li> <li><code>kubectl</code> installed and configured to communicate with your cluster</li> <li><code>git</code> installed (for plugin installation from source)</li> <li>Basic familiarity with the command line</li> </ul>"},{"location":"25-krew/#introduction","title":"Introduction","text":""},{"location":"25-krew/#what-is-krew","title":"What is Krew?","text":"<ul> <li><code>Krew</code> is a plugin manager for <code>kubectl</code>, the Kubernetes command-line tool.</li> <li>It works similarly to <code>apt</code> for Debian, <code>brew</code> for macOS, or <code>npm</code> for Node.js - but specifically for <code>kubectl</code> plugins.</li> <li><code>Krew</code> helps you discover, install, and manage plugins that extend <code>kubectl</code> with additional commands and capabilities.</li> </ul>"},{"location":"25-krew/#why-use-krew","title":"Why use Krew?","text":"<ul> <li>Discoverability: Browse 200+ community-maintained plugins from a centralized index</li> <li>Easy installation: Install plugins with a single command (<code>kubectl krew install &lt;plugin&gt;</code>)</li> <li>Version management: Update all installed plugins at once</li> <li>Cross-platform: Works on Linux, macOS, and Windows</li> <li>No sudo required: Plugins are installed in your home directory</li> </ul>"},{"location":"25-krew/#how-kubectl-plugins-work","title":"How kubectl plugins work","text":"<ul> <li><code>kubectl</code> has a built-in plugin mechanism: any executable in your <code>PATH</code> named <code>kubectl-&lt;plugin_name&gt;</code> becomes a <code>kubectl</code> subcommand.</li> <li>For example, an executable named <code>kubectl-whoami</code> can be invoked as <code>kubectl whoami</code>.</li> <li><code>Krew</code> manages the installation of these executables into <code>~/.krew/bin/</code>.</li> </ul> <pre><code>graph LR\n    A[kubectl krew install foo] --&gt; B[Download plugin binary]\n    B --&gt; C[Place in ~/.krew/bin/kubectl-foo]\n    C --&gt; D[kubectl foo is now available]\n    style A fill:#326CE5,color:#fff\n    style D fill:#326CE5,color:#fff</code></pre>"},{"location":"25-krew/#krew-architecture","title":"Krew Architecture","text":"<pre><code>~/.krew/\n\u251c\u2500\u2500 bin/                  # Plugin binaries (added to PATH)\n\u2502   \u251c\u2500\u2500 kubectl-krew      # Krew itself\n\u2502   \u251c\u2500\u2500 kubectl-ctx        # Context switcher plugin\n\u2502   \u251c\u2500\u2500 kubectl-ns         # Namespace switcher plugin\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 index/                # Plugin index (metadata)\n\u2502   \u2514\u2500\u2500 default/\n\u2502       \u2514\u2500\u2500 plugins/\n\u251c\u2500\u2500 receipts/             # Installation records\n\u2502   \u251c\u2500\u2500 ctx.yaml\n\u2502   \u251c\u2500\u2500 ns.yaml\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 store/                # Downloaded plugin archives\n</code></pre>"},{"location":"25-krew/#common-krew-commands","title":"Common <code>Krew</code> Commands","text":"<p>Below are the most common <code>Krew</code> commands you\u2019ll use when working with <code>kubectl</code> plugins.</p> <code>kubectl krew install</code> - Install a plugin <p>Syntax: <code>kubectl krew install &lt;plugin-name&gt;</code></p> <p>Description: Downloads and installs a plugin from the Krew index.</p> <ul> <li>Downloads the plugin binary for your OS and architecture</li> <li>Places the binary in <code>~/.krew/bin/</code></li> <li> <p>The plugin becomes available as <code>kubectl &lt;plugin-name&gt;</code></p> <pre><code># Install a single plugin\nkubectl krew install ctx\n\n# Install multiple plugins at once\nkubectl krew install ctx ns view-secret\n\n# The plugin is now available\nkubectl ctx\n</code></pre> </li> </ul> <code>kubectl krew list</code> - List installed plugins <p>Syntax: <code>kubectl krew list</code></p> <p>Description: Shows all currently installed plugins managed by Krew.</p> <ul> <li>Displays plugin name and installed version</li> <li> <p>Only shows Krew-managed plugins (not manually installed ones)</p> <pre><code># List all installed plugins\nkubectl krew list\n\n# Example output:\n# PLUGIN          VERSION\n# ctx             v0.9.5\n# krew            v0.4.4\n# ns              v0.9.5\n# view-secret     v0.12.0\n</code></pre> </li> </ul> <code>kubectl krew search</code> - Search for plugins <p>Syntax: <code>kubectl krew search [keyword]</code></p> <p>Description: Searches the Krew plugin index for available plugins.</p> <ul> <li>Without arguments, lists all available plugins</li> <li>With a keyword, filters plugins by name and description</li> <li> <p>Shows plugin name, description, installed status, and stability</p> <pre><code># List all available plugins\nkubectl krew search\n\n# Search for plugins by keyword\nkubectl krew search secret\n\n# Search for resource-related plugins\nkubectl krew search resource\n\n# Example output:\n# NAME             DESCRIPTION                                  INSTALLED\n# view-secret      Decode Kubernetes secrets                    yes\n# modify-secret    Edit secrets in-place                        no\n</code></pre> </li> </ul> <code>kubectl krew info</code> - Show plugin details <p>Syntax: <code>kubectl krew info &lt;plugin-name&gt;</code></p> <p>Description: Displays detailed information about a specific plugin.</p> <ul> <li>Shows plugin name, version, homepage, and description</li> <li> <p>Includes supported platforms and caveats</p> <pre><code># Show plugin details\nkubectl krew info ctx\n\n# Example output:\n# NAME: ctx\n# URI: https://github.com/ahmetb/kubectx/...\n# SHA256: ...\n# VERSION: v0.9.5\n# HOMEPAGE: https://github.com/ahmetb/kubectx\n# DESCRIPTION:\n# ...\n</code></pre> </li> </ul> <code>kubectl krew update</code> - Update the plugin index <p>Syntax: <code>kubectl krew update</code></p> <p>Description: Fetches the latest plugin index from the Krew plugin repository.</p> <ul> <li>Downloads the latest plugin metadata</li> <li>Does NOT update installed plugins</li> <li> <p>Should be run periodically to discover new plugins</p> <pre><code># Update the plugin index\nkubectl krew update\n\n# Output:\n# Updated the local copy of plugin index.\n</code></pre> </li> </ul> <code>kubectl krew upgrade</code> - Upgrade installed plugins <p>Syntax: <code>kubectl krew upgrade [plugin-name]</code></p> <p>Description: Upgrades installed plugins to their latest versions.</p> <ul> <li>Without arguments, upgrades ALL installed plugins</li> <li> <p>With a plugin name, upgrades only that specific plugin</p> <pre><code># Upgrade all installed plugins\nkubectl krew upgrade\n\n# Upgrade a specific plugin\nkubectl krew upgrade ctx\n\n# Output:\n# Updated the local copy of plugin index.\n# Upgrading plugin: ctx\n# Upgraded plugin: ctx\n</code></pre> </li> </ul> <code>kubectl krew uninstall</code> - Remove a plugin <p>Syntax: <code>kubectl krew uninstall &lt;plugin-name&gt;</code></p> <p>Description: Removes an installed plugin.</p> <ul> <li>Deletes the plugin binary and installation receipt</li> <li> <p>The <code>kubectl &lt;plugin-name&gt;</code> command will no longer be available</p> <pre><code># Uninstall a plugin\nkubectl krew uninstall ctx\n\n# Verify it's removed\nkubectl krew list\n</code></pre> </li> </ul>"},{"location":"25-krew/#essential-kubectl-plugins","title":"Essential kubectl Plugins","text":"<p>Below is a curated list of the most useful <code>kubectl</code> plugins organized by category. These plugins can dramatically improve your daily Kubernetes workflow.</p>"},{"location":"25-krew/#cluster-navigation","title":"Cluster Navigation","text":"Plugin Description Usage <code>ctx</code> Switch between Kubernetes contexts quickly <code>kubectl ctx &lt;context&gt;</code> <code>ns</code> Switch between namespaces quickly <code>kubectl ns &lt;namespace&gt;</code> <code>get-all</code> List ALL resources in a namespace (not just common ones) <code>kubectl get-all</code>"},{"location":"25-krew/#debugging-inspection","title":"Debugging &amp; Inspection","text":"Plugin Description Usage <code>debug-shell</code> Create a debug container in a running pod <code>kubectl debug-shell &lt;pod&gt;</code> <code>pod-inspect</code> Detailed pod inspection with events and logs <code>kubectl pod-inspect &lt;pod&gt;</code> <code>node-shell</code> Open a shell into a Kubernetes node <code>kubectl node-shell &lt;node&gt;</code> <code>blame</code> Show who last modified Kubernetes resources <code>kubectl blame &lt;resource&gt; &lt;name&gt;</code> <code>tail</code> Stream logs from multiple pods (like <code>stern</code>) <code>kubectl tail --ns &lt;ns&gt;</code>"},{"location":"25-krew/#security-secrets","title":"Security &amp; Secrets","text":"Plugin Description Usage <code>view-secret</code> Decode and view Kubernetes secrets easily <code>kubectl view-secret &lt;secret&gt;</code> <code>modify-secret</code> Edit secrets in-place with base64 encoding handled <code>kubectl modify-secret &lt;secret&gt;</code> <code>access-matrix</code> Show RBAC access matrix for a resource <code>kubectl access-matrix</code> <code>who-can</code> Show who has RBAC permissions for an action <code>kubectl who-can get pods</code> <code>view-cert</code> View certificate details from secrets <code>kubectl view-cert &lt;secret&gt;</code>"},{"location":"25-krew/#resource-management","title":"Resource Management","text":"Plugin Description Usage <code>resource-capacity</code> Show resource requests/limits and utilization <code>kubectl resource-capacity</code> <code>view-utilization</code> Show cluster resource utilization <code>kubectl view-utilization</code> <code>count</code> Count resources by kind <code>kubectl count pods</code> <code>images</code> List container images running in the cluster <code>kubectl images</code> <code>neat</code> Remove clutter from Kubernetes YAML output <code>kubectl get pod -o yaml \\| kubectl neat</code>"},{"location":"25-krew/#networking","title":"Networking","text":"Plugin Description Usage <code>ingress-rule</code> List Ingress rules across the cluster <code>kubectl ingress-rule</code> <code>sniff</code> Capture network traffic from a pod using tcpdump/Wireshark <code>kubectl sniff &lt;pod&gt;</code>"},{"location":"25-krew/#lab","title":"Lab","text":""},{"location":"25-krew/#step-01-install-krew","title":"Step 01 - Install <code>Krew</code>","text":"<ul> <li>Before using <code>Krew</code>, you need to install it on your local machine.</li> </ul> macOSLinuxWindows (PowerShell) <pre><code># Using Homebrew (recommended)\nbrew install krew\n</code></pre> <pre><code>(\n  set -x; cd \"$(mktemp -d)\" &amp;&amp;\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\n  KREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\n  tar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n  ./\"${KREW}\" install krew\n)\n</code></pre> <pre><code># Download and install Krew\nInvoke-WebRequest -Uri \"https://github.com/kubernetes-sigs/krew/releases/latest/download/krew-windows_amd64.exe\" -OutFile krew.exe\n.\\krew.exe install krew\n</code></pre>"},{"location":"25-krew/#add-krew-to-path","title":"Add Krew to PATH","text":"<p>After installation, you must add <code>Krew</code> to your shell <code>PATH</code>:</p> bashzsh <pre><code># Add to ~/.bashrc\necho 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <pre><code># Add to ~/.zshrc\necho 'export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n</code></pre>"},{"location":"25-krew/#verify-installation","title":"Verify Installation","text":"<pre><code>kubectl krew version\n\n## Expected output (version may vary):\n# OPTION            VALUE\n# GitTag            v0.4.4\n# GitCommit         ...\n# IndexURI          https://github.com/kubernetes-sigs/krew-index.git\n# BasePath          /home/user/.krew\n# IndexPath         /home/user/.krew/index/default\n# InstallPath       /home/user/.krew/store\n# BinPath           /home/user/.krew/bin\n</code></pre>"},{"location":"25-krew/#step-02-update-the-plugin-index","title":"Step 02 - Update the Plugin Index","text":"<ul> <li>Before installing plugins, update the local plugin index to get the latest available plugins:</li> </ul> <pre><code>kubectl krew update\n</code></pre> <ul> <li>You should see output similar to:</li> </ul> <pre><code>Updated the local copy of plugin index.\n</code></pre>"},{"location":"25-krew/#step-03-discover-plugins","title":"Step 03 - Discover Plugins","text":"<ul> <li>Browse the available plugins to find tools that match your needs:</li> </ul> <pre><code># List all available plugins (200+)\nkubectl krew search\n\n# Search for specific functionality\nkubectl krew search secret\nkubectl krew search debug\nkubectl krew search resource\n\n# Get detailed info about a specific plugin\nkubectl krew info view-secret\n</code></pre>"},{"location":"25-krew/#step-04-install-essential-plugins","title":"Step 04 - Install Essential Plugins","text":"<ul> <li>Install a curated set of essential plugins for daily Kubernetes work:</li> </ul> <pre><code># Context and namespace switching\nkubectl krew install ctx\nkubectl krew install ns\n\n# Secret management\nkubectl krew install view-secret\n\n# Resource inspection\nkubectl krew install get-all\nkubectl krew install resource-capacity\nkubectl krew install count\nkubectl krew install images\n\n# RBAC\nkubectl krew install access-matrix\n</code></pre> <ul> <li>Verify the installed plugins:</li> </ul> <pre><code>kubectl krew list\n</code></pre>"},{"location":"25-krew/#step-05-use-context-namespace-switchers","title":"Step 05 - Use Context &amp; Namespace Switchers","text":"<ul> <li>The <code>ctx</code> and <code>ns</code> plugins make switching between contexts and namespaces effortless:</li> </ul>"},{"location":"25-krew/#switch-contexts-with-ctx","title":"Switch contexts with <code>ctx</code>","text":"<pre><code># List all available contexts (current context is highlighted)\nkubectl ctx\n\n# Switch to a different context\nkubectl ctx my-other-cluster\n\n# Switch back to the previous context\nkubectl ctx -\n</code></pre>"},{"location":"25-krew/#switch-namespaces-with-ns","title":"Switch namespaces with <code>ns</code>","text":"<pre><code># List all namespaces (current namespace is highlighted)\nkubectl ns\n\n# Switch to a different namespace\nkubectl ns kube-system\n\n# Switch back to the previous namespace\nkubectl ns -\n\n# Verify the current namespace\nkubectl config view --minify -o jsonpath='{.contexts[0].context.namespace}'\n</code></pre>"},{"location":"25-krew/#step-06-inspect-secrets","title":"Step 06 - Inspect Secrets","text":"<ul> <li>The <code>view-secret</code> plugin makes it easy to decode and view Kubernetes secrets:</li> </ul> <pre><code># First, create a test secret\nkubectl create secret generic demo-secret \\\n  --from-literal=username=admin \\\n  --from-literal=password=s3cr3t\n\n# List all secrets in the current namespace\nkubectl view-secret\n\n# View all keys in a specific secret\nkubectl view-secret demo-secret\n\n# View a specific key (decoded automatically)\nkubectl view-secret demo-secret username\n\n# View a specific key from a specific namespace\nkubectl view-secret demo-secret password -n default\n\n# Compare with the standard kubectl approach (base64 encoded)\nkubectl get secret demo-secret -o jsonpath='{.data.password}' | base64 -d\n</code></pre>"},{"location":"25-krew/#step-07-explore-cluster-resources","title":"Step 07 - Explore Cluster Resources","text":""},{"location":"25-krew/#get-all-resources-with-get-all","title":"Get all resources with <code>get-all</code>","text":"<ul> <li>Unlike <code>kubectl get all</code>, which only shows common resources, <code>get-all</code> lists every resource in a namespace:</li> </ul> <pre><code># List ALL resources in the current namespace\nkubectl get-all\n\n# List all resources in a specific namespace\nkubectl get-all -n kube-system\n</code></pre> <p>Why <code>get-all</code> instead of <code>kubectl get all</code>?</p> <p><code>kubectl get all</code> only shows a subset of resources (Pods, Services, Deployments, ReplicaSets, etc.). It does not show ConfigMaps, Secrets, Ingresses, ServiceAccounts, RBAC resources, CRDs, and many others. The <code>get-all</code> plugin discovers and lists every resource type in the namespace.</p>"},{"location":"25-krew/#count-resources-with-count","title":"Count resources with <code>count</code>","text":"<pre><code># Count all pods across all namespaces\nkubectl count pods\n\n# Count deployments in a specific namespace\nkubectl count deployments -n kube-system\n\n# Count all resource types\nkubectl count all\n</code></pre>"},{"location":"25-krew/#view-resource-capacity","title":"View resource capacity","text":"<pre><code># Show node resource requests, limits, and utilization\nkubectl resource-capacity\n\n# Show with pods breakdown\nkubectl resource-capacity --pods\n\n# Show utilization percentages\nkubectl resource-capacity --util\n\n# Show specific resource type\nkubectl resource-capacity --pods --util --sort cpu.util\n</code></pre>"},{"location":"25-krew/#step-08-check-rbac-permissions","title":"Step 08 - Check RBAC Permissions","text":"<ul> <li>The <code>access-matrix</code> plugin helps you understand who can do what in your cluster:</li> </ul> <pre><code># Show access matrix for pods in the current namespace\nkubectl access-matrix --for pods\n\n# Show access matrix for all resources\nkubectl access-matrix\n\n# Show what a specific service account can do\nkubectl access-matrix --sa default:default\n</code></pre>"},{"location":"25-krew/#step-09-list-container-images","title":"Step 09 - List Container Images","text":"<ul> <li>The <code>images</code> plugin shows all container images running in the cluster:</li> </ul> <pre><code># List all images across all namespaces\nkubectl images --all-namespaces\n\n# List images in a specific namespace\nkubectl images -n kube-system\n\n# Show image columns\nkubectl images --columns namespace,name,image\n</code></pre>"},{"location":"25-krew/#step-10-update-and-manage-plugins","title":"Step 10 - Update and Manage Plugins","text":"<pre><code># Update the plugin index (fetch new metadata)\nkubectl krew update\n\n# Upgrade all installed plugins to latest versions\nkubectl krew upgrade\n\n# Upgrade a specific plugin\nkubectl krew upgrade ctx\n\n# Uninstall a plugin you no longer need\nkubectl krew uninstall sniff\n\n# Check for outdated plugins\nkubectl krew list\n</code></pre>"},{"location":"25-krew/#step-11-create-your-own-kubectl-plugin","title":"Step 11 - Create Your Own kubectl Plugin","text":"<ul> <li>Any executable in your <code>PATH</code> named <code>kubectl-&lt;name&gt;</code> becomes a <code>kubectl</code> plugin.</li> <li>Let\u2019s create a simple plugin that shows pod resource usage:</li> </ul>"},{"location":"25-krew/#create-the-plugin-script","title":"Create the plugin script","text":"<pre><code>cat &lt;&lt; 'EOF' &gt; kubectl-pod-status\n#!/bin/bash\n# kubectl-pod-status: Show a summary of pod statuses in a namespace\n\nNAMESPACE=\"${1:---all-namespaces}\"\n\nif [ \"$NAMESPACE\" = \"--all-namespaces\" ]; then\n  NS_FLAG=\"--all-namespaces\"\nelse\n  NS_FLAG=\"-n $NAMESPACE\"\nfi\n\necho \"=== Pod Status Summary ===\"\necho \"\"\n\n# Count pods by status\nkubectl get pods $NS_FLAG --no-headers 2&gt;/dev/null | \\\n  awk '{print $NF}' | \\\n  sort | \\\n  uniq -c | \\\n  sort -rn | \\\n  while read count status; do\n    printf \"  %-15s %s\\n\" \"$status\" \"$count\"\n  done\n\necho \"\"\necho \"=== Total Pods ===\"\nTOTAL=$(kubectl get pods $NS_FLAG --no-headers 2&gt;/dev/null | wc -l | tr -d ' ')\necho \"  Total: $TOTAL\"\nEOF\n</code></pre>"},{"location":"25-krew/#install-and-test-the-plugin","title":"Install and test the plugin","text":"<pre><code># Make it executable\nchmod +x kubectl-pod-status\n\n# Move it to a directory in your PATH\nsudo mv kubectl-pod-status /usr/local/bin/\n\n# Verify kubectl recognizes it\nkubectl plugin list\n\n# Use your custom plugin\nkubectl pod-status\nkubectl pod-status kube-system\nkubectl pod-status default\n</code></pre>"},{"location":"25-krew/#clean-up","title":"Clean up","text":"<pre><code># Remove the custom plugin\nsudo rm /usr/local/bin/kubectl-pod-status\n</code></pre>"},{"location":"25-krew/#exercises","title":"Exercises","text":"<p>The following exercises will test your understanding of <code>Krew</code> and <code>kubectl</code> plugins. Try to solve each exercise on your own before revealing the solution.</p>"},{"location":"25-krew/#01-find-and-install-a-plugin-by-use-case","title":"01. Find and Install a Plugin by Use Case","text":"<p>You need to find a plugin that can show you the certificate expiration dates stored in Kubernetes secrets. Find it, install it, and test it.</p>"},{"location":"25-krew/#scenario","title":"Scenario:","text":"<ul> <li>You manage a cluster with TLS certificates stored as secrets.</li> <li>You need a quick way to inspect certificate details without manual base64 decoding and openssl commands.</li> </ul> <p>Hint: Use <code>kubectl krew search cert</code> to find relevant plugins.</p> Solution <pre><code># Search for certificate-related plugins\nkubectl krew search cert\n\n# Install the view-cert plugin\nkubectl krew install view-cert\n\n# Create a self-signed certificate for testing\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\\n  -keyout tls.key -out tls.crt \\\n  -subj \"/CN=test.example.com\"\n\n# Create a TLS secret\nkubectl create secret tls test-tls \\\n  --cert=tls.crt --key=tls.key\n\n# View the certificate details\nkubectl view-cert test-tls\n\n# Clean up\nkubectl delete secret test-tls\nrm tls.key tls.crt\n</code></pre>"},{"location":"25-krew/#02-compare-kubectl-get-all-vs-kubectl-get-all","title":"02. Compare <code>kubectl get all</code> vs <code>kubectl get-all</code>","text":"<p>Run both commands in the <code>kube-system</code> namespace and document the differences. How many more resource types does <code>get-all</code> discover?</p>"},{"location":"25-krew/#scenario_1","title":"Scenario:","text":"<ul> <li>You need to audit all resources in a namespace for compliance purposes.</li> <li>The standard <code>kubectl get all</code> misses many resource types.</li> </ul> <p>Hint: Pipe both outputs through <code>grep \"kind:\"</code> or count the lines.</p> Solution <pre><code># Standard kubectl - limited resource types\nkubectl get all -n kube-system 2&gt;/dev/null | head -50\n\n# get-all plugin - discovers ALL resource types\nkubectl get-all -n kube-system 2&gt;/dev/null | head -100\n\n# Count resource types from standard kubectl\necho \"=== kubectl get all ===\"\nkubectl get all -n kube-system --no-headers 2&gt;/dev/null | wc -l\n\n# Count resource types from get-all\necho \"=== kubectl get-all ===\"\nkubectl get-all -n kube-system --no-headers 2&gt;/dev/null | wc -l\n\n# The get-all plugin typically finds 2-5x more resources including:\n# - ConfigMaps, Secrets, ServiceAccounts\n# - Roles, RoleBindings, ClusterRoles\n# - Events, Endpoints, EndpointSlices\n# - PodDisruptionBudgets, NetworkPolicies\n# - Custom Resources (CRDs)\n</code></pre>"},{"location":"25-krew/#03-audit-cluster-rbac-permissions","title":"03. Audit Cluster RBAC Permissions","text":"<p>Use <code>Krew</code> plugins to answer: \u201cWhich service accounts in the <code>default</code> namespace can create Deployments?\u201d</p>"},{"location":"25-krew/#scenario_2","title":"Scenario:","text":"<ul> <li>As a cluster administrator, you need to audit RBAC to ensure least-privilege access.</li> <li>Understanding who can create workloads is critical for security.</li> </ul> <p>Hint: Install <code>who-can</code> or use <code>access-matrix</code> to check RBAC permissions.</p> Solution <pre><code># Install the who-can plugin\nkubectl krew install who-can\n\n# Check who can create deployments in the default namespace\nkubectl who-can create deployments -n default\n\n# Check who can delete pods\nkubectl who-can delete pods -n default\n\n# Check who can get secrets (sensitive!)\nkubectl who-can get secrets -n default\n\n# Use access-matrix for a broader view\nkubectl access-matrix --for deployments -n default\n</code></pre>"},{"location":"25-krew/#04-check-cluster-resource-utilization","title":"04. Check Cluster Resource Utilization","text":"<p>Install and use the <code>resource-capacity</code> plugin to identify nodes with the highest CPU and memory utilization. Then check if any node is over 80% utilized.</p>"},{"location":"25-krew/#scenario_3","title":"Scenario:","text":"<ul> <li>You\u2019re troubleshooting slow pod scheduling and suspect resource exhaustion.</li> <li>You need a quick overview of cluster capacity vs. usage.</li> </ul> <p>Hint: Use <code>kubectl resource-capacity --util --sort cpu.util</code>.</p> Solution <pre><code># Install the resource-capacity plugin (if not already installed)\nkubectl krew install resource-capacity\n\n# Show basic resource capacity\nkubectl resource-capacity\n\n# Show with utilization percentages\nkubectl resource-capacity --util\n\n# Sort by CPU utilization (highest first)\nkubectl resource-capacity --util --sort cpu.util\n\n# Show per-pod breakdown\nkubectl resource-capacity --pods --util\n\n# Show resource capacity with specific output\nkubectl resource-capacity --util --pod-count\n\n# To check if any node is over 80%, examine the output percentages\n# Nodes with CPU% or Memory% above 80% may need attention\n</code></pre>"},{"location":"25-krew/#05-create-a-multi-function-kubectl-plugin","title":"05. Create a Multi-Function kubectl Plugin","text":"<p>Create a custom <code>kubectl</code> plugin called <code>kubectl-cluster-info-extended</code> that shows:</p> <ol> <li>Current context and namespace</li> <li>Node count and status</li> <li>Pod count by namespace (top 5)</li> <li>Resource utilization summary</li> </ol>"},{"location":"25-krew/#scenario_4","title":"Scenario:","text":"<ul> <li>You want a single command that gives you a quick cluster health overview.</li> <li>This is useful as a morning check or after a deployment.</li> </ul> <p>Hint: Combine multiple <code>kubectl</code> commands in a bash script named <code>kubectl-cluster_info_extended</code>.</p> Solution <pre><code>cat &lt;&lt; 'PLUGINEOF' &gt; kubectl-cluster_info_extended\n#!/bin/bash\n# kubectl-cluster-info-extended: Quick cluster health overview\n\necho \"============================================\"\necho \"  Kubernetes Cluster Overview\"\necho \"============================================\"\necho \"\"\n\n# Current context\necho \"--- Context &amp; Namespace ---\"\nCONTEXT=$(kubectl config current-context 2&gt;/dev/null)\nNAMESPACE=$(kubectl config view --minify -o jsonpath='{.contexts[0].context.namespace}' 2&gt;/dev/null)\necho \"  Context:   ${CONTEXT:-N/A}\"\necho \"  Namespace: ${NAMESPACE:-default}\"\necho \"\"\n\n# Node status\necho \"--- Nodes ---\"\nkubectl get nodes --no-headers 2&gt;/dev/null | \\\n  awk '{printf \"  %-30s %-10s %s\\n\", $1, $2, $5}'\nNODE_COUNT=$(kubectl get nodes --no-headers 2&gt;/dev/null | wc -l | tr -d ' ')\necho \"  Total: $NODE_COUNT nodes\"\necho \"\"\n\n# Pod count by namespace (top 5)\necho \"--- Pods by Namespace (Top 5) ---\"\nkubectl get pods --all-namespaces --no-headers 2&gt;/dev/null | \\\n  awk '{print $1}' | \\\n  sort | uniq -c | sort -rn | head -5 | \\\n  while read count ns; do\n    printf \"  %-30s %s pods\\n\" \"$ns\" \"$count\"\n  done\necho \"\"\n\n# Pod status summary\necho \"--- Pod Status Summary ---\"\nkubectl get pods --all-namespaces --no-headers 2&gt;/dev/null | \\\n  awk '{print $4}' | \\\n  sort | uniq -c | sort -rn | \\\n  while read count status; do\n    printf \"  %-15s %s\\n\" \"$status\" \"$count\"\n  done\necho \"\"\necho \"============================================\"\nPLUGINEOF\n\n# Install and test\nchmod +x kubectl-cluster_info_extended\nsudo mv kubectl-cluster_info_extended /usr/local/bin/\n\n# Run the plugin (note: hyphens in the name become spaces or underscores)\nkubectl cluster-info-extended\n\n# Clean up\nsudo rm /usr/local/bin/kubectl-cluster_info_extended\n</code></pre>"},{"location":"25-krew/#06-manage-plugin-lifecycle","title":"06. Manage Plugin Lifecycle","text":"<p>Perform a full plugin lifecycle: search, install, use, update, and uninstall. Track the disk space used by <code>Krew</code> plugins before and after.</p>"},{"location":"25-krew/#scenario_5","title":"Scenario:","text":"<ul> <li>You\u2019re managing a shared jump host where disk space matters.</li> <li>You need to keep installed plugins lean and up-to-date.</li> </ul> <p>Hint: Check the <code>~/.krew/store/</code> directory size with <code>du -sh</code>.</p> Solution <pre><code># Check initial disk usage\necho \"=== Before ===\"\ndu -sh ~/.krew/ 2&gt;/dev/null || echo \"Krew not yet installed\"\n\n# Update the plugin index\nkubectl krew update\n\n# Search and install a plugin\nkubectl krew search neat\nkubectl krew install neat\n\n# Use the plugin\nkubectl get pod -n kube-system -o yaml | head -1 | kubectl neat\n\n# List installed plugins\nkubectl krew list\n\n# Check disk usage after install\necho \"=== After Install ===\"\ndu -sh ~/.krew/\ndu -sh ~/.krew/store/\n\n# Upgrade all plugins\nkubectl krew upgrade\n\n# Uninstall the plugin\nkubectl krew uninstall neat\n\n# Check disk usage after uninstall\necho \"=== After Uninstall ===\"\ndu -sh ~/.krew/\n\n# List remaining plugins\nkubectl krew list\n</code></pre>"},{"location":"25-krew/#finalize-cleanup","title":"Finalize &amp; Cleanup","text":"<ul> <li>To remove all plugins installed during this lab:</li> </ul> <pre><code># List all installed plugins\nkubectl krew list\n\n# Uninstall specific plugins\nkubectl krew uninstall ctx ns view-secret get-all \\\n  resource-capacity count images access-matrix\n\n# Remove Krew completely (optional)\nrm -rf ~/.krew\n</code></pre> <ul> <li>Remove the <code>PATH</code> entry from your shell configuration file if you no longer want <code>Krew</code>.</li> </ul>"},{"location":"25-krew/#troubleshooting","title":"Troubleshooting","text":"<ul> <li><code>kubectl krew</code> command not found:</li> </ul> <p>Make sure <code>~/.krew/bin</code> is in your <code>PATH</code>. Add it to your shell configuration:</p> <pre><code>export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n</code></pre> <p></p> <ul> <li>Plugin installation fails:</li> </ul> <p>Update the plugin index and try again:</p> <pre><code>kubectl krew update\nkubectl krew install &lt;plugin-name&gt;\n</code></pre> <p></p> <ul> <li>Plugin not found after installation:</li> </ul> <p>Verify the plugin is in the Krew bin directory and your <code>PATH</code> is correct:</p> <pre><code>ls ~/.krew/bin/\necho $PATH | tr ':' '\\n' | grep krew\n</code></pre> <p></p> <ul> <li>Custom plugin not recognized by kubectl:</li> </ul> <p>Ensure the plugin file name follows the pattern <code>kubectl-&lt;name&gt;</code>, is executable, and is in a directory listed in your <code>PATH</code>:</p> <pre><code># Check kubectl can find your plugins\nkubectl plugin list\n\n# Verify the file is executable\nls -la /usr/local/bin/kubectl-*\n</code></pre> <p></p> <ul> <li>Permission errors during installation:</li> </ul> <p><code>Krew</code> installs plugins in your home directory and should not require <code>sudo</code>. If you encounter permission issues:</p> <pre><code># Check Krew directory ownership\nls -la ~/.krew/\n\n# Fix ownership if needed\nsudo chown -R $(whoami) ~/.krew/\n</code></pre>"},{"location":"25-krew/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the full Krew Plugin Index for more plugins</li> <li>Learn about writing kubectl plugins in any language</li> <li>Submit your own plugin to the Krew Index</li> <li>Combine <code>Krew</code> plugins with shell aliases for even faster workflows</li> <li>Explore Krew Custom Indexes for private plugin distribution</li> </ul>"},{"location":"26-kubeapps/","title":"Kubeapps - Application Dashboard for Kubernetes","text":"<ul> <li>Welcome to the <code>Kubeapps</code> hands-on lab! In this tutorial, you\u2019ll learn how to install and use <code>Kubeapps</code>, a web-based UI for deploying and managing applications on your Kubernetes cluster using Helm charts.</li> <li>You\u2019ll set up Kubeapps, configure authentication, browse application catalogs, deploy applications, and manage their lifecycle through the dashboard.</li> </ul>"},{"location":"26-kubeapps/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What <code>Kubeapps</code> is and why it is useful</li> <li>How to install <code>Kubeapps</code> using Helm</li> <li>How to configure authentication and RBAC for <code>Kubeapps</code></li> <li>How to browse, deploy, upgrade, and delete applications from the dashboard</li> <li>How to add custom Helm repositories and private registries</li> <li>How to manage application catalogs and configurations</li> <li>Troubleshooting and best practices</li> </ul>"},{"location":"26-kubeapps/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link Kubeapps Official Site kubeapps.dev Kubeapps GitHub Repository github.com/vmware-tanzu/kubeapps Kubeapps Documentation kubeapps.dev/docs Bitnami Kubeapps Helm Chart artifacthub.io/packages/helm/bitnami/kubeapps Helm Official Docs helm.sh/docs Kubernetes RBAC kubernetes.io/docs/reference/access-authn-authz/rbac"},{"location":"26-kubeapps/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (minikube, kind, Docker Desktop, or cloud-managed)</li> <li><code>kubectl</code> installed and configured to communicate with your cluster</li> <li><code>Helm</code> (v3+) installed</li> <li>A web browser to access the Kubeapps dashboard</li> <li>Basic understanding of Helm charts (see Lab 13 - HelmChart)</li> </ul>"},{"location":"26-kubeapps/#introduction","title":"Introduction","text":""},{"location":"26-kubeapps/#what-is-kubeapps","title":"What is Kubeapps?","text":"<ul> <li><code>Kubeapps</code> is an in-cluster web-based application that enables users to deploy and manage applications on a Kubernetes cluster using Helm charts.</li> <li>It provides a graphical user interface (GUI) for browsing, deploying, upgrading, and deleting Helm-based applications.</li> <li>Think of it as an \u201cApp Store\u201d for your Kubernetes cluster.</li> </ul>"},{"location":"26-kubeapps/#why-use-kubeapps","title":"Why use Kubeapps?","text":"<ul> <li>Visual application catalog: Browse Helm charts from multiple repositories with a rich UI</li> <li>One-click deployments: Deploy complex applications without writing <code>helm install</code> commands</li> <li>Self-service: Enable developers to deploy applications without deep Kubernetes knowledge</li> <li>Multi-repository support: Aggregate charts from multiple Helm repositories and OCI registries</li> <li>Upgrade management: View available upgrades and apply them through the UI</li> <li>Multi-cluster support: Manage applications across multiple Kubernetes clusters</li> <li>RBAC integration: Control who can deploy what, based on Kubernetes RBAC</li> </ul>"},{"location":"26-kubeapps/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    subgraph \"User\"\n        A[Browser]\n    end\n\n    subgraph \"Kubernetes Cluster\"\n        subgraph \"Kubeapps Namespace\"\n            B[Kubeapps Frontend&lt;br&gt;nginx]\n            C[Kubeapps APIs&lt;br&gt;kubeapps-internal-kubeappsapis]\n            D[AppRepository Controller]\n            E[PostgreSQL / Redis&lt;br&gt;Asset Cache]\n        end\n\n        subgraph \"Helm Repositories\"\n            F[Bitnami]\n            G[Custom Repos]\n            H[OCI Registries]\n        end\n\n        subgraph \"Target Namespace\"\n            I[Deployed Application&lt;br&gt;Pods, Services, etc.]\n        end\n    end\n\n    A --&gt;|HTTPS| B\n    B --&gt; C\n    C --&gt; D\n    D --&gt; F\n    D --&gt; G\n    D --&gt; H\n    C --&gt;|Helm SDK| I\n    D --&gt; E\n\n    style A fill:#326CE5,color:#fff\n    style B fill:#326CE5,color:#fff\n    style I fill:#326CE5,color:#fff</code></pre>"},{"location":"26-kubeapps/#key-components","title":"Key Components","text":"Component Description Kubeapps Frontend Nginx-based web UI that users interact with through the browser Kubeapps APIs Backend service that handles Helm operations, catalog browsing, and auth AppRepository Controller Syncs Helm chart metadata from configured repositories into the asset cache Asset Cache (DB) PostgreSQL or Redis instance storing chart metadata for fast catalog browsing"},{"location":"26-kubeapps/#common-operations","title":"Common Operations","text":"<p>Below is a reference of common operations you\u2019ll perform with <code>Kubeapps</code>, both through the UI and via command line.</p>"},{"location":"26-kubeapps/#helm-cli-reference-for-kubeapps","title":"Helm CLI Reference for Kubeapps","text":"<code>helm install</code> - Install Kubeapps <p>Syntax: <code>helm install kubeapps bitnami/kubeapps [options]</code></p> <p>Description: Deploys Kubeapps to your Kubernetes cluster.</p> <ul> <li>Installs all Kubeapps components (frontend, API server, controller, database)</li> <li>Creates the necessary RBAC resources</li> <li> <p>Configures the default chart repositories</p> <pre><code># Basic install\nhelm install kubeapps bitnami/kubeapps \\\n  --namespace kubeapps --create-namespace\n\n# Install with custom values\nhelm install kubeapps bitnami/kubeapps \\\n  --namespace kubeapps --create-namespace \\\n  -f custom-values.yaml\n\n# Install with specific chart version\nhelm install kubeapps bitnami/kubeapps \\\n  --namespace kubeapps --create-namespace \\\n  --version 15.0.0\n</code></pre> </li> </ul> Create ServiceAccount and Token <p>Description: Create a ServiceAccount with cluster-admin permissions for Kubeapps authentication.</p> <ul> <li>Kubeapps uses Kubernetes tokens for authentication</li> <li>The ServiceAccount must have appropriate RBAC permissions</li> <li> <p>For production, use more restrictive roles than cluster-admin</p> <pre><code># Create a namespace for the operator\nkubectl create namespace kubeapps-user\n\n# Create a ServiceAccount\nkubectl create serviceaccount kubeapps-operator \\\n  -n kubeapps-user\n\n# Bind cluster-admin role\nkubectl create clusterrolebinding kubeapps-operator \\\n  --clusterrole=cluster-admin \\\n  --serviceaccount=kubeapps-user:kubeapps-operator\n\n# Create a token secret\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kubeapps-operator-token\n  namespace: kubeapps-user\n  annotations:\n    kubernetes.io/service-account.name: kubeapps-operator\ntype: kubernetes.io/service-account-token\nEOF\n\n# Get the token\nkubectl get secret kubeapps-operator-token \\\n  -n kubeapps-user \\\n  -o go-template='{{.data.token | base64decode}}'\n</code></pre> </li> </ul> Access Kubeapps Dashboard <p>Description: Access the Kubeapps web UI through port-forwarding or Ingress.</p> <ul> <li>Port-forwarding is the simplest approach for development</li> <li> <p>For production, configure an Ingress with TLS</p> <pre><code># Port-forward to access the dashboard\nkubectl port-forward svc/kubeapps \\\n  -n kubeapps 8080:80\n\n# Open in browser: http://localhost:8080\n# Paste the token from the previous step to log in\n</code></pre> </li> </ul> Add Custom Helm Repository <p>Description: Add a custom Helm chart repository to Kubeapps.</p> <ul> <li>Can be done through the UI or using kubectl</li> <li>Supports public and private repositories</li> <li> <p>Supports OCI-based registries</p> <pre><code># Add a custom repository via kubectl\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: kubeapps.com/v1alpha1\nkind: AppRepository\nmetadata:\n  name: my-custom-repo\n  namespace: kubeapps\nspec:\n  url: https://charts.example.com\n  # For private repos, add auth:\n  # auth:\n  #   header:\n  #     secretKeyRef:\n  #       name: my-repo-auth\n  #       key: authorizationHeader\nEOF\n</code></pre> </li> </ul>"},{"location":"26-kubeapps/#rbac-configuration","title":"RBAC Configuration","text":"<p><code>Kubeapps</code> leverages Kubernetes RBAC to control access. Different roles provide different levels of access.</p>"},{"location":"26-kubeapps/#role-examples","title":"Role Examples","text":""},{"location":"26-kubeapps/#read-only-user-view-only","title":"Read-Only User (View Only)","text":"<pre><code># kubeapps-viewer-role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: kubeapps-viewer\nrules:\n  # Allow listing Helm releases\n  - apiGroups: [\"\"]\n    resources: [\"secrets\"]\n    verbs: [\"get\", \"list\"]\n  # Allow viewing workloads\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments\", \"statefulsets\", \"daemonsets\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"services\", \"configmaps\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n</code></pre>"},{"location":"26-kubeapps/#namespace-deployer-deploy-to-specific-namespace","title":"Namespace Deployer (Deploy to Specific Namespace)","text":"<pre><code># kubeapps-deployer-role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: kubeapps-deployer\n  namespace: my-team\nrules:\n  - apiGroups: [\"*\"]\n    resources: [\"*\"]\n    verbs: [\"*\"]\n</code></pre>"},{"location":"26-kubeapps/#cluster-admin-full-access","title":"Cluster Admin (Full Access)","text":"<pre><code># For lab/development purposes, bind to cluster-admin\nkubectl create clusterrolebinding kubeapps-admin \\\n  --clusterrole=cluster-admin \\\n  --serviceaccount=kubeapps-user:kubeapps-operator\n</code></pre> <p>Production RBAC</p> <p>Never use <code>cluster-admin</code> in production environments. Create specific roles that grant only the minimum permissions needed for each user or team.</p>"},{"location":"26-kubeapps/#lab","title":"Lab","text":""},{"location":"26-kubeapps/#step-01-install-kubeapps","title":"Step 01 - Install <code>Kubeapps</code>","text":""},{"location":"26-kubeapps/#add-the-bitnami-helm-repository","title":"Add the Bitnami Helm repository","text":"<pre><code># Add the Bitnami repository\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\n# Update the repository index\nhelm repo update\n</code></pre>"},{"location":"26-kubeapps/#install-kubeapps","title":"Install Kubeapps","text":"<pre><code># Install Kubeapps in its own namespace\nhelm install kubeapps bitnami/kubeapps \\\n  --namespace kubeapps \\\n  --create-namespace \\\n  --wait\n</code></pre>"},{"location":"26-kubeapps/#verify-the-installation","title":"Verify the installation","text":"<pre><code># Check all pods are running\nkubectl get pods -n kubeapps\n\n# Expected output (pod names will vary):\n# NAME                                                READY   STATUS    RESTARTS   AGE\n# kubeapps-...                                        1/1     Running   0          2m\n# kubeapps-internal-kubeappsapis-...                   1/1     Running   0          2m\n# kubeapps-internal-apprepository-controller-...       1/1     Running   0          2m\n# kubeapps-postgresql-0                                1/1     Running   0          2m\n\n# Check all services\nkubectl get svc -n kubeapps\n</code></pre>"},{"location":"26-kubeapps/#step-02-create-authentication-credentials","title":"Step 02 - Create Authentication Credentials","text":"<ul> <li><code>Kubeapps</code> uses Kubernetes tokens for authentication. We need to create a ServiceAccount and generate a token:</li> </ul> <pre><code># Create a dedicated namespace for Kubeapps users\nkubectl create namespace kubeapps-user\n</code></pre> <pre><code># Create a ServiceAccount for the Kubeapps operator\nkubectl create serviceaccount kubeapps-operator \\\n  -n kubeapps-user\n</code></pre> <pre><code># Bind the cluster-admin role to the service account\nkubectl create clusterrolebinding kubeapps-operator \\\n  --clusterrole=cluster-admin \\\n  --serviceaccount=kubeapps-user:kubeapps-operator\n</code></pre> <pre><code># Create a Secret to generate the token\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kubeapps-operator-token\n  namespace: kubeapps-user\n  annotations:\n    kubernetes.io/service-account.name: kubeapps-operator\ntype: kubernetes.io/service-account-token\nEOF\n</code></pre>"},{"location":"26-kubeapps/#retrieve-the-token","title":"Retrieve the token","text":"<pre><code># Get the authentication token (save this - you'll need it to log in)\nkubectl get secret kubeapps-operator-token \\\n  -n kubeapps-user \\\n  -o go-template='{{.data.token | base64decode}}'\n</code></pre> <p>Save the token</p> <p>Copy the token output and save it somewhere accessible. You\u2019ll paste it into the Kubeapps login page.</p>"},{"location":"26-kubeapps/#step-03-access-the-dashboard","title":"Step 03 - Access the Dashboard","text":""},{"location":"26-kubeapps/#option-a-port-forwarding-development","title":"Option A: Port Forwarding (Development)","text":"<pre><code># Forward port 8080 to the Kubeapps service\nkubectl port-forward svc/kubeapps -n kubeapps 8080:80\n</code></pre> <ul> <li>Open your browser and navigate to: http://localhost:8080</li> <li>Paste the token from Step 02 into the login field</li> <li>Click Submit</li> </ul>"},{"location":"26-kubeapps/#option-b-ingress-production-like","title":"Option B: Ingress (Production-like)","text":"<pre><code># kubeapps-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kubeapps-ingress\n  namespace: kubeapps\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: kubeapps.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubeapps\n                port:\n                  number: 80\n</code></pre> <pre><code># Apply the ingress\nkubectl apply -f kubeapps-ingress.yaml\n\n# Add to /etc/hosts (Linux/macOS)\necho \"127.0.0.1 kubeapps.local\" | sudo tee -a /etc/hosts\n\n# Access via: http://kubeapps.local\n</code></pre>"},{"location":"26-kubeapps/#step-04-browse-the-application-catalog","title":"Step 04 - Browse the Application Catalog","text":"<p>Once logged in, you\u2019ll see the Kubeapps dashboard:</p> <ol> <li>Catalog tab: Browse available Helm charts from configured repositories</li> <li>Applications tab: View deployed Helm releases</li> <li>Configuration tab: Manage app repositories and settings</li> </ol>"},{"location":"26-kubeapps/#browse-available-charts","title":"Browse available charts","text":"<ul> <li>Click on the Catalog tab</li> <li>Use the search bar to find applications (e.g., \u201cnginx\u201d, \u201credis\u201d, \u201cpostgresql\u201d)</li> <li>Click on a chart card to see details, including:</li> <li>Chart description and README</li> <li>Available versions</li> <li>Default configuration values</li> <li>Installation instructions</li> </ul>"},{"location":"26-kubeapps/#step-05-deploy-an-application-via-the-ui","title":"Step 05 - Deploy an Application via the UI","text":"<p>Let\u2019s deploy NGINX using the Kubeapps dashboard:</p> <ol> <li>Click Catalog in the navigation</li> <li>Search for nginx</li> <li>Select bitnami/nginx from the results</li> <li>Click Deploy</li> <li>Configure the deployment:</li> <li>Name: <code>my-nginx</code></li> <li>Namespace: <code>default</code> (or create a new one)</li> <li>Review the default values in the YAML editor</li> <li>Modify <code>replicaCount</code> to <code>2</code> if desired</li> <li>Click Deploy to install</li> </ol>"},{"location":"26-kubeapps/#verify-from-the-command-line","title":"Verify from the command line","text":"<pre><code># Check the Helm release\nhelm list --all-namespaces\n\n# Check the deployed resources\nkubectl get all -l app.kubernetes.io/instance=my-nginx\n\n# Check the pods\nkubectl get pods -l app.kubernetes.io/instance=my-nginx\n</code></pre>"},{"location":"26-kubeapps/#step-06-deploy-an-application-via-cli","title":"Step 06 - Deploy an Application via CLI","text":"<p>You can also deploy applications that will appear in the Kubeapps dashboard using the Helm CLI:</p> <pre><code># Deploy Redis via Helm (Kubeapps will detect it automatically)\nhelm install my-redis bitnami/redis \\\n  --namespace default \\\n  --set architecture=standalone \\\n  --set auth.enabled=false\n</code></pre> <ul> <li>Go back to the Kubeapps dashboard</li> <li>Click on the Applications tab</li> <li>You should see both <code>my-nginx</code> and <code>my-redis</code> listed</li> </ul>"},{"location":"26-kubeapps/#step-07-upgrade-an-application","title":"Step 07 - Upgrade an Application","text":""},{"location":"26-kubeapps/#via-the-dashboard","title":"Via the Dashboard","text":"<ol> <li>Click on Applications tab</li> <li>Click on my-nginx</li> <li>Click Upgrade</li> <li>Modify the values (e.g., change <code>replicaCount</code> to <code>3</code>)</li> <li>Click Deploy to apply the upgrade</li> </ol>"},{"location":"26-kubeapps/#via-cli-also-reflected-in-the-dashboard","title":"Via CLI (also reflected in the dashboard)","text":"<pre><code># Upgrade Redis to enable auth\nhelm upgrade my-redis bitnami/redis \\\n  --set architecture=standalone \\\n  --set auth.enabled=true \\\n  --set auth.password=mypassword\n\n# Check the upgrade in the dashboard\n# The revision number should increment\nhelm history my-redis\n</code></pre>"},{"location":"26-kubeapps/#step-08-add-a-custom-repository","title":"Step 08 - Add a Custom Repository","text":""},{"location":"26-kubeapps/#via-the-dashboard_1","title":"Via the Dashboard","text":"<ol> <li>Click on Configuration in the navigation (gear icon)</li> <li>Click App Repositories</li> <li>Click Add App Repository</li> <li>Fill in:</li> <li>Name: <code>codecentric</code></li> <li>URL: <code>https://codecentric.github.io/helm-charts</code></li> <li>Click Install Repository</li> </ol>"},{"location":"26-kubeapps/#via-kubectl","title":"Via kubectl","text":"<pre><code># Add a custom repository via kubectl manifest\ncat &lt;&lt;EOF | kubectl apply -n kubeapps -f -\napiVersion: kubeapps.com/v1alpha1\nkind: AppRepository\nmetadata:\n  name: codecentric\n  namespace: kubeapps\nspec:\n  url: https://codecentric.github.io/helm-charts\nEOF\n</code></pre> <pre><code># Verify the repository was added\nkubectl get apprepositories -n kubeapps\n</code></pre> <ul> <li>Go to the Catalog tab and you should now see charts from the new repository.</li> </ul>"},{"location":"26-kubeapps/#step-09-view-application-details","title":"Step 09 - View Application Details","text":"<ul> <li>In the Kubeapps dashboard, click on any deployed application to see:</li> <li>Status: The current state of the Helm release</li> <li>Resources: All Kubernetes resources created by the chart (Pods, Services, ConfigMaps, etc.)</li> <li>Notes: Post-install notes from the chart</li> <li>Values: The configuration values used for the deployment</li> <li>Revision History: All previous versions and their configurations</li> </ul>"},{"location":"26-kubeapps/#compare-with-cli-output","title":"Compare with CLI output","text":"<pre><code># View the same information from the command line\nhelm status my-nginx\nhelm get values my-nginx\nhelm history my-nginx\nkubectl get all -l app.kubernetes.io/instance=my-nginx\n</code></pre>"},{"location":"26-kubeapps/#step-10-delete-an-application","title":"Step 10 - Delete an Application","text":""},{"location":"26-kubeapps/#via-the-dashboard_2","title":"Via the Dashboard","text":"<ol> <li>Click on Applications tab</li> <li>Click on the application you want to delete (e.g., my-nginx)</li> <li>Click Delete</li> <li>Confirm the deletion</li> </ol>"},{"location":"26-kubeapps/#via-cli","title":"Via CLI","text":"<pre><code># Delete the Redis deployment\nhelm uninstall my-redis\n</code></pre>"},{"location":"26-kubeapps/#step-11-create-a-restricted-user","title":"Step 11 - Create a Restricted User","text":"<p>Create a ServiceAccount with limited permissions (namespace-scoped only):</p> <pre><code># Create a namespace for the restricted user\nkubectl create namespace team-dev\n\n# Create a ServiceAccount\nkubectl create serviceaccount kubeapps-dev-user -n team-dev\n\n# Create a Role with limited permissions\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: kubeapps-dev-role\n  namespace: team-dev\nrules:\n  - apiGroups: [\"*\"]\n    resources: [\"*\"]\n    verbs: [\"*\"]\nEOF\n\n# Bind the role to the ServiceAccount\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: kubeapps-dev-binding\n  namespace: team-dev\nsubjects:\n  - kind: ServiceAccount\n    name: kubeapps-dev-user\n    namespace: team-dev\nroleRef:\n  kind: Role\n  name: kubeapps-dev-role\n  apiGroup: rbac.authorization.k8s.io\nEOF\n\n# Create a token for the dev user\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kubeapps-dev-token\n  namespace: team-dev\n  annotations:\n    kubernetes.io/service-account.name: kubeapps-dev-user\ntype: kubernetes.io/service-account-token\nEOF\n\n# Get the token\nkubectl get secret kubeapps-dev-token \\\n  -n team-dev \\\n  -o go-template='{{.data.token | base64decode}}'\n</code></pre> <ul> <li>Log out of Kubeapps and log in with this new token</li> <li>You should only be able to deploy applications to the <code>team-dev</code> namespace</li> </ul>"},{"location":"26-kubeapps/#exercises","title":"Exercises","text":"<p>The following exercises will test your understanding of <code>Kubeapps</code>. Try to solve each exercise on your own before revealing the solution.</p>"},{"location":"26-kubeapps/#01-deploy-wordpress-with-custom-values","title":"01. Deploy WordPress with Custom Values","text":"<p>Deploy a WordPress instance using the Kubeapps dashboard with custom database credentials and a specific number of replicas.</p>"},{"location":"26-kubeapps/#scenario","title":"Scenario:","text":"<ul> <li>Your team needs a quick WordPress deployment for a staging environment.</li> <li>You need to customize the database password and blog name before deploying.</li> </ul> <p>Hint: Search for \u201cwordpress\u201d in the catalog, modify the <code>wordpressPassword</code>, <code>wordpressBlogName</code>, and <code>replicaCount</code> values.</p> Solution <pre><code># Option 1: Via the Kubeapps UI\n# 1. Go to Catalog &gt; search \"wordpress\"\n# 2. Select bitnami/wordpress\n# 3. Click Deploy\n# 4. In the values editor, set:\n#    - wordpressUsername: admin\n#    - wordpressPassword: my-staging-password\n#    - wordpressBlogName: \"Staging Blog\"\n#    - replicaCount: 2\n# 5. Click Deploy\n\n# Option 2: Via CLI (reflected in the dashboard)\nhelm install my-wordpress bitnami/wordpress \\\n  --namespace default \\\n  --set wordpressUsername=admin \\\n  --set wordpressPassword=my-staging-password \\\n  --set wordpressBlogName=\"Staging Blog\" \\\n  --set replicaCount=2\n\n# Verify the deployment\nkubectl get pods -l app.kubernetes.io/instance=my-wordpress\nkubectl get svc -l app.kubernetes.io/instance=my-wordpress\n\n# Access WordPress via port-forward\nkubectl port-forward svc/my-wordpress 8081:80\n\n# Open http://localhost:8081 in your browser\n\n# Clean up\nhelm uninstall my-wordpress\n</code></pre>"},{"location":"26-kubeapps/#02-add-a-private-helm-repository","title":"02. Add a Private Helm Repository","text":"<p>Add a private Helm repository to Kubeapps that requires authentication. Use basic auth credentials.</p>"},{"location":"26-kubeapps/#scenario_1","title":"Scenario:","text":"<ul> <li>Your organization hosts internal Helm charts in a private repository.</li> <li>The repository requires basic authentication (username/password).</li> </ul> <p>Hint: Create a Kubernetes Secret with auth credentials first, then reference it in the AppRepository.</p> Solution <pre><code># 1. Create a secret with the repository credentials\nkubectl create secret generic my-private-repo-auth \\\n  -n kubeapps \\\n  --from-literal=authorizationHeader=\"Basic $(echo -n 'myuser:mypassword' | base64)\"\n\n# 2. Create the AppRepository with auth reference\ncat &lt;&lt;EOF | kubectl apply -n kubeapps -f -\napiVersion: kubeapps.com/v1alpha1\nkind: AppRepository\nmetadata:\n  name: my-private-repo\n  namespace: kubeapps\nspec:\n  url: https://charts.example.com\n  auth:\n    header:\n      secretKeyRef:\n        name: my-private-repo-auth\n        key: authorizationHeader\nEOF\n\n# 3. Verify the repository was added\nkubectl get apprepositories -n kubeapps\n\n# 4. Check the sync status\nkubectl get pods -n kubeapps -l app=apprepo-sync-my-private-repo\n\n# Clean up\nkubectl delete apprepository my-private-repo -n kubeapps\nkubectl delete secret my-private-repo-auth -n kubeapps\n</code></pre>"},{"location":"26-kubeapps/#03-upgrade-an-application-and-rollback","title":"03. Upgrade an Application and Rollback","text":"<p>Deploy PostgreSQL, upgrade it with new configuration, then rollback to the previous version.</p>"},{"location":"26-kubeapps/#scenario_2","title":"Scenario:","text":"<ul> <li>You deployed PostgreSQL with default settings.</li> <li>After upgrading with new memory limits, the pods fail to start.</li> <li>You need to rollback to the working version.</li> </ul> <p>Hint: Use the Kubeapps UI or <code>helm rollback</code> to revert to a previous revision.</p> Solution <pre><code># 1. Deploy PostgreSQL\nhelm install my-postgres bitnami/postgresql \\\n  --namespace default \\\n  --set auth.postgresPassword=initial-password\n\n# Verify it's running\nkubectl get pods -l app.kubernetes.io/instance=my-postgres\n\n# 2. Upgrade with new configuration\nhelm upgrade my-postgres bitnami/postgresql \\\n  --set auth.postgresPassword=initial-password \\\n  --set primary.resources.requests.memory=2Gi \\\n  --set primary.resources.limits.memory=4Gi\n\n# 3. Check the history\nhelm history my-postgres\n\n# 4. Rollback to the previous revision (via CLI)\nhelm rollback my-postgres 1\n\n# OR via the Kubeapps dashboard:\n# - Go to Applications &gt; my-postgres\n# - Click on the revision dropdown\n# - Select revision 1\n# - Click Rollback\n\n# 5. Verify the rollback\nhelm history my-postgres\nkubectl get pods -l app.kubernetes.io/instance=my-postgres\n\n# Clean up\nhelm uninstall my-postgres\n</code></pre>"},{"location":"26-kubeapps/#04-multi-namespace-deployment","title":"04. Multi-Namespace Deployment","text":"<p>Create two separate ServiceAccounts with permissions for different namespaces, deploy applications in each namespace using the appropriate token.</p>"},{"location":"26-kubeapps/#scenario_3","title":"Scenario:","text":"<ul> <li>Your organization has two teams: <code>team-frontend</code> and <code>team-backend</code>.</li> <li>Each team should only be able to deploy to their own namespace.</li> <li>Test that RBAC prevents cross-namespace deployments.</li> </ul> <p>Hint: Create two namespaces, two ServiceAccounts, and namespace-scoped RoleBindings.</p> Solution <pre><code># 1. Create namespaces\nkubectl create namespace team-frontend\nkubectl create namespace team-backend\n\n# 2. Create ServiceAccounts\nkubectl create serviceaccount frontend-deployer -n team-frontend\nkubectl create serviceaccount backend-deployer -n team-backend\n\n# 3. Create namespace-scoped Roles and RoleBindings\nfor TEAM in frontend backend; do\n  cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: deployer-role\n  namespace: team-${TEAM}\nrules:\n  - apiGroups: [\"*\"]\n    resources: [\"*\"]\n    verbs: [\"*\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: deployer-binding\n  namespace: team-${TEAM}\nsubjects:\n  - kind: ServiceAccount\n    name: ${TEAM}-deployer\n    namespace: team-${TEAM}\nroleRef:\n  kind: Role\n  name: deployer-role\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ${TEAM}-deployer-token\n  namespace: team-${TEAM}\n  annotations:\n    kubernetes.io/service-account.name: ${TEAM}-deployer\ntype: kubernetes.io/service-account-token\nEOF\ndone\n\n# 4. Get tokens for each team\necho \"=== Frontend Token ===\"\nkubectl get secret frontend-deployer-token \\\n  -n team-frontend \\\n  -o go-template='{{.data.token | base64decode}}'\n\necho \"\"\necho \"=== Backend Token ===\"\nkubectl get secret backend-deployer-token \\\n  -n team-backend \\\n  -o go-template='{{.data.token | base64decode}}'\n\n# 5. Log into Kubeapps with the frontend token\n# - You should only see the team-frontend namespace\n# - Try deploying nginx to team-frontend (should succeed)\n# - Try deploying to team-backend (should fail)\n\n# Clean up\nkubectl delete namespace team-frontend team-backend\n</code></pre>"},{"location":"26-kubeapps/#05-configure-kubeapps-with-custom-values","title":"05. Configure Kubeapps with Custom Values","text":"<p>Reinstall Kubeapps with custom configuration: enable Ingress, change the number of frontend replicas, and configure a specific set of default repositories.</p>"},{"location":"26-kubeapps/#scenario_4","title":"Scenario:","text":"<ul> <li>You\u2019re setting up Kubeapps for a production environment.</li> <li>You need to configure it with Ingress, high availability, and specific repositories.</li> </ul> <p>Hint: Create a custom <code>values.yaml</code> and pass it to <code>helm upgrade --install</code>.</p> Solution <pre><code># 1. Show the default values\nhelm show values bitnami/kubeapps &gt; kubeapps-default-values.yaml\n\n# 2. Create a custom values file\ncat &lt;&lt;EOF &gt; kubeapps-custom-values.yaml\n# Frontend configuration\nfrontend:\n  replicaCount: 2\n\n# Ingress configuration\ningress:\n  enabled: true\n  hostname: kubeapps.local\n  ingressClassName: nginx\n  annotations:\n    nginx.ingress.kubernetes.io/proxy-body-size: \"50m\"\n\n# Default app repositories\napprepository:\n  initialRepos:\n    - name: bitnami\n      url: https://charts.bitnami.com/bitnami\n    - name: ingress-nginx\n      url: https://kubernetes.github.io/ingress-nginx\n    - name: jetstack\n      url: https://charts.jetstack.io\n    - name: prometheus-community\n      url: https://prometheus-community.github.io/helm-charts\nEOF\n\n# 3. Upgrade Kubeapps with custom values\nhelm upgrade --install kubeapps bitnami/kubeapps \\\n  --namespace kubeapps \\\n  --create-namespace \\\n  -f kubeapps-custom-values.yaml \\\n  --wait\n\n# 4. Verify the changes\nkubectl get pods -n kubeapps\nkubectl get ingress -n kubeapps\nkubectl get apprepositories -n kubeapps\n\n# Clean up\nrm kubeapps-default-values.yaml kubeapps-custom-values.yaml\n</code></pre>"},{"location":"26-kubeapps/#06-monitor-application-health","title":"06. Monitor Application Health","text":"<p>Deploy an application through Kubeapps and use the dashboard to monitor its health. Intentionally break it and observe the status changes.</p>"},{"location":"26-kubeapps/#scenario_5","title":"Scenario:","text":"<ul> <li>You\u2019ve deployed an application and need to ensure it stays healthy.</li> <li>You want to understand how Kubeapps reports application health status.</li> </ul> <p>Hint: Deploy nginx, then force-delete a pod or scale down to 0 and observe the dashboard.</p> Solution <pre><code># 1. Deploy nginx via Kubeapps or CLI\nhelm install health-test bitnami/nginx \\\n  --namespace default \\\n  --set replicaCount=3\n\n# 2. Check the healthy state in Kubeapps dashboard\n# - Go to Applications &gt; health-test\n# - All pods should show as Running\n# - Status should be \"Deployed\"\n\n# 3. Break the application - delete a pod\nkubectl delete pod -l app.kubernetes.io/instance=health-test --wait=false\n\n# 4. Observe in the dashboard:\n# - One pod will briefly show as Terminating\n# - A new pod will appear as ContainerCreating\n# - Eventually all pods return to Running\n\n# 5. Scale down to 0 replicas\nkubectl scale deployment health-test-nginx --replicas=0\n\n# 6. Observe in the dashboard:\n# - No pods running\n# - Application shows degraded state\n\n# 7. Scale back up\nkubectl scale deployment health-test-nginx --replicas=3\n\n# 8. Verify recovery in the dashboard\n\n# Clean up\nhelm uninstall health-test\n</code></pre>"},{"location":"26-kubeapps/#finalize-cleanup","title":"Finalize &amp; Cleanup","text":"<ul> <li>To remove all resources created by this lab:</li> </ul> <pre><code># Remove deployed applications\nhelm uninstall my-nginx 2&gt;/dev/null\nhelm uninstall my-redis 2&gt;/dev/null\nhelm uninstall my-wordpress 2&gt;/dev/null\nhelm uninstall my-postgres 2&gt;/dev/null\nhelm uninstall health-test 2&gt;/dev/null\n\n# Remove Kubeapps\nhelm uninstall kubeapps -n kubeapps\n\n# Remove namespaces\nkubectl delete namespace kubeapps kubeapps-user team-dev 2&gt;/dev/null\n\n# Remove ClusterRoleBinding\nkubectl delete clusterrolebinding kubeapps-operator 2&gt;/dev/null\n</code></pre>"},{"location":"26-kubeapps/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Kubeapps pods not starting:</li> </ul> <p>Check pod status and events:</p> <pre><code>kubectl get pods -n kubeapps\nkubectl describe pod &lt;pod-name&gt; -n kubeapps\nkubectl logs &lt;pod-name&gt; -n kubeapps\n</code></pre> <p></p> <ul> <li>Cannot log in to Kubeapps:</li> </ul> <p>Ensure the ServiceAccount token is valid and the ClusterRoleBinding exists:</p> <pre><code># Verify the token secret exists\nkubectl get secret kubeapps-operator-token -n kubeapps-user\n\n# Verify the ClusterRoleBinding exists\nkubectl get clusterrolebinding kubeapps-operator\n\n# Regenerate the token if needed\nkubectl delete secret kubeapps-operator-token -n kubeapps-user\n# Then recreate it (see Step 02)\n</code></pre> <p></p> <ul> <li>Catalog shows no charts:</li> </ul> <p>The AppRepository controller may need time to sync. Check its logs:</p> <pre><code># Check sync pods\nkubectl get pods -n kubeapps -l app=apprepo\n\n# Check controller logs\nkubectl logs -n kubeapps -l app.kubernetes.io/component=apprepository-controller\n\n# Verify AppRepositories exist\nkubectl get apprepositories -n kubeapps\n</code></pre> <p></p> <ul> <li>Port-forward not working:</li> </ul> <p>Ensure the service is running and no other process uses port 8080:</p> <pre><code># Check the service exists\nkubectl get svc kubeapps -n kubeapps\n\n# Try a different local port\nkubectl port-forward svc/kubeapps -n kubeapps 9090:80\n</code></pre> <p></p> <ul> <li>Application deployment fails from the dashboard:</li> </ul> <p>Check the Kubeapps API server logs for details:</p> <pre><code>kubectl logs -n kubeapps -l app.kubernetes.io/component=kubeappsapis\n</code></pre> <p></p> <ul> <li>RBAC errors (403 Forbidden):</li> </ul> <p>The token\u2019s ServiceAccount lacks necessary permissions. Check and update the RoleBinding:</p> <pre><code># Check current bindings\nkubectl get clusterrolebindings | grep kubeapps\nkubectl get rolebindings -n &lt;target-namespace&gt; | grep kubeapps\n\n# Verify the ServiceAccount exists\nkubectl get serviceaccount -n kubeapps-user\n</code></pre>"},{"location":"26-kubeapps/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Kubeapps multi-cluster support for managing apps across clusters</li> <li>Explore OCI registry support for Helm charts stored in container registries</li> <li>Integrate Kubeapps with OIDC providers (Dex, Keycloak) for SSO authentication</li> <li>Set up Kubeapps with Operators to manage operator-based applications</li> <li>Explore Carvel packages as an alternative packaging format</li> <li>Configure private chart repositories with Docker registry integration</li> </ul>"},{"location":"27-kubeadm/","title":"kubeadm - Bootstrap a Kubernetes Cluster from Scratch","text":"<ul> <li>In this lab we will learn how to use kubeadm to bootstrap a fully functional Kubernetes cluster. We will set up a control-plane node, join worker nodes, install a CNI plugin, and verify the cluster is operational.</li> </ul>"},{"location":"27-kubeadm/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What <code>kubeadm</code> is and its role in the Kubernetes ecosystem</li> <li>How to prepare nodes (control-plane and workers) for cluster creation</li> <li>How to initialize a control-plane node with <code>kubeadm init</code></li> <li>How to join worker nodes with <code>kubeadm join</code></li> <li>How to install a Container Network Interface (CNI) plugin</li> <li>How to configure <code>kubectl</code> for the new cluster</li> <li>How to upgrade a cluster using <code>kubeadm upgrade</code></li> <li>How to reset and tear down a cluster with <code>kubeadm reset</code></li> <li>Best practices for production-grade cluster bootstrapping</li> </ul>"},{"location":"27-kubeadm/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link kubeadm Overview kubernetes.io/docs Creating a cluster with kubeadm kubernetes.io/docs kubeadm init kubernetes.io/docs kubeadm join kubernetes.io/docs kubeadm upgrade kubernetes.io/docs kubeadm reset kubernetes.io/docs Installing kubeadm kubernetes.io/docs Container Runtimes kubernetes.io/docs CNI Plugins kubernetes.io/docs"},{"location":"27-kubeadm/#prerequisites","title":"Prerequisites","text":"<ul> <li>Two or more Linux machines (physical or virtual) - one for the control-plane and one or more for workers</li> <li>Each machine must have at least 2 CPUs and 2 GB RAM (recommended)</li> <li>Full network connectivity between all machines</li> <li>Unique hostname, MAC address, and <code>product_uuid</code> for each node</li> <li>Swap disabled on all nodes</li> <li>A supported container runtime installed (containerd, CRI-O, or Docker with cri-dockerd)</li> </ul> <p>Lab Environment Options</p> <p>You can run this lab using:</p> <ul> <li>Multipass VMs on macOS/Linux</li> <li>Vagrant + VirtualBox or libvirt</li> <li>Cloud VMs (AWS EC2, GCP Compute Engine, Azure VMs)</li> <li>Docker containers for a quick local experiment (see the included <code>setup-k8s.sh</code>)</li> <li>LXD containers on Linux</li> </ul> <p>For this lab, instructions are given for Ubuntu/Debian-based systems. Adapt package commands as needed for other distributions.</p>"},{"location":"27-kubeadm/#kubeadm-architecture-overview","title":"kubeadm Architecture Overview","text":"<pre><code>graph TB\n    subgraph control[\"Control-Plane Node\"]\n        api[\"kube-apiserver\"]\n        etcd[\"etcd\"]\n        sched[\"kube-scheduler\"]\n        cm[\"kube-controller-manager\"]\n        kubelet_cp[\"kubelet\"]\n        cni_cp[\"CNI Plugin\"]\n    end\n\n    subgraph worker1[\"Worker Node 1\"]\n        kubelet_w1[\"kubelet\"]\n        proxy_w1[\"kube-proxy\"]\n        cni_w1[\"CNI Plugin\"]\n        pods_w1[\"Pods\"]\n    end\n\n    subgraph worker2[\"Worker Node 2\"]\n        kubelet_w2[\"kubelet\"]\n        proxy_w2[\"kube-proxy\"]\n        cni_w2[\"CNI Plugin\"]\n        pods_w2[\"Pods\"]\n    end\n\n    api &lt;--&gt; etcd\n    api &lt;--&gt; sched\n    api &lt;--&gt; cm\n    kubelet_cp --&gt; api\n    kubelet_w1 --&gt; api\n    kubelet_w2 --&gt; api\n\n    style control fill:#326CE5,color:#fff\n    style worker1 fill:#4a9,color:#fff\n    style worker2 fill:#4a9,color:#fff</code></pre> Component Role <code>kubeadm init</code> Bootstraps the control-plane node <code>kubeadm join</code> Joins a worker (or additional control-plane) node to the cluster <code>kubeadm upgrade</code> Upgrades the cluster to a newer Kubernetes version <code>kubeadm reset</code> Tears down the cluster on a node (undo <code>init</code> or <code>join</code>) <code>kubeadm token</code> Manages bootstrap tokens for node joining <code>kubeadm certs</code> Manages cluster certificates"},{"location":"27-kubeadm/#01-prepare-all-nodes","title":"01. Prepare all nodes","text":"<p>Run these steps on every node (control-plane and workers).</p>"},{"location":"27-kubeadm/#0101-disable-swap","title":"01.01 Disable swap","text":"<p>Kubernetes requires swap to be disabled:</p> <pre><code># Disable swap immediately\nsudo swapoff -a\n\n# Disable swap permanently (comment out swap in fstab)\nsudo sed -i '/ swap / s/^/#/' /etc/fstab\n</code></pre>"},{"location":"27-kubeadm/#0102-load-required-kernel-modules","title":"01.02 Load required kernel modules","text":"<pre><code># Load modules needed by containerd/Kubernetes networking\ncat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n</code></pre>"},{"location":"27-kubeadm/#0103-set-sysctl-parameters-for-kubernetes-networking","title":"01.03 Set sysctl parameters for Kubernetes networking","text":"<pre><code>cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\nEOF\n\n# Apply without reboot\nsudo sysctl --system\n</code></pre>"},{"location":"27-kubeadm/#0104-install-the-container-runtime-containerd","title":"01.04 Install the container runtime (containerd)","text":"<pre><code># Install containerd\nsudo apt-get update\nsudo apt-get install -y containerd\n\n# Generate default config\nsudo mkdir -p /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\n\n# Enable SystemdCgroup (required for kubeadm)\nsudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml\n\n# Restart containerd\nsudo systemctl restart containerd\nsudo systemctl enable containerd\n</code></pre> <p>Why containerd?</p> <ul> <li>Since Kubernetes 1.24, dockershim was removed from <code>kubelet</code>.</li> <li>The recommended container runtimes are containerd or CRI-O.</li> <li>If you need Docker CLI tools, install Docker separately - it will use containerd under the hood.</li> </ul>"},{"location":"27-kubeadm/#0105-install-kubeadm-kubelet-and-kubectl","title":"01.05 Install kubeadm, kubelet, and kubectl","text":"Ubuntu / DebianRHEL / CentOS / Fedora <pre><code># Install prerequisites\nsudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gpg\n\n# Add the Kubernetes apt repository signing key\nsudo mkdir -p -m 755 /etc/apt/keyrings\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | \\\n    sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# Add the Kubernetes apt repository\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | \\\n    sudo tee /etc/apt/sources.list.d/kubernetes.list\n\n# Install kubeadm, kubelet, and kubectl\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\n\n# Pin their versions to prevent accidental upgrades\nsudo apt-mark hold kubelet kubeadm kubectl\n</code></pre> <pre><code># Add the Kubernetes yum repository\ncat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://pkgs.k8s.io/core:/stable:/v1.31/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://pkgs.k8s.io/core:/stable:/v1.31/rpm/repodata/repomd.xml.key\nEOF\n\n# Install kubeadm, kubelet, and kubectl\nsudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes\n\n# Enable kubelet service\nsudo systemctl enable kubelet\n</code></pre>"},{"location":"27-kubeadm/#0106-enable-the-kubelet-service","title":"01.06 Enable the kubelet service","text":"<pre><code>sudo systemctl enable --now kubelet\n</code></pre> <p>Note</p> <p>At this point the kubelet will crash-loop every few seconds - this is expected. It is waiting for <code>kubeadm init</code> or <code>kubeadm join</code> to provide its configuration.</p>"},{"location":"27-kubeadm/#02-initialize-the-control-plane-node","title":"02. Initialize the control-plane node","text":"<p>Run these steps only on the control-plane node.</p>"},{"location":"27-kubeadm/#0201-run-kubeadm-init","title":"02.01 Run kubeadm init","text":"<pre><code>sudo kubeadm init \\\n    --pod-network-cidr=10.244.0.0/16 \\\n    --apiserver-advertise-address=&lt;CONTROL_PLANE_IP&gt;\n</code></pre> <p>Replace Placeholders</p> <p>Replace <code>&lt;CONTROL_PLANE_IP&gt;</code> with the actual IP address of your control-plane node. The <code>--pod-network-cidr</code> must match the CIDR expected by your CNI plugin (10.244.0.0/16 for Flannel, 192.168.0.0/16 for Calico).</p> <p>Expected output (excerpt):</p> <pre><code>Your Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nThen you can join any number of worker nodes by running the following\non each as root:\n\n  kubeadm join &lt;CONTROL_PLANE_IP&gt;:6443 --token &lt;TOKEN&gt; \\\n      --discovery-token-ca-cert-hash sha256:&lt;HASH&gt;\n</code></pre>"},{"location":"27-kubeadm/#0202-configure-kubectl","title":"02.02 Configure kubectl","text":"<pre><code># Set up kubectl for the current user\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre>"},{"location":"27-kubeadm/#0203-verify-control-plane-status","title":"02.03 Verify control-plane status","text":"<pre><code># The node should appear with status NotReady (until CNI is installed)\nkubectl get nodes\n\n# Check that control-plane pods are running\nkubectl get pods -n kube-system\n</code></pre> <p>Expected output:</p> <pre><code>NAME                                   READY   STATUS    RESTARTS   AGE\ncoredns-xxxxxxxxxx-xxxxx               0/1     Pending   0          1m\ncoredns-xxxxxxxxxx-xxxxx               0/1     Pending   0          1m\netcd-control-plane                     1/1     Running   0          1m\nkube-apiserver-control-plane           1/1     Running   0          1m\nkube-controller-manager-control-plane  1/1     Running   0          1m\nkube-proxy-xxxxx                       1/1     Running   0          1m\nkube-scheduler-control-plane           1/1     Running   0          1m\n</code></pre> <p>Note</p> <p>CoreDNS pods will stay in Pending state until a CNI plugin is installed. This is normal.</p>"},{"location":"27-kubeadm/#03-install-a-cni-plugin","title":"03. Install a CNI plugin","text":"<p>A CNI (Container Network Interface) plugin is required so pods can communicate across nodes. Choose one of the following:</p> FlannelCalicoCilium <pre><code>kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n</code></pre> <p>Note</p> <p>Flannel requires <code>--pod-network-cidr=10.244.0.0/16</code> to be set during <code>kubeadm init</code>.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml\n</code></pre> <p>Note</p> <p>If you used <code>--pod-network-cidr=192.168.0.0/16</code> during <code>kubeadm init</code>, Calico works with zero additional config. For custom CIDRs, edit the <code>CALICO_IPV4POOL_CIDR</code> environment variable in the manifest.</p> <pre><code># Install Cilium CLI\nCILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)\nCLI_ARCH=amd64\ncurl -L --fail --remote-name-all \\\n    https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz\nsudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin\nrm cilium-linux-${CLI_ARCH}.tar.gz\n\n# Install Cilium into the cluster\ncilium install\n</code></pre> <p>After installing the CNI, verify all pods come up:</p> <pre><code># Wait for all system pods to be running\nkubectl get pods -n kube-system -w\n\n# Node should now show Ready\nkubectl get nodes\n</code></pre> <p>Expected output:</p> <pre><code>NAME            STATUS   ROLES           AGE   VERSION\ncontrol-plane   Ready    control-plane   5m    v1.31.x\n</code></pre>"},{"location":"27-kubeadm/#04-join-worker-nodes","title":"04. Join worker nodes","text":"<p>Run these steps on each worker node.</p>"},{"location":"27-kubeadm/#0401-use-the-join-command-from-kubeadm-init-output","title":"04.01 Use the join command from kubeadm init output","text":"<pre><code>sudo kubeadm join &lt;CONTROL_PLANE_IP&gt;:6443 \\\n    --token &lt;TOKEN&gt; \\\n    --discovery-token-ca-cert-hash sha256:&lt;HASH&gt;\n</code></pre> <p>Forgot the join command?</p> <p>If you lost the join command, generate a new token on the control-plane node:</p> <pre><code>kubeadm token create --print-join-command\n</code></pre>"},{"location":"27-kubeadm/#0402-verify-nodes-from-the-control-plane","title":"04.02 Verify nodes from the control-plane","text":"<pre><code># Run on the control-plane node\nkubectl get nodes\n</code></pre> <p>Expected output:</p> <pre><code>NAME            STATUS   ROLES           AGE   VERSION\ncontrol-plane   Ready    control-plane   10m   v1.31.x\nworker-1        Ready    &lt;none&gt;          2m    v1.31.x\nworker-2        Ready    &lt;none&gt;          1m    v1.31.x\n</code></pre>"},{"location":"27-kubeadm/#0403-optional-label-worker-nodes","title":"04.03 (Optional) Label worker nodes","text":"<pre><code>kubectl label node worker-1 node-role.kubernetes.io/worker=worker\nkubectl label node worker-2 node-role.kubernetes.io/worker=worker\n</code></pre>"},{"location":"27-kubeadm/#05-verify-the-cluster","title":"05. Verify the cluster","text":""},{"location":"27-kubeadm/#0501-deploy-a-test-application","title":"05.01 Deploy a test application","text":"<pre><code>kubectl create deployment nginx-test --image=nginx --replicas=3\nkubectl expose deployment nginx-test --port=80 --type=NodePort\n</code></pre>"},{"location":"27-kubeadm/#0502-check-deployment-status","title":"05.02 Check deployment status","text":"<pre><code># All 3 replicas should be running across the nodes\nkubectl get pods -o wide\n\n# Get the NodePort\nkubectl get svc nginx-test\n</code></pre>"},{"location":"27-kubeadm/#0503-test-connectivity","title":"05.03 Test connectivity","text":"<pre><code># Get the NodePort assigned\nNODE_PORT=$(kubectl get svc nginx-test -o jsonpath='{.spec.ports[0].nodePort}')\n\n# Test from any node (replace with an actual node IP)\ncurl http://&lt;NODE_IP&gt;:${NODE_PORT}\n</code></pre>"},{"location":"27-kubeadm/#0504-run-a-cluster-health-check","title":"05.04 Run a cluster health check","text":"<pre><code># Check component status\nkubectl get componentstatuses 2&gt;/dev/null || kubectl get --raw='/readyz?verbose'\n\n# Check all namespaces\nkubectl get pods --all-namespaces\n\n# Verify DNS resolution\nkubectl run dns-test --image=busybox:1.36 --rm -it --restart=Never -- \\\n    nslookup kubernetes.default.svc.cluster.local\n</code></pre>"},{"location":"27-kubeadm/#0505-clean-up-the-test-deployment","title":"05.05 Clean up the test deployment","text":"<pre><code>kubectl delete deployment nginx-test\nkubectl delete svc nginx-test\n</code></pre>"},{"location":"27-kubeadm/#06-optional-allow-scheduling-on-the-control-plane","title":"06. (Optional) Allow scheduling on the control-plane","text":"<p>By default, the control-plane node has a taint that prevents workload pods from being scheduled on it. For single-node clusters or development environments, remove it:</p> <pre><code># Remove the control-plane taint\nkubectl taint nodes --all node-role.kubernetes.io/control-plane-\n</code></pre> <p>Warning</p> <p>Do not do this in production. The control-plane node should be dedicated to running control-plane components for stability and security.</p>"},{"location":"27-kubeadm/#07-managing-bootstrap-tokens","title":"07. Managing bootstrap tokens","text":"<pre><code># List existing tokens\nkubeadm token list\n\n# Create a new token (default TTL: 24h)\nkubeadm token create\n\n# Create a token with a custom TTL\nkubeadm token create --ttl 2h\n\n# Create a token and print the full join command\nkubeadm token create --print-join-command\n\n# Delete a specific token\nkubeadm token delete &lt;TOKEN&gt;\n</code></pre>"},{"location":"27-kubeadm/#08-upgrade-the-cluster","title":"08. Upgrade the cluster","text":"<p>To upgrade a cluster from one Kubernetes minor version to the next:</p>"},{"location":"27-kubeadm/#0801-upgrade-the-control-plane","title":"08.01 Upgrade the control-plane","text":"Ubuntu / DebianRHEL / CentOS / Fedora <pre><code># Update the Kubernetes repo to the target version (e.g., v1.32)\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | \\\n    sudo tee /etc/apt/sources.list.d/kubernetes.list\n\n# Update apt and upgrade kubeadm\nsudo apt-get update\nsudo apt-mark unhold kubeadm\nsudo apt-get install -y kubeadm\nsudo apt-mark hold kubeadm\n\n# Verify kubeadm version\nkubeadm version\n</code></pre> <pre><code># Update the repo baseurl to v1.32\nsudo sed -i 's/v1.31/v1.32/' /etc/yum.repos.d/kubernetes.repo\n\nsudo yum install -y kubeadm --disableexcludes=kubernetes\n</code></pre> <pre><code># Check the upgrade plan\nsudo kubeadm upgrade plan\n\n# Apply the upgrade (replace with actual target version)\nsudo kubeadm upgrade apply v1.32.0\n</code></pre>"},{"location":"27-kubeadm/#0802-upgrade-kubelet-and-kubectl-on-the-control-plane","title":"08.02 Upgrade kubelet and kubectl on the control-plane","text":"<pre><code>sudo apt-mark unhold kubelet kubectl\nsudo apt-get install -y kubelet kubectl\nsudo apt-mark hold kubelet kubectl\n\n# Restart kubelet\nsudo systemctl daemon-reload\nsudo systemctl restart kubelet\n</code></pre>"},{"location":"27-kubeadm/#0803-upgrade-worker-nodes","title":"08.03 Upgrade worker nodes","text":"<p>On each worker node:</p> <pre><code># Drain the node (run from the control-plane)\nkubectl drain &lt;WORKER_NODE&gt; --ignore-daemonsets --delete-emptydir-data\n\n# On the worker node: upgrade kubeadm, then kubelet + kubectl\nsudo apt-mark unhold kubeadm kubelet kubectl\nsudo apt-get update\nsudo apt-get install -y kubeadm kubelet kubectl\nsudo apt-mark hold kubeadm kubelet kubectl\n\n# Upgrade the node configuration\nsudo kubeadm upgrade node\n\n# Restart kubelet\nsudo systemctl daemon-reload\nsudo systemctl restart kubelet\n</code></pre> <pre><code># Uncordon the node (run from the control-plane)\nkubectl uncordon &lt;WORKER_NODE&gt;\n</code></pre>"},{"location":"27-kubeadm/#0804-verify-the-upgrade","title":"08.04 Verify the upgrade","text":"<pre><code>kubectl get nodes\n</code></pre> <p>Expected output:</p> <pre><code>NAME            STATUS   ROLES           AGE   VERSION\ncontrol-plane   Ready    control-plane   1d    v1.32.0\nworker-1        Ready    worker          1d    v1.32.0\nworker-2        Ready    worker          1d    v1.32.0\n</code></pre>"},{"location":"27-kubeadm/#09-certificate-management","title":"09. Certificate management","text":"<p>kubeadm manages cluster certificates automatically. Here are useful commands:</p> <pre><code># Check certificate expiration dates\nsudo kubeadm certs check-expiration\n\n# Renew all certificates\nsudo kubeadm certs renew all\n\n# Renew a specific certificate\nsudo kubeadm certs renew apiserver\n</code></pre> <p>Certificate Validity</p> <p>By default, kubeadm certificates are valid for 1 year. The CA certificate is valid for 10 years. Plan certificate renewal before expiration to avoid cluster downtime.</p>"},{"location":"27-kubeadm/#10-using-a-kubeadm-configuration-file","title":"10. Using a kubeadm configuration file","text":"<p>Instead of passing many command-line flags, you can use a configuration file:</p> <pre><code># manifests/kubeadm-config.yaml\napiVersion: kubeadm.k8s.io/v1beta4\nkind: ClusterConfiguration\nkubernetesVersion: v1.31.0\ncontrolPlaneEndpoint: \"control-plane:6443\"\nnetworking:\n  podSubnet: \"10.244.0.0/16\"\n  serviceSubnet: \"10.96.0.0/12\"\n  dnsDomain: \"cluster.local\"\napiServer:\n  extraArgs:\n    - name: audit-log-path\n      value: /var/log/kubernetes/audit.log\n    - name: audit-log-maxage\n      value: \"30\"\netcd:\n  local:\n    dataDir: /var/lib/etcd\n---\napiVersion: kubeadm.k8s.io/v1beta4\nkind: InitConfiguration\nnodeRegistration:\n  criSocket: unix:///var/run/containerd/containerd.sock\n  taints:\n    - key: \"node-role.kubernetes.io/control-plane\"\n      effect: \"NoSchedule\"\n</code></pre> <pre><code># Initialize using the config file\nsudo kubeadm init --config manifests/kubeadm-config.yaml\n</code></pre> <p>Generating a Default Config</p> <p>You can generate a default configuration to customize:</p> <pre><code>kubeadm config print init-defaults &gt; kubeadm-config.yaml\n</code></pre>"},{"location":"27-kubeadm/#11-reset-and-tear-down","title":"11. Reset and tear down","text":"<p>To completely remove Kubernetes from a node:</p> <pre><code># Reset the node (removes all cluster state)\nsudo kubeadm reset -f\n\n# Clean up networking rules and CNI configs\nsudo iptables -F &amp;&amp; sudo iptables -t nat -F &amp;&amp; sudo iptables -t mangle -F &amp;&amp; sudo iptables -X\nsudo rm -rf /etc/cni/net.d\n\n# Remove kubeconfig\nrm -rf $HOME/.kube\n\n# (Optional) Uninstall packages\nsudo apt-mark unhold kubelet kubeadm kubectl\nsudo apt-get purge -y kubelet kubeadm kubectl\n</code></pre>"},{"location":"27-kubeadm/#summary","title":"Summary","text":"Concept Key Takeaway kubeadm init Bootstraps a control-plane node with all required components kubeadm join Adds worker (or HA control-plane) nodes to the cluster CNI plugin Required for pod networking - install immediately after <code>kubeadm init</code> kubeadm upgrade Safely upgrades cluster version one minor release at a time kubeadm reset Cleanly tears down cluster state on a node kubeadm token Manages tokens for joining nodes (default 24h TTL) kubeadm certs Manages TLS certificates (default 1-year validity) Swap must be off kubelet will not start if swap is enabled Container runtime containerd or CRI-O required (dockershim removed since K8s 1.24) Config file <code>--config</code> flag allows declarative cluster configuration"},{"location":"27-kubeadm/#exercises","title":"Exercises","text":"<p>The following exercises will test your understanding of kubeadm and cluster bootstrapping. Try to solve each exercise on your own before revealing the solution.</p>"},{"location":"27-kubeadm/#01-create-a-single-node-cluster-that-can-run-workloads","title":"01. Create a single-node cluster that can run workloads","text":"<p>Create a cluster with <code>kubeadm init</code> that allows scheduling pods on the control-plane node.</p> Solution <pre><code># Initialize the cluster\nsudo kubeadm init --pod-network-cidr=10.244.0.0/16\n\n# Set up kubeconfig\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n# Install CNI (Flannel)\nkubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n\n# Remove the control-plane taint to allow scheduling\nkubectl taint nodes --all node-role.kubernetes.io/control-plane-\n\n# Verify the node is Ready\nkubectl get nodes\n</code></pre>"},{"location":"27-kubeadm/#02-generate-a-new-join-token-and-add-a-worker-node","title":"02. Generate a new join token and add a worker node","text":"<p>The original join token from <code>kubeadm init</code> has expired. Generate a new one and join a worker.</p> Solution <pre><code># On the control-plane node - generate a new token with the full join command\nkubeadm token create --print-join-command\n\n# On the worker node - run the printed command, e.g.:\nsudo kubeadm join 192.168.1.100:6443 \\\n    --token abc123.abcdefghijklmnop \\\n    --discovery-token-ca-cert-hash sha256:abc123...\n\n# On the control-plane - verify the worker joined\nkubectl get nodes\n</code></pre>"},{"location":"27-kubeadm/#03-check-and-renew-cluster-certificates","title":"03. Check and renew cluster certificates","text":"<p>Check when the cluster certificates expire and renew the API server certificate.</p> Solution <pre><code># Check expiration dates for all certificates\nsudo kubeadm certs check-expiration\n\n# Renew only the API server certificate\nsudo kubeadm certs renew apiserver\n\n# Restart the API server to pick up the new cert\n# (API server runs as a static pod, so restart kubelet or move the manifest)\nsudo systemctl restart kubelet\n\n# Verify the new certificate\nsudo kubeadm certs check-expiration | grep apiserver\n</code></pre>"},{"location":"27-kubeadm/#04-perform-a-cluster-upgrade-from-v131-to-v132","title":"04. Perform a cluster upgrade from v1.31 to v1.32","text":"<p>Plan and execute a full cluster upgrade across control-plane and worker nodes.</p> Solution <pre><code># ---- Control-Plane Node ----\n\n# 1. Update the apt repo to v1.32\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | \\\n    sudo tee /etc/apt/sources.list.d/kubernetes.list\n\n# 2. Upgrade kubeadm\nsudo apt-get update\nsudo apt-mark unhold kubeadm\nsudo apt-get install -y kubeadm\nsudo apt-mark hold kubeadm\n\n# 3. Check the plan\nsudo kubeadm upgrade plan\n\n# 4. Apply the upgrade\nsudo kubeadm upgrade apply v1.32.0\n\n# 5. Upgrade kubelet and kubectl\nsudo apt-mark unhold kubelet kubectl\nsudo apt-get install -y kubelet kubectl\nsudo apt-mark hold kubelet kubectl\nsudo systemctl daemon-reload\nsudo systemctl restart kubelet\n\n# ---- Each Worker Node ----\n\n# 6. Drain the worker (from control-plane)\nkubectl drain worker-1 --ignore-daemonsets --delete-emptydir-data\n\n# 7. On the worker: upgrade all components\nsudo apt-mark unhold kubeadm kubelet kubectl\nsudo apt-get update\nsudo apt-get install -y kubeadm kubelet kubectl\nsudo apt-mark hold kubeadm kubelet kubectl\n\n# 8. Upgrade the node config\nsudo kubeadm upgrade node\n\n# 9. Restart kubelet\nsudo systemctl daemon-reload\nsudo systemctl restart kubelet\n\n# 10. Uncordon the worker (from control-plane)\nkubectl uncordon worker-1\n\n# Verify\nkubectl get nodes\n</code></pre>"},{"location":"27-kubeadm/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>kubelet crash-loops after <code>kubeadm init</code>: <pre><code># Check kubelet logs\nsudo journalctl -xeu kubelet --no-pager | tail -50\n</code></pre>   Common causes: swap is enabled, cgroup driver mismatch, containerd not running.</p> </li> <li> <p>CoreDNS pods stuck in Pending:   A CNI plugin must be installed. CoreDNS cannot schedule without a pod network.   <pre><code># Verify no CNI is installed\nls /etc/cni/net.d/\n# Install your chosen CNI plugin (see section 03)\n</code></pre></p> </li> <li> <p>Node remains NotReady: <pre><code># Check the kubelet status\nsudo systemctl status kubelet\n\n# Check for container runtime issues\nsudo crictl ps\nsudo crictl pods\n\n# Check node conditions\nkubectl describe node &lt;NODE_NAME&gt; | grep -A5 Conditions\n</code></pre></p> </li> <li> <p>Token expired when trying to join: <pre><code># Generate a new token\nkubeadm token create --print-join-command\n</code></pre></p> </li> <li> <p><code>kubeadm init</code> fails with preflight errors: <pre><code># Run preflight checks to see all issues\nsudo kubeadm init phase preflight\n\n# Common fix: disable swap\nsudo swapoff -a\n\n# Common fix: ensure container runtime is running\nsudo systemctl status containerd\n</code></pre></p> </li> <li> <p>Certificate errors after cluster has been running for a year: <pre><code># Check certificate expiration\nsudo kubeadm certs check-expiration\n\n# Renew all certificates\nsudo kubeadm certs renew all\n\n# Restart control-plane components\nsudo systemctl restart kubelet\n</code></pre></p> </li> <li> <p>cgroup driver mismatch: <pre><code># Ensure containerd uses SystemdCgroup\ngrep SystemdCgroup /etc/containerd/config.toml\n# Should show: SystemdCgroup = true\n\n# If not, fix it and restart\nsudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml\nsudo systemctl restart containerd\n</code></pre></p> </li> </ul>"},{"location":"27-kubeadm/#next-steps","title":"Next Steps","text":"<ul> <li>Set up a highly available (HA) control-plane with multiple control-plane nodes and an external load balancer - see HA topology</li> <li>Add etcd encryption at rest for secrets - see Encrypting Secret Data</li> <li>Configure audit logging to track API server requests</li> <li>Set up RBAC (see Lab 31 - RBAC) for fine-grained access control</li> <li>Explore kubeadm phases (<code>kubeadm init phase</code>) for granular control over cluster bootstrapping</li> <li>Consider managed alternatives for production: EKS, GKE, or AKS if cluster management overhead is too high</li> </ul>"},{"location":"28-Telepresence/","title":"Telepresence","text":""},{"location":"28-Telepresence/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What Telepresence is and why it\u2019s useful for Kubernetes development</li> <li>How to install and configure Telepresence on your system</li> <li>Creating and managing different types of service intercepts</li> <li>Debugging applications locally while connected to a cluster</li> <li>Team collaboration using personal intercepts and preview URLs</li> <li>Best practices for cloud-native development workflows</li> </ul>"},{"location":"28-Telepresence/#introduction","title":"Introduction","text":"<ul> <li>Telepresence is a powerful tool that allows developers to code and test microservices locally while connecting to a remote Kubernetes cluster</li> <li>It bridges the gap between local development and cloud-native environments, enabling fast feedback loops</li> <li>Eliminates the need to continuously build, push, and deploy containers during development</li> </ul>"},{"location":"28-Telepresence/#what-is-telepresence","title":"What is Telepresence?","text":"<ul> <li>Telepresence is an open-source tool that creates a bi-directional network proxy between your local development machine and a Kubernetes cluster</li> </ul> <p>Telepresence Capabilities</p> <p>Local Development with Cluster Access</p> <ul> <li>Run a single service locally while connecting to remote Kubernetes services</li> <li>Debug services using your local IDE, debuggers, and development tools</li> <li>Test integrations with other services running in the cluster</li> <li>Develop without waiting for container builds and deployments</li> <li>Work with realistic data and dependencies from the cluster</li> </ul>"},{"location":"28-Telepresence/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n  A[Local Machine] --&gt; B[Telepresence Client]\n  B --&gt;|Network Tunnel| C[Kubernetes Cluster]\n  C --&gt; D[Traffic Manager]\n  D --&gt; E[Your Service - Local]\n  D --&gt; F[Other Services]\n  C --&gt; G[Database]\n  C --&gt; H[Message Queue]\n  C --&gt; I[APIs]\n  F --&gt; G\n  F --&gt; H\n  F --&gt; I\n\n  style A fill:#667eea\n  style B fill:#764ba2\n  style C fill:#48bb78\n  style D fill:#ed8936\n  style E fill:#f6ad55</code></pre>"},{"location":"28-Telepresence/#terminology","title":"Terminology","text":"Term Description Traffic Manager Controller deployed in the cluster that manages intercepts and routing Intercept Route traffic from a service in the cluster to your local machine Global Intercept All traffic to a service goes to your local machine Personal Intercept Only traffic matching specific headers goes to your local machine Preview URL Shareable URL that routes to your local development environment"},{"location":"28-Telepresence/#key-features","title":"Key Features","text":"<ul> <li> <p> Fast Inner Loop Development</p> <ul> <li>Edit code locally and see changes immediately.</li> <li>No container rebuilds required.</li> <li>Instant feedback on code changes.</li> </ul> </li> <li> <p> Full Network Access</p> <ul> <li>Access cluster resources as if running in the cluster.</li> <li>Connect to databases, message queues, and other services.</li> <li>Test with production-like data and dependencies.</li> </ul> </li> <li> <p> Service Intercepts</p> <ul> <li>Route cluster traffic to your local machine.</li> <li>Debug production issues safely.</li> <li>Test changes without affecting teammates.</li> </ul> </li> <li> <p> Preview URLs</p> <ul> <li>Share your local changes with stakeholders.</li> <li>Get feedback without deploying.</li> <li>Collaborate seamlessly.</li> </ul> </li> <li> <p> Personal Intercepts</p> <ul> <li>Header-based traffic routing.</li> <li>Multiple developers on the same service. Safe parallel development.</li> </ul> </li> <li> <p> Volume Mounts</p> <p>Access ConfigMaps and Secrets locally. Test with real cluster configurations. Develop with production-like settings.</p> </li> </ul>"},{"location":"28-Telepresence/#prerequisites","title":"Prerequisites","text":"<p>Required Setup</p> <ul> <li>Running Kubernetes cluster (Minikube, Kind, or cloud provider)</li> <li><code>kubectl</code> configured and working</li> <li>Admin access to the cluster (for Traffic Manager installation)</li> <li>Docker Desktop or Docker Engine running</li> <li>Internet connectivity</li> <li>Code editor (VS Code recommended)</li> </ul>"},{"location":"28-Telepresence/#system-requirements","title":"System Requirements","text":"Requirement Specification OS macOS, Linux, or Windows (WSL2) Memory At least 4GB RAM available Disk 2GB free space Network Stable internet connection"},{"location":"28-Telepresence/#installation","title":"Installation","text":""},{"location":"28-Telepresence/#step-01-install-telepresence-cli","title":"Step 01 - Install Telepresence CLI","text":"macOS (Homebrew) macOS (Manual)\ud83d\udc27 Linux\u229e Windows (WSL2) <pre><code># Install using Homebrew\nbrew install datawire/blackbird/telepresence\n\n# Verify installation\ntelepresence version\n</code></pre> <pre><code># Download the binary\nsudo curl -fL https://app.getambassador.io/download/tel2/darwin/amd64/latest/telepresence \\\n  -o /usr/local/bin/telepresence\n\n# Make it executable\nsudo chmod a+x /usr/local/bin/telepresence\n\n# Verify installation\ntelepresence version\n</code></pre> <pre><code># Download the binary\nsudo curl -fL https://app.getambassador.io/download/tel2/linux/amd64/latest/telepresence \\\n  -o /usr/local/bin/telepresence\n\n# Make it executable\nsudo chmod a+x /usr/local/bin/telepresence\n\n# Verify installation\ntelepresence version\n</code></pre> <pre><code># In WSL2 terminal\nsudo curl -fL https://app.getambassador.io/download/tel2/linux/amd64/latest/telepresence \\\n  -o /usr/local/bin/telepresence\n\nsudo chmod a+x /usr/local/bin/telepresence\n\ntelepresence version\n</code></pre>"},{"location":"28-Telepresence/#step-02-verify-cluster-access","title":"Step 02 - Verify Cluster Access","text":"<pre><code># Check cluster connectivity\nkubectl cluster-info\n\n# Check your context\nkubectl config current-context\n\n# Verify you have admin permissions\nkubectl auth can-i create deployments --all-namespaces\n</code></pre>"},{"location":"28-Telepresence/#step-03-connect-telepresence","title":"Step 03 - Connect Telepresence","text":"<pre><code># Connect to the cluster\n# This installs the Traffic Manager in your cluster\ntelepresence connect\n\n# Check connection status\ntelepresence status\n\n# List available services\ntelepresence list\n</code></pre> <p>Expected Output</p> <pre><code>Connected to context &lt;your-context&gt; (https://your-cluster)\n\nTraffic Manager:\n  Version    : v2.x.x\n  Status     : Running\n  Namespace  : ambassador\n</code></pre>"},{"location":"28-Telepresence/#step-04-verify-dns-access","title":"Step 04 - Verify DNS Access","text":"<pre><code># Test DNS resolution of cluster services\ncurl -k https://kubernetes.default.svc.cluster.local:443\n</code></pre>"},{"location":"28-Telepresence/#demo-architecture","title":"Demo Architecture","text":"<p>Our demo application consists of three microservices:</p> <pre><code>graph LR\n    A[Frontend&lt;br/&gt;Nginx] --&gt;|HTTP| B[Backend&lt;br/&gt;Python/Flask]\n    B --&gt;|HTTP| C[Data Service&lt;br/&gt;Python/Flask]\n\n    style A fill:#48bb78\n    style B fill:#ed8936\n    style C fill:#667eea</code></pre> <p>Service Description</p> <p>Frontend (Nginx)</p> <ul> <li>Web UI for the application</li> <li>Proxies API calls to backend</li> </ul> <p>Backend (Python/Flask) - INTERCEPT POINT</p> <ul> <li>REST API with multiple endpoints</li> <li>Communicates with data service</li> <li>This is the service we\u2019ll intercept</li> </ul> <p>Data Service (Python/Flask)</p> <ul> <li>Provides metrics and sample data</li> <li>Simulates data layer</li> </ul>"},{"location":"28-Telepresence/#lab-setup","title":"Lab Setup","text":""},{"location":"28-Telepresence/#step-01-quick-setup","title":"Step 01 - Quick Setup","text":"<p>Use the automated setup script:</p> <pre><code># Navigate to lab directory\ncd Labs/32-Telepresence\n\n# Run setup script\n./setup.sh\n</code></pre> What does setup.sh do? <ul> <li>Creates the <code>telepresence-demo</code> namespace</li> <li>Deploys all three microservices</li> <li>Waits for services to be ready</li> <li>Displays access information</li> </ul>"},{"location":"28-Telepresence/#step-02-manual-setup-alternative","title":"Step 02 - Manual Setup (Alternative)","text":"<p>If you prefer manual setup:</p> <pre><code># Create namespace\nkubectl create namespace telepresence-demo\n\n# Set default namespace\nkubectl config set-context --current --namespace=telepresence-demo\n\n# Deploy services\nkubectl apply -f resources/01-namespace.yaml\nkubectl apply -f resources/02-dataservice.yaml\nkubectl apply -f resources/03-backend.yaml\nkubectl apply -f resources/04-frontend.yaml\n\n# Wait for pods to be ready\nkubectl wait --for=condition=ready pod --all --timeout=120s\n</code></pre>"},{"location":"28-Telepresence/#step-03-verify-deployment","title":"Step 03 - Verify Deployment","text":"<pre><code># Check all resources\nkubectl get all -n telepresence-demo\n\n# Test frontend\nkubectl port-forward -n telepresence-demo svc/frontend 8080:80\n</code></pre> <p>Then open your browser to <code>http://localhost:8080</code></p>"},{"location":"28-Telepresence/#exercise-1-basic-intercept","title":"Exercise 1: Basic Intercept","text":"<p>Goal</p> <p>Intercept the backend service and route all traffic to your local development environment.</p>"},{"location":"28-Telepresence/#step-1-prepare-local-environment","title":"Step 1 - Prepare Local Environment","text":"<pre><code># Navigate to backend source code\ncd resources/backend-app\n\n# Create Python virtual environment\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"28-Telepresence/#step-2-connect-telepresence","title":"Step 2 - Connect Telepresence","text":"<pre><code># Ensure you're connected\ntelepresence connect\n\n# List available services\ntelepresence list --namespace telepresence-demo\n</code></pre>"},{"location":"28-Telepresence/#step-3-create-intercept","title":"Step 3 - Create Intercept","text":"<pre><code># Intercept the backend service on port 5000\ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo\n</code></pre> <p>Intercept Created</p> <pre><code>backend: intercepted\n  Intercept name: backend\n  State         : ACTIVE\n  Workload kind : Deployment\n  Destination   : 127.0.0.1:5000\n  Intercepting  : all TCP connections\n</code></pre>"},{"location":"28-Telepresence/#step-4-run-local-service","title":"Step 4 - Run Local Service","text":"<pre><code># Run the backend service locally\npython app.py\n\n# The service starts on port 5000\n</code></pre>"},{"location":"28-Telepresence/#step-5-test-the-intercept","title":"Step 5 - Test the Intercept","text":"<pre><code># In a new terminal, forward the frontend\nkubectl port-forward -n telepresence-demo svc/frontend 8080:80\n\n# Open http://localhost:8080 in your browser\n# All backend requests now go to your local machine!\n</code></pre>"},{"location":"28-Telepresence/#step-6-make-live-changes","title":"Step 6 - Make Live Changes","text":"<p>Try This</p> <ol> <li>Edit <code>resources/backend-app/app.py</code></li> <li>Modify a response message</li> <li>Save the file (Flask auto-reloads in debug mode)</li> <li>Refresh your browser</li> <li>See your changes immediately!</li> </ol>"},{"location":"28-Telepresence/#step-7-remove-intercept","title":"Step 7 - Remove Intercept","text":"<pre><code># Leave the intercept\ntelepresence leave backend\n\n# Verify it's removed\ntelepresence list\n</code></pre>"},{"location":"28-Telepresence/#exercise-2-preview-urls","title":"Exercise 2: Preview URLs","text":"<p>Goal</p> <p>Create a shareable preview URL for your local changes.</p>"},{"location":"28-Telepresence/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Ambassador Cloud account (free tier available)</li> <li>Internet-accessible cluster</li> </ul>"},{"location":"28-Telepresence/#step-1-login-to-ambassador-cloud","title":"Step 1 - Login to Ambassador Cloud","text":"<pre><code># Login (opens browser)\ntelepresence login\n</code></pre>"},{"location":"28-Telepresence/#step-2-create-preview-intercept","title":"Step 2 - Create Preview Intercept","text":"<pre><code># Create an intercept with preview URL\ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo \\\n  --preview-url=true\n</code></pre> <p>Preview URL Created</p> <p>You\u2019ll receive a URL like: <pre><code>https://[random-id].preview.edgestack.me\n</code></pre></p>"},{"location":"28-Telepresence/#step-3-share-and-test","title":"Step 3 - Share and Test","text":"<ol> <li>Copy the preview URL</li> <li>Share it with teammates or stakeholders</li> <li>Make changes to your local code</li> <li>Others see changes in real-time via the preview URL!</li> </ol>"},{"location":"28-Telepresence/#exercise-3-global-intercept","title":"Exercise 3: Global Intercept","text":"<p>Goal</p> <p>Route ALL traffic for a service to your local machine.</p> <p>Use with Caution</p> <p>Global intercepts affect all users of the service. Only use in development environments or when you have exclusive access.</p> <pre><code># Create a global intercept\ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo\n\n# All traffic to backend now goes to localhost:5000\n</code></pre> <p>Use Cases:</p> <ul> <li>Testing breaking changes</li> <li>Debugging production-like issues</li> <li>Performance testing with real load</li> </ul>"},{"location":"28-Telepresence/#exercise-4-personal-intercept","title":"Exercise 4: Personal Intercept","text":"<p>Goal</p> <p>Only intercept requests that match specific HTTP headers. This allows multiple developers to work on the same service simultaneously.</p>"},{"location":"28-Telepresence/#step-1-create-selective-intercept","title":"Step 1 - Create Selective Intercept","text":"<pre><code># Intercept only requests with specific header\ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo \\\n  --http-match=auto\n</code></pre>"},{"location":"28-Telepresence/#step-2-get-your-intercept-id","title":"Step 2 - Get Your Intercept ID","text":"<pre><code># List intercepts to see your ID\ntelepresence list --namespace telepresence-demo\n</code></pre>"},{"location":"28-Telepresence/#step-3-test-with-header","title":"Step 3 - Test with Header","text":"<pre><code># Requests WITHOUT your header go to cluster\ncurl http://backend.telepresence-demo.svc.cluster.local:5000/api/health\n\n# Requests WITH your header go to your local machine\ncurl -H \"x-telepresence-intercept-id: [your-id]\" \\\n  http://backend.telepresence-demo.svc.cluster.local:5000/api/health\n</code></pre>"},{"location":"28-Telepresence/#step-4-browser-testing","title":"Step 4 - Browser Testing","text":"<p>Install a browser extension to add custom headers:</p> <ul> <li>ModHeader (Chrome/Firefox)</li> <li>Modify Headers (Firefox)</li> </ul> <p>Add the header with your intercept ID, and only your browser will see your local changes!</p>"},{"location":"28-Telepresence/#advanced-features","title":"Advanced Features","text":""},{"location":"28-Telepresence/#environment-variables","title":"Environment Variables","text":"<p>Capture cluster environment variables:</p> <pre><code># Intercept and capture environment\ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo \\\n  --env-file=.env.cluster\n\n# Load them in your app\nsource .env.cluster\npython app.py\n</code></pre>"},{"location":"28-Telepresence/#volume-mounts","title":"Volume Mounts","text":"<p>Access remote volumes locally:</p> <pre><code># Intercept with volume mounts\ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo \\\n  --mount=true\n\n# Volumes mounted at:\n# ~/telepresence/[namespace]/[pod-name]/[volume-name]\n</code></pre>"},{"location":"28-Telepresence/#docker-mode","title":"Docker Mode","text":"<p>Run your local service in Docker:</p> <pre><code># Intercept and connect Docker container\ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo \\\n  --docker-run -- \\\n  -v $(pwd):/app \\\n  my-backend-image:dev\n</code></pre>"},{"location":"28-Telepresence/#common-commands-reference","title":"Common Commands Reference","text":"Command Description <code>telepresence connect</code> Connect to cluster <code>telepresence status</code> Show connection status <code>telepresence list</code> List available services <code>telepresence intercept SERVICE --port PORT</code> Create basic intercept <code>telepresence leave SERVICE</code> Remove specific intercept <code>telepresence leave --all</code> Remove all intercepts <code>telepresence quit</code> Disconnect from cluster <code>telepresence loglevel debug</code> Enable debug logging"},{"location":"28-Telepresence/#troubleshooting","title":"Troubleshooting","text":""},{"location":"28-Telepresence/#connection-issues","title":"Connection Issues","text":"<pre><code># Check status\ntelepresence status\n\n# Reconnect\ntelepresence quit\ntelepresence connect\n\n# Check logs\ntelepresence loglevel debug\n</code></pre>"},{"location":"28-Telepresence/#dns-not-working","title":"DNS Not Working","text":"<pre><code># Try alternative DNS method\ntelepresence quit\ntelepresence connect --dns=google\n</code></pre>"},{"location":"28-Telepresence/#intercept-not-working","title":"Intercept Not Working","text":"<pre><code># Check intercept status\ntelepresence list\n\n# View traffic manager logs\nkubectl logs -n ambassador deployment/traffic-manager\n</code></pre>"},{"location":"28-Telepresence/#port-conflicts","title":"Port Conflicts","text":"<pre><code># Use different local port\ntelepresence intercept backend \\\n  --port 5001:5000 \\\n  --namespace telepresence-demo\n</code></pre> <p>More Help</p> <p>See TROUBLESHOOTING.md for detailed solutions to common issues.</p>"},{"location":"28-Telepresence/#best-practices","title":"Best Practices","text":"<p>Development Workflow</p> <p>Keep Connection Active</p> <ul> <li>Keep telepresence connected during development sessions</li> </ul> <p>Use Personal Intercepts</p> <ul> <li>Avoid disrupting teammates in shared environments</li> </ul> <p>Environment Parity</p> <ul> <li>Capture and use cluster environment variables</li> </ul> <p>Clean Up</p> <ul> <li>Always remove intercepts when done</li> <li>Use <code>telepresence leave --all</code></li> </ul> <p>Security</p> <ul> <li>Be cautious with sensitive environment variables</li> <li>Don\u2019t expose internal services unnecessarily</li> <li>Use preview URLs with expiration times</li> <li>Follow your organization\u2019s security policies</li> </ul> <p>Performance</p> <ul> <li>Run heavy services (databases) in cluster</li> <li>Understand that remote calls have network latency</li> <li>Use selective intercepts to minimize overhead</li> <li>Maintain stable internet connection</li> </ul>"},{"location":"28-Telepresence/#cleanup","title":"Cleanup","text":""},{"location":"28-Telepresence/#remove-intercepts","title":"Remove Intercepts","text":"<pre><code># Leave specific intercept\ntelepresence leave backend\n\n# Leave all intercepts\ntelepresence leave --all\n</code></pre>"},{"location":"28-Telepresence/#disconnect-from-cluster","title":"Disconnect from Cluster","text":"<pre><code># Disconnect but leave traffic manager\ntelepresence quit\n\n# Disconnect and remove traffic manager\ntelepresence uninstall --everything\n</code></pre>"},{"location":"28-Telepresence/#delete-demo-resources","title":"Delete Demo Resources","text":"<pre><code># Use cleanup script\n./cleanup.sh\n\n# Or manually\nkubectl delete namespace telepresence-demo\n</code></pre>"},{"location":"28-Telepresence/#additional-resources","title":"Additional Resources","text":"<p>Learn More</p> <ul> <li>Official Documentation</li> <li>GitHub Repository</li> <li>Community Slack</li> <li>EXAMPLES.md - 12 practical examples</li> <li>TROUBLESHOOTING.md - Detailed problem solving</li> </ul>"},{"location":"28-Telepresence/#key-takeaways","title":"Key Takeaways","text":"<p>What You Learned</p> <p>\u2705 Telepresence bridges local development and Kubernetes clusters</p> <p>\u2705 Enables fast inner-loop development without container rebuilds</p> <p>\u2705 Supports multiple intercept types for different scenarios</p> <p>\u2705 Works with existing development tools and IDEs</p> <p>\u2705 Dramatically improves developer productivity</p> <p>\u2705 Essential tool for modern cloud-native development</p>"},{"location":"28-Telepresence/#next-steps","title":"Next Steps","text":"<ul> <li>Experiment with different intercept types</li> <li>Integrate Telepresence into your CI/CD pipeline</li> <li>Create team workflows using personal intercepts</li> <li>Explore EXAMPLES.md for more use cases</li> <li>Set up automated testing with Telepresence</li> <li>Configure Telepresence for your specific stack</li> </ul> <p>Happy Coding! \ud83d\ude80</p>"},{"location":"28-Telepresence/EXAMPLES/","title":"Telepresence Lab 28 - Examples","text":"<p>This document provides practical examples for different Telepresence use cases.</p>"},{"location":"28-Telepresence/EXAMPLES/#example-1-basic-development-workflow","title":"Example 1: Basic Development Workflow","text":"<p>Scenario: You\u2019re developing a new feature for the backend service.</p> <pre><code># 1. Connect to cluster\ntelepresence connect\n\n# 2. Navigate to backend code\ncd resources/backend-app\n\n# 3. Set up local environment (first time only)\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n# 4. Start intercept\ntelepresence intercept backend --port 5000 --namespace telepresence-demo\n\n# 5. Run locally\npython app.py\n\n# 6. Access via frontend\nkubectl port-forward -n telepresence-demo svc/frontend 8080:80\n# Open http://localhost:8080\n\n# 7. Make changes to app.py and see them live!\n\n# 8. When done, clean up\ntelepresence leave backend\n</code></pre>"},{"location":"28-Telepresence/EXAMPLES/#example-2-debugging-with-vs-code","title":"Example 2: Debugging with VS Code","text":"<p>Scenario: You want to debug the backend service with breakpoints.</p> <pre><code># 1. Start intercept\ntelepresence intercept backend --port 5000 --namespace telepresence-demo\n\n# 2. In VS Code, create .vscode/launch.json:\n</code></pre> <pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Flask\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"module\": \"flask\",\n            \"env\": {\n                \"FLASK_APP\": \"app.py\",\n                \"FLASK_DEBUG\": \"1\",\n                \"DATASERVICE_URL\": \"http://dataservice.telepresence-demo.svc.cluster.local:5001\"\n            },\n            \"args\": [\n                \"run\",\n                \"--no-debugger\",\n                \"--no-reload\",\n                \"--port\",\n                \"5000\"\n            ],\n            \"jinja\": true,\n            \"justMyCode\": true\n        }\n    ]\n}\n</code></pre> <pre><code># 3. Set breakpoints in app.py\n# 4. Press F5 to start debugging\n# 5. Access the service and hit your breakpoints!\n</code></pre>"},{"location":"28-Telepresence/EXAMPLES/#example-3-testing-with-real-database","title":"Example 3: Testing with Real Database","text":"<p>Scenario: Access cluster database from local code.</p> <pre><code># 1. Connect telepresence\ntelepresence connect\n\n# 2. Now you can access cluster services as if you're in the cluster\npsql -h postgres.default.svc.cluster.local -U myuser -d mydb\n\n# 3. Run your app locally with database access\npython app.py\n\n# Your local app now talks to the cluster database!\n</code></pre>"},{"location":"28-Telepresence/EXAMPLES/#example-4-personal-intercept-for-team-development","title":"Example 4: Personal Intercept for Team Development","text":"<p>Scenario: Multiple developers working on the same service.</p> <pre><code># Developer 1 (Alice)\ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo \\\n  --http-header=x-dev-user=alice\n\n# Developer 2 (Bob)  \ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo \\\n  --http-header=x-dev-user=bob\n\n# Alice's requests (with header x-dev-user=alice) go to her local machine\n# Bob's requests (with header x-dev-user=bob) go to his local machine\n# Everyone else's requests go to the cluster\n\n# Test with curl:\ncurl -H \"x-dev-user=alice\" http://backend.telepresence-demo.svc.cluster.local:5000/api/health\n</code></pre>"},{"location":"28-Telepresence/EXAMPLES/#example-5-preview-urls-for-stakeholder-review","title":"Example 5: Preview URLs for Stakeholder Review","text":"<p>Scenario: Share your local changes with product manager.</p> <pre><code># 1. Login to Ambassador Cloud (free tier)\ntelepresence login\n\n# 2. Create preview intercept\ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo \\\n  --preview-url=true\n\n# Output shows preview URL:\n# Preview URL: https://abc123.preview.edgestack.me\n\n# 3. Share URL with stakeholders\n# They can access your local version without any setup!\n\n# 4. Make changes and they see them immediately\n</code></pre>"},{"location":"28-Telepresence/EXAMPLES/#example-6-environment-variable-sync","title":"Example 6: Environment Variable Sync","text":"<p>Scenario: Capture all cluster environment variables locally.</p> <pre><code># 1. Create intercept and capture env vars\ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo \\\n  --env-file=.env.cluster \\\n  --env-json=env.json\n\n# 2. Load in shell\nsource .env.cluster\n\n# 3. View as JSON\ncat env.json | jq\n\n# 4. Use in docker-compose.yml\ncat .env.cluster &gt;&gt; .env\n\n# 5. Run your app with cluster config\npython app.py\n</code></pre>"},{"location":"28-Telepresence/EXAMPLES/#example-7-volume-access","title":"Example 7: Volume Access","text":"<p>Scenario: Access ConfigMaps and Secrets locally.</p> <pre><code># 1. Intercept with volume mounts\ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo \\\n  --mount=true\n\n# 2. Volumes are mounted at:\nls ~/telepresence/telepresence-demo/backend-*/volumes/\n\n# 3. Access ConfigMaps and Secrets\ncat ~/telepresence/telepresence-demo/backend-*/volumes/config/app-config\n\n# 4. Your local app can read these files\n</code></pre>"},{"location":"28-Telepresence/EXAMPLES/#example-8-docker-mode","title":"Example 8: Docker Mode","text":"<p>Scenario: Run local container that accesses cluster.</p> <pre><code># 1. Build your image\ncd resources/backend-app\ndocker build -t my-backend:dev .\n\n# 2. Run with telepresence in Docker mode\ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo \\\n  --docker-run -- \\\n  -v $(pwd):/app \\\n  -e ENVIRONMENT=local-docker \\\n  my-backend:dev\n\n# Your container now runs locally but has full cluster access!\n</code></pre>"},{"location":"28-Telepresence/EXAMPLES/#example-9-integration-testing","title":"Example 9: Integration Testing","text":"<p>Scenario: Run integration tests against real cluster services.</p> <pre><code># 1. Connect telepresence\ntelepresence connect\n\n# 2. Run tests - they can access cluster services\npytest tests/integration/ -v\n\n# Example test:\n# def test_backend_to_dataservice():\n#     response = requests.get('http://dataservice.telepresence-demo.svc.cluster.local:5001/data')\n#     assert response.status_code == 200\n\n# 3. No need for mocks - test against real services!\n</code></pre>"},{"location":"28-Telepresence/EXAMPLES/#example-10-multi-service-development","title":"Example 10: Multi-Service Development","text":"<p>Scenario: Develop two services simultaneously.</p> <pre><code># Terminal 1 - Backend\ncd resources/backend-app\ntelepresence intercept backend --port 5000 --namespace telepresence-demo\npython app.py\n\n# Terminal 2 - Data Service\ncd resources/dataservice-app\ntelepresence intercept dataservice --port 5001 --namespace telepresence-demo\npython app.py\n\n# Both services now run locally and can communicate!\n# Frontend still runs in cluster\n</code></pre>"},{"location":"28-Telepresence/EXAMPLES/#example-11-performance-profiling","title":"Example 11: Performance Profiling","text":"<p>Scenario: Profile your service with real cluster traffic.</p> <pre><code># 1. Start intercept\ntelepresence intercept backend --port 5000 --namespace telepresence-demo\n\n# 2. Run with profiler\npython -m cProfile -o profile.stats app.py\n\n# 3. Generate traffic from cluster\n\n# 4. Analyze profile\npython -m pstats profile.stats\n\n# 5. Visualize with snakeviz\npip install snakeviz\nsnakeviz profile.stats\n</code></pre>"},{"location":"28-Telepresence/EXAMPLES/#example-12-hot-reload-development","title":"Example 12: Hot Reload Development","text":"<p>Scenario: Use Flask hot reload with cluster access.</p> <pre><code># 1. Start intercept\ntelepresence intercept backend --port 5000 --namespace telepresence-demo\n\n# 2. Run with debug mode\nexport FLASK_APP=app.py\nexport FLASK_ENV=development\nflask run --port 5000\n\n# 3. Make changes to app.py\n# Flask automatically reloads!\n# Changes immediately visible in cluster\n</code></pre>"},{"location":"28-Telepresence/EXAMPLES/#quick-reference-commands","title":"Quick Reference Commands","text":"<pre><code># Connect\ntelepresence connect\n\n# List services\ntelepresence list --namespace telepresence-demo\n\n# Basic intercept\ntelepresence intercept SERVICE --port LOCAL_PORT --namespace NAMESPACE\n\n# Intercept with preview URL\ntelepresence intercept SERVICE --port PORT --preview-url=true\n\n# Personal intercept\ntelepresence intercept SERVICE --port PORT --http-match=auto\n\n# Leave intercept\ntelepresence leave SERVICE\n\n# Disconnect\ntelepresence quit\n\n# Status\ntelepresence status\n\n# Debug mode\ntelepresence loglevel debug\n</code></pre>"},{"location":"28-Telepresence/EXAMPLES/#tips-and-tricks","title":"Tips and Tricks","text":"<ol> <li> <p>Use shell aliases for common commands:    <pre><code>alias tpc='telepresence connect'\nalias tpq='telepresence quit'\nalias tps='telepresence status'\nalias tpl='telepresence list'\n</code></pre></p> </li> <li> <p>Keep telepresence connected during your work session - connection is cheap to maintain</p> </li> <li> <p>Use watch mode in your framework (Flask, nodemon, etc.) for hot reload</p> </li> <li> <p>Combine with kubectl port-forward to access additional services</p> </li> <li> <p>Use preview URLs for quick stakeholder demos without VPN setup</p> </li> <li> <p>Personal intercepts are safer in shared clusters - use them by default</p> </li> <li> <p>Monitor traffic manager logs if issues arise:    <pre><code>kubectl logs -n ambassador deployment/traffic-manager -f\n</code></pre></p> </li> <li> <p>Export environment once and reuse:    <pre><code>telepresence intercept backend --env-file=.env --namespace telepresence-demo\nsource .env\n# Now run any command with cluster environment\n</code></pre></p> </li> </ol>"},{"location":"28-Telepresence/QUICKREF/","title":"Lab 28 - Telepresence Demo","text":""},{"location":"28-Telepresence/QUICKREF/#quick-start","title":"\ud83c\udfaf Quick Start","text":"<p>Deploy the demo and start intercepting in minutes:</p> <pre><code># 1. Setup everything\n./setup.sh\n\n# 2. Quick start intercept\n./quickstart.sh\n\n# 3. Navigate to backend app\ncd resources/backend-app\n\n# 4. Setup Python environment (first time)\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n# 5. Start intercepting\ntelepresence intercept backend --port 5000 --namespace telepresence-demo\n\n# 6. Run locally\npython app.py\n\n# 7. Test via frontend\nkubectl port-forward -n telepresence-demo svc/frontend 8080:80\n# Open http://localhost:8080\n</code></pre>"},{"location":"28-Telepresence/QUICKREF/#lab-structure","title":"\ud83d\udcc1 Lab Structure","text":"<pre><code>32-Telepresence/\n\u251c\u2500\u2500 README.md              # Complete guide with theory and exercises\n\u251c\u2500\u2500 EXAMPLES.md            # 12 practical examples\n\u251c\u2500\u2500 TROUBLESHOOTING.md     # Common issues and solutions\n\u251c\u2500\u2500 setup.sh              # Automated setup script\n\u251c\u2500\u2500 cleanup.sh            # Cleanup script\n\u251c\u2500\u2500 test.sh               # Test script\n\u251c\u2500\u2500 quickstart.sh         # Quick start guide\n\u2514\u2500\u2500 resources/\n    \u251c\u2500\u2500 01-namespace.yaml           # Namespace definition\n    \u251c\u2500\u2500 02-dataservice.yaml         # Data service deployment\n    \u251c\u2500\u2500 03-backend.yaml             # Backend service deployment\n    \u251c\u2500\u2500 04-frontend.yaml            # Frontend deployment\n    \u251c\u2500\u2500 BUILD.md                     # Docker build instructions\n    \u251c\u2500\u2500 backend-app/\n    \u2502   \u251c\u2500\u2500 app.py                   # Backend Python application\n    \u2502   \u251c\u2500\u2500 requirements.txt         # Python dependencies\n    \u2502   \u2514\u2500\u2500 Dockerfile              # Backend Docker image\n    \u251c\u2500\u2500 dataservice-app/\n    \u2502   \u251c\u2500\u2500 app.py                   # Data service application\n    \u2502   \u251c\u2500\u2500 requirements.txt         # Python dependencies\n    \u2502   \u2514\u2500\u2500 Dockerfile              # Data service Docker image\n    \u2514\u2500\u2500 frontend-app/\n        \u251c\u2500\u2500 index.html               # Frontend HTML/JS\n        \u251c\u2500\u2500 nginx.conf              # Nginx configuration\n        \u2514\u2500\u2500 Dockerfile              # Frontend Docker image\n</code></pre>"},{"location":"28-Telepresence/QUICKREF/#what-youll-learn","title":"\ud83d\ude80 What You\u2019ll Learn","text":"<ol> <li>Installation &amp; Setup</li> <li>Install Telepresence CLI</li> <li>Connect to Kubernetes cluster</li> <li> <p>Deploy Traffic Manager</p> </li> <li> <p>Basic Intercepts</p> </li> <li>Global intercepts (all traffic)</li> <li>Personal intercepts (header-based)</li> <li> <p>Preview URLs (shareable links)</p> </li> <li> <p>Development Workflows</p> </li> <li>Local debugging with cluster access</li> <li>Hot reload development</li> <li> <p>Integration testing</p> </li> <li> <p>Advanced Features</p> </li> <li>Volume mounts</li> <li>Environment variable sync</li> <li>Docker mode</li> </ol>"},{"location":"28-Telepresence/QUICKREF/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>README.md - Complete guide (50+ pages)</li> <li>Theory and concepts</li> <li>Installation steps</li> <li>4 hands-on exercises</li> <li>Best practices</li> <li> <p>Troubleshooting</p> </li> <li> <p>EXAMPLES.md - 12 Practical Examples</p> </li> <li>Basic workflows</li> <li>VS Code debugging</li> <li>Team collaboration</li> <li> <p>Integration testing</p> </li> <li> <p>TROUBLESHOOTING.md - Problem Solving</p> </li> <li>10 common issues</li> <li>Solutions and workarounds</li> <li>Debug commands</li> </ul>"},{"location":"28-Telepresence/QUICKREF/#scripts","title":"\ud83d\udee0\ufe0f Scripts","text":"<ul> <li>setup.sh - Deploy all resources automatically</li> <li>cleanup.sh - Remove all demo resources</li> <li>test.sh - Verify deployment and connectivity</li> <li>quickstart.sh - Quick start guide for intercepting</li> </ul>"},{"location":"28-Telepresence/QUICKREF/#exercises","title":"\ud83c\udf93 Exercises","text":""},{"location":"28-Telepresence/QUICKREF/#exercise-1-basic-intercept","title":"Exercise 1: Basic Intercept","text":"<p>Route all backend traffic to your local machine</p>"},{"location":"28-Telepresence/QUICKREF/#exercise-2-preview-urls","title":"Exercise 2: Preview URLs","text":"<p>Create shareable links for stakeholder review</p>"},{"location":"28-Telepresence/QUICKREF/#exercise-3-global-intercept","title":"Exercise 3: Global Intercept","text":"<p>Test breaking changes safely</p>"},{"location":"28-Telepresence/QUICKREF/#exercise-4-personal-intercept","title":"Exercise 4: Personal Intercept","text":"<p>Use header-based routing for team development</p>"},{"location":"28-Telepresence/QUICKREF/#demo-architecture","title":"\ud83c\udfd7\ufe0f Demo Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Frontend      \u2502\n\u2502   (Nginx)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Backend       \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 Data Service \u2502\n\u2502   (Python)      \u2502      \u2502  (Python)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Intercept Point: Backend service - Local development with cluster access - Real-time testing with other services - No container builds required</p>"},{"location":"28-Telepresence/QUICKREF/#key-features-demonstrated","title":"\u26a1 Key Features Demonstrated","text":"<ul> <li>\u2705 Fast inner-loop development</li> <li>\u2705 Service mesh integration</li> <li>\u2705 Real-time code changes</li> <li>\u2705 Team collaboration</li> <li>\u2705 Production debugging</li> <li>\u2705 Integration testing</li> <li>\u2705 Preview URLs</li> <li>\u2705 Environment sync</li> </ul>"},{"location":"28-Telepresence/QUICKREF/#prerequisites","title":"\ud83d\udd27 Prerequisites","text":"<ul> <li>Kubernetes cluster (Minikube, Kind, or cloud)</li> <li>kubectl configured</li> <li>Admin access to cluster</li> <li>Python 3.11+</li> <li>Docker (optional)</li> </ul>"},{"location":"28-Telepresence/QUICKREF/#common-commands","title":"\ud83d\udcdd Common Commands","text":"<pre><code># Connect\ntelepresence connect\n\n# List services\ntelepresence list --namespace telepresence-demo\n\n# Intercept\ntelepresence intercept backend --port 5000 --namespace telepresence-demo\n\n# Status\ntelepresence status\n\n# Leave intercept\ntelepresence leave backend\n\n# Disconnect\ntelepresence quit\n\n# Cleanup\ntelepresence uninstall --everything\n</code></pre>"},{"location":"28-Telepresence/QUICKREF/#learning-outcomes","title":"\ud83c\udfaf Learning Outcomes","text":"<p>After completing this lab, you will:</p> <ol> <li>Understand Telepresence architecture</li> <li>Install and configure Telepresence</li> <li>Create and manage intercepts</li> <li>Debug services with local tools</li> <li>Collaborate using personal intercepts</li> <li>Share changes via preview URLs</li> <li>Integrate Telepresence into workflows</li> <li>Troubleshoot common issues</li> </ol>"},{"location":"28-Telepresence/QUICKREF/#best-practices","title":"\ud83c\udf1f Best Practices","text":"<ol> <li>Use personal intercepts in shared environments</li> <li>Keep telepresence connected during development</li> <li>Leverage hot reload for fast feedback</li> <li>Export environment variables once, reuse</li> <li>Monitor Traffic Manager logs</li> <li>Clean up intercepts when done</li> <li>Use preview URLs for demos</li> <li>Document team intercept conventions</li> </ol>"},{"location":"28-Telepresence/QUICKREF/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":"<p>If you encounter issues:</p> <ol> <li>Check TROUBLESHOOTING.md</li> <li>Run <code>./test.sh</code> to verify setup</li> <li>Check <code>telepresence status</code></li> <li>Enable debug: <code>telepresence loglevel debug</code></li> <li>View logs: <code>kubectl logs -n ambassador deployment/traffic-manager</code></li> </ol>"},{"location":"28-Telepresence/QUICKREF/#resources","title":"\ud83d\udd17 Resources","text":"<ul> <li>Official Docs: https://www.telepresence.io/docs/</li> <li>GitHub: https://github.com/telepresenceio/telepresence</li> <li>Community Slack: https://a8r.io/slack</li> <li>Video Tutorials: https://www.youtube.com/c/Datawire</li> </ul>"},{"location":"28-Telepresence/QUICKREF/#cleanup","title":"\ud83e\uddf9 Cleanup","text":"<pre><code># Remove demo resources\n./cleanup.sh\n\n# Or manually\nkubectl delete namespace telepresence-demo\ntelepresence quit\ntelepresence uninstall --everything\n</code></pre>"},{"location":"28-Telepresence/QUICKREF/#next-steps","title":"\ud83d\udca1 Next Steps","text":"<p>After mastering this lab:</p> <ol> <li>Integrate Telepresence into CI/CD</li> <li>Create team workflows</li> <li>Explore Ambassador Cloud features</li> <li>Set up automated testing</li> <li>Configure for your specific stack</li> <li>Share knowledge with team</li> </ol> <p>Happy Coding! \ud83d\ude80</p> <p>Questions? Check the README.md for detailed information or EXAMPLES.md for practical use cases.</p>"},{"location":"28-Telepresence/TROUBLESHOOTING/","title":"Telepresence Troubleshooting Guide","text":""},{"location":"28-Telepresence/TROUBLESHOOTING/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"28-Telepresence/TROUBLESHOOTING/#1-cannot-connect-to-cluster","title":"1. Cannot Connect to Cluster","text":"<p>Symptom: <code>telepresence connect</code> fails</p> <p>Solutions: <pre><code># Check kubectl connectivity\nkubectl cluster-info\n\n# Check current context\nkubectl config current-context\n\n# Try with specific context\ntelepresence connect --context your-context\n\n# Check RBAC permissions\nkubectl auth can-i create mutatingwebhookconfigurations\n</code></pre></p>"},{"location":"28-Telepresence/TROUBLESHOOTING/#2-traffic-manager-not-installing","title":"2. Traffic Manager Not Installing","text":"<p>Symptom: Traffic Manager pod in <code>ambassador</code> namespace is not running</p> <p>Solutions: <pre><code># Check traffic manager status\nkubectl get pods -n ambassador\n\n# View logs\nkubectl logs -n ambassador deployment/traffic-manager\n\n# Uninstall and reinstall\ntelepresence uninstall --everything\ntelepresence connect\n\n# Check for resource constraints\nkubectl describe pod -n ambassador -l app=traffic-manager\n</code></pre></p>"},{"location":"28-Telepresence/TROUBLESHOOTING/#3-intercept-not-working","title":"3. Intercept Not Working","text":"<p>Symptom: Traffic not reaching local service</p> <p>Solutions: <pre><code># Check intercept status\ntelepresence list\ntelepresence status\n\n# Verify local service is running\nnetstat -an | grep 5000\n# or\nlsof -i :5000\n\n# Check for port conflicts\ntelepresence intercept backend --port 5001:5000 --namespace telepresence-demo\n\n# View detailed logs\ntelepresence quit\ntelepresence loglevel debug\ntelepresence connect\n</code></pre></p>"},{"location":"28-Telepresence/TROUBLESHOOTING/#4-dns-resolution-issues","title":"4. DNS Resolution Issues","text":"<p>Symptom: Cannot resolve cluster service names</p> <p>Solutions: <pre><code># Check DNS configuration\ntelepresence status\n\n# Test DNS resolution\nping dataservice.telepresence-demo.svc.cluster.local\n\n# Try alternative DNS\ntelepresence quit\ntelepresence connect --dns=google\n\n# On macOS, reset DNS\nsudo killall -HUP mDNSResponder\n</code></pre></p>"},{"location":"28-Telepresence/TROUBLESHOOTING/#5-permission-denied-errors","title":"5. Permission Denied Errors","text":"<p>Symptom: Permission errors when connecting</p> <p>Solutions: <pre><code># Check if running with proper permissions\n# On macOS/Linux, might need to run with sudo for the first connection\nsudo telepresence connect\n\n# Check cluster admin permissions\nkubectl auth can-i '*' '*' --all-namespaces\n\n# Contact cluster admin if lacking permissions\n</code></pre></p>"},{"location":"28-Telepresence/TROUBLESHOOTING/#6-port-already-in-use","title":"6. Port Already in Use","text":"<p>Symptom: \u201cAddress already in use\u201d error</p> <p>Solutions: <pre><code># Find what's using the port\nlsof -i :5000\n\n# Kill the process\nkill -9 &lt;PID&gt;\n\n# Use a different local port\ntelepresence intercept backend --port 5001:5000 --namespace telepresence-demo\n# Then run your app on port 5001\n</code></pre></p>"},{"location":"28-Telepresence/TROUBLESHOOTING/#7-slow-connection-or-timeouts","title":"7. Slow Connection or Timeouts","text":"<p>Symptom: Slow responses or connection timeouts</p> <p>Solutions: <pre><code># Check network connectivity\nping 8.8.8.8\n\n# Restart telepresence\ntelepresence quit\ntelepresence connect\n\n# Check cluster health\nkubectl get nodes\nkubectl top nodes\n\n# Reduce intercept scope\ntelepresence intercept backend --port 5000 --http-match=auto\n</code></pre></p>"},{"location":"28-Telepresence/TROUBLESHOOTING/#8-environment-variables-not-loading","title":"8. Environment Variables Not Loading","text":"<p>Symptom: App can\u2019t access cluster environment variables</p> <p>Solutions: <pre><code># Capture environment to file\ntelepresence intercept backend \\\n  --port 5000 \\\n  --namespace telepresence-demo \\\n  --env-file=.env.cluster\n\n# Load in your shell\nsource .env.cluster\n\n# Then run your app\npython app.py\n\n# Or use with Docker\ntelepresence intercept backend \\\n  --port 5000 \\\n  --docker-run -- \\\n  --env-file .env.cluster \\\n  my-image\n</code></pre></p>"},{"location":"28-Telepresence/TROUBLESHOOTING/#9-multiple-intercepts-conflict","title":"9. Multiple Intercepts Conflict","text":"<p>Symptom: Only one intercept works at a time</p> <p>Solutions: <pre><code># List all intercepts\ntelepresence list\n\n# Leave specific intercept\ntelepresence leave backend\n\n# Leave all intercepts\ntelepresence leave --all\n\n# Use different ports for different services\ntelepresence intercept backend --port 5000\ntelepresence intercept frontend --port 3000\n</code></pre></p>"},{"location":"28-Telepresence/TROUBLESHOOTING/#10-cant-access-volumes","title":"10. Can\u2019t Access Volumes","text":"<p>Symptom: Volume mounts not accessible locally</p> <p>Solutions: <pre><code># Enable volume mounts with intercept\ntelepresence intercept backend \\\n  --port 5000 \\\n  --mount=true \\\n  --mount-type=sshfs\n\n# Check mount location\nls ~/telepresence/telepresence-demo/\n\n# Alternative: use docker mode\ntelepresence intercept backend \\\n  --port 5000 \\\n  --docker-run \\\n  -- \\\n  -v ~/telepresence/telepresence-demo:/mnt/volumes \\\n  my-image\n</code></pre></p>"},{"location":"28-Telepresence/TROUBLESHOOTING/#debugging-commands","title":"Debugging Commands","text":"<pre><code># Show detailed status\ntelepresence status\n\n# Enable debug logging\ntelepresence loglevel debug\n\n# Show version information\ntelepresence version\n\n# Show all intercepts across namespaces\ntelepresence list --all-namespaces\n\n# Test cluster connectivity\ntelepresence test\n\n# View background daemon logs (macOS)\ntail -f ~/Library/Logs/telepresence/*.log\n\n# View background daemon logs (Linux)\njournalctl -u telepresence -f\n</code></pre>"},{"location":"28-Telepresence/TROUBLESHOOTING/#getting-help","title":"Getting Help","text":"<ul> <li>Official Docs: https://www.telepresence.io/docs/</li> <li>GitHub Issues: https://github.com/telepresenceio/telepresence/issues</li> <li>Community Slack: https://a8r.io/slack</li> <li>Stack Overflow: Tag <code>telepresence</code></li> </ul>"},{"location":"28-Telepresence/TROUBLESHOOTING/#reporting-issues","title":"Reporting Issues","text":"<p>When reporting issues, include:</p> <ol> <li>Telepresence version: <code>telepresence version</code></li> <li>Kubernetes version: <code>kubectl version</code></li> <li>Operating system and version</li> <li>Output of <code>telepresence status</code></li> <li>Relevant logs with debug enabled</li> <li>Steps to reproduce the issue</li> </ol>"},{"location":"28-Telepresence/resources/BUILD/","title":"Telepresence Demo - Docker Images Build","text":"<p>This directory contains instructions for building the Docker images if you want to use custom images instead of inline deployments.</p>"},{"location":"28-Telepresence/resources/BUILD/#building-images","title":"Building Images","text":""},{"location":"28-Telepresence/resources/BUILD/#backend-service","title":"Backend Service","text":"<pre><code>cd backend-app\ndocker build -t your-registry/telepresence-backend:v1 .\ndocker push your-registry/telepresence-backend:v1\n</code></pre>"},{"location":"28-Telepresence/resources/BUILD/#data-service","title":"Data Service","text":"<pre><code>cd dataservice-app\ndocker build -t your-registry/telepresence-dataservice:v1 .\ndocker push your-registry/telepresence-dataservice:v1\n</code></pre>"},{"location":"28-Telepresence/resources/BUILD/#frontend","title":"Frontend","text":"<pre><code>cd frontend-app\ndocker build -t your-registry/telepresence-frontend:v1 .\ndocker push your-registry/telepresence-frontend:v1\n</code></pre>"},{"location":"28-Telepresence/resources/BUILD/#using-custom-images","title":"Using Custom Images","text":"<p>If you build custom images, update the Kubernetes manifests to use your images:</p> <ol> <li>Edit <code>02-dataservice.yaml</code> and replace the inline Python image with your dataservice image</li> <li>Edit <code>03-backend.yaml</code> and replace the inline Python image with your backend image</li> <li>Edit <code>04-frontend.yaml</code> and replace nginx:alpine with your frontend image</li> </ol>"},{"location":"28-Telepresence/resources/BUILD/#note","title":"Note","text":"<p>The current setup uses inline deployments (building the app within the container at runtime) for simplicity. This is perfect for demos but not recommended for production use. For production, always use pre-built images.</p>"},{"location":"29-EFK/","title":"EFK Stack - Elasticsearch, Filebeat, Kibana","text":"<ul> <li>The EFK stack is a popular Kubernetes-native logging solution combining Elasticsearch (storage), Filebeat (collection), and Kibana (visualization).</li> <li>This lab deploys a file-based processing architecture: Filebeat writes logs to a shared PVC instead of directly to Elasticsearch, decoupling collection from indexing.</li> <li>Full air-gapped / offline installation support via Harbor registry is included.</li> <li>The entire stack can be deployed via ArgoCD using the App of Apps pattern from Lab 18.</li> </ul>"},{"location":"29-EFK/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>Deploy Elasticsearch, Filebeat, and Kibana on Kubernetes using Helm</li> <li>Implement a file-based log processing pipeline with a CronJob</li> <li>Use a shared PersistentVolumeClaim to buffer logs between collection and indexing</li> <li>Access Kibana via Nginx Ingress</li> <li>Query logs with KQL (Kibana Query Language)</li> <li>Deploy the EFK stack via ArgoCD App of Apps (from Lab 18)</li> <li>Perform air-gapped offline installation using Harbor as a local registry</li> </ul>"},{"location":"29-EFK/#what-is-the-efk-stack","title":"What is the EFK Stack?","text":"Component Role Elasticsearch Search and analytics engine - stores and indexes log data Filebeat Lightweight log shipper (DaemonSet) - collects container logs Kibana Web UI for searching, visualizing, and dashboarding log data"},{"location":"29-EFK/#why-file-based-processing","title":"Why File-Based Processing?","text":"<p>Traditional EFK sends logs directly from Filebeat to Elasticsearch. This lab uses an intermediate file approach:</p> Aspect Direct (traditional) File-Based (this lab) Reliability Logs lost if ES is down Logs persist on PVC even if ES is down Debugging No raw log access Raw JSON files always available Reprocessing Not possible Reprocess any time by rerunning the CronJob Monitoring Single pipeline Clear separation: collection vs. indexing"},{"location":"29-EFK/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    subgraph cluster[\"Kubernetes Cluster\"]\n        subgraph nodes[\"All Nodes\"]\n            fb[\"Filebeat DaemonSet\\ncollects /var/log/containers/*.log\"]\n        end\n\n        subgraph pods[\"Application Pods\"]\n            lg[\"Log Generator\\n(3 replicas)\"]\n            other[\"Other application\\npods\"]\n        end\n\n        subgraph storage[\"Shared Storage\"]\n            pvc[\"PersistentVolumeClaim\\n5Gi - /filebeat-logs/\"]\n        end\n\n        subgraph processing[\"Processing\"]\n            cron[\"Log Processor\\n(CronJob - every 2 min)\"]\n        end\n\n        subgraph efk[\"efk namespace\"]\n            es[\"Elasticsearch\\n(StatefulSet)\"]\n            kibana[\"Kibana\\n(Deployment)\"]\n            ing[\"Nginx Ingress\\nkibana.local\"]\n        end\n    end\n\n    user[\"User / Browser\"] --&gt; ing\n    lg -. logs .-&gt; fb\n    other -. logs .-&gt; fb\n    fb --&gt; pvc\n    pvc --&gt; cron\n    cron --&gt; es\n    es --&gt; kibana\n    ing --&gt; kibana</code></pre>"},{"location":"29-EFK/#data-flow","title":"Data Flow","text":"<pre><code>sequenceDiagram\n    participant App as Application Pods\n    participant FB as Filebeat (DaemonSet)\n    participant PVC as Shared PVC\n    participant Proc as Log Processor (CronJob)\n    participant ES as Elasticsearch\n    participant Kib as Kibana\n\n    App-&gt;&gt;FB: Write stdout/stderr logs\n    FB-&gt;&gt;PVC: Write JSON log files (/filebeat-logs/)\n    Note over PVC: Files persisted on disk\n    Proc-&gt;&gt;PVC: Read unprocessed files (every 2 min)\n    Proc-&gt;&gt;ES: Bulk-send log entries via REST API\n    Proc-&gt;&gt;PVC: Mark files as processed (keep originals)\n    Kib-&gt;&gt;ES: Query logs via REST API</code></pre>"},{"location":"29-EFK/#directory-structure","title":"Directory Structure","text":"<pre><code>33-EFK/\n\u251c\u2500\u2500 README.md                       # This file\n\u251c\u2500\u2500 .env                            # Configuration (image tags, Harbor settings)\n\u251c\u2500\u2500 demo.sh                         # Online deployment script\n\u251c\u2500\u2500 monitor.sh                      # Monitoring and testing script\n\u251c\u2500\u2500 access-kibana.sh                # Kibana access helper\n\u251c\u2500\u2500 fix-kibana.sh                   # Dashboard re-import utility\n\u251c\u2500\u2500 airgap.sh                       # Offline/air-gapped installation orchestrator\n\u2502\n\u251c\u2500\u2500 argocd-apps/                    # ArgoCD Application manifests (App of Apps)\n\u2502   \u251c\u2500\u2500 elasticsearch.yaml          # ArgoCD App: Elasticsearch Helm chart\n\u2502   \u251c\u2500\u2500 filebeat.yaml               # ArgoCD App: Filebeat Helm chart (wave 1)\n\u2502   \u251c\u2500\u2500 kibana.yaml                 # ArgoCD App: Kibana Helm chart (wave 1)\n\u2502   \u251c\u2500\u2500 log-generator.yaml          # ArgoCD App: Log Generator Helm chart (wave 2)\n\u2502   \u2514\u2500\u2500 log-processor.yaml          # ArgoCD App: Log Processor Helm chart (wave 2)\n\u2502\n\u251c\u2500\u2500 helm/\n\u2502   \u251c\u2500\u2500 elasticsearch/              # Elasticsearch Helm chart\n\u2502   \u251c\u2500\u2500 filebeat/                   # Filebeat Helm chart (file output mode)\n\u2502   \u251c\u2500\u2500 kibana/                     # Kibana Helm chart (+ dashboard importer)\n\u2502   \u2502   \u2514\u2500\u2500 dashboards/             # 8 pre-built NDJSON dashboard files\n\u2502   \u251c\u2500\u2500 log-processor/              # Log Processor CronJob Helm chart\n\u2502   \u2514\u2500\u2500 log-generator/              # Log Generator Helm chart\n\u2502\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 common.sh                   # Shared functions and color helpers\n\u2502   \u251c\u2500\u2500 install-harbor.sh           # Install Harbor registry on K8s\n\u2502   \u251c\u2500\u2500 install-ingress.sh          # Install Nginx Ingress Controller\n\u2502   \u251c\u2500\u2500 retag-and-push-images.sh    # Retag images for Harbor and push\n\u2502   \u251c\u2500\u2500 upload-charts-to-harbor.sh  # Push Helm charts to Harbor OCI\n\u2502   \u251c\u2500\u2500 generate-harbor-values.sh   # Generate registry override values\n\u2502   \u251c\u2500\u2500 offline-install.sh          # Install EFK from Harbor\n\u2502   \u2514\u2500\u2500 verify-deployment.sh        # Verify offline deployment\n\u2502\n\u2514\u2500\u2500 artifacts/                      # Offline artifacts (generated by airgap.sh)\n    \u251c\u2500\u2500 download-all.sh\n    \u251c\u2500\u2500 images/                     # Container images as .tar files\n    \u251c\u2500\u2500 charts/                     # Packaged Helm charts (.tgz)\n    \u2514\u2500\u2500 harbor/                     # Harbor chart and images\n</code></pre>"},{"location":"29-EFK/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (v1.20+) with at least 8 GB RAM</li> <li><code>kubectl</code> configured to access your cluster</li> <li><code>Helm 3.x</code> installed</li> <li>(Optional) Nginx Ingress Controller for Kibana access</li> </ul> <pre><code># Install kubectl (macOS)\nbrew install kubectl\n\n# Install Helm\nbrew install helm\n\n# Verify\nkubectl version --client\nhelm version\n</code></pre>"},{"location":"29-EFK/#lab","title":"Lab","text":""},{"location":"29-EFK/#part-01-deploy-the-efk-stack","title":"Part 01 - Deploy the EFK Stack","text":""},{"location":"29-EFK/#01-deploy-all-components","title":"01. Deploy All Components","text":"<pre><code>chmod +x demo.sh\n./demo.sh deploy\n</code></pre> <p>The script will:</p> <ul> <li>Create the <code>efk</code> namespace</li> <li>Deploy Elasticsearch (StatefulSet)</li> <li>Deploy Kibana with Nginx Ingress</li> <li>Deploy Filebeat DaemonSet (writes logs to PVC files)</li> <li>Deploy Log Generator pods (3 replicas generating structured logs)</li> <li>Deploy Log Processor CronJob + run an initial Job immediately</li> <li>Wait for all pods to be ready</li> <li>Print Kibana access information</li> </ul>"},{"location":"29-EFK/#02-access-kibana","title":"02. Access Kibana","text":""},{"location":"29-EFK/#option-a-ingress-recommended","title":"Option A - Ingress (Recommended)","text":"<pre><code># Get the Ingress IP\nINGRESS_IP=$(kubectl get ingress -n efk kibana \\\n    -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n\n# Add to /etc/hosts if not already present\ngrep -q \"kibana.local\" /etc/hosts || \\\n    echo \"${INGRESS_IP:-192.168.49.2} kibana.local\" | sudo tee -a /etc/hosts\n\nopen http://kibana.local\n</code></pre>"},{"location":"29-EFK/#option-b-port-forward","title":"Option B - Port-Forward","text":"<pre><code>kubectl port-forward -n efk svc/kibana 5601:5601 &amp;\nopen http://localhost:5601\n</code></pre>"},{"location":"29-EFK/#part-02-kibana-dashboards","title":"Part 02 - Kibana Dashboards","text":"<p>Dashboards are automatically imported during deployment via the Kibana Helm chart\u2019s dashboard importer init container.</p>"},{"location":"29-EFK/#available-dashboards-8","title":"Available Dashboards (8)","text":"Dashboard Description General Logs Dashboard Overview of all logs by level, component, and time Error Analysis Dashboard Comprehensive error monitoring and analysis Warning Analysis Dashboard Track and analyze WARNING level logs Component Activity Dashboard Detailed per-component log breakdown Performance Overview Dashboard Key metrics, volume trends, and health indicators HTTP Access Dashboard HTTP request logs and access patterns K8s Monitoring Dashboard Kubernetes cluster monitoring APM Dashboard Application performance monitoring"},{"location":"29-EFK/#access-steps","title":"Access Steps","text":"<ol> <li>Open Kibana at <code>http://kibana.local</code></li> <li>Click Dashboard in the left sidebar</li> <li>Select any dashboard to view logs</li> </ol>"},{"location":"29-EFK/#verify-or-re-import-dashboards","title":"Verify or Re-import Dashboards","text":"<pre><code># Check import job status\nkubectl logs -n efk -l app=kibana,component=dashboard-importer\n\n# Manually re-import by upgrading the chart\nhelm upgrade kibana ./helm/kibana -n efk\n</code></pre>"},{"location":"29-EFK/#part-03-log-pipeline","title":"Part 03 - Log Pipeline","text":""},{"location":"29-EFK/#log-generator","title":"Log Generator","text":"<p>The log generator creates structured JSON logs with varying severity levels and simulated service names:</p> <pre><code>{\n  \"timestamp\": \"2026-02-22T10:30:45Z\",\n  \"level\": \"ERROR\",\n  \"component\": \"PaymentService\",\n  \"message\": \"Transaction failed: timeout\",\n  \"request_id\": \"req-1740217845-12345\",\n  \"counter\": 42\n}\n</code></pre> <p>Components that generate logs: <code>UserService</code>, <code>OrderService</code>, <code>PaymentService</code>, <code>AuthService</code>, <code>DatabaseService</code>, <code>CacheService</code></p>"},{"location":"29-EFK/#file-based-pipeline-flow","title":"File-Based Pipeline Flow","text":"<pre><code>graph LR\n    fb[\"Filebeat DaemonSet\"] --&gt;|\"writes JSON\"| pvc[\"Shared PVC\\n/filebeat-logs/\"]\n    pvc --&gt;|\"reads every 2min\"| proc[\"Log Processor\\n(CronJob)\"]\n    proc --&gt;|\"bulk REST API\"| es[\"Elasticsearch\"]\n    proc -. \"keeps original\" .-&gt; pvc\n    es --&gt; kib[\"Kibana\"]</code></pre>"},{"location":"29-EFK/#monitor-the-pipeline","title":"Monitor the Pipeline","text":"<pre><code># Interactive monitor\n./monitor.sh\n\n# Quick summary\n./monitor.sh summary\n\n# End-to-end pipeline test\n./monitor.sh test\n\n# Full detailed report\n./monitor.sh full\n</code></pre>"},{"location":"29-EFK/#manual-pipeline-checks","title":"Manual Pipeline Checks","text":"<pre><code># Verify Filebeat is writing log files\nkubectl exec -n efk -l app=filebeat -- ls -lh /filebeat-logs/\n\n# Count documents in Elasticsearch\nkubectl exec -n efk elasticsearch-0 -- \\\n    curl -s http://localhost:9200/filebeat-*/_count\n\n# View CronJob schedule\nkubectl get cronjob -n efk\n\n# Manually trigger the log processor\nkubectl create job -n efk --from=cronjob/log-processor manual-$(date +%s)\nkubectl logs -n efk job/manual-* --tail=30\n</code></pre>"},{"location":"29-EFK/#part-04-kibana-query-language-kql","title":"Part 04 - Kibana Query Language (KQL)","text":"<pre><code># Show only ERROR logs\njson.level: \"ERROR\"\n\n# Show logs from a specific component\njson.component: \"PaymentService\"\n\n# Show ERROR or WARN logs\njson.level: (\"ERROR\" OR \"WARN\")\n\n# Show logs with a keyword in the message\njson.message: *timeout*\n\n# Combine multiple conditions\njson.level: \"ERROR\" AND json.component: \"PaymentService\"\n</code></pre>"},{"location":"29-EFK/#useful-prometheus-style-elasticsearch-queries","title":"Useful Prometheus-style Elasticsearch Queries","text":"<pre><code># List all indices\nkubectl exec -n efk elasticsearch-0 -- \\\n    curl -s http://localhost:9200/_cat/indices?v\n\n# Cluster health\nkubectl exec -n efk elasticsearch-0 -- \\\n    curl -s http://localhost:9200/_cluster/health?pretty\n\n# Count documents\nkubectl exec -n efk elasticsearch-0 -- \\\n    curl -s http://localhost:9200/filebeat-*/_count?pretty\n\n# Recent 5 log entries\nkubectl exec -n efk elasticsearch-0 -- \\\n    curl -s \"http://localhost:9200/filebeat-*/_search?size=5&amp;sort=@timestamp:desc&amp;pretty\"\n</code></pre>"},{"location":"29-EFK/#part-05-deploy-via-argocd-app-of-apps","title":"Part 05 - Deploy via ArgoCD (App of Apps)","text":"<p>The EFK stack can be deployed via ArgoCD from Lab 18 using the App of Apps pattern. The <code>argocd-apps/</code> directory contains individual ArgoCD Application manifests for each Helm chart.</p>"},{"location":"29-EFK/#deploy-via-app-of-apps-from-lab-18","title":"Deploy via App of Apps (from Lab 18)","text":"<pre><code># From Lab 18 directory - deploy the root App of Apps\nkubectl apply -f ../18-ArgoCD/apps/app-of-apps.yaml\n</code></pre> <p>ArgoCD will discover <code>Labs/33-EFK/argocd-apps/</code> and deploy each component with proper sync waves:</p> <ul> <li>Wave 0 - Elasticsearch (deployed first)</li> <li>Wave 1 - Filebeat, Kibana (deployed after Elasticsearch is healthy)</li> <li>Wave 2 - Log Generator, Log Processor (deployed last)</li> </ul>"},{"location":"29-EFK/#deploy-efk-app-of-apps-directly","title":"Deploy EFK App of Apps Directly","text":"<pre><code># Apply only the EFK App of Apps (without the full Lab 18 setup)\nkubectl apply -f argocd-apps/elasticsearch.yaml\nkubectl apply -f argocd-apps/filebeat.yaml\nkubectl apply -f argocd-apps/kibana.yaml\nkubectl apply -f argocd-apps/log-generator.yaml\nkubectl apply -f argocd-apps/log-processor.yaml\n</code></pre>"},{"location":"29-EFK/#monitor-via-argocd","title":"Monitor via ArgoCD","text":"<pre><code>argocd app list | grep efk\nargocd app get efk-elasticsearch\nkubectl get applications -n argocd | grep efk\n</code></pre>"},{"location":"29-EFK/#part-06-air-gapped-offline-installation","title":"Part 06 - Air-Gapped / Offline Installation","text":"<p>This lab supports fully offline deployment using Harbor as a local Docker and Helm chart registry.</p>"},{"location":"29-EFK/#air-gapped-flow","title":"Air-Gapped Flow","text":"<pre><code>graph LR\n    subgraph internet[\"Internet-Connected Machine\"]\n        prep[\"1. ./airgap.sh prepare\\nPull images + package charts\"]\n    end\n\n    subgraph transfer[\"Transfer\"]\n        tar[\"artifacts/ folder\\n(images + charts + harbor)\"]\n    end\n\n    subgraph airgap[\"Air-Gapped Cluster\"]\n        install[\"2. ./airgap.sh install\\nHarbor + push + EFK deploy\"]\n        verify[\"3. ./airgap.sh verify\\nValidate all components\"]\n    end\n\n    prep --&gt; tar --&gt; install --&gt; verify</code></pre>"},{"location":"29-EFK/#configuration-env","title":"Configuration (.env)","text":"<pre><code># Harbor settings\nHARBOR_DOMAIN=\"harbor.local\"\nHARBOR_ADMIN_PASSWORD=\"Harbor12345\"\nHARBOR_PROJECT=\"efk\"\n\n# Image versions\nES_TAG=\"8.11.0\"\nFILEBEAT_TAG=\"8.11.0\"\nKIBANA_TAG=\"8.11.0\"\n</code></pre>"},{"location":"29-EFK/#step-1-prepare-artifacts-requires-internet","title":"Step 1 - Prepare Artifacts (requires internet)","text":"<pre><code># Download all container images, Helm charts, and Harbor installer\n./airgap.sh prepare\n</code></pre>"},{"location":"29-EFK/#step-2-transfer-to-air-gapped-machine","title":"Step 2 - Transfer to Air-Gapped Machine","text":"<pre><code>tar czf efk-offline.tar.gz 33-EFK/\n# Copy efk-offline.tar.gz to the air-gapped machine\n</code></pre>"},{"location":"29-EFK/#step-3-full-offline-install","title":"Step 3 - Full Offline Install","text":"<pre><code># One command: installs Harbor, pushes content, deploys EFK\n./airgap.sh install\n</code></pre>"},{"location":"29-EFK/#step-4-verify","title":"Step 4 - Verify","text":"<pre><code>./airgap.sh verify\n</code></pre>"},{"location":"29-EFK/#all-air-gap-commands","title":"All Air-Gap Commands","text":"<pre><code>./airgap.sh prepare       # Download artifacts (needs internet)\n./airgap.sh install       # Full install: Harbor + push + EFK\n./airgap.sh harbor        # Install Harbor registry only\n./airgap.sh push          # Push images and charts to Harbor\n./airgap.sh efk           # Install EFK from Harbor\n./airgap.sh verify        # Run verification tests\n./airgap.sh status        # Show deployment status\n./airgap.sh cleanup       # Remove EFK (keep Harbor)\n./airgap.sh cleanup-all   # Remove everything\n</code></pre>"},{"location":"29-EFK/#part-07-configuration","title":"Part 07 - Configuration","text":""},{"location":"29-EFK/#elasticsearch","title":"Elasticsearch","text":"<p>Edit <code>helm/elasticsearch/values.yaml</code>:</p> <pre><code>resources:\n  requests:\n    memory: \"2Gi\"\n    cpu: \"1000m\"\n  limits:\n    memory: \"2Gi\"\n    cpu: \"1000m\"\npersistence:\n  size: 10Gi\n</code></pre>"},{"location":"29-EFK/#log-processor-cronjob-schedule","title":"Log Processor (CronJob Schedule)","text":"<p>Edit <code>helm/log-processor/values.yaml</code>:</p> <pre><code># How often to process log files\nschedule: \"*/2 * * * *\"    # Every 2 minutes (default)\n# schedule: \"*/1 * * * *\"  # Every 1 minute\n# schedule: \"*/5 * * * *\"  # Every 5 minutes\n\nprocessing:\n  keepOriginalFiles: true   # Keep files in /filebeat-logs/ for inspection\n  createBackups: true       # Also create copies in /filebeat-logs/processed/\n</code></pre>"},{"location":"29-EFK/#log-generator_1","title":"Log Generator","text":"<p>Edit <code>helm/log-generator/values.yaml</code>:</p> <pre><code>replicaCount: 3     # Pods generating logs\nlogInterval: 5      # Seconds between log messages per pod\n</code></pre>"},{"location":"29-EFK/#part-08-troubleshooting","title":"Part 08 - Troubleshooting","text":""},{"location":"29-EFK/#pods-not-starting","title":"Pods Not Starting","text":"<pre><code>kubectl get events -n efk --sort-by='.lastTimestamp'\nkubectl describe pod &lt;pod-name&gt; -n efk\n</code></pre>"},{"location":"29-EFK/#filebeat-not-collecting-logs","title":"Filebeat Not Collecting Logs","text":"<pre><code># Check DaemonSet coverage\nkubectl get daemonset -n efk filebeat\n\n# Verify RBAC\nkubectl get clusterrole filebeat\nkubectl get clusterrolebinding filebeat\n\n# Check log files are being written\nkubectl exec -n efk -l app=filebeat -- ls -lh /filebeat-logs/\n</code></pre>"},{"location":"29-EFK/#log-processor-not-running","title":"Log Processor Not Running","text":"<pre><code># Check CronJob status\nkubectl get cronjob -n efk log-processor\n\n# View recent job executions\nkubectl get jobs -n efk -l app=log-processor --sort-by=.metadata.creationTimestamp\n\n# View processor logs\nkubectl logs -n efk -l app=log-processor --tail=100\n\n# Manually trigger for testing\nkubectl create job -n efk --from=cronjob/log-processor test-run-$(date +%s)\n</code></pre>"},{"location":"29-EFK/#no-data-in-kibana","title":"No Data in Kibana","text":"<pre><code># 1. Verify Filebeat is writing files\nkubectl exec -n efk -l app=filebeat -- ls -lh /filebeat-logs/\n\n# 2. Check Log Processor has run\nkubectl get jobs -n efk -l app=log-processor\n\n# 3. Confirm data in Elasticsearch\nkubectl exec -n efk elasticsearch-0 -- \\\n    curl -s http://localhost:9200/filebeat-*/_count\n\n# 4. Check index pattern in Kibana matches: filebeat-*\n# 5. Adjust the time range in Kibana (top right corner)\n</code></pre> <p>Note</p> <p>The first log data appears in Kibana after the Log Processor CronJob runs (up to 2 minutes after deployment).</p>"},{"location":"29-EFK/#kibana-dashboard-import-failed","title":"Kibana Dashboard Import Failed","text":"<pre><code>kubectl logs -n efk -l app=kibana,component=dashboard-importer\n\n# Re-import by upgrading the chart\nhelm upgrade kibana ./helm/kibana -n efk\n</code></pre>"},{"location":"29-EFK/#cleanup","title":"Cleanup","text":"<pre><code># Full cleanup\n./demo.sh cleanup\n\n# Manual cleanup\nhelm uninstall elasticsearch filebeat kibana log-processor log-generator -n efk\nkubectl delete namespace efk\n</code></pre>"},{"location":"29-EFK/#resources","title":"Resources","text":"<ul> <li>Elasticsearch Documentation</li> <li>Filebeat Documentation</li> <li>Kibana Documentation</li> <li>Kubernetes Logging Architecture</li> <li>Harbor Registry</li> </ul>"},{"location":"30-Keda/","title":"KEDA - Kubernetes Event-Driven Autoscaling","text":"<ul> <li><code>KEDA</code> is a Kubernetes-based Event Driven Autoscaler that extends the native Kubernetes <code>HorizontalPodAutoscaler</code> (HPA).</li> <li>It allows you to scale any container in Kubernetes based on the number of events from virtually any event source - queues, streams, databases, HTTP traffic, cron schedules, and more.</li> <li><code>KEDA</code> is a CNCF Graduated project (since 2023), widely adopted and production-proven.</li> </ul>"},{"location":"30-Keda/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What <code>KEDA</code> is and how it differs from native HPA</li> <li>How KEDA architecture works (Operator, Metrics Adapter, Scalers)</li> <li>KEDA core CRDs: <code>ScaledObject</code>, <code>ScaledJob</code>, <code>TriggerAuthentication</code></li> <li>How to install KEDA via Helm</li> <li>Scale to zero and scale from zero using event-driven triggers</li> <li>Real-world scalers: CPU/Memory, Cron, Redis, Kafka, HTTP, Prometheus</li> <li>Using <code>TriggerAuthentication</code> with Kubernetes Secrets</li> <li>Scaling Jobs (not Deployments) with <code>ScaledJob</code></li> <li>Combining KEDA with ArgoCD (Lab 18) for GitOps-managed autoscaling</li> </ul>"},{"location":"30-Keda/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link KEDA Official Documentation keda.sh KEDA Scalers Reference keda.sh/docs/scalers KEDA on ArtifactHub (Helm) artifacthub.io/keda KEDA GitHub Repository github.com/kedacore/keda KEDA HTTP Add-on github.com/kedacore/http-add-on CNCF Project Page cncf.io/projects/keda"},{"location":"30-Keda/#the-problem-keda-solves","title":"The Problem KEDA Solves","text":""},{"location":"30-Keda/#native-hpa-limitations","title":"Native HPA Limitations","text":"<p>Kubernetes\u2019 built-in <code>HorizontalPodAutoscaler</code> only scales on CPU and memory metrics (or custom metrics via the Metrics API, which is complex to set up). This means you can\u2019t natively:</p> <ul> <li>Scale a worker deployment to zero when a job queue is empty</li> <li>Scale up when a Kafka topic has unread messages</li> <li>Scale based on a cron schedule (e.g., double capacity every weekday morning)</li> <li>Scale based on a Redis list length, database row count, or HTTP request rate</li> </ul> <pre><code>graph LR\n    hpa[\"Native HPA\"] -- \"Only\" --&gt; cpu[\"CPU Metrics\"]\n    hpa -- \"Only\" --&gt; mem[\"Memory Metrics\"]\n    hpa -- \"Complex setup\" --&gt; custom[\"Custom Metrics API\"]\n\n    keda[\"KEDA\"] -- \"50+ scalers\" --&gt; kafka[\"Kafka\"]\n    keda --&gt; rabbitmq[\"RabbitMQ\"]\n    keda --&gt; redis[\"Redis\"]\n    keda --&gt; cron[\"Cron Schedule\"]\n    keda --&gt; http[\"HTTP Traffic\"]\n    keda --&gt; prometheus[\"Prometheus\"]\n    keda --&gt; aws[\"AWS SQS / SNS\"]\n    keda --&gt; azure[\"Azure Service Bus\"]\n    keda --&gt; gcp[\"GCP Pub/Sub\"]\n    keda --&gt; \"...\"</code></pre>"},{"location":"30-Keda/#scale-to-zero-the-game-changer","title":"Scale-to-Zero: The Game Changer","text":"<p>KEDA\u2019s most powerful feature is scale-to-zero: when there are no events, pods scale down to 0 replicas, saving resources. When events arrive, KEDA scales back up instantly.</p> Scenario Without KEDA With KEDA Idle queue worker 1-3 pods always running 0 pods (scale to zero) Morning traffic spike Manual scaling or slow HPA Pre-warmed via Cron scaler Kafka consumer lag Fixed replica count Dynamic scaling on lag metric Batch job Long-running Deployment Short-lived Jobs, scaled by queue depth"},{"location":"30-Keda/#keda-architecture","title":"KEDA Architecture","text":"<pre><code>graph TB\n    subgraph cluster[\"Kubernetes Cluster\"]\n        subgraph keda_ns[\"keda namespace\"]\n            operator[\"KEDA Operator\\n(keda-operator)\"]\n            metrics[\"KEDA Metrics Adapter\\n(keda-operator-metrics-apiserver)\"]\n            hooks[\"KEDA Admission Webhooks\\n(keda-admission-webhooks)\"]\n        end\n\n        subgraph app_ns[\"app namespace\"]\n            so[\"ScaledObject CRD\"]\n            sj[\"ScaledJob CRD\"]\n            ta[\"TriggerAuthentication CRD\"]\n            deployment[\"Deployment / StatefulSet\"]\n        end\n\n        hpa_k8s[\"Kubernetes HPA\\n(managed by KEDA)\"]\n        k8s_api[\"Kubernetes API Server\"]\n    end\n\n    subgraph external[\"External Event Sources\"]\n        kafka_ext[\"Kafka Cluster\"]\n        redis_ext[\"Redis\"]\n        rabbitmq_ext[\"RabbitMQ\"]\n        prom_ext[\"Prometheus\"]\n        cron_ext[\"Cron Schedule\"]\n    end\n\n    so --&gt; operator\n    sj --&gt; operator\n    ta --&gt; operator\n\n    operator -- \"Creates &amp; manages\" --&gt; hpa_k8s\n    operator -- \"Queries metrics\" --&gt; external\n\n    hpa_k8s -- \"Scales\" --&gt; deployment\n    metrics -- \"Exposes custom metrics\" --&gt; k8s_api\n    k8s_api --&gt; hpa_k8s\n\n    kafka_ext --&gt; operator\n    redis_ext --&gt; operator\n    rabbitmq_ext --&gt; operator\n    prom_ext --&gt; operator\n    cron_ext --&gt; operator</code></pre>"},{"location":"30-Keda/#keda-components","title":"KEDA Components","text":"Component Description keda-operator Watches <code>ScaledObject</code>/<code>ScaledJob</code> CRDs; creates/manages HPA objects; polls event sources keda-operator-metrics-apiserver Exposes custom metrics to the Kubernetes Metrics API so native HPA can read them keda-admission-webhooks Validates KEDA CRDs on admission (prevents misconfigurations)"},{"location":"30-Keda/#how-keda-works-step-by-step","title":"How KEDA Works (Step by Step)","text":"<pre><code>sequenceDiagram\n    participant Dev as Developer\n    participant Git as Git/kubectl\n    participant K8s as Kubernetes API\n    participant KEDA as KEDA Operator\n    participant Scaler as Event Source (e.g. Redis)\n    participant HPA as Kubernetes HPA\n    participant Pod as Application Pods\n\n    Dev-&gt;&gt;Git: Apply ScaledObject manifest\n    Git-&gt;&gt;K8s: Create ScaledObject CRD\n    K8s--&gt;&gt;KEDA: ScaledObject admitted &amp; stored\n    KEDA-&gt;&gt;K8s: Create/update HPA for the target Deployment\n    loop Every polling interval (default 30s)\n        KEDA-&gt;&gt;Scaler: Query metric (e.g. Redis list length)\n        Scaler--&gt;&gt;KEDA: Current value (e.g. 150 messages)\n        KEDA-&gt;&gt;K8s: Update HPA with current metric value\n        K8s-&gt;&gt;HPA: HPA calculates desired replicas\n        HPA-&gt;&gt;Pod: Scale Deployment up/down\n    end</code></pre>"},{"location":"30-Keda/#keda-terminology","title":"KEDA Terminology","text":"Term Kind Description ScaledObject CRD Links a Deployment/StatefulSet/custom workload to one or more scalers. KEDA creates a managed HPA for it. ScaledJob CRD Like ScaledObject but for Kubernetes <code>Jobs</code> - creates one Job per event (or batches) instead of scaling pods TriggerAuthentication CRD Stores authentication configs (secrets, pod identity) for scalers that need credentials ClusterTriggerAuthentication CRD Same as TriggerAuthentication but cluster-scoped (reusable across namespaces) Scaler Built-in A plugin inside KEDA that knows how to query a specific event/metric source Trigger Config A single scaler configuration inside a ScaledObject/ScaledJob minReplicaCount Config Minimum replicas (can be <code>0</code> for scale-to-zero) maxReplicaCount Config Maximum replicas KEDA is allowed to scale to cooldownPeriod Config Seconds KEDA waits after last event before scaling back to <code>minReplicaCount</code> pollingInterval Config How often KEDA queries the scaler (default: 30 seconds)"},{"location":"30-Keda/#available-scalers-50","title":"Available Scalers (50+)","text":"<p>KEDA ships with scalers for virtually every major event/metric source:</p> Category Scalers Message Queues Apache Kafka, RabbitMQ, Azure Service Bus, AWS SQS, GCP Pub/Sub, NATS JetStream, IBM MQ Databases Redis (List/Stream/Cluster/Sentinel), PostgreSQL, MySQL, MSSQL, MongoDB, CouchDB Storage AWS S3, Azure Blob Storage, GCS Bucket Monitoring Prometheus, Datadog, Graphite, InfluxDB, New Relic HTTP HTTP Add-on (external component) Compute CPU, Memory (same as HPA but combined with other scalers) Time Cron Cloud Native ArgoCD, KEDA HTTP Add-on, Kubernetes Event-driven Jobs Cloud-Specific Azure Event Hub, Azure Log Analytics, AWS CloudWatch, GCP Stackdriver"},{"location":"30-Keda/#directory-structure","title":"Directory Structure","text":"<pre><code>34-Keda/\n\u251c\u2500\u2500 README.md                          # This file\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 install.sh                     # Install KEDA via Helm\n\u2502   \u2514\u2500\u2500 demo.sh                        # Full automated demo\n\u2514\u2500\u2500 manifests/\n    \u251c\u2500\u2500 00-namespace.yaml              # Namespace for demo workloads\n    \u251c\u2500\u2500 01-demo-deployment.yaml        # A simple nginx deployment to scale\n    \u251c\u2500\u2500 02-scaled-object-cpu.yaml      # ScaledObject: CPU-based scaling\n    \u251c\u2500\u2500 03-scaled-object-cron.yaml     # ScaledObject: Cron-based scheduling\n    \u251c\u2500\u2500 04-redis-stack.yaml            # Redis deployment for queue demo\n    \u251c\u2500\u2500 05-scaled-object-redis.yaml    # ScaledObject: Redis List scaler\n    \u251c\u2500\u2500 06-trigger-auth.yaml           # TriggerAuthentication with Secret\n    \u251c\u2500\u2500 07-scaled-object-redis-auth.yaml # ScaledObject with auth\n    \u251c\u2500\u2500 08-prometheus-scaler.yaml      # ScaledObject: Prometheus scaler\n    \u2514\u2500\u2500 09-scaled-job.yaml             # ScaledJob: batch job per queue message\n</code></pre>"},{"location":"30-Keda/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster (v1.24+)</li> <li><code>kubectl</code> configured to access your cluster</li> <li><code>Helm 3.x</code> installed</li> </ul> <pre><code># Verify prerequisites\nkubectl version --client --short\nhelm version --short\n</code></pre>"},{"location":"30-Keda/#installation","title":"Installation","text":""},{"location":"30-Keda/#part-01-install-keda-via-helm","title":"Part 01 - Install KEDA via Helm","text":"<p>Helm is the recommended installation method for KEDA.</p>"},{"location":"30-Keda/#01-add-the-keda-helm-repository","title":"01. Add the KEDA Helm repository","text":"<pre><code>helm repo add kedacore https://kedacore.github.io/charts\nhelm repo update kedacore\n\n# Confirm available charts\nhelm search repo kedacore/keda\n</code></pre> <p>Expected output: <pre><code>NAME            CHART VERSION   APP VERSION\nkedacore/keda   2.x.x           2.x.x\n</code></pre></p>"},{"location":"30-Keda/#02-install-keda","title":"02. Install KEDA","text":"<pre><code>helm upgrade --install keda kedacore/keda \\\n    --namespace keda \\\n    --create-namespace \\\n    --wait\n</code></pre>"},{"location":"30-Keda/#03-verify-the-installation","title":"03. Verify the installation","text":"<pre><code>kubectl get pods -n keda\n</code></pre> <p>Expected output (all <code>Running</code>): <pre><code>NAME                                                READY   STATUS    RESTARTS\nkeda-admission-webhooks-xxxx                       1/1     Running   0\nkeda-operator-xxxx                                 1/1     Running   0\nkeda-operator-metrics-apiserver-xxxx               1/1     Running   0\n</code></pre></p>"},{"location":"30-Keda/#04-verify-keda-crds-are-registered","title":"04. Verify KEDA CRDs are registered","text":"<pre><code>kubectl get crd | grep keda\n</code></pre> <p>Expected output: <pre><code>clustertriggerauthentications.keda.sh\nscaledjobs.keda.sh\nscaledobjects.keda.sh\ntriggerauthentications.keda.sh\n</code></pre></p>"},{"location":"30-Keda/#05-verify-the-metrics-api-is-available","title":"05. Verify the metrics API is available","text":"<pre><code>kubectl get apiservice | grep keda\n</code></pre> <p>Expected: <pre><code>v1beta1.external.metrics.k8s.io   keda/keda-operator-metrics-apiserver   True\n</code></pre></p>"},{"location":"30-Keda/#part-02-install-keda-via-kubectl-alternative","title":"Part 02 - Install KEDA via kubectl (Alternative)","text":"<pre><code># Install KEDA using the official release manifest\nkubectl apply --server-side \\\n    -f https://github.com/kedacore/keda/releases/latest/download/keda-2.x.x.yaml\n</code></pre> <p>Note</p> <p>Replace <code>2.x.x</code> with the latest KEDA version from github.com/kedacore/keda/releases.</p>"},{"location":"30-Keda/#core-concepts-labs","title":"Core Concepts &amp; Labs","text":""},{"location":"30-Keda/#part-03-your-first-scaledobject-cpu-scaler","title":"Part 03 - Your First ScaledObject (CPU Scaler)","text":"<p>The <code>CPU</code> scaler is the simplest way to start with KEDA - it works like HPA but lets you combine it with other KEDA scalers.</p>"},{"location":"30-Keda/#understanding-the-scaledobject","title":"Understanding the ScaledObject","text":"<p>A <code>ScaledObject</code> has three key sections:</p> <pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: my-scaler\nspec:\n  # 1. What to scale\n  scaleTargetRef:\n    name: my-deployment         # Must match a Deployment/StatefulSet name\n\n  # 2. Scaling bounds\n  minReplicaCount: 1            # 0 = scale to zero\n  maxReplicaCount: 10\n\n  # 3. When to scale (triggers)\n  triggers:\n    - type: cpu                 # Scaler type\n      metadata:\n        type: Utilization       # AverageValue or Utilization\n        value: \"60\"             # Scale when CPU &gt; 60%\n</code></pre>"},{"location":"30-Keda/#lab-deploy-a-workload-and-scale-it-on-cpu","title":"Lab: Deploy a workload and scale it on CPU","text":"<p>Step 01 - Create the namespace and a demo deployment:</p> <pre><code>kubectl apply -f manifests/00-namespace.yaml\nkubectl apply -f manifests/01-demo-deployment.yaml\n</code></pre> <p>Verify: <pre><code>kubectl get deployment -n keda-demo\n</code></pre></p> <p>Step 02 - Apply the CPU ScaledObject:</p> <pre><code>kubectl apply -f manifests/02-scaled-object-cpu.yaml\n</code></pre> <p>Step 03 - Verify KEDA created an HPA:</p> <pre><code># KEDA creates and manages an HPA automatically\nkubectl get hpa -n keda-demo\n</code></pre> <p>Expected: <pre><code>NAME                          REFERENCE              TARGETS      MINPODS   MAXPODS\nkeda-hpa-demo-cpu-scaler      Deployment/nginx-demo  5%/60%       1         10\n</code></pre></p> <p>Step 04 - Generate CPU load and watch scaling:</p> <pre><code># Terminal 1: Watch pods\nkubectl get pods -n keda-demo -w\n\n# Terminal 2: Generate CPU load\nkubectl run -it --rm load-generator \\\n    --image=busybox \\\n    --namespace=keda-demo \\\n    --restart=Never \\\n    -- /bin/sh -c \"while true; do wget -q -O- http://nginx-demo:80; done\"\n</code></pre> <p>Step 05 - Inspect the ScaledObject status:</p> <pre><code>kubectl get scaledobject -n keda-demo\nkubectl describe scaledobject demo-cpu-scaler -n keda-demo\n</code></pre>"},{"location":"30-Keda/#part-04-cron-scaler-scheduled-scaling","title":"Part 04 - Cron Scaler (Scheduled Scaling)","text":"<p>The Cron scaler lets you define time windows with specific replica counts. This is ideal for predictable traffic patterns - e.g., pre-warm your API servers every weekday morning.</p>"},{"location":"30-Keda/#scaledobject-with-cron-trigger","title":"ScaledObject with Cron Trigger","text":"<pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: nginx-cron-scaler\n  namespace: keda-demo\nspec:\n  scaleTargetRef:\n    name: nginx-demo\n  minReplicaCount: 1      # Night/off-hours minimum\n  maxReplicaCount: 10\n  triggers:\n    - type: cron\n      metadata:\n        timezone: \"Asia/Jerusalem\"     # Any valid IANA timezone\n        start: \"0 8 * * 1-5\"          # Weekdays at 08:00\n        end:   \"0 18 * * 1-5\"         # Weekdays at 18:00\n        desiredReplicas: \"5\"           # Scale to 5 during business hours\n</code></pre>"},{"location":"30-Keda/#multiple-cron-triggers","title":"Multiple Cron Triggers","text":"<p>You can combine multiple cron triggers for different time windows:</p> <pre><code>triggers:\n  # Business hours: Mon-Fri, 08:00-18:00 \u2192 5 replicas\n  - type: cron\n    metadata:\n      timezone: \"Asia/Jerusalem\"\n      start: \"0 8 * * 1-5\"\n      end: \"0 18 * * 1-5\"\n      desiredReplicas: \"5\"\n\n  # Lunch peak: Mon-Fri, 12:00-14:00 \u2192 8 replicas\n  - type: cron\n    metadata:\n      timezone: \"Asia/Jerusalem\"\n      start: \"0 12 * * 1-5\"\n      end: \"0 14 * * 1-5\"\n      desiredReplicas: \"8\"\n\n  # Weekend reduced: Sat-Sun, 10:00-16:00 \u2192 2 replicas\n  - type: cron\n    metadata:\n      timezone: \"Asia/Jerusalem\"\n      start: \"0 10 * * 6-7\"\n      end: \"0 16 * * 6-7\"\n      desiredReplicas: \"2\"\n</code></pre> <p>How multiple triggers work</p> <p>When multiple triggers are active at the same time, KEDA uses the maximum desired replica count across all active triggers.</p>"},{"location":"30-Keda/#lab-apply-the-cron-scaledobject","title":"Lab: Apply the Cron ScaledObject","text":"<pre><code>kubectl apply -f manifests/03-scaled-object-cron.yaml\n\n# Check the current replica count\nkubectl get scaledobject nginx-cron-scaler -n keda-demo\n\n# Inspect the details including the active trigger\nkubectl describe scaledobject nginx-cron-scaler -n keda-demo\n</code></pre>"},{"location":"30-Keda/#part-05-scale-to-zero-with-redis-queue-scaler","title":"Part 05 - Scale to Zero with Redis Queue Scaler","text":"<p>The Redis List scaler monitors a Redis list length and scales the consumer Deployment up (or from 0) when there are items in the queue - and back down to zero when the queue is empty.</p> <p>This is the classic \u201cworker pool\u201d autoscaling pattern:</p> <pre><code>graph LR\n    producer[\"Producer\\n(pushes jobs to queue)\"] --&gt; redis_list[\"Redis List\\n(jobs:queue)\"]\n    redis_list --&gt; keda_op[\"KEDA Operator\\n(polls list length)\"]\n    keda_op -- \"length &gt; 0 \u2192 scale up\" --&gt; workers[\"Worker Pods\\n(0 \u2192 N replicas)\"]\n    keda_op -- \"length == 0 \u2192 scale to zero\" --&gt; zero[\"0 Pods\\n(cost savings)\"]\n    workers -- \"LPOP jobs\" --&gt; redis_list</code></pre>"},{"location":"30-Keda/#step-01-deploy-redis","title":"Step 01 - Deploy Redis","text":"<pre><code>kubectl apply -f manifests/04-redis-stack.yaml\n\n# Wait for Redis to be ready\nkubectl rollout status deployment/redis -n keda-demo\n</code></pre>"},{"location":"30-Keda/#step-02-deploy-a-worker-deployment-starts-at-0-replicas","title":"Step 02 - Deploy a Worker Deployment (starts at 0 replicas)","text":"<p>The worker deployment starts at 0 replicas - KEDA will scale it up when jobs arrive:</p> <pre><code># Part of manifests/04-redis-stack.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-worker\n  namespace: keda-demo\nspec:\n  replicas: 0           # Start at zero - KEDA controls this\n  selector:\n    matchLabels:\n      app: redis-worker\n  template:\n    metadata:\n      labels:\n        app: redis-worker\n    spec:\n      containers:\n        - name: worker\n          image: redis:7-alpine\n          # Simulates a worker: pops one job, sleeps 2s, repeat\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - |\n              while true; do\n                JOB=$(redis-cli -h redis LPOP jobs:queue)\n                if [ -n \"$JOB\" ]; then\n                  echo \"Processing: $JOB\"\n                  sleep 2\n                else\n                  sleep 1\n                fi\n              done\n</code></pre>"},{"location":"30-Keda/#step-03-apply-the-redis-scaledobject","title":"Step 03 - Apply the Redis ScaledObject","text":"<pre><code>kubectl apply -f manifests/05-scaled-object-redis.yaml\n</code></pre> <p>The ScaledObject:</p> <pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: redis-worker-scaler\n  namespace: keda-demo\nspec:\n  scaleTargetRef:\n    name: redis-worker\n  minReplicaCount: 0          # Scale to ZERO when queue is empty\n  maxReplicaCount: 20\n  cooldownPeriod: 30          # Wait 30s after last job before scaling to zero\n  pollingInterval: 5          # Check every 5 seconds\n  triggers:\n    - type: redis\n      metadata:\n        address: redis:6379   # Redis host:port (within cluster)\n        listName: jobs:queue  # The Redis list to monitor\n        listLength: \"5\"       # One replica per 5 items in the queue\n</code></pre>"},{"location":"30-Keda/#step-04-verify-scale-to-zero","title":"Step 04 - Verify scale-to-zero","text":"<pre><code># Should show 0 pods (no jobs in queue yet)\nkubectl get pods -n keda-demo -l app=redis-worker\nkubectl get scaledobject redis-worker-scaler -n keda-demo\n</code></pre>"},{"location":"30-Keda/#step-05-enqueue-jobs-and-watch-scale-up","title":"Step 05 - Enqueue jobs and watch scale-up","text":"<pre><code># Terminal 1: Watch pods\nkubectl get pods -n keda-demo -l app=redis-worker -w\n\n# Terminal 2: Push 50 jobs to the queue\nkubectl exec -it deployment/redis -n keda-demo -- \\\n    redis-cli RPUSH jobs:queue \\\n    job-1 job-2 job-3 job-4 job-5 \\\n    job-6 job-7 job-8 job-9 job-10 \\\n    job-11 job-12 job-13 job-14 job-15 \\\n    job-16 job-17 job-18 job-19 job-20 \\\n    job-21 job-22 job-23 job-24 job-25 \\\n    job-26 job-27 job-28 job-29 job-30 \\\n    job-31 job-32 job-33 job-34 job-35 \\\n    job-36 job-37 job-38 job-39 job-40 \\\n    job-41 job-42 job-43 job-44 job-45 \\\n    job-46 job-47 job-48 job-49 job-50\n\n# Check queue length\nkubectl exec -it deployment/redis -n keda-demo -- redis-cli LLEN jobs:queue\n</code></pre> <p>Observe the events: 1. KEDA detects 50 jobs in queue (50 / 5 = 10 replicas desired) 2. Pods scale up from 0 \u2192 10 3. Workers consume the jobs 4. Queue drains \u2192 worker pods scale back down to 0</p>"},{"location":"30-Keda/#part-06-triggerauthentication","title":"Part 06 - TriggerAuthentication","text":"<p>Many scalers require credentials to connect to external services (password, token, connection string). <code>TriggerAuthentication</code> prevents putting secrets directly in the <code>ScaledObject</code>.</p>"},{"location":"30-Keda/#creating-a-triggerauthentication","title":"Creating a TriggerAuthentication","text":"<pre><code>graph LR\n    secret[\"Kubernetes Secret\\n(redis-auth-secret)\"] --&gt; ta[\"TriggerAuthentication\\n(redis-auth)\"]\n    ta --&gt; so[\"ScaledObject\"]\n    so --&gt; keda_op[\"KEDA Operator\"]\n    keda_op -- \"Reads credentials\\nfrom Secret\" --&gt; redis_ext[\"Redis with\\npassword auth\"]</code></pre> <p>Step 01 - Create a Secret:</p> <pre><code>kubectl create secret generic redis-auth-secret \\\n    --namespace keda-demo \\\n    --from-literal=redis-password='super-secret-password'\n</code></pre> <p>Step 02 - Create the TriggerAuthentication (references the Secret):</p> <pre><code># manifests/06-trigger-auth.yaml\napiVersion: keda.sh/v1alpha1\nkind: TriggerAuthentication\nmetadata:\n  name: redis-auth\n  namespace: keda-demo\nspec:\n  secretTargetRef:\n    - parameter: password          # The scaler parameter this maps to\n      name: redis-auth-secret      # Kubernetes Secret name\n      key: redis-password          # Key within the Secret\n</code></pre> <p>Step 03 - Reference it in the ScaledObject:</p> <pre><code># manifests/07-scaled-object-redis-auth.yaml\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: redis-auth-worker-scaler\n  namespace: keda-demo\nspec:\n  scaleTargetRef:\n    name: redis-worker\n  minReplicaCount: 0\n  maxReplicaCount: 10\n  triggers:\n    - type: redis\n      authenticationRef:\n        name: redis-auth          # Reference to TriggerAuthentication\n      metadata:\n        address: redis:6379\n        listName: jobs:queue\n        listLength: \"5\"\n</code></pre> <p>Apply: <pre><code>kubectl apply -f manifests/06-trigger-auth.yaml\nkubectl apply -f manifests/07-scaled-object-redis-auth.yaml\n</code></pre></p>"},{"location":"30-Keda/#clustertriggerauthentication-cluster-wide","title":"ClusterTriggerAuthentication (Cluster-Wide)","text":"<p>For credentials used across multiple namespaces:</p> <pre><code>apiVersion: keda.sh/v1alpha1\nkind: ClusterTriggerAuthentication\nmetadata:\n  name: global-redis-auth        # No namespace needed\nspec:\n  secretTargetRef:\n    - parameter: password\n      name: redis-auth-secret    # Secret must exist in the KEDA namespace\n      key: redis-password\n</code></pre> <p>Reference with <code>kind</code>: <pre><code>authenticationRef:\n  name: global-redis-auth\n  kind: ClusterTriggerAuthentication\n</code></pre></p>"},{"location":"30-Keda/#part-07-prometheus-scaler","title":"Part 07 - Prometheus Scaler","text":"<p>The Prometheus scaler lets you scale based on any Prometheus metric - custom application metrics, business metrics, or infrastructure metrics.</p> <pre><code># manifests/08-prometheus-scaler.yaml\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: prometheus-scaler\n  namespace: keda-demo\nspec:\n  scaleTargetRef:\n    name: nginx-demo\n  minReplicaCount: 1\n  maxReplicaCount: 20\n  triggers:\n    - type: prometheus\n      metadata:\n        # Prometheus server URL (in-cluster)\n        serverAddress: http://prometheus-server.monitoring.svc:9090\n\n        # The PromQL query to evaluate\n        # This example scales on HTTP request rate\n        query: |\n          sum(rate(http_requests_total{namespace=\"keda-demo\"}[1m]))\n\n        # Scale threshold: add one replica per 100 req/sec\n        threshold: \"100\"\n\n        # Optional: activation threshold (below this = stay at minReplicaCount)\n        activationThreshold: \"10\"\n</code></pre> <p>PromQL Tips for KEDA</p> <ul> <li>The query must return a single scalar value</li> <li>Use <code>activationThreshold</code> to prevent scaling when traffic is very low</li> <li>KEDA uses the formula: <code>desiredReplicas = ceil(metricValue / threshold)</code></li> </ul>"},{"location":"30-Keda/#common-prometheus-scaling-patterns","title":"Common Prometheus Scaling Patterns","text":"<pre><code># Scale on response latency (p95 &gt; 500ms)\nquery: |\n  histogram_quantile(0.95,\n    sum(rate(http_request_duration_seconds_bucket{\n      namespace=\"keda-demo\"\n    }[2m])) by (le)\n  ) * 1000\n\n# Scale on queue depth from application metric\nquery: |\n  myapp_queue_depth{namespace=\"keda-demo\"}\n\n# Scale on active WebSocket connections\nquery: |\n  sum(websocket_active_connections{namespace=\"keda-demo\"})\n</code></pre>"},{"location":"30-Keda/#part-08-scaledjob-batch-processing","title":"Part 08 - ScaledJob (Batch Processing)","text":"<p><code>ScaledJob</code> is designed for batch workloads where each event should be processed by its own short-lived Kubernetes <code>Job</code> (not a long-running pod).</p> <p>Use cases: - Video/image transcoding (one Job per file) - Report generation (one Job per report request) - ML batch inference (one Job per data chunk)</p>"},{"location":"30-Keda/#scaledobject-vs-scaledjob","title":"ScaledObject vs ScaledJob","text":"Feature ScaledObject ScaledJob Target Deployment / StatefulSet Kubernetes Job Scaling model Adjust replica count Create new Jobs per event Idle behavior Scale to zero replicas No Jobs running Best for Long-running workers Short-lived batch tasks Parallelism All pods share the workload Each Job handles its own event"},{"location":"30-Keda/#scaledjob-example","title":"ScaledJob Example","text":"<pre><code># manifests/09-scaled-job.yaml\napiVersion: keda.sh/v1alpha1\nkind: ScaledJob\nmetadata:\n  name: redis-batch-job\n  namespace: keda-demo\nspec:\n  jobTargetRef:\n    # This Job template is instantiated for each batch of events\n    parallelism: 1\n    completions: 1\n    backoffLimit: 2\n    template:\n      spec:\n        restartPolicy: Never\n        containers:\n          - name: batch-processor\n            image: redis:7-alpine\n            command: [\"/bin/sh\", \"-c\"]\n            args:\n              - |\n                echo \"Batch job started\"\n                # Pop and process up to 5 items from the queue\n                for i in $(seq 1 5); do\n                  JOB=$(redis-cli -h redis LPOP batch:queue)\n                  if [ -n \"$JOB\" ]; then\n                    echo \"Processing batch item: $JOB\"\n                    sleep 1\n                  fi\n                done\n                echo \"Batch job done\"\n\n  # Scaling configuration\n  minReplicaCount: 0           # No Jobs when queue is empty\n  maxReplicaCount: 50          # At most 50 parallel Jobs\n  pollingInterval: 10\n  successfulJobsHistoryLimit: 5\n  failedJobsHistoryLimit: 5\n\n  # Scaling strategy\n  scalingStrategy:\n    strategy: \"default\"        # \"default\", \"custom\", or \"accurate\"\n    # customScalingQueueLengthDeduction: 0\n    # customScalingRunningJobPercentage: \"0.5\"\n\n  triggers:\n    - type: redis\n      metadata:\n        address: redis:6379\n        listName: batch:queue\n        listLength: \"5\"        # One Job per 5 items\n</code></pre>"},{"location":"30-Keda/#push-items-and-watch-jobs","title":"Push items and watch Jobs","text":"<pre><code># Push 25 items to the batch queue\nkubectl exec -it deployment/redis -n keda-demo -- \\\n    redis-cli RPUSH batch:queue \\\n    batch-1 batch-2 batch-3 batch-4 batch-5 \\\n    batch-6 batch-7 batch-8 batch-9 batch-10 \\\n    batch-11 batch-12 batch-13 batch-14 batch-15 \\\n    batch-16 batch-17 batch-18 batch-19 batch-20 \\\n    batch-21 batch-22 batch-23 batch-24 batch-25\n\n# Watch Jobs being created\nkubectl get jobs -n keda-demo -w\n\n# Watch the ScaledJob\nkubectl get scaledjob -n keda-demo\n</code></pre>"},{"location":"30-Keda/#part-09-scaling-behavior-tuning","title":"Part 09 - Scaling Behavior Tuning","text":"<p>KEDA inherits HPA\u2019s scaling behavior configuration, giving you fine-grained control over how fast pods scale up and down.</p> <pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: tuned-scaler\n  namespace: keda-demo\nspec:\n  scaleTargetRef:\n    name: nginx-demo\n  minReplicaCount: 1\n  maxReplicaCount: 20\n\n  # Advanced HPA scaling behavior tuning\n  advanced:\n    # How quickly to scale UP\n    horizontalPodAutoscalerConfig:\n      behavior:\n        scaleUp:\n          stabilizationWindowSeconds: 0      # React immediately to scale up\n          policies:\n            - type: Pods\n              value: 4                        # Add at most 4 pods per period\n              periodSeconds: 15\n            - type: Percent\n              value: 100                      # Or double the pod count\n              periodSeconds: 15\n          selectPolicy: Max                   # Use whichever adds more pods\n\n        scaleDown:\n          stabilizationWindowSeconds: 120    # Wait 2 minutes before scaling down\n          policies:\n            - type: Pods\n              value: 2                        # Remove at most 2 pods per period\n              periodSeconds: 60\n\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus-server.monitoring.svc:9090\n        query: sum(rate(http_requests_total{namespace=\"keda-demo\"}[1m]))\n        threshold: \"100\"\n</code></pre>"},{"location":"30-Keda/#common-tuning-patterns","title":"Common Tuning Patterns","text":"Pattern Config Use Case Aggressive scale-up, slow scale-down <code>scaleUp.stabilizationWindowSeconds: 0</code>, <code>scaleDown.stabilizationWindowSeconds: 300</code> Spiky traffic - respond fast, avoid flapping Gradual scale-up <code>scaleUp.policies: [{type: Pods, value: 2, periodSeconds: 60}]</code> Expensive pods, avoid overwhelming downstreams No downscale <code>scaleDown: {selectPolicy: Disabled}</code> Stateful workloads, long-lived connections Fast scale-down <code>scaleDown.stabilizationWindowSeconds: 0</code> Short-lived jobs, cost optimization"},{"location":"30-Keda/#part-10-kafka-scaler","title":"Part 10 - Kafka Scaler","text":"<p>The Kafka scaler scales Consumers based on consumer group lag - the number of messages in a topic that haven\u2019t been processed yet.</p> <pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: kafka-consumer-scaler\n  namespace: keda-demo\nspec:\n  scaleTargetRef:\n    name: kafka-consumer\n  minReplicaCount: 0\n  maxReplicaCount: 20\n  triggers:\n    - type: kafka\n      metadata:\n        bootstrapServers: kafka-broker:9092\n        consumerGroup: my-consumer-group     # The consumer group to monitor\n        topic: my-topic                       # The topic to watch\n        lagThreshold: \"10\"                    # One replica per 10 unprocessed messages\n        offsetResetPolicy: latest             # \"latest\" or \"earliest\"\n</code></pre>"},{"location":"30-Keda/#kafka-with-sasltls-authentication","title":"Kafka with SASL/TLS Authentication","text":"<pre><code>apiVersion: keda.sh/v1alpha1\nkind: TriggerAuthentication\nmetadata:\n  name: kafka-auth\n  namespace: keda-demo\nspec:\n  secretTargetRef:\n    - parameter: sasl          # \"plaintext\" | \"scram_sha256\" | \"scram_sha512\"\n      name: kafka-credentials\n      key: sasl-type\n    - parameter: username\n      name: kafka-credentials\n      key: username\n    - parameter: password\n      name: kafka-credentials\n      key: password\n    - parameter: tls\n      name: kafka-credentials\n      key: tls-enabled          # \"enable\" | \"disable\"\n</code></pre>"},{"location":"30-Keda/#part-11-http-scaler-keda-http-add-on","title":"Part 11 - HTTP Scaler (KEDA HTTP Add-on)","text":"<p>The HTTP scaler requires installing the separate KEDA HTTP Add-on. It intercepts HTTP traffic and scales the target service based on request rate (including scale to zero).</p>"},{"location":"30-Keda/#install-the-http-add-on","title":"Install the HTTP Add-on","text":"<pre><code>helm upgrade --install http-add-on kedacore/keda-add-ons-http \\\n    --namespace keda \\\n    --wait\n</code></pre>"},{"location":"30-Keda/#httpscaledobject","title":"HTTPScaledObject","text":"<pre><code>apiVersion: http.keda.sh/v1alpha1\nkind: HTTPScaledObject\nmetadata:\n  name: nginx-http-scaler\n  namespace: keda-demo\nspec:\n  hosts:\n    - nginx-demo.keda-demo.svc    # The Kubernetes service hostname\n  pathPrefixes:\n    - /                            # Scale on all paths (optional filter)\n  scaledownPeriod: 300             # Scale to zero after 5 minutes of no traffic\n  scaleTargetRef:\n    deployment: nginx-demo\n    service: nginx-demo\n    port: 80\n  replicas:\n    min: 0                         # Scale to zero when no HTTP traffic\n    max: 10\n  scalingMetric:\n    requestRate:\n      targetValue: 100             # One replica per 100 req/sec\n      granularity: 1s\n      window: 1m\n</code></pre>"},{"location":"30-Keda/#part-12-combining-multiple-triggers","title":"Part 12 - Combining Multiple Triggers","text":"<p>KEDA allows multiple triggers in a single <code>ScaledObject</code>. The scaling decision uses the maximum desired replica count across all active triggers.</p> <pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: multi-trigger-scaler\n  namespace: keda-demo\nspec:\n  scaleTargetRef:\n    name: nginx-demo\n  minReplicaCount: 1\n  maxReplicaCount: 20\n  triggers:\n    # Trigger 1: CPU-based (baseline HPA-like behavior)\n    - type: cpu\n      metadata:\n        type: Utilization\n        value: \"60\"\n\n    # Trigger 2: Scale during business hours\n    - type: cron\n      metadata:\n        timezone: \"UTC\"\n        start: \"0 8 * * 1-5\"\n        end: \"0 18 * * 1-5\"\n        desiredReplicas: \"5\"\n\n    # Trigger 3: Queue depth\n    - type: redis\n      metadata:\n        address: redis:6379\n        listName: jobs:queue\n        listLength: \"10\"\n</code></pre> <p>Multiple Trigger Evaluation</p> <p>KEDA evaluates ALL triggers simultaneously and scales to whichever trigger demands the most replicas. If CPU wants 3, cron wants 5, and Redis wants 8 - KEDA scales to 8.</p>"},{"location":"30-Keda/#part-13-keda-with-argocd-gitops","title":"Part 13 - KEDA with ArgoCD (GitOps)","text":"<p>Managing <code>ScaledObject</code> and <code>TriggerAuthentication</code> through ArgoCD brings the GitOps benefits of version control, drift detection, and automatic reconciliation to your autoscaling configs.</p>"},{"location":"30-Keda/#argocd-application-for-keda-resources","title":"ArgoCD Application for KEDA Resources","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: keda-autoscaling\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/nirgeier/KubernetesLabs.git\n    targetRevision: HEAD\n    path: Labs/34-Keda/manifests    # All ScaledObject manifests\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: keda-demo\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true               # ArgoCD reverts manual ScaledObject changes\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <p>When you push a new or updated <code>ScaledObject</code> manifest to Git, ArgoCD automatically applies it and KEDA starts the new scaling behavior - no manual kubectl needed.</p>"},{"location":"30-Keda/#workflow","title":"Workflow","text":"<pre><code>sequenceDiagram\n    participant Dev as Developer\n    participant Git as Git Repository\n    participant ArgoCD as ArgoCD\n    participant K8s as Kubernetes\n    participant KEDA as KEDA Operator\n\n    Dev-&gt;&gt;Git: Push updated ScaledObject YAML\n    ArgoCD-&gt;&gt;Git: Polls for changes (or webhook)\n    Git--&gt;&gt;ArgoCD: New ScaledObject detected\n    ArgoCD-&gt;&gt;K8s: Apply ScaledObject\n    K8s-&gt;&gt;KEDA: KEDA Operator notified\n    KEDA-&gt;&gt;K8s: Creates/updates managed HPA\n    Note over KEDA,K8s: KEDA now scales based on new trigger config</code></pre>"},{"location":"30-Keda/#part-14-monitoring-keda","title":"Part 14 - Monitoring KEDA","text":""},{"location":"30-Keda/#keda-metrics-prometheus","title":"KEDA Metrics (Prometheus)","text":"<p>KEDA exposes its own metrics that you can scrape with Prometheus:</p> <pre><code># Check KEDA metrics endpoint\nkubectl port-forward svc/keda-operator-metrics-apiserver -n keda 8080:8080 &amp;\ncurl http://localhost:8080/metrics\n</code></pre> <p>Key metrics:</p> Metric Description <code>keda_scaler_active</code> Whether a scaler is currently active (1=active, 0=inactive) <code>keda_scaler_metrics_value</code> The current metric value from a scaler <code>keda_scaler_errors_total</code> Number of errors encountered by a scaler <code>keda_scaled_object_paused</code> Whether a ScaledObject is paused <code>keda_resource_totals</code> Number of KEDA CRD resources"},{"location":"30-Keda/#servicemonitor-for-prometheus-operator","title":"ServiceMonitor for Prometheus Operator","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: keda-metrics\n  namespace: keda\n  labels:\n    release: prometheus     # Must match your Prometheus release label\nspec:\n  selector:\n    matchLabels:\n      app: keda-operator-metrics-apiserver\n  endpoints:\n    - port: metrics\n      interval: 30s\n      path: /metrics\n</code></pre>"},{"location":"30-Keda/#useful-grafana-dashboard","title":"Useful Grafana Dashboard","text":"<p>Import KEDA community dashboard from Grafana.com: Dashboard ID <code>16543</code></p> <pre><code># Quick verification with kubectl\nkubectl get scaledobjects -A\nkubectl get scaledjobs -A\n\n# Describe for events and conditions\nkubectl describe scaledobject &lt;name&gt; -n &lt;namespace&gt;\n\n# Check KEDA operator logs\nkubectl logs -n keda -l app=keda-operator --tail=50\n</code></pre>"},{"location":"30-Keda/#part-15-pause-and-resume-scaling","title":"Part 15 - Pause and Resume Scaling","text":"<p>Sometimes you need to temporarily stop KEDA from scaling (e.g., during maintenance):</p> <pre><code># Pause a ScaledObject (KEDA stops reconciling, current replicas stay)\nkubectl annotate scaledobject nginx-cron-scaler \\\n    -n keda-demo \\\n    autoscaling.keda.sh/paused-replicas=\"2\"\n\n# Resume (delete the annotation)\nkubectl annotate scaledobject nginx-cron-scaler \\\n    -n keda-demo \\\n    autoscaling.keda.sh/paused-replicas-\n\n# Pause all scaling on a ScaledObject\nkubectl annotate scaledobject nginx-cron-scaler \\\n    -n keda-demo \\\n    autoscaling.keda.sh/paused=true\n\n# Resume\nkubectl annotate scaledobject nginx-cron-scaler \\\n    -n keda-demo \\\n    autoscaling.keda.sh/paused-\n</code></pre>"},{"location":"30-Keda/#part-16-fallback-configuration","title":"Part 16 - Fallback Configuration","text":"<p>Fallback allows KEDA to use a safe replica value if the scaler fails to query the metric source:</p> <pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: resilient-scaler\n  namespace: keda-demo\nspec:\n  scaleTargetRef:\n    name: nginx-demo\n  minReplicaCount: 1\n  maxReplicaCount: 10\n\n  # Fallback: if the metric source is unavailable, use these settings\n  fallback:\n    failureThreshold: 3          # Fail 3 times before using fallback replicas\n    replicas: 3                  # Fallback to 3 replicas when metric unavailable\n\n  triggers:\n    - type: redis\n      metadata:\n        address: redis:6379\n        listName: jobs:queue\n        listLength: \"5\"\n</code></pre>"},{"location":"30-Keda/#part-17-keda-cli-kubectl-keda-plugin","title":"Part 17 - KEDA CLI (kubectl-keda Plugin)","text":"<p>The <code>keda</code> plugin for <code>kubectl</code> simplifies common KEDA operations:</p>"},{"location":"30-Keda/#install","title":"Install","text":"<pre><code># macOS via Homebrew\nbrew tap kedacore/keda\nbrew install keda\n\n# Or via Krew (kubectl plugin manager)\nkubectl krew install keda\n</code></pre>"},{"location":"30-Keda/#common-commands","title":"Common Commands","text":"<pre><code># List all ScaledObjects across all namespaces\nkubectl keda list scaledobjects -A\n\n# List ScaledJobs\nkubectl keda list scaledjobs -A\n\n# Check metric values for a ScaledObject\nkubectl keda get scaledobject nginx-cron-scaler -n keda-demo\n\n# Show events for a ScaledObject\nkubectl keda describe scaledobject nginx-cron-scaler -n keda-demo\n\n# Pause / resume a ScaledObject\nkubectl keda pause scaledobject nginx-cron-scaler -n keda-demo\nkubectl keda resume scaledobject nginx-cron-scaler -n keda-demo\n</code></pre>"},{"location":"30-Keda/#part-18-troubleshooting","title":"Part 18 - Troubleshooting","text":""},{"location":"30-Keda/#scaledobject-not-scaling","title":"ScaledObject Not Scaling","text":"<pre><code># 1. Check ScaledObject status and conditions\nkubectl describe scaledobject &lt;name&gt; -n &lt;namespace&gt;\n\n# Look for conditions like:\n# Ready: True/False\n# Active: True/False\n\n# 2. Check KEDA operator logs\nkubectl logs -n keda -l app=keda-operator --tail=100\n\n# 3. Check the HPA managed by KEDA\nkubectl get hpa -n &lt;namespace&gt;\nkubectl describe hpa keda-hpa-&lt;name&gt; -n &lt;namespace&gt;\n\n# 4. Check if metric is being received\nkubectl get --raw \"/apis/external.metrics.k8s.io/v1beta1\" | jq .\n</code></pre>"},{"location":"30-Keda/#scale-to-zero-not-working","title":"Scale to Zero Not Working","text":"<pre><code># Verify minReplicaCount is 0 in ScaledObject\nkubectl get scaledobject &lt;name&gt; -n &lt;namespace&gt; -o yaml | grep minReplicaCount\n\n# Check the cooldownPeriod hasn't passed yet\nkubectl describe scaledobject &lt;name&gt; -n &lt;namespace&gt; | grep -A5 \"Conditions\"\n\n# Verify the metric/queue is truly empty\nkubectl exec -it deployment/redis -n keda-demo -- redis-cli LLEN jobs:queue\n</code></pre>"},{"location":"30-Keda/#metric-source-connectivity-issues","title":"Metric Source Connectivity Issues","text":"<pre><code># Check KEDA can reach the metric source\nkubectl run debug-pod --image=busybox -n keda -it --rm --restart=Never \\\n    -- sh -c \"nc -zv redis.keda-demo.svc.cluster.local 6379\"\n\n# Verify TriggerAuthentication is correct\nkubectl describe triggerauthentication &lt;name&gt; -n &lt;namespace&gt;\n\n# Check if secrets referenced in TriggerAuthentication exist\nkubectl get secret &lt;secret-name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"30-Keda/#keda-webhook-errors","title":"KEDA Webhook Errors","text":"<pre><code># Check admission webhook\nkubectl get validatingwebhookconfigurations | grep keda\nkubectl describe validatingwebhookconfiguration keda-admission\n\n# Restart KEDA webhooks\nkubectl rollout restart deployment/keda-admission-webhooks -n keda\n</code></pre>"},{"location":"30-Keda/#common-keda-cheatsheet","title":"Common KEDA Cheatsheet","text":"<pre><code># --- Installation ---\nhelm repo add kedacore https://kedacore.github.io/charts\nhelm upgrade --install keda kedacore/keda --namespace keda --create-namespace --wait\n\n# --- Inspect ---\nkubectl get scaledobjects -A                               # List all ScaledObjects\nkubectl get scaledjobs -A                                  # List all ScaledJobs\nkubectl get triggerauthentications -A                      # List TriggerAuths\nkubectl describe scaledobject &lt;name&gt; -n &lt;ns&gt;               # Full details + events\nkubectl get hpa -n &lt;ns&gt;                                    # KEDA-managed HPAs\n\n# --- Troubleshoot ---\nkubectl logs -n keda -l app=keda-operator --tail=100       # Operator logs\nkubectl logs -n keda -l app=keda-operator-metrics-apiserver --tail=50\n\n# --- Pause / Resume ---\nkubectl annotate scaledobject &lt;name&gt; -n &lt;ns&gt; autoscaling.keda.sh/paused=true\nkubectl annotate scaledobject &lt;name&gt; -n &lt;ns&gt; autoscaling.keda.sh/paused-\n\n# --- ScaledObject quick template ---\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: my-scaler\n  namespace: default\nspec:\n  scaleTargetRef:\n    name: my-deployment\n  minReplicaCount: 0\n  maxReplicaCount: 10\n  triggers:\n    - type: redis\n      metadata:\n        address: redis:6379\n        listName: my:queue\n        listLength: \"5\"\nEOF\n</code></pre>"},{"location":"30-Keda/#exercises","title":"Exercises","text":"<p>The following exercises will test your understanding of KEDA concepts. Try to solve each exercise on your own before revealing the solution.</p>"},{"location":"30-Keda/#01-scale-a-deployment-based-on-a-custom-redis-key","title":"01. Scale a Deployment Based on a Custom Redis Key","text":"<p>Create a ScaledObject that monitors a Redis Sorted Set score instead of a list length. Use the <code>redis</code> scaler with <code>listName</code> pointing to a different key, and scale from 0 to 5 replicas.</p>"},{"location":"30-Keda/#scenario","title":"Scenario:","text":"<p>\u25e6 Your application uses a Redis sorted set for priority-based job queues. \u25e6 You want workers to scale up when high-priority jobs are enqueued.</p> <p>Hint: Create a new Redis list key and a corresponding ScaledObject with <code>minReplicaCount: 0</code>.</p> Solution <pre><code># 1. Create a ScaledObject for a different queue\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: priority-worker-scaler\n  namespace: keda-demo\nspec:\n  scaleTargetRef:\n    name: redis-worker\n  minReplicaCount: 0\n  maxReplicaCount: 5\n  cooldownPeriod: 30\n  pollingInterval: 5\n  triggers:\n    - type: redis\n      metadata:\n        address: redis:6379\n        listName: priority:queue\n        listLength: \"3\"\nEOF\n\n# 2. Push items to the priority queue\nkubectl exec -it deployment/redis -n keda-demo -- \\\n    redis-cli RPUSH priority:queue job-a job-b job-c job-d job-e job-f\n\n# 3. Watch pods scale up\nkubectl get pods -n keda-demo -l app=redis-worker -w\n\n# 4. Verify ScaledObject\nkubectl get scaledobject priority-worker-scaler -n keda-demo\n</code></pre>"},{"location":"30-Keda/#02-combine-cron-and-cpu-triggers","title":"02. Combine Cron and CPU Triggers","text":"<p>Create a ScaledObject that uses both a Cron trigger (scale to 3 during business hours) and a CPU trigger (scale beyond 3 when CPU exceeds 70%).</p>"},{"location":"30-Keda/#scenario_1","title":"Scenario:","text":"<p>\u25e6 Your API needs a baseline of 3 pods during work hours but should burst higher under load. \u25e6 Outside business hours, the minimum can drop to 1.</p> <p>Hint: Use multiple triggers in a single ScaledObject. KEDA uses the maximum across active triggers.</p> Solution <pre><code># Apply this ScaledObject\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: hybrid-scaler\n  namespace: keda-demo\nspec:\n  scaleTargetRef:\n    name: nginx-demo\n  minReplicaCount: 1\n  maxReplicaCount: 10\n  triggers:\n    - type: cron\n      metadata:\n        timezone: \"UTC\"\n        start: \"0 8 * * 1-5\"\n        end: \"0 18 * * 1-5\"\n        desiredReplicas: \"3\"\n    - type: cpu\n      metadata:\n        type: Utilization\n        value: \"70\"\n</code></pre> <pre><code>kubectl apply -f - &lt;&lt;EOF\n# (paste the YAML above)\nEOF\n\n# Verify KEDA created an HPA with both triggers\nkubectl get hpa -n keda-demo\nkubectl describe scaledobject hybrid-scaler -n keda-demo\n</code></pre>"},{"location":"30-Keda/#03-use-triggerauthentication-with-a-kubernetes-secret","title":"03. Use TriggerAuthentication with a Kubernetes Secret","text":"<p>Create a TriggerAuthentication that references a Kubernetes Secret containing a Redis password, then create a ScaledObject that uses it.</p>"},{"location":"30-Keda/#scenario_2","title":"Scenario:","text":"<p>\u25e6 Your production Redis requires authentication. \u25e6 You need to keep credentials out of the ScaledObject manifest.</p> <p>Hint: Create a Secret, then a TriggerAuthentication referencing it, then a ScaledObject with <code>authenticationRef</code>.</p> Solution <pre><code># 1. Create the Secret\nkubectl create secret generic my-redis-secret \\\n    --namespace keda-demo \\\n    --from-literal=redis-password='my-secure-password'\n\n# 2. Create the TriggerAuthentication\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: keda.sh/v1alpha1\nkind: TriggerAuthentication\nmetadata:\n  name: my-redis-auth\n  namespace: keda-demo\nspec:\n  secretTargetRef:\n    - parameter: password\n      name: my-redis-secret\n      key: redis-password\nEOF\n\n# 3. Create the ScaledObject with authenticationRef\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: auth-redis-scaler\n  namespace: keda-demo\nspec:\n  scaleTargetRef:\n    name: redis-worker\n  minReplicaCount: 0\n  maxReplicaCount: 10\n  triggers:\n    - type: redis\n      authenticationRef:\n        name: my-redis-auth\n      metadata:\n        address: redis:6379\n        listName: secure:queue\n        listLength: \"5\"\nEOF\n\n# 4. Verify\nkubectl get triggerauthentication -n keda-demo\nkubectl describe scaledobject auth-redis-scaler -n keda-demo\n</code></pre>"},{"location":"30-Keda/#04-create-a-scaledjob-for-batch-processing","title":"04. Create a ScaledJob for Batch Processing","text":"<p>Create a ScaledJob that spawns one Kubernetes Job for every 3 items in a Redis list, with a maximum of 10 concurrent Jobs.</p>"},{"location":"30-Keda/#scenario_3","title":"Scenario:","text":"<p>\u25e6 Your data pipeline receives files for processing. Each Job should handle a small batch. \u25e6 When the queue is empty, no Jobs should be running.</p> <p>Hint: Use <code>kind: ScaledJob</code> with <code>jobTargetRef</code> and the <code>redis</code> trigger.</p> Solution <pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledJob\nmetadata:\n  name: batch-processor\n  namespace: keda-demo\nspec:\n  jobTargetRef:\n    parallelism: 1\n    completions: 1\n    backoffLimit: 2\n    template:\n      spec:\n        restartPolicy: Never\n        containers:\n          - name: processor\n            image: redis:7-alpine\n            command: [\"/bin/sh\", \"-c\"]\n            args:\n              - |\n                for i in $(seq 1 3); do\n                  JOB=$(redis-cli -h redis LPOP processing:queue)\n                  if [ -n \"$JOB\" ]; then\n                    echo \"Processing: $JOB\"\n                    sleep 2\n                  fi\n                done\n                echo \"Batch complete\"\n  minReplicaCount: 0\n  maxReplicaCount: 10\n  pollingInterval: 10\n  successfulJobsHistoryLimit: 5\n  failedJobsHistoryLimit: 3\n  triggers:\n    - type: redis\n      metadata:\n        address: redis:6379\n        listName: processing:queue\n        listLength: \"3\"\n</code></pre> <pre><code># Apply the ScaledJob\nkubectl apply -f - &lt;&lt;EOF\n# (paste the YAML above)\nEOF\n\n# Push items to trigger Job creation\nkubectl exec -it deployment/redis -n keda-demo -- \\\n    redis-cli RPUSH processing:queue file-1 file-2 file-3 file-4 file-5 file-6 file-7 file-8 file-9\n\n# Watch Jobs being created\nkubectl get jobs -n keda-demo -w\n\n# Check the ScaledJob\nkubectl get scaledjob -n keda-demo\n</code></pre>"},{"location":"30-Keda/#05-pause-and-resume-a-scaledobject","title":"05. Pause and Resume a ScaledObject","text":"<p>Pause an active ScaledObject at a fixed replica count, then resume normal scaling.</p>"},{"location":"30-Keda/#scenario_4","title":"Scenario:","text":"<p>\u25e6 You need to perform maintenance on the metric source and want to keep a stable replica count. \u25e6 After maintenance, resume event-driven scaling.</p> <p>Hint: Use the <code>autoscaling.keda.sh/paused-replicas</code> annotation.</p> Solution <pre><code># 1. Pause the ScaledObject at 3 replicas\nkubectl annotate scaledobject redis-worker-scaler \\\n    -n keda-demo \\\n    autoscaling.keda.sh/paused-replicas=\"3\"\n\n# 2. Verify it's paused\nkubectl get scaledobject redis-worker-scaler -n keda-demo -o yaml | grep -A2 annotations\n\n# 3. Check that replicas stay at 3 regardless of queue depth\nkubectl get deployment redis-worker -n keda-demo\n\n# 4. Resume normal scaling\nkubectl annotate scaledobject redis-worker-scaler \\\n    -n keda-demo \\\n    autoscaling.keda.sh/paused-replicas-\n\n# 5. Verify scaling resumes\nkubectl describe scaledobject redis-worker-scaler -n keda-demo\n</code></pre>"},{"location":"30-Keda/#cleanup","title":"Cleanup","text":"<pre><code># Remove all KEDA demo resources\nkubectl delete namespace keda-demo\n\n# Uninstall KEDA\nhelm uninstall keda --namespace keda\nkubectl delete namespace keda\n\n# Remove KEDA CRDs\nkubectl delete crd \\\n    scaledobjects.keda.sh \\\n    scaledjobs.keda.sh \\\n    triggerauthentications.keda.sh \\\n    clustertriggerauthentications.keda.sh\n</code></pre>"},{"location":"30-Keda/#summary","title":"Summary","text":"Concept Key Takeaway ScaledObject Links Deployments to any event source; KEDA manages an HPA for you ScaledJob Creates a new Job per event batch; ideal for batch processing TriggerAuthentication Externalizes credentials from ScaledObject specs Scale to Zero Set <code>minReplicaCount: 0</code> for complete cost savings when idle Multiple Triggers Combine triggers; KEDA uses whichever demands the most replicas Fallback Define safe replica counts when the metric source is unreachable Pause Temporarily halt KEDA scaling without removing resources GitOps Manage ScaledObjects via ArgoCD for full GitOps autoscaling"},{"location":"30-Keda/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the full KEDA Scalers catalog - 60+ event sources including AWS SQS, GCP Pub/Sub, and Azure Service Bus.</li> <li>Try the KEDA HTTP Add-on for HTTP-based scale-to-zero.</li> <li>Combine KEDA with ArgoCD (Lab 18) for GitOps-managed autoscaling configurations.</li> <li>Learn about Prometheus &amp; Grafana (Lab 15) for monitoring KEDA metrics.</li> <li>Explore the KEDA community Grafana dashboard for visualizing scaling behavior.</li> <li>Practice KEDA tasks in the Kubernetes KEDA Tasks section.</li> </ul>"},{"location":"31-RBAC/","title":"RBAC - Role-Based Access Control","text":"<ul> <li>In this lab we will learn how Kubernetes Role-Based Access Control (RBAC) works and how to use Roles, ClusterRoles, RoleBindings, ClusterRoleBindings, and ServiceAccounts to control who can do what inside a cluster.</li> </ul>"},{"location":"31-RBAC/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What RBAC is and why it is essential for Kubernetes security</li> <li>The four RBAC API objects: <code>Role</code>, <code>ClusterRole</code>, <code>RoleBinding</code>, <code>ClusterRoleBinding</code></li> <li>How to create and assign fine-grained permissions to users and ServiceAccounts</li> <li>How to test permissions with <code>kubectl auth can-i</code></li> <li>How to grant a pod access to the Kubernetes API using a ServiceAccount</li> <li>Difference between namespace-scoped and cluster-scoped permissions</li> <li>Best practices: principle of least privilege</li> </ul>"},{"location":"31-RBAC/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link RBAC Authorization kubernetes.io/docs Using RBAC Authorization kubernetes.io/docs ServiceAccounts kubernetes.io/docs kubectl auth can-i kubernetes.io/docs"},{"location":"31-RBAC/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> </ul>"},{"location":"31-RBAC/#rbac-overview","title":"RBAC Overview","text":"<pre><code>graph LR\n    subgraph identity[\"Who?\"]\n        user[\"User / Group\"]\n        sa[\"ServiceAccount\"]\n    end\n\n    subgraph binding[\"Binding\"]\n        rb[\"RoleBinding\\n(namespace-scoped)\"]\n        crb[\"ClusterRoleBinding\\n(cluster-scoped)\"]\n    end\n\n    subgraph permission[\"What can they do?\"]\n        role[\"Role\\n(namespace-scoped)\"]\n        cr[\"ClusterRole\\n(cluster-scoped)\"]\n    end\n\n    user --&gt; rb\n    sa --&gt; rb\n    user --&gt; crb\n    sa --&gt; crb\n    rb --&gt; role\n    rb --&gt; cr\n    crb --&gt; cr</code></pre> Object Scope Purpose <code>Role</code> Namespace Defines a set of permissions within a namespace <code>ClusterRole</code> Cluster Defines a set of permissions cluster-wide <code>RoleBinding</code> Namespace Grants a Role/ClusterRole to a subject within a namespace <code>ClusterRoleBinding</code> Cluster Grants a ClusterRole to a subject cluster-wide <code>ServiceAccount</code> Namespace Identity for processes running in pods"},{"location":"31-RBAC/#01-create-namespace","title":"01. Create namespace","text":"<pre><code># Clean up if it already exists\nkubectl delete namespace rbac-lab --ignore-not-found\n\n# Create the lab namespace\nkubectl create namespace rbac-lab\n</code></pre>"},{"location":"31-RBAC/#02-create-a-role-namespace-scoped","title":"02. Create a Role (namespace-scoped)","text":"<p>A <code>Role</code> grants permissions within a specific namespace. Create a Role that allows read-only access to pods:</p> <pre><code># manifests/role-pod-reader.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: rbac-lab\n  name: pod-reader\nrules:\n  - apiGroups: [\"\"]           # \"\" = core API group\n    resources: [\"pods\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n</code></pre> <pre><code>kubectl apply -f manifests/role-pod-reader.yaml\n</code></pre> <p>Understanding Rules</p> <ul> <li>apiGroups: <code>\"\"</code> is the core API group (pods, services, configmaps). Use <code>\"apps\"</code> for deployments, <code>\"batch\"</code> for jobs, etc.</li> <li>resources: Kubernetes resource types (pods, services, deployments, secrets, etc.)</li> <li>verbs: Actions - <code>get</code>, <code>list</code>, <code>watch</code>, <code>create</code>, <code>update</code>, <code>patch</code>, <code>delete</code></li> </ul>"},{"location":"31-RBAC/#03-create-a-serviceaccount","title":"03. Create a ServiceAccount","text":"<pre><code># Create a ServiceAccount to bind our role to\nkubectl create serviceaccount app-reader -n rbac-lab\n</code></pre>"},{"location":"31-RBAC/#04-create-a-rolebinding","title":"04. Create a RoleBinding","text":"<p>Bind the <code>pod-reader</code> Role to our <code>app-reader</code> ServiceAccount:</p> <pre><code># manifests/rolebinding-pod-reader.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: rbac-lab\nsubjects:\n  - kind: ServiceAccount\n    name: app-reader\n    namespace: rbac-lab\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <pre><code>kubectl apply -f manifests/rolebinding-pod-reader.yaml\n</code></pre>"},{"location":"31-RBAC/#05-test-permissions-with-kubectl-auth-can-i","title":"05. Test permissions with <code>kubectl auth can-i</code>","text":"<pre><code># Check: can the ServiceAccount list pods in rbac-lab?\nkubectl auth can-i list pods \\\n  --namespace rbac-lab \\\n  --as system:serviceaccount:rbac-lab:app-reader\n# Expected output: yes\n\n# Check: can it delete pods? (should be denied)\nkubectl auth can-i delete pods \\\n  --namespace rbac-lab \\\n  --as system:serviceaccount:rbac-lab:app-reader\n# Expected output: no\n\n# Check: can it list pods in the default namespace? (should be denied)\nkubectl auth can-i list pods \\\n  --namespace default \\\n  --as system:serviceaccount:rbac-lab:app-reader\n# Expected output: no\n</code></pre>"},{"location":"31-RBAC/#06-use-a-serviceaccount-in-a-pod","title":"06. Use a ServiceAccount in a Pod","text":"<p>Deploy a pod that uses the <code>app-reader</code> ServiceAccount to query the Kubernetes API from within the pod:</p> <pre><code># manifests/pod-with-sa.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: api-explorer\n  namespace: rbac-lab\nspec:\n  serviceAccountName: app-reader\n  containers:\n    - name: kubectl\n      image: bitnami/kubectl:latest\n      command: [\"sleep\", \"3600\"]\n</code></pre> <pre><code>kubectl apply -f manifests/pod-with-sa.yaml\n\n# Wait for the pod to be running\nkubectl wait --for=condition=Ready pod/api-explorer -n rbac-lab --timeout=60s\n</code></pre> <p>Now exec into the pod and test the API access:</p> <pre><code># Exec into the pod\nkubectl exec -it api-explorer -n rbac-lab -- bash\n\n# Inside the pod - list pods (should work)\nkubectl get pods -n rbac-lab\n\n# Inside the pod - try to delete a pod (should fail with Forbidden)\nkubectl delete pod api-explorer -n rbac-lab\n\n# Inside the pod - try to list services (should fail - not in our Role)\nkubectl get services -n rbac-lab\n\n# Exit the pod\nexit\n</code></pre>"},{"location":"31-RBAC/#07-create-a-clusterrole-and-clusterrolebinding","title":"07. Create a ClusterRole and ClusterRoleBinding","text":"<p>A <code>ClusterRole</code> + <code>ClusterRoleBinding</code> grants permissions across all namespaces:</p> <pre><code># manifests/clusterrole-namespace-viewer.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: namespace-viewer\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"namespaces\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <pre><code># manifests/clusterrolebinding-namespace-viewer.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: view-namespaces\nsubjects:\n  - kind: ServiceAccount\n    name: app-reader\n    namespace: rbac-lab\nroleRef:\n  kind: ClusterRole\n  name: namespace-viewer\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <pre><code>kubectl apply -f manifests/clusterrole-namespace-viewer.yaml\nkubectl apply -f manifests/clusterrolebinding-namespace-viewer.yaml\n</code></pre> <p>Test it:</p> <pre><code># Can now list namespaces cluster-wide\nkubectl auth can-i list namespaces \\\n  --as system:serviceaccount:rbac-lab:app-reader\n# Expected output: yes\n</code></pre>"},{"location":"31-RBAC/#08-aggregate-clusterroles","title":"08. Aggregate ClusterRoles","text":"<p>Kubernetes supports aggregation - automatically combining ClusterRoles via labels. The built-in <code>view</code>, <code>edit</code>, and <code>admin</code> ClusterRoles use this pattern:</p> <pre><code># manifests/clusterrole-custom-view.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: custom-metrics-viewer\n  labels:\n    # This label makes it auto-aggregate into the built-in \"view\" ClusterRole\n    rbac.authorization.k8s.io/aggregate-to-view: \"true\"\nrules:\n  - apiGroups: [\"metrics.k8s.io\"]\n    resources: [\"pods\", \"nodes\"]\n    verbs: [\"get\", \"list\"]\n</code></pre> <pre><code>kubectl apply -f manifests/clusterrole-custom-view.yaml\n\n# Verify it was aggregated into the \"view\" ClusterRole\nkubectl get clusterrole view -o yaml | grep -A5 \"aggregationRule\"\n</code></pre>"},{"location":"31-RBAC/#09-explore-default-clusterroles","title":"09. Explore default ClusterRoles","text":"<p>Kubernetes ships with several built-in ClusterRoles:</p> <pre><code># List all ClusterRoles\nkubectl get clusterroles\n\n# Inspect the built-in \"view\" role (read-only across most resources)\nkubectl describe clusterrole view\n\n# Inspect the built-in \"edit\" role (read-write but no RBAC changes)\nkubectl describe clusterrole edit\n\n# Inspect the built-in \"admin\" role (full access within a namespace)\nkubectl describe clusterrole admin\n\n# Inspect \"cluster-admin\" (full access to everything)\nkubectl describe clusterrole cluster-admin\n</code></pre> Built-in ClusterRole Permissions <code>view</code> Read-only access to most resources (no secrets) <code>edit</code> Read-write access to most resources (no RBAC or namespace) <code>admin</code> Full control within a namespace (including RBAC) <code>cluster-admin</code> Unrestricted access to everything (use with extreme caution!) <p>Security Best Practice</p> <p>Never bind <code>cluster-admin</code> to application ServiceAccounts. Follow the principle of least privilege - grant only the minimum permissions required.</p>"},{"location":"31-RBAC/#10-cleanup","title":"10. Cleanup","text":"<pre><code>kubectl delete namespace rbac-lab\nkubectl delete clusterrole namespace-viewer custom-metrics-viewer --ignore-not-found\nkubectl delete clusterrolebinding view-namespaces --ignore-not-found\n</code></pre>"},{"location":"31-RBAC/#summary","title":"Summary","text":"Concept Key Takeaway Role Namespace-scoped permissions ClusterRole Cluster-scoped permissions (or reusable across NS) RoleBinding Assigns Role/ClusterRole within a namespace ClusterRoleBinding Assigns ClusterRole cluster-wide ServiceAccount Pod identity - attach Roles to pods via ServiceAccounts <code>kubectl auth can-i</code> Test permissions without trial-and-error Principle of Least Privilege Always grant the minimum permissions required"},{"location":"31-RBAC/#exercises","title":"Exercises","text":"<p>The following exercises will test your understanding of Kubernetes RBAC. Try to solve each exercise on your own before revealing the solution.</p>"},{"location":"31-RBAC/#01-create-a-role-that-grants-full-access-to-configmaps","title":"01. Create a Role That Grants Full Access to ConfigMaps","text":"<p>Create a Role named <code>configmap-admin</code> in the <code>rbac-lab</code> namespace that allows all operations (<code>get</code>, <code>list</code>, <code>watch</code>, <code>create</code>, <code>update</code>, <code>patch</code>, <code>delete</code>) on ConfigMaps. Bind it to a new ServiceAccount named <code>config-manager</code>.</p>"},{"location":"31-RBAC/#scenario","title":"Scenario:","text":"<p>\u25e6 Your application needs to dynamically create and update ConfigMaps for feature flags. \u25e6 The ServiceAccount must have full CRUD access to ConfigMaps but nothing else.</p> <p>Hint: Use <code>kubectl create role</code> with <code>--verb='*'</code> or list all verbs, and <code>kubectl create rolebinding</code> to bind it.</p> Solution <pre><code>## Create the namespace (if not already created)\nkubectl create namespace rbac-lab --dry-run=client -o yaml | kubectl apply -f -\n\n## Create the ServiceAccount\nkubectl create serviceaccount config-manager -n rbac-lab\n\n## Create the Role with full ConfigMap access\nkubectl create role configmap-admin \\\n  --namespace rbac-lab \\\n  --verb=get,list,watch,create,update,patch,delete \\\n  --resource=configmaps\n\n## Bind the Role to the ServiceAccount\nkubectl create rolebinding configmap-admin-binding \\\n  --namespace rbac-lab \\\n  --role=configmap-admin \\\n  --serviceaccount=rbac-lab:config-manager\n\n## Test: can the ServiceAccount create configmaps?\nkubectl auth can-i create configmaps \\\n  --namespace rbac-lab \\\n  --as system:serviceaccount:rbac-lab:config-manager\n## Expected: yes\n\n## Test: can it delete configmaps?\nkubectl auth can-i delete configmaps \\\n  --namespace rbac-lab \\\n  --as system:serviceaccount:rbac-lab:config-manager\n## Expected: yes\n\n## Test: can it access secrets? (should be denied)\nkubectl auth can-i get secrets \\\n  --namespace rbac-lab \\\n  --as system:serviceaccount:rbac-lab:config-manager\n## Expected: no\n\n## Clean up\nkubectl delete rolebinding configmap-admin-binding -n rbac-lab\nkubectl delete role configmap-admin -n rbac-lab\nkubectl delete serviceaccount config-manager -n rbac-lab\n</code></pre>"},{"location":"31-RBAC/#02-use-kubectl-auth-can-i-list-to-audit-permissions","title":"02. Use <code>kubectl auth can-i --list</code> to Audit Permissions","text":"<p>List all permissions that the <code>app-reader</code> ServiceAccount has in the <code>rbac-lab</code> namespace and cluster-wide. Identify which permissions come from the Role vs the ClusterRole.</p>"},{"location":"31-RBAC/#scenario_1","title":"Scenario:","text":"<p>\u25e6 A security audit requires you to document all permissions granted to a ServiceAccount. \u25e6 You need to distinguish between namespace-scoped and cluster-scoped permissions.</p> <p>Hint: Use <code>kubectl auth can-i --list --namespace rbac-lab --as system:serviceaccount:rbac-lab:app-reader</code>.</p> Solution <pre><code>## Ensure the namespace and bindings exist (from the main lab)\nkubectl create namespace rbac-lab --dry-run=client -o yaml | kubectl apply -f -\nkubectl create serviceaccount app-reader -n rbac-lab --dry-run=client -o yaml | kubectl apply -f -\nkubectl apply -f manifests/role-pod-reader.yaml\nkubectl apply -f manifests/rolebinding-pod-reader.yaml\nkubectl apply -f manifests/clusterrole-namespace-viewer.yaml\nkubectl apply -f manifests/clusterrolebinding-namespace-viewer.yaml\n\n## List namespace-scoped permissions in rbac-lab\nkubectl auth can-i --list \\\n  --namespace rbac-lab \\\n  --as system:serviceaccount:rbac-lab:app-reader\n## Expected output includes:\n## pods      []    []    [get watch list]   &lt;-- from Role pod-reader\n## namespaces []   []    [get list watch]   &lt;-- from ClusterRole namespace-viewer\n\n## List cluster-scoped permissions (no namespace)\nkubectl auth can-i --list \\\n  --as system:serviceaccount:rbac-lab:app-reader\n## Expected output includes:\n## namespaces []   []    [get list watch]   &lt;-- from ClusterRoleBinding\n\n## Check a specific permission\nkubectl auth can-i list pods \\\n  --namespace rbac-lab \\\n  --as system:serviceaccount:rbac-lab:app-reader\n## Expected: yes\n\nkubectl auth can-i list pods \\\n  --namespace default \\\n  --as system:serviceaccount:rbac-lab:app-reader\n## Expected: no (pod-reader Role is only in rbac-lab)\n</code></pre>"},{"location":"31-RBAC/#03-create-a-clusterrole-that-allows-reading-logs","title":"03. Create a ClusterRole That Allows Reading Logs","text":"<p>Create a ClusterRole named <code>log-reader</code> that grants access to <code>pods/log</code> (a subresource). Bind it to a ServiceAccount <code>log-collector</code> in the <code>rbac-lab</code> namespace using a RoleBinding (not a ClusterRoleBinding) to limit it to the namespace.</p>"},{"location":"31-RBAC/#scenario_2","title":"Scenario:","text":"<p>\u25e6 Your centralized logging agent needs to read pod logs but only in a specific namespace. \u25e6 You want to reuse a ClusterRole via a namespace-scoped RoleBinding.</p> <p>Hint: Use <code>pods/log</code> as the resource in the ClusterRole. A RoleBinding can reference a ClusterRole but limits its scope to the binding\u2019s namespace.</p> Solution <pre><code>## Create the ServiceAccount\nkubectl create serviceaccount log-collector -n rbac-lab\n\n## Create a ClusterRole for reading pod logs\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: log-reader\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"pods/log\"]\n    verbs: [\"get\", \"list\"]\nEOF\n\n## Use a RoleBinding (not ClusterRoleBinding) to limit scope to rbac-lab\nkubectl create rolebinding log-reader-binding \\\n  --namespace rbac-lab \\\n  --clusterrole=log-reader \\\n  --serviceaccount=rbac-lab:log-collector\n\n## Test: can read logs in rbac-lab?\nkubectl auth can-i get pods/log \\\n  --namespace rbac-lab \\\n  --as system:serviceaccount:rbac-lab:log-collector\n## Expected: yes\n\n## Test: can read logs in default namespace? (should be denied)\nkubectl auth can-i get pods/log \\\n  --namespace default \\\n  --as system:serviceaccount:rbac-lab:log-collector\n## Expected: no\n\n## Clean up\nkubectl delete rolebinding log-reader-binding -n rbac-lab\nkubectl delete clusterrole log-reader\nkubectl delete serviceaccount log-collector -n rbac-lab\n</code></pre>"},{"location":"31-RBAC/#04-restrict-a-serviceaccount-to-only-exec-into-pods","title":"04. Restrict a ServiceAccount to Only <code>exec</code> into Pods","text":"<p>Create a Role that grants only <code>create</code> access on the <code>pods/exec</code> subresource, and basic <code>get</code> on pods. Bind it to a new ServiceAccount and verify it can exec but cannot delete or list pods.</p>"},{"location":"31-RBAC/#scenario_3","title":"Scenario:","text":"<p>\u25e6 A debugging tool needs to exec into running pods for troubleshooting. \u25e6 It should not be able to list, delete, or modify pods - only exec into them.</p> <p>Hint: Use two rules in the Role: one for <code>pods</code> with <code>get</code>, and one for <code>pods/exec</code> with <code>create</code>.</p> Solution <pre><code>## Create the ServiceAccount\nkubectl create serviceaccount exec-debugger -n rbac-lab\n\n## Create the Role\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: rbac-lab\n  name: exec-only\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\"]\n    verbs: [\"get\"]\n  - apiGroups: [\"\"]\n    resources: [\"pods/exec\"]\n    verbs: [\"create\"]\nEOF\n\n## Bind the Role\nkubectl create rolebinding exec-debugger-binding \\\n  --namespace rbac-lab \\\n  --role=exec-only \\\n  --serviceaccount=rbac-lab:exec-debugger\n\n## Test: can exec?\nkubectl auth can-i create pods/exec \\\n  --namespace rbac-lab \\\n  --as system:serviceaccount:rbac-lab:exec-debugger\n## Expected: yes\n\n## Test: can get pods?\nkubectl auth can-i get pods \\\n  --namespace rbac-lab \\\n  --as system:serviceaccount:rbac-lab:exec-debugger\n## Expected: yes\n\n## Test: can list pods? (should be denied)\nkubectl auth can-i list pods \\\n  --namespace rbac-lab \\\n  --as system:serviceaccount:rbac-lab:exec-debugger\n## Expected: no\n\n## Test: can delete pods? (should be denied)\nkubectl auth can-i delete pods \\\n  --namespace rbac-lab \\\n  --as system:serviceaccount:rbac-lab:exec-debugger\n## Expected: no\n\n## Clean up\nkubectl delete rolebinding exec-debugger-binding -n rbac-lab\nkubectl delete role exec-only -n rbac-lab\nkubectl delete serviceaccount exec-debugger -n rbac-lab\n</code></pre>"},{"location":"31-RBAC/#05-verify-that-a-pod-uses-the-correct-serviceaccount-token","title":"05. Verify That a Pod Uses the Correct ServiceAccount Token","text":"<p>Deploy a pod with a custom ServiceAccount and verify from inside the pod that it uses the correct token by querying the Kubernetes API directly (without kubectl).</p>"},{"location":"31-RBAC/#scenario_4","title":"Scenario:","text":"<p>\u25e6 You need to verify that a pod\u2019s ServiceAccount token is correctly mounted and functional. \u25e6 The pod should be able to authenticate to the Kubernetes API using its token.</p> <p>Hint: The ServiceAccount token is mounted at <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code>. Use <code>curl</code> with the token to hit the API server.</p> Solution <pre><code>## Create the ServiceAccount and Role (reuse from main lab)\nkubectl create serviceaccount app-reader -n rbac-lab --dry-run=client -o yaml | kubectl apply -f -\nkubectl apply -f manifests/role-pod-reader.yaml\nkubectl apply -f manifests/rolebinding-pod-reader.yaml\n\n## Deploy a pod with curl available\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: token-verifier\n  namespace: rbac-lab\nspec:\n  serviceAccountName: app-reader\n  containers:\n    - name: curl\n      image: curlimages/curl:latest\n      command: [\"sleep\", \"3600\"]\nEOF\n\nkubectl wait --for=condition=Ready pod/token-verifier -n rbac-lab --timeout=60s\n\n## Verify the token is mounted\nkubectl exec token-verifier -n rbac-lab -- \\\n  ls /var/run/secrets/kubernetes.io/serviceaccount/\n\n## Read the ServiceAccount name\nkubectl exec token-verifier -n rbac-lab -- \\\n  cat /var/run/secrets/kubernetes.io/serviceaccount/namespace\necho  ## Newline\n## Expected: rbac-lab\n\n## Use the token to query the API server\nkubectl exec token-verifier -n rbac-lab -- sh -c '\n  TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\n  CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n  curl -s --cacert $CACERT \\\n    -H \"Authorization: Bearer $TOKEN\" \\\n    https://kubernetes.default.svc/api/v1/namespaces/rbac-lab/pods | head -20\n'\n## Expected: JSON response listing pods in rbac-lab\n\n## Clean up\nkubectl delete pod token-verifier -n rbac-lab\n</code></pre>"},{"location":"31-RBAC/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Permission denied (Forbidden):</li> </ul> <p>Verify the Role, RoleBinding, and subject names match exactly:</p> <pre><code>## Check Role exists and has correct rules\nkubectl get role pod-reader -n rbac-lab -o yaml\n\n## Check RoleBinding exists and references the correct Role and subject\nkubectl get rolebinding read-pods -n rbac-lab -o yaml\n\n## Common mistake: ServiceAccount name or namespace mismatch\nkubectl get serviceaccount -n rbac-lab\n</code></pre> <p></p> <ul> <li><code>kubectl auth can-i</code> returns unexpected results:</li> </ul> <p>Ensure you are using the correct impersonation format:</p> <pre><code>## Correct format for ServiceAccounts:\nkubectl auth can-i list pods \\\n  --as system:serviceaccount:&lt;namespace&gt;:&lt;sa-name&gt;\n\n## Correct format for users:\nkubectl auth can-i list pods --as &lt;username&gt;\n\n## List all permissions for a ServiceAccount\nkubectl auth can-i --list --as system:serviceaccount:rbac-lab:app-reader -n rbac-lab\n</code></pre> <p></p> <ul> <li>ClusterRole not working across namespaces:</li> </ul> <p>Ensure you used a <code>ClusterRoleBinding</code> (not a <code>RoleBinding</code>). A <code>RoleBinding</code> limits a ClusterRole to a single namespace:</p> <pre><code>## Check the binding type\nkubectl get clusterrolebinding view-namespaces -o yaml\n\n## If it's a RoleBinding, it only works in the binding's namespace\nkubectl get rolebinding -n rbac-lab\n</code></pre> <p></p> <ul> <li>Pod cannot access the Kubernetes API:</li> </ul> <p>Check the ServiceAccount token is mounted and the pod identity is correct:</p> <pre><code>## Check if the pod uses the expected ServiceAccount\nkubectl get pod api-explorer -n rbac-lab -o jsonpath='{.spec.serviceAccountName}'\necho\n\n## Check if the ServiceAccount has the expected bindings\nkubectl get rolebinding,clusterrolebinding -A -o wide | grep app-reader\n</code></pre> <p></p> <ul> <li>Aggregated ClusterRole not working:</li> </ul> <p>Verify the label matches the aggregation label selector:</p> <pre><code>## Check the aggregation rule on the target ClusterRole\nkubectl get clusterrole view -o yaml | grep -A5 aggregationRule\n\n## The label must match exactly\nkubectl get clusterrole custom-metrics-viewer -o yaml | grep -A2 labels\n</code></pre>"},{"location":"31-RBAC/#next-steps","title":"Next Steps","text":"<ul> <li>Explore OPA Gatekeeper or Kyverno for policy enforcement beyond RBAC.</li> <li>Learn about Pod Security Standards and Pod Security Admission to control what pods can do.</li> <li>Set up Kubernetes Audit Logging to monitor RBAC events (who accessed what and when).</li> <li>Integrate RBAC with external identity providers (OIDC, LDAP) using Dex or your cloud provider\u2019s IAM integration.</li> <li>Explore Hierarchical Namespaces for multi-tenant RBAC patterns.</li> <li>Study the RBAC Good Practices guide from the official Kubernetes documentation.</li> </ul>"},{"location":"32-Secrets/","title":"Kubernetes Secrets","text":"<ul> <li>Welcome to the Kubernetes <code>Secrets</code> hands-on lab! In this tutorial, you\u2019ll learn everything about Kubernetes Secrets \u2013 how to create, manage, consume, secure, and rotate them.</li> <li>Secrets are first-class Kubernetes objects designed to hold sensitive data such as passwords, OAuth tokens, TLS certificates, and SSH keys.</li> <li>You\u2019ll gain practical experience creating Secrets imperatively and declaratively, mounting them into Pods as environment variables and volumes, working with TLS and docker-registry Secrets, using projected volumes, making Secrets immutable, and enabling encryption at rest.</li> </ul>"},{"location":"32-Secrets/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What Kubernetes Secrets are and why they exist</li> <li>The different types of Secrets and when to use each one</li> <li>How Secrets differ from ConfigMaps</li> <li>How to create Secrets imperatively (from literals, files, and env-files)</li> <li>How to create Secrets declaratively using YAML manifests</li> <li>How to mount Secrets as environment variables in Pods</li> <li>How to mount Secrets as files (volumes) in Pods</li> <li>How to create and use <code>docker-registry</code> Secrets for private image registries</li> <li>How to create and use TLS Secrets for HTTPS termination</li> <li>How to use projected volumes to combine Secrets and ConfigMaps</li> <li>How to make Secrets immutable for safety and performance</li> <li>How to rotate Secrets and trigger Pod restarts</li> <li>How to enable encryption at rest with <code>EncryptionConfiguration</code></li> <li>Security best practices and common pitfalls</li> </ul>"},{"location":"32-Secrets/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link Kubernetes Secrets kubernetes.io/docs/concepts/configuration/secret Managing Secrets with kubectl kubernetes.io/docs/tasks/configmap-secret/managing-secret-using-kubectl Managing Secrets with Config File kubernetes.io/docs/tasks/configmap-secret/managing-secret-using-config-file Distribute Credentials via Secrets kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure Encrypting Secrets at Rest kubernetes.io/docs/tasks/administer-cluster/encrypt-data Good Practices for Secrets kubernetes.io/docs/concepts/security/secrets-good-practices Projected Volumes kubernetes.io/docs/concepts/storage/projected-volumes Pull Image from Private Registry kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry TLS Secrets kubernetes.io/docs/concepts/configuration/secret/#tls-secrets External Secrets Operator external-secrets.io Sealed Secrets (Bitnami) github.com/bitnami-labs/sealed-secrets HashiCorp Vault vaultproject.io"},{"location":"32-Secrets/#introduction","title":"Introduction","text":""},{"location":"32-Secrets/#what-are-kubernetes-secrets","title":"What Are Kubernetes Secrets?","text":"<ul> <li>A <code>Secret</code> is a Kubernetes object that holds a small amount of sensitive data, such as a password, a token, or a key.</li> <li>Secrets decouple sensitive information from Pod specs and container images, reducing the risk of accidental exposure.</li> <li>Without Secrets, you would need to embed credentials directly in Pod manifests, Dockerfiles, or application code \u2013 all of which are insecure practices.</li> <li>Secrets are stored in <code>etcd</code>, the Kubernetes cluster\u2019s key-value store, and are made available to Pods through environment variables or volume mounts.</li> </ul>"},{"location":"32-Secrets/#secret-types","title":"Secret Types","text":"<p>Kubernetes supports several built-in Secret types, each designed for a specific use case:</p> Type Description Usage <code>Opaque</code> Generic Secret for arbitrary key-value pairs (default type) Passwords, API keys, connection strings <code>kubernetes.io/dockerconfigjson</code> Docker registry credentials for pulling private images <code>imagePullSecrets</code> in Pod specs <code>kubernetes.io/tls</code> TLS certificate and private key pair HTTPS termination, Ingress TLS <code>kubernetes.io/basic-auth</code> Credentials for basic HTTP authentication Username/password for HTTP auth <code>kubernetes.io/ssh-auth</code> SSH private key for authentication Git clones, SSH connections <code>bootstrap.kubernetes.io/token</code> Bootstrap token for node joining <code>kubeadm join</code> operations <code>kubernetes.io/service-account-token</code> Service account token (auto-created by Kubernetes) Pod-to-API-server authentication <p>Default Type</p> <p>If you do not specify a <code>type</code> when creating a Secret, Kubernetes defaults to <code>Opaque</code>. This is the most common type and accepts any arbitrary data.</p>"},{"location":"32-Secrets/#secrets-vs-configmaps","title":"Secrets vs. ConfigMaps","text":"<p>Both Secrets and ConfigMaps store configuration data, but they serve different purposes:</p> Feature Secret ConfigMap Purpose Sensitive data (passwords, tokens, keys) Non-sensitive configuration (settings, properties) Data encoding Values stored as base64-encoded strings Values stored as plain text Size limit 1 MiB per Secret 1 MiB per ConfigMap RBAC Typically restricted with fine-grained RBAC Often more broadly accessible tmpfs mounting Mounted in <code>tmpfs</code> (RAM) \u2013 never written to disk on nodes Mounted on disk Encryption Can be encrypted at rest in etcd Not encrypted at rest Environment vars Supported via <code>secretKeyRef</code> Supported via <code>configMapKeyRef</code> Volume mounts Supported (files in tmpfs) Supported (files on disk) <p>Base64 Is NOT Encryption</p> <p>Kubernetes stores Secret values as base64-encoded strings. Base64 is an encoding scheme, not an encryption algorithm. Anyone with <code>get</code> access to Secrets can decode them trivially with <code>base64 --decode</code>. Always combine Secrets with proper RBAC and encryption at rest.</p>"},{"location":"32-Secrets/#secret-encoding-base64-vs-encryption-at-rest","title":"Secret Encoding: base64 vs. Encryption at Rest","text":"<p>Understanding the difference between encoding and encryption is critical:</p> <ul> <li>base64 encoding: Secrets stored in the <code>data</code> field must be base64-encoded. This is a reversible encoding (not encryption) that allows binary data to be represented as text. You can use the <code>stringData</code> field to provide plain-text values that Kubernetes will automatically base64-encode.</li> <li>Encryption at rest: By default, Secrets are stored unencrypted in etcd. Anyone with access to etcd can read all Secrets. To protect Secrets in etcd, you must enable encryption at rest using an <code>EncryptionConfiguration</code> resource and configure the API server to use it.</li> </ul>"},{"location":"32-Secrets/#security-best-practices","title":"Security Best Practices","text":"<p>Critical Security Considerations</p> <ol> <li>Enable RBAC: Restrict who can <code>get</code>, <code>list</code>, and <code>watch</code> Secrets. A user who can <code>list</code> Secrets in a namespace can see all Secret data.</li> <li>Enable encryption at rest: Configure the API server with <code>--encryption-provider-config</code> to encrypt Secrets in etcd.</li> <li>Avoid Secrets in Git: Never commit Secret manifests with real credentials to version control. Use tools like Sealed Secrets, External Secrets Operator, or SOPS.</li> <li>Use least privilege: Grant only the minimum RBAC permissions needed. Avoid giving <code>*</code> (wildcard) access to Secrets.</li> <li>Rotate Secrets regularly: Establish a rotation schedule and automate the process.</li> <li>Prefer volume mounts over env vars: Environment variables can leak through crash dumps, logs, or child processes. Volume-mounted Secrets are more secure.</li> <li>Use immutable Secrets: Mark Secrets as <code>immutable: true</code> when the data should never change, improving security and API server performance.</li> <li>Audit Secret access: Enable Kubernetes audit logging to track who accesses Secrets and when.</li> </ol>"},{"location":"32-Secrets/#secret-lifecycle","title":"Secret Lifecycle","text":"<p>The following diagram illustrates how Secrets flow from creation to consumption in a Kubernetes cluster:</p> <pre><code>flowchart TB\n    subgraph Creation [\"Secret Creation\"]\n        A[\"kubectl create secret\"] --&gt; D[\"API Server\"]\n        B[\"YAML Manifest\\n(stringData / data)\"] --&gt; D\n        C[\"External Secrets\\nOperator / Vault\"] --&gt; D\n    end\n\n    subgraph Storage [\"Storage Layer\"]\n        D --&gt; E{\"Encryption\\nat Rest?\"}\n        E --&gt;|Yes| F[\"Encrypted in etcd\"]\n        E --&gt;|No| G[\"Plain base64 in etcd\"]\n    end\n\n    subgraph Consumption [\"Pod Consumption\"]\n        F --&gt; H[\"kubelet fetches Secret\"]\n        G --&gt; H\n        H --&gt; I[\"Environment\\nVariables\"]\n        H --&gt; J[\"Volume Mounts\\n(tmpfs)\"]\n        H --&gt; K[\"imagePullSecrets\"]\n    end\n\n    subgraph Pod [\"Pod Runtime\"]\n        I --&gt; L[\"Container Process\\nreads env var\"]\n        J --&gt; M[\"Container Process\\nreads file\"]\n        K --&gt; N[\"kubelet pulls\\nprivate image\"]\n    end\n\n    style Creation fill:#e1f5fe,stroke:#0277bd\n    style Storage fill:#fff3e0,stroke:#ef6c00\n    style Consumption fill:#e8f5e9,stroke:#2e7d32\n    style Pod fill:#f3e5f5,stroke:#7b1fa2</code></pre>"},{"location":"32-Secrets/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (minikube, kind, k3d, Docker Desktop, or a cloud-managed cluster)</li> <li><code>kubectl</code> installed and configured to communicate with your cluster</li> <li><code>openssl</code> installed (for generating TLS certificates in Step 06)</li> <li>Basic familiarity with Kubernetes Pods, Deployments, and YAML manifests</li> </ul> <p>Verify your cluster is accessible:</p> <pre><code>## Verify kubectl is configured and the cluster is reachable\nkubectl cluster-info\n\n## Verify you can list namespaces\nkubectl get namespaces\n</code></pre>"},{"location":"32-Secrets/#lab","title":"Lab","text":""},{"location":"32-Secrets/#step-01-create-the-lab-namespace","title":"Step 01 - Create the Lab Namespace","text":"<ul> <li>Before we begin working with Secrets, let\u2019s create a dedicated namespace to keep our lab resources isolated.</li> </ul> <pre><code>## Create the secrets-lab namespace from the manifest file\nkubectl apply -f manifests/namespace.yaml\n\n## Verify the namespace was created\nkubectl get namespace secrets-lab\n\n## Set the default namespace for this lab session so we don't need\n## to add -n secrets-lab to every command\nkubectl config set-context --current --namespace=secrets-lab\n</code></pre> <p>Using a Dedicated Namespace</p> <p>Working in a dedicated namespace makes cleanup easy \u2013 just delete the namespace at the end and all resources are removed. It also provides an isolation boundary for RBAC policies.</p>"},{"location":"32-Secrets/#step-02-create-secrets-imperatively","title":"Step 02 - Create Secrets Imperatively","text":"<ul> <li>The fastest way to create Secrets is using <code>kubectl create secret</code> directly from the command line.</li> <li>Kubernetes supports creating Secrets from literal values, from files, and from environment files.</li> </ul>"},{"location":"32-Secrets/#from-literal-values","title":"From Literal Values","text":"<pre><code>## Create an Opaque Secret with literal key-value pairs\n## The --from-literal flag accepts key=value pairs\nkubectl create secret generic db-credentials \\\n  --from-literal=username=admin \\\n  --from-literal=password='S3cur3P@ssw0rd!' \\\n  --from-literal=host=db.example.com \\\n  --from-literal=port=5432\n\n## Verify the Secret was created\nkubectl get secret db-credentials\n\n## View the Secret details (values are base64-encoded)\nkubectl get secret db-credentials -o yaml\n</code></pre> <p>Quoting Special Characters</p> <p>When using <code>--from-literal</code>, wrap values containing special characters (<code>$</code>, <code>!</code>, <code>@</code>, <code>#</code>, etc.) in single quotes to prevent shell interpretation. For example: <code>--from-literal=password='My$ecret!'</code>.</p>"},{"location":"32-Secrets/#from-files","title":"From Files","text":"<pre><code>## Create sample credential files\necho -n \"admin\" &gt; /tmp/username.txt\necho -n \"S3cur3P@ssw0rd!\" &gt; /tmp/password.txt\n\n## Create a Secret from files\n## Each file becomes a key (filename) with the file contents as the value\nkubectl create secret generic file-credentials \\\n  --from-file=/tmp/username.txt \\\n  --from-file=/tmp/password.txt\n\n## Verify the Secret was created\nkubectl get secret file-credentials -o yaml\n\n## You can also specify a custom key name using key=path syntax\nkubectl create secret generic file-credentials-custom \\\n  --from-file=db-user=/tmp/username.txt \\\n  --from-file=db-pass=/tmp/password.txt\n\n## Verify the custom key names\nkubectl get secret file-credentials-custom -o jsonpath='{.data}' | python3 -m json.tool\n\n## Clean up temporary files\nrm /tmp/username.txt /tmp/password.txt\n</code></pre> <p>Trailing Newlines</p> <p>Use <code>echo -n</code> (without trailing newline) when creating files for Secrets. A trailing newline character will be included in the Secret value and can cause authentication failures.</p>"},{"location":"32-Secrets/#from-an-env-file","title":"From an Env File","text":"<pre><code>## Create an env file with key=value pairs (one per line)\ncat &lt;&lt;'EOF' &gt; /tmp/app-secrets.env\nDB_HOST=db.example.com\nDB_PORT=5432\nDB_USER=admin\nDB_PASSWORD=S3cur3P@ssw0rd!\nAPI_KEY=sk-1234567890abcdef\nEOF\n\n## Create a Secret from the env file\n## Each line becomes a separate key-value pair in the Secret\nkubectl create secret generic env-credentials \\\n  --from-env-file=/tmp/app-secrets.env\n\n## Verify the Secret keys match the env file entries\nkubectl get secret env-credentials -o jsonpath='{.data}' | python3 -m json.tool\n\n## Clean up\nrm /tmp/app-secrets.env\n</code></pre>"},{"location":"32-Secrets/#inspect-the-created-secrets","title":"Inspect the Created Secrets","text":"<pre><code>## List all Secrets in the namespace\nkubectl get secrets\n\n## Describe a Secret to see metadata (values are hidden)\nkubectl describe secret db-credentials\n\n## Decode a specific value from a Secret\nkubectl get secret db-credentials -o jsonpath='{.data.password}' | base64 --decode\necho  ## Add a newline for readability\n\n## Decode ALL values from a Secret using a one-liner\nkubectl get secret db-credentials -o json | \\\n  python3 -c \"import json,sys,base64; \\\n  data=json.load(sys.stdin)['data']; \\\n  [print(f'{k}: {base64.b64decode(v).decode()}') for k,v in data.items()]\"\n</code></pre>"},{"location":"32-Secrets/#step-03-create-secrets-declaratively","title":"Step 03 - Create Secrets Declaratively","text":"<ul> <li>For production workflows, you typically define Secrets in YAML manifests and manage them through GitOps pipelines.</li> <li>Kubernetes supports two fields for providing Secret data: <code>data</code> (base64-encoded) and <code>stringData</code> (plain text).</li> </ul>"},{"location":"32-Secrets/#using-stringdata-recommended-for-readability","title":"Using <code>stringData</code> (Recommended for Readability)","text":"<pre><code>## Apply the Opaque Secret manifest that uses stringData\nkubectl apply -f manifests/secret-opaque.yaml\n\n## Verify the Secret was created\nkubectl get secret app-credentials\n\n## View the Secret -- notice that stringData values are now base64-encoded\n## under the 'data' field (stringData is a write-only convenience field)\nkubectl get secret app-credentials -o yaml\n</code></pre>"},{"location":"32-Secrets/#using-data-base64-encoded","title":"Using <code>data</code> (Base64-Encoded)","text":"<pre><code>## When using the 'data' field, you must base64-encode values yourself\n## Encode a value to base64\necho -n \"my-secret-value\" | base64\n## Output: bXktc2VjcmV0LXZhbHVl\n\n## Decode a base64 value back to plain text\necho \"bXktc2VjcmV0LXZhbHVl\" | base64 --decode\n## Output: my-secret-value\n</code></pre> <pre><code>## Example: Secret using the 'data' field with pre-encoded values\n## (You do NOT need to apply this -- it is shown for comparison)\napiVersion: v1\nkind: Secret\nmetadata:\n  name: base64-example\n  namespace: secrets-lab\ntype: Opaque\ndata:\n  ## Each value must be base64-encoded\n  username: YWRtaW4=          ## base64(\"admin\")\n  password: UzNjdXIzUEBzc3cwcmQh  ## base64(\"S3cur3P@ssw0rd!\")\n</code></pre> <p><code>stringData</code> vs. <code>data</code></p> <ul> <li>Use <code>stringData</code> for readability during development and when values are plain text.</li> <li>Use <code>data</code> when you have already-encoded values or when generating manifests programmatically.</li> <li>If both <code>stringData</code> and <code>data</code> contain the same key, the <code>stringData</code> value takes precedence.</li> <li>After a Secret is created, Kubernetes always stores and returns values in the base64-encoded <code>data</code> field. The <code>stringData</code> field does not appear in <code>kubectl get secret -o yaml</code> output.</li> </ul>"},{"location":"32-Secrets/#verify-the-data-is-base64-encoded","title":"Verify the Data Is Base64-Encoded","text":"<pre><code>## Retrieve the Secret and decode the username\nkubectl get secret app-credentials -o jsonpath='{.data.username}' | base64 --decode\necho  ## Newline\n## Output: admin\n\n## Retrieve and decode the password\nkubectl get secret app-credentials -o jsonpath='{.data.password}' | base64 --decode\necho  ## Newline\n## Output: S3cur3P@ssw0rd!\n\n## Retrieve and decode the database URL\nkubectl get secret app-credentials -o jsonpath='{.data.database-url}' | base64 --decode\necho  ## Newline\n## Output: postgres://admin:S3cur3P%40ssw0rd!@db-host:5432/mydb\n</code></pre>"},{"location":"32-Secrets/#step-04-mount-secrets-as-environment-variables","title":"Step 04 - Mount Secrets as Environment Variables","text":"<ul> <li>One of the most common ways to consume Secrets is by injecting them as environment variables into a container.</li> <li>The Pod references specific keys from a Secret using <code>secretKeyRef</code>.</li> </ul> <pre><code>## Ensure the app-credentials Secret exists (from Step 03)\nkubectl get secret app-credentials\n\n## Apply the Pod that mounts Secret values as environment variables\nkubectl apply -f manifests/pod-env-secret.yaml\n\n## Wait for the Pod to be running\nkubectl wait --for=condition=Ready pod/secret-env-demo --timeout=60s\n\n## View the Pod logs to see the environment variables in action\nkubectl logs secret-env-demo\n</code></pre> <p>Expected output:</p> <pre><code>=== Secret values loaded as environment variables ===\nDB_USERNAME=admin\nDB_PASSWORD=S3cur3P@ssw0rd!\nDATABASE_URL=postgres://admin:S3cur3P%40ssw0rd!@db-host:5432/mydb\n=== Sleeping to keep pod alive for inspection ===\n</code></pre> <pre><code>## You can also exec into the Pod and inspect the environment\nkubectl exec secret-env-demo -- env | grep -E \"DB_|DATABASE_\"\n\n## Verify a specific variable\nkubectl exec secret-env-demo -- sh -c 'echo $DB_USERNAME'\n</code></pre>"},{"location":"32-Secrets/#loading-all-keys-with-envfrom","title":"Loading All Keys with <code>envFrom</code>","text":"<p>Instead of mapping individual keys, you can inject all keys from a Secret as environment variables:</p> <pre><code>## Create a Pod that loads all Secret keys as env vars using envFrom\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-envfrom-demo\n  namespace: secrets-lab\nspec:\n  containers:\n    - name: demo\n      image: busybox:1.36\n      command: [\"sh\", \"-c\", \"env | sort | grep -v PATH &amp;&amp; sleep 3600\"]\n      envFrom:\n        - secretRef:\n            name: db-credentials\n          ## Optional: add a prefix to all env var names\n          prefix: SECRET_\n  restartPolicy: Never\nEOF\n\n## Wait for the Pod and check the environment variables\nkubectl wait --for=condition=Ready pod/secret-envfrom-demo --timeout=60s\nkubectl logs secret-envfrom-demo | grep SECRET_\n</code></pre> <p>Environment Variable Risks</p> <p>Environment variables are visible in process listings (<code>/proc/&lt;pid&gt;/environ</code>), crash dumps, and may be logged by applications. For highly sensitive data (private keys, TLS certificates), prefer volume mounts over environment variables.</p>"},{"location":"32-Secrets/#step-05-mount-secrets-as-volumes","title":"Step 05 - Mount Secrets as Volumes","text":"<ul> <li>Mounting Secrets as volumes creates files in the container\u2019s filesystem.</li> <li>Each key in the Secret becomes a file, and the file content is the decoded Secret value.</li> <li>Volume-mounted Secrets are stored in <code>tmpfs</code> (RAM-backed filesystem) and are never written to disk on the node.</li> </ul> <pre><code>## Ensure the app-credentials Secret exists\nkubectl get secret app-credentials\n\n## Apply the Pod that mounts Secret as a volume\nkubectl apply -f manifests/pod-volume-secret.yaml\n\n## Wait for the Pod to be running\nkubectl wait --for=condition=Ready pod/secret-volume-demo --timeout=60s\n\n## View the Pod logs to see the mounted files\nkubectl logs secret-volume-demo\n</code></pre> <p>Expected output:</p> <pre><code>=== Secret files mounted as volume ===\n--- Listing /etc/secrets ---\ntotal 0\ndrwxrwx--T    2 root     root           120 ...  .\ndrwxr-xr-x    1 root     root            28 ...  ..\n-r--------    1 root     root             5 ...  username\n-r--------    1 root     root            16 ...  password\n-r--------    1 root     root            52 ...  database-url\n--- Contents of each file ---\n/etc/secrets/database-url: postgres://admin:S3cur3P%40ssw0rd!@db-host:5432/mydb\n/etc/secrets/password: S3cur3P@ssw0rd!\n/etc/secrets/username: admin\n</code></pre> <pre><code>## Exec into the Pod and read individual Secret files\nkubectl exec secret-volume-demo -- cat /etc/secrets/username\nkubectl exec secret-volume-demo -- cat /etc/secrets/password\n\n## Verify file permissions (should be 0400 as set in the manifest)\nkubectl exec secret-volume-demo -- ls -la /etc/secrets/\n\n## Verify the mount is tmpfs (in-memory filesystem)\nkubectl exec secret-volume-demo -- df -T /etc/secrets/\n</code></pre>"},{"location":"32-Secrets/#mounting-specific-keys-only","title":"Mounting Specific Keys Only","text":"<p>You can select which keys to mount and control their file paths using the <code>items</code> field:</p> <pre><code>## Create a Pod that mounts only the password key with a custom filename\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-selective-mount\n  namespace: secrets-lab\nspec:\n  containers:\n    - name: demo\n      image: busybox:1.36\n      command: [\"sh\", \"-c\", \"ls -la /etc/secrets/ &amp;&amp; cat /etc/secrets/db-password &amp;&amp; sleep 3600\"]\n      volumeMounts:\n        - name: secret-vol\n          mountPath: /etc/secrets\n          readOnly: true\n  volumes:\n    - name: secret-vol\n      secret:\n        secretName: app-credentials\n        items:\n          ## Only mount the 'password' key, renaming the file to 'db-password'\n          - key: password\n            path: db-password\n            mode: 0400\n  restartPolicy: Never\nEOF\n\n## Wait and verify\nkubectl wait --for=condition=Ready pod/secret-selective-mount --timeout=60s\nkubectl logs secret-selective-mount\n</code></pre> <p>Automatic Updates</p> <p>When a Secret is updated, volume-mounted Secrets are automatically updated by the kubelet (with a delay of up to the kubelet sync period, typically 1-2 minutes). Environment variables are NOT updated \u2013 the Pod must be restarted.</p>"},{"location":"32-Secrets/#step-06-create-and-use-docker-registry-secrets","title":"Step 06 - Create and Use Docker Registry Secrets","text":"<ul> <li>When pulling container images from a private registry, Kubernetes needs credentials.</li> <li>The <code>docker-registry</code> Secret type stores these credentials in the format that the kubelet expects.</li> </ul> <pre><code>## Create a docker-registry Secret\n## Replace the placeholder values with your actual registry credentials\nkubectl create secret docker-registry my-registry-secret \\\n  --docker-server=https://index.docker.io/v1/ \\\n  --docker-username=your-username \\\n  --docker-password=your-password \\\n  --docker-email=your-email@example.com\n\n## View the created Secret\nkubectl get secret my-registry-secret -o yaml\n\n## The Secret contains a .dockerconfigjson key with the registry auth data\n## Decode it to see the structure\nkubectl get secret my-registry-secret \\\n  -o jsonpath='{.data.\\.dockerconfigjson}' | base64 --decode | python3 -m json.tool\n</code></pre>"},{"location":"32-Secrets/#using-imagepullsecrets-in-a-pod","title":"Using imagePullSecrets in a Pod","text":"<pre><code>## Create a Pod that references the registry Secret for image pulling\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: private-image-pod\n  namespace: secrets-lab\nspec:\n  containers:\n    - name: app\n      image: your-registry.example.com/your-app:latest\n      ports:\n        - containerPort: 8080\n  imagePullSecrets:\n    - name: my-registry-secret\n  restartPolicy: Never\nEOF\n</code></pre>"},{"location":"32-Secrets/#attaching-imagepullsecrets-to-a-serviceaccount","title":"Attaching imagePullSecrets to a ServiceAccount","text":"<p>Instead of adding <code>imagePullSecrets</code> to every Pod, you can attach it to a ServiceAccount so all Pods using that ServiceAccount automatically use the registry credentials:</p> <pre><code>## Create a ServiceAccount with the imagePullSecret attached\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: private-registry-sa\n  namespace: secrets-lab\nimagePullSecrets:\n  - name: my-registry-secret\nEOF\n\n## Any Pod using this ServiceAccount will automatically pull from the private registry\n## kubectl get serviceaccount private-registry-sa -o yaml\n</code></pre> <p>ServiceAccount Pattern</p> <p>Attaching <code>imagePullSecrets</code> to a ServiceAccount is the recommended pattern for teams. It avoids repeating the Secret reference in every Pod spec and centralizes registry credential management.</p>"},{"location":"32-Secrets/#step-07-create-and-use-tls-secrets","title":"Step 07 - Create and Use TLS Secrets","text":"<ul> <li>TLS Secrets hold a certificate and its associated private key.</li> <li>They are commonly used for HTTPS termination in Ingress controllers or directly in application Pods.</li> </ul>"},{"location":"32-Secrets/#generate-a-self-signed-certificate","title":"Generate a Self-Signed Certificate","text":"<pre><code>## Generate a self-signed TLS certificate and private key using openssl\n## This creates a certificate valid for 365 days for localhost\nopenssl req -x509 \\\n  -nodes \\\n  -days 365 \\\n  -newkey rsa:2048 \\\n  -keyout /tmp/tls.key \\\n  -out /tmp/tls.crt \\\n  -subj \"/CN=localhost/O=secrets-lab\"\n\n## Verify the certificate details\nopenssl x509 -in /tmp/tls.crt -text -noout | head -20\n</code></pre>"},{"location":"32-Secrets/#create-the-tls-secret","title":"Create the TLS Secret","text":"<pre><code>## Create a TLS Secret from the certificate and key files\n## kubectl validates that the cert and key are valid and match\nkubectl create secret tls tls-secret \\\n  --cert=/tmp/tls.crt \\\n  --key=/tmp/tls.key\n\n## Verify the Secret was created with the correct type\nkubectl get secret tls-secret\n## The TYPE column should show kubernetes.io/tls\n\n## View the Secret structure\nkubectl get secret tls-secret -o yaml\n## It contains two keys: tls.crt and tls.key\n\n## Clean up local files\nrm /tmp/tls.crt /tmp/tls.key\n</code></pre>"},{"location":"32-Secrets/#deploy-nginx-with-tls","title":"Deploy Nginx with TLS","text":"<pre><code>## Apply the nginx TLS pod and its ConfigMap\nkubectl apply -f manifests/tls-pod.yaml\n\n## Wait for the Pod to be running\nkubectl wait --for=condition=Ready pod/nginx-tls-demo --timeout=60s\n\n## Test the HTTPS endpoint from inside the cluster\n## The -k flag tells curl to accept the self-signed certificate\nkubectl exec nginx-tls-demo -- curl -k https://localhost:443 2&gt;/dev/null\n## Output: TLS is working! Served by nginx with a self-signed certificate.\n\n## Verify the certificate details served by nginx\nkubectl exec nginx-tls-demo -- \\\n  sh -c \"echo | openssl s_client -connect localhost:443 2&gt;/dev/null | openssl x509 -noout -subject -dates\"\n</code></pre> <p>TLS in Ingress</p> <p>In production, TLS Secrets are typically referenced in Ingress resources rather than mounted directly in Pods. Ingress controllers handle TLS termination centrally. Tools like cert-manager can automate certificate issuance and renewal.</p>"},{"location":"32-Secrets/#step-08-using-projected-volumes","title":"Step 08 - Using Projected Volumes","text":"<ul> <li>Projected volumes allow you to combine multiple volume sources into a single mount directory.</li> <li>This is useful when a container needs both Secrets and ConfigMaps accessible from the same path.</li> </ul> <pre><code>## Apply the projected volume demo (includes both a ConfigMap and a Pod)\nkubectl apply -f manifests/pod-projected-volume.yaml\n\n## Wait for the Pod to be running\nkubectl wait --for=condition=Ready pod/projected-volume-demo --timeout=60s\n\n## View the Pod logs to see all projected files\nkubectl logs projected-volume-demo\n</code></pre> <p>Expected output:</p> <pre><code>=== Projected volume contents ===\n--- Listing /etc/app-config ---\n...\n--- Secret: username ---\nadmin\n--- Secret: password ---\nS3cur3P@ssw0rd!\n--- ConfigMap: app.properties ---\napp.name=secrets-demo\napp.version=1.0.0\napp.environment=development\napp.log.level=info\n--- ConfigMap: feature-flags.json ---\n{\n  \"enableNewUI\": true,\n  \"enableBetaFeatures\": false,\n  \"maxRetries\": 3\n}\n=== All config and secrets unified under one mount ===\n</code></pre> <pre><code>## Exec into the Pod and explore the projected mount\nkubectl exec projected-volume-demo -- ls -la /etc/app-config/\n\n## Read a specific file\nkubectl exec projected-volume-demo -- cat /etc/app-config/app.properties\n</code></pre> <p>Projected Volume Sources</p> <p>Projected volumes can combine: <code>secret</code>, <code>configMap</code>, <code>downwardAPI</code>, and <code>serviceAccountToken</code> sources. This is powerful for applications that expect all configuration in a single directory.</p>"},{"location":"32-Secrets/#step-09-immutable-secrets","title":"Step 09 - Immutable Secrets","text":"<ul> <li>Kubernetes supports marking Secrets as immutable starting from v1.21 (GA).</li> <li>Once a Secret is marked as <code>immutable: true</code>, its <code>data</code> and <code>stringData</code> fields cannot be updated.</li> <li>The only way to change an immutable Secret is to delete it and recreate it.</li> </ul>"},{"location":"32-Secrets/#benefits-of-immutable-secrets","title":"Benefits of Immutable Secrets","text":"<ol> <li>Security: Prevents accidental or malicious modifications to critical credentials</li> <li>Performance: The kubelet does not need to set up watches for immutable Secrets, reducing API server load</li> <li>Reliability: Guarantees that the Secret data remains consistent throughout its lifetime</li> </ol> <pre><code>## Apply the immutable Secret\nkubectl apply -f manifests/immutable-secret.yaml\n\n## Verify it was created\nkubectl get secret immutable-api-key\nkubectl get secret immutable-api-key -o jsonpath='{.immutable}'\necho  ## Newline\n## Output: true\n</code></pre>"},{"location":"32-Secrets/#attempt-to-modify-an-immutable-secret","title":"Attempt to Modify an Immutable Secret","text":"<pre><code>## Try to update the immutable Secret -- this will FAIL\nkubectl patch secret immutable-api-key \\\n  --type='json' \\\n  -p='[{\"op\": \"replace\", \"path\": \"/data/api-key\", \"value\": \"bmV3LWtleQ==\"}]'\n\n## Expected error:\n## Error from server (Forbidden): secrets \"immutable-api-key\" is forbidden:\n## field is immutable when `immutable` is set\n</code></pre> <pre><code>## Also try editing -- this will also fail on save\n## kubectl edit secret immutable-api-key\n## (Uncomment and try if you want to see the error)\n\n## The ONLY way to change an immutable Secret is to delete and recreate it\nkubectl delete secret immutable-api-key\nkubectl apply -f manifests/immutable-secret.yaml\n</code></pre> <p>Immutable Cannot Be Reversed</p> <p>Once <code>immutable: true</code> is set on a Secret, you cannot change it back to <code>false</code>. The only option is to delete and recreate the Secret. Plan accordingly before marking Secrets as immutable.</p>"},{"location":"32-Secrets/#step-10-secret-rotation-and-pod-restart-strategies","title":"Step 10 - Secret Rotation and Pod Restart Strategies","text":"<ul> <li>In production, credentials must be rotated regularly for security compliance.</li> <li>When a Secret is updated, volume-mounted Secrets are automatically refreshed, but environment variables are not.</li> <li>For environment variable-based Secrets, you need to trigger a Pod restart.</li> </ul>"},{"location":"32-Secrets/#update-a-secret","title":"Update a Secret","text":"<pre><code>## Update the db-credentials Secret with a new password\nkubectl create secret generic db-credentials \\\n  --from-literal=username=admin \\\n  --from-literal=password='N3wR0t@tedP@ss!' \\\n  --from-literal=host=db.example.com \\\n  --from-literal=port=5432 \\\n  --dry-run=client -o yaml | kubectl apply -f -\n\n## Verify the password was updated\nkubectl get secret db-credentials -o jsonpath='{.data.password}' | base64 --decode\necho  ## Newline\n## Output: N3wR0t@tedP@ss!\n</code></pre>"},{"location":"32-Secrets/#verify-volume-mount-auto-update","title":"Verify Volume Mount Auto-Update","text":"<pre><code>## If you still have the secret-volume-demo Pod running from Step 05,\n## check that the mounted files reflect the new Secret data.\n## (There may be a delay of up to the kubelet sync period, typically 1-2 minutes)\n\n## Wait a moment for the kubelet to sync\nsleep 120\n\n## Check the updated file content\nkubectl exec secret-volume-demo -- cat /etc/secrets/password 2&gt;/dev/null || \\\n  echo \"Pod secret-volume-demo is not running. Recreate it to test volume auto-update.\"\n</code></pre>"},{"location":"32-Secrets/#trigger-a-rolling-restart-for-deployments","title":"Trigger a Rolling Restart for Deployments","text":"<p>When Pods consume Secrets as environment variables, updating the Secret does not restart the Pod. Use one of these strategies:</p> <pre><code>## Strategy 1: kubectl rollout restart (simplest, requires Deployment/StatefulSet)\n## Create a sample Deployment first\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: secret-consumer\n  namespace: secrets-lab\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: secret-consumer\n  template:\n    metadata:\n      labels:\n        app: secret-consumer\n    spec:\n      containers:\n        - name: app\n          image: busybox:1.36\n          command: [\"sh\", \"-c\", \"while true; do echo DB_PASSWORD=$DB_PASSWORD; sleep 30; done\"]\n          env:\n            - name: DB_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: db-credentials\n                  key: password\nEOF\n\n## Wait for the Deployment to be ready\nkubectl rollout status deployment/secret-consumer --timeout=60s\n\n## Now trigger a rolling restart to pick up the updated Secret\nkubectl rollout restart deployment/secret-consumer\n\n## Wait for the rollout to complete\nkubectl rollout status deployment/secret-consumer --timeout=60s\n\n## Verify the new pods have the updated password\nkubectl logs deployment/secret-consumer | head -5\n</code></pre> <pre><code>## Strategy 2: Annotation-based trigger (useful in Helm/GitOps workflows)\n## Add a hash of the Secret data as a pod annotation.\n## When the Secret changes, the annotation changes, triggering a rollout.\nSECRET_HASH=$(kubectl get secret db-credentials -o jsonpath='{.data}' | sha256sum | cut -d' ' -f1)\n\nkubectl patch deployment secret-consumer \\\n  -p \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"secret-hash\\\":\\\"$SECRET_HASH\\\"}}}}}\"\n\n## Verify the annotation was added\nkubectl get deployment secret-consumer -o jsonpath='{.spec.template.metadata.annotations}' | python3 -m json.tool\n</code></pre> <pre><code>## Strategy 3: Automated rotation script\n## This script updates a Secret and triggers a rolling restart\n\ncat &lt;&lt;'SCRIPT'\n#!/bin/bash\n## rotate-secret.sh - Rotate a Secret and restart the consuming Deployment\n\nSECRET_NAME=\"${1:?Usage: rotate-secret.sh &lt;secret-name&gt; &lt;deployment-name&gt;}\"\nDEPLOYMENT_NAME=\"${2:?Usage: rotate-secret.sh &lt;secret-name&gt; &lt;deployment-name&gt;}\"\nNAMESPACE=\"${3:-secrets-lab}\"\n\n## Generate a new password\nNEW_PASSWORD=$(openssl rand -base64 24)\n\n## Update the Secret\nkubectl create secret generic \"$SECRET_NAME\" \\\n  --from-literal=username=admin \\\n  --from-literal=password=\"$NEW_PASSWORD\" \\\n  --from-literal=host=db.example.com \\\n  --from-literal=port=5432 \\\n  --namespace=\"$NAMESPACE\" \\\n  --dry-run=client -o yaml | kubectl apply -f -\n\necho \"Secret '$SECRET_NAME' updated with new password.\"\n\n## Trigger rolling restart\nkubectl rollout restart deployment/\"$DEPLOYMENT_NAME\" -n \"$NAMESPACE\"\necho \"Rolling restart triggered for deployment '$DEPLOYMENT_NAME'.\"\n\n## Wait for rollout\nkubectl rollout status deployment/\"$DEPLOYMENT_NAME\" -n \"$NAMESPACE\" --timeout=120s\necho \"Rotation complete.\"\nSCRIPT\n</code></pre>"},{"location":"32-Secrets/#step-11-enable-encryption-at-rest","title":"Step 11 - Enable Encryption at Rest","text":"<ul> <li>By default, Secrets are stored unencrypted in etcd.</li> <li>Encryption at rest ensures that Secrets are encrypted before being written to etcd.</li> <li>This step requires access to the control-plane node and the ability to modify kube-apiserver flags.</li> </ul> <p>Control Plane Access Required</p> <p>This step can only be performed on clusters where you have control-plane access (e.g., kubeadm clusters, bare-metal). Managed Kubernetes services (EKS, GKE, AKS) typically handle encryption at rest automatically or through their own configuration mechanisms.</p>"},{"location":"32-Secrets/#review-the-encryptionconfiguration","title":"Review the EncryptionConfiguration","text":"<pre><code>## View the example EncryptionConfiguration\ncat manifests/encryption-config.yaml\n</code></pre> <pre><code>## The EncryptionConfiguration specifies:\n## - Which resources to encrypt (secrets)\n## - Which encryption provider to use (aescbc)\n## - The encryption key (base64-encoded 32-byte key)\n## - A fallback identity provider (for reading unencrypted data)\n</code></pre>"},{"location":"32-Secrets/#generate-an-encryption-key","title":"Generate an Encryption Key","text":"<pre><code>## Generate a random 32-byte encryption key and base64-encode it\nhead -c 32 /dev/urandom | base64\n## Use this output as the 'secret' value in the EncryptionConfiguration\n</code></pre>"},{"location":"32-Secrets/#apply-encryption-kubeadm-clusters","title":"Apply Encryption (kubeadm clusters)","text":"<p>Cluster-Specific Instructions</p> <p>The following steps apply to kubeadm-based clusters. For managed Kubernetes services, consult your provider\u2019s documentation.</p> <pre><code>## 1. Copy the EncryptionConfiguration to the control-plane node\n## (Replace the key in the file with your generated key first!)\n## sudo cp encryption-config.yaml /etc/kubernetes/encryption-config.yaml\n\n## 2. Edit the kube-apiserver manifest to add the encryption flag\n## sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml\n## Add under spec.containers.command:\n##   - --encryption-provider-config=/etc/kubernetes/encryption-config.yaml\n## Add under spec.containers.volumeMounts:\n##   - name: encryption-config\n##     mountPath: /etc/kubernetes/encryption-config.yaml\n##     readOnly: true\n## Add under spec.volumes:\n##   - name: encryption-config\n##     hostPath:\n##       path: /etc/kubernetes/encryption-config.yaml\n##       type: File\n\n## 3. The kube-apiserver will restart automatically (it's a static Pod)\n## Wait for it to come back up\n## kubectl get pods -n kube-system -l component=kube-apiserver\n\n## 4. Re-encrypt all existing Secrets so they are stored encrypted\n## kubectl get secrets --all-namespaces -o json | kubectl replace -f -\n\n## 5. Verify encryption is working by checking etcd directly\n## ETCDCTL_API=3 etcdctl get /registry/secrets/default/my-secret \\\n##   --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n##   --cert=/etc/kubernetes/pki/etcd/server.crt \\\n##   --key=/etc/kubernetes/pki/etcd/server.key\n## The output should show encrypted (non-readable) data prefixed with \"k8s:enc:aescbc:v1:key1\"\n</code></pre> <p>Managed Kubernetes Encryption</p> <ul> <li>GKE: Secrets are encrypted at rest by default. You can use customer-managed encryption keys (CMEK) via Cloud KMS.</li> <li>EKS: Enable envelope encryption with AWS KMS through the <code>--encryption-config</code> flag in the cluster config.</li> <li>AKS: Enable encryption at rest with customer-managed keys through Azure Key Vault.</li> </ul>"},{"location":"32-Secrets/#exercises","title":"Exercises","text":"<p>The following exercises will test your understanding of Kubernetes Secrets. Try to solve each exercise on your own before revealing the solution.</p>"},{"location":"32-Secrets/#01-create-a-secret-from-multiple-files-and-verify-its-content","title":"01. Create a Secret from Multiple Files and Verify Its Content","text":"<p>Create a Secret named <code>multi-file-secret</code> from three separate files containing a username, password, and API token. Then verify you can retrieve and decode each value.</p>"},{"location":"32-Secrets/#scenario","title":"Scenario:","text":"<p>\u25e6 You have received three credential files from your security team. \u25e6 Each file contains a single credential value that must be stored in Kubernetes. \u25e6 You need to create a single Secret containing all three credentials.</p> <p>Hint: Use <code>kubectl create secret generic --from-file</code> with multiple <code>--from-file</code> flags or a directory.</p> Solution <pre><code>## Create temporary credential files\necho -n \"lab-admin\" &gt; /tmp/username\necho -n \"Ex3rc1se#1!\" &gt; /tmp/password\necho -n \"tok-abc123def456\" &gt; /tmp/api-token\n\n## Create the Secret from multiple files\nkubectl create secret generic multi-file-secret \\\n  --from-file=/tmp/username \\\n  --from-file=/tmp/password \\\n  --from-file=/tmp/api-token\n\n## Verify the Secret exists and has 3 data entries\nkubectl get secret multi-file-secret\n## DATA column should show 3\n\n## Decode each value to verify correctness\nkubectl get secret multi-file-secret -o jsonpath='{.data.username}' | base64 --decode\necho  ## Newline -- Output: lab-admin\n\nkubectl get secret multi-file-secret -o jsonpath='{.data.password}' | base64 --decode\necho  ## Newline -- Output: Ex3rc1se#1!\n\nkubectl get secret multi-file-secret -o jsonpath='{.data.api-token}' | base64 --decode\necho  ## Newline -- Output: tok-abc123def456\n\n## Clean up\nrm /tmp/username /tmp/password /tmp/api-token\nkubectl delete secret multi-file-secret\n</code></pre>"},{"location":"32-Secrets/#02-mount-a-secret-as-an-environment-variable-and-prove-the-pod-can-read-it","title":"02. Mount a Secret as an Environment Variable and Prove the Pod Can Read It","text":"<p>Create a Secret named <code>greeting-secret</code> with a key <code>message</code> containing the value <code>Hello from Kubernetes Secrets!</code>. Then create a Pod that reads this value as an environment variable and prints it.</p>"},{"location":"32-Secrets/#scenario_1","title":"Scenario:","text":"<p>\u25e6 Your application reads its greeting message from an environment variable named <code>GREETING</code>. \u25e6 The message is sensitive (perhaps it contains an internal URL) and should be stored in a Secret. \u25e6 You need to verify the application can read the value correctly.</p> <p>Hint: Use <code>secretKeyRef</code> in the Pod\u2019s <code>env</code> section and <code>kubectl logs</code> to verify.</p> Solution <pre><code>## Create the Secret with the greeting message\nkubectl create secret generic greeting-secret \\\n  --from-literal=message=\"Hello from Kubernetes Secrets!\"\n\n## Create a Pod that reads the Secret as an environment variable\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: greeting-pod\n  namespace: secrets-lab\nspec:\n  containers:\n    - name: greeter\n      image: busybox:1.36\n      command: [\"sh\", \"-c\", \"echo $GREETING\"]\n      env:\n        - name: GREETING\n          valueFrom:\n            secretKeyRef:\n              name: greeting-secret\n              key: message\n  restartPolicy: Never\nEOF\n\n## Wait for the Pod to complete\nkubectl wait --for=condition=Ready pod/greeting-pod --timeout=30s 2&gt;/dev/null || sleep 5\n\n## Check the logs -- should print the greeting\nkubectl logs greeting-pod\n## Output: Hello from Kubernetes Secrets!\n\n## Clean up\nkubectl delete pod greeting-pod\nkubectl delete secret greeting-secret\n</code></pre>"},{"location":"32-Secrets/#03-create-a-secret-with-stringdata-and-verify-it-gets-base64-encoded","title":"03. Create a Secret with <code>stringData</code> and Verify It Gets Base64-Encoded","text":"<p>Create a Secret declaratively using the <code>stringData</code> field with a username of <code>developer</code> and a password of <code>PlainText123</code>. After applying, verify that the values are stored as base64 in the <code>data</code> field.</p>"},{"location":"32-Secrets/#scenario_2","title":"Scenario:","text":"<p>\u25e6 You want to create a Secret using plain-text values for convenience. \u25e6 You need to confirm that Kubernetes correctly encodes the values before storing them. \u25e6 Understanding this encoding behavior is essential for debugging Secret issues.</p> <p>Hint: Use <code>kubectl apply</code> with a YAML manifest containing <code>stringData</code>, then inspect with <code>kubectl get secret -o yaml</code>.</p> Solution <pre><code>## Create the Secret using stringData\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: stringdata-demo\n  namespace: secrets-lab\ntype: Opaque\nstringData:\n  username: developer\n  password: PlainText123\nEOF\n\n## View the Secret -- notice stringData is converted to data with base64\nkubectl get secret stringdata-demo -o yaml\n\n## Manually verify the base64 encoding\necho -n \"developer\" | base64\n## Output: ZGV2ZWxvcGVy\n\necho -n \"PlainText123\" | base64\n## Output: UGxhaW5UZXh0MTIz\n\n## Compare with what Kubernetes stored\nkubectl get secret stringdata-demo -o jsonpath='{.data.username}'\necho  ## Should output: ZGV2ZWxvcGVy\n\nkubectl get secret stringdata-demo -o jsonpath='{.data.password}'\necho  ## Should output: UGxhaW5UZXh0MTIz\n\n## Decode to confirm round-trip integrity\nkubectl get secret stringdata-demo -o jsonpath='{.data.username}' | base64 --decode\necho  ## Output: developer\n\nkubectl get secret stringdata-demo -o jsonpath='{.data.password}' | base64 --decode\necho  ## Output: PlainText123\n\n## Clean up\nkubectl delete secret stringdata-demo\n</code></pre>"},{"location":"32-Secrets/#04-mount-a-secret-as-a-volume-and-read-the-file-inside-the-pod","title":"04. Mount a Secret as a Volume and Read the File Inside the Pod","text":"<p>Create a Secret named <code>config-secret</code> with two keys: <code>db.conf</code> containing <code>host=db.local\\nport=5432</code> and <code>cache.conf</code> containing <code>host=redis.local\\nport=6379</code>. Mount it as a volume at <code>/etc/config</code> and read both files from inside the Pod.</p>"},{"location":"32-Secrets/#scenario_3","title":"Scenario:","text":"<p>\u25e6 Your application reads configuration from files in <code>/etc/config/</code>. \u25e6 Some configuration values are sensitive (database and cache connection details). \u25e6 You need the files to appear as regular files that the application can read.</p> <p>Hint: Create the Secret with <code>--from-literal</code>, mount as a volume with <code>secret.secretName</code>, and use <code>kubectl exec</code> to read files.</p> Solution <pre><code>## Create the Secret with configuration data\nkubectl create secret generic config-secret \\\n  --from-literal=db.conf=$'host=db.local\\nport=5432' \\\n  --from-literal=cache.conf=$'host=redis.local\\nport=6379'\n\n## Create a Pod that mounts the Secret as a volume\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: config-reader\n  namespace: secrets-lab\nspec:\n  containers:\n    - name: reader\n      image: busybox:1.36\n      command: [\"sh\", \"-c\", \"sleep 3600\"]\n      volumeMounts:\n        - name: config-vol\n          mountPath: /etc/config\n          readOnly: true\n  volumes:\n    - name: config-vol\n      secret:\n        secretName: config-secret\n  restartPolicy: Never\nEOF\n\n## Wait for the Pod\nkubectl wait --for=condition=Ready pod/config-reader --timeout=60s\n\n## List the files in the mounted directory\nkubectl exec config-reader -- ls -la /etc/config/\n\n## Read the database configuration\nkubectl exec config-reader -- cat /etc/config/db.conf\n## Output:\n## host=db.local\n## port=5432\n\n## Read the cache configuration\nkubectl exec config-reader -- cat /etc/config/cache.conf\n## Output:\n## host=redis.local\n## port=6379\n\n## Clean up\nkubectl delete pod config-reader\nkubectl delete secret config-secret\n</code></pre>"},{"location":"32-Secrets/#05-create-a-tls-secret-and-mount-it-in-an-nginx-pod","title":"05. Create a TLS Secret and Mount It in an Nginx Pod","text":"<p>Generate a self-signed TLS certificate, create a <code>kubernetes.io/tls</code> Secret, and deploy an nginx Pod that serves HTTPS traffic using the certificate.</p>"},{"location":"32-Secrets/#scenario_4","title":"Scenario:","text":"<p>\u25e6 Your team needs to test HTTPS termination at the Pod level. \u25e6 You must generate a self-signed certificate, store it as a Kubernetes TLS Secret, and verify nginx can serve HTTPS responses.</p> <p>Hint: Use <code>openssl req</code> to generate the cert/key, <code>kubectl create secret tls</code> to create the Secret, and <code>kubectl exec</code> with <code>curl -k</code> to test.</p> Solution <pre><code>## Generate a self-signed certificate\nopenssl req -x509 -nodes -days 30 \\\n  -newkey rsa:2048 \\\n  -keyout /tmp/exercise-tls.key \\\n  -out /tmp/exercise-tls.crt \\\n  -subj \"/CN=exercise.local/O=exercise\"\n\n## Create the TLS Secret\nkubectl create secret tls exercise-tls-secret \\\n  --cert=/tmp/exercise-tls.crt \\\n  --key=/tmp/exercise-tls.key\n\n## Verify the Secret type\nkubectl get secret exercise-tls-secret\n## TYPE should be kubernetes.io/tls\n\n## Create an nginx ConfigMap for HTTPS\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: exercise-nginx-conf\n  namespace: secrets-lab\ndata:\n  default.conf: |\n    server {\n        listen 443 ssl;\n        ssl_certificate     /etc/nginx/ssl/tls.crt;\n        ssl_certificate_key /etc/nginx/ssl/tls.key;\n        location / {\n            return 200 'Exercise 05: TLS is working!\\n';\n            add_header Content-Type text/plain;\n        }\n    }\nEOF\n\n## Deploy nginx with the TLS Secret\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: exercise-nginx-tls\n  namespace: secrets-lab\nspec:\n  containers:\n    - name: nginx\n      image: nginx:1.25-alpine\n      ports:\n        - containerPort: 443\n      volumeMounts:\n        - name: tls-certs\n          mountPath: /etc/nginx/ssl\n          readOnly: true\n        - name: nginx-config\n          mountPath: /etc/nginx/conf.d\n          readOnly: true\n  volumes:\n    - name: tls-certs\n      secret:\n        secretName: exercise-tls-secret\n    - name: nginx-config\n      configMap:\n        name: exercise-nginx-conf\n  restartPolicy: Never\nEOF\n\n## Wait for the Pod\nkubectl wait --for=condition=Ready pod/exercise-nginx-tls --timeout=60s\n\n## Test HTTPS\nkubectl exec exercise-nginx-tls -- curl -k https://localhost:443 2&gt;/dev/null\n## Output: Exercise 05: TLS is working!\n\n## Clean up\nrm /tmp/exercise-tls.crt /tmp/exercise-tls.key\nkubectl delete pod exercise-nginx-tls\nkubectl delete configmap exercise-nginx-conf\nkubectl delete secret exercise-tls-secret\n</code></pre>"},{"location":"32-Secrets/#06-create-an-imagepullsecret-and-reference-it-in-a-deployment","title":"06. Create an imagePullSecret and Reference It in a Deployment","text":"<p>Create a <code>docker-registry</code> Secret for a fictional private registry and create a Deployment that references it via <code>imagePullSecrets</code>.</p>"},{"location":"32-Secrets/#scenario_5","title":"Scenario:","text":"<p>\u25e6 Your organization uses a private container registry at <code>registry.internal.example.com</code>. \u25e6 All Pods pulling from this registry need credentials. \u25e6 You need to set up the credentials and reference them in a Deployment.</p> <p>Hint: Use <code>kubectl create secret docker-registry</code> and add <code>imagePullSecrets</code> to the Deployment\u2019s Pod template spec.</p> Solution <pre><code>## Create the docker-registry Secret\nkubectl create secret docker-registry internal-registry \\\n  --docker-server=registry.internal.example.com \\\n  --docker-username=deploy-bot \\\n  --docker-password='R3g1stryP@ss!' \\\n  --docker-email=deploy@example.com\n\n## Verify the Secret\nkubectl get secret internal-registry\n## TYPE should be kubernetes.io/dockerconfigjson\n\n## Create a Deployment referencing the imagePullSecret\n## (This will fail to pull since the registry is fictional, but the\n## configuration is correct -- check the Pod spec for the reference)\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: private-app\n  namespace: secrets-lab\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: private-app\n  template:\n    metadata:\n      labels:\n        app: private-app\n    spec:\n      containers:\n        - name: app\n          ## Using nginx as a stand-in; in production this would be\n          ## registry.internal.example.com/my-app:latest\n          image: nginx:1.25-alpine\n          ports:\n            - containerPort: 80\n      imagePullSecrets:\n        - name: internal-registry\nEOF\n\n## Verify the Deployment references the imagePullSecret\nkubectl get deployment private-app -o jsonpath='{.spec.template.spec.imagePullSecrets}' | python3 -m json.tool\n\n## Clean up\nkubectl delete deployment private-app\nkubectl delete secret internal-registry\n</code></pre>"},{"location":"32-Secrets/#07-update-a-secret-and-verify-the-volume-mount-reflects-the-change","title":"07. Update a Secret and Verify the Volume Mount Reflects the Change","text":"<p>Create a Secret, mount it as a volume in a Pod, update the Secret, and verify that the file content inside the Pod changes automatically.</p>"},{"location":"32-Secrets/#scenario_6","title":"Scenario:","text":"<p>\u25e6 You need to update an API key without restarting the application. \u25e6 Volume-mounted Secrets are automatically refreshed by the kubelet. \u25e6 You need to prove this auto-update behavior works.</p> <p>Hint: Create a Secret and a long-running Pod with a volume mount. Use <code>kubectl create --dry-run=client -o yaml | kubectl apply -f -</code> to update the Secret, then wait and re-read the file.</p> Solution <pre><code>## Create the initial Secret\nkubectl create secret generic rotating-secret \\\n  --from-literal=api-key=\"original-key-v1\"\n\n## Create a long-running Pod that mounts the Secret\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-watcher\n  namespace: secrets-lab\nspec:\n  containers:\n    - name: watcher\n      image: busybox:1.36\n      command: [\"sh\", \"-c\", \"while true; do echo \\\"$(date): $(cat /etc/secrets/api-key)\\\"; sleep 10; done\"]\n      volumeMounts:\n        - name: secret-vol\n          mountPath: /etc/secrets\n          readOnly: true\n  volumes:\n    - name: secret-vol\n      secret:\n        secretName: rotating-secret\n  restartPolicy: Never\nEOF\n\n## Wait for the Pod\nkubectl wait --for=condition=Ready pod/secret-watcher --timeout=60s\n\n## Verify the current value\nkubectl exec secret-watcher -- cat /etc/secrets/api-key\n## Output: original-key-v1\n\n## Update the Secret with a new value\nkubectl create secret generic rotating-secret \\\n  --from-literal=api-key=\"rotated-key-v2\" \\\n  --dry-run=client -o yaml | kubectl apply -f -\n\n## Wait for the kubelet to sync (up to ~2 minutes)\necho \"Waiting for kubelet to sync the updated Secret...\"\nsleep 120\n\n## Verify the value has been updated inside the Pod\nkubectl exec secret-watcher -- cat /etc/secrets/api-key\n## Output: rotated-key-v2\n\n## Check the logs for the transition\nkubectl logs secret-watcher | tail -20\n\n## Clean up\nkubectl delete pod secret-watcher\nkubectl delete secret rotating-secret\n</code></pre>"},{"location":"32-Secrets/#08-create-an-immutable-secret-and-try-to-modify-it","title":"08. Create an Immutable Secret and Try to Modify It","text":"<p>Create a Secret marked as <code>immutable: true</code> and attempt to update its data. Verify that the update is rejected by the API server.</p>"},{"location":"32-Secrets/#scenario_7","title":"Scenario:","text":"<p>\u25e6 Your compliance team requires that production API keys cannot be modified after deployment. \u25e6 You need to prove that the immutable flag prevents changes. \u25e6 You also need to understand the only way to change an immutable Secret.</p> <p>Hint: Create a Secret with <code>immutable: true</code> in the YAML, then try <code>kubectl patch</code> or <code>kubectl edit</code> to modify it.</p> Solution <pre><code>## Create an immutable Secret\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: locked-secret\n  namespace: secrets-lab\ntype: Opaque\nimmutable: true\nstringData:\n  token: \"immutable-token-abc123\"\nEOF\n\n## Verify it's immutable\nkubectl get secret locked-secret -o jsonpath='{.immutable}'\necho  ## Output: true\n\n## Attempt to modify the Secret data -- this WILL FAIL\nkubectl patch secret locked-secret \\\n  --type='json' \\\n  -p='[{\"op\": \"replace\", \"path\": \"/stringData\", \"value\": {\"token\": \"new-token\"}}]' 2&gt;&amp;1 || true\n## Expected error: field is immutable when `immutable` is set\n\n## Attempt to add a new key -- this WILL also FAIL\nkubectl patch secret locked-secret \\\n  --type='merge' \\\n  -p='{\"stringData\": {\"new-key\": \"new-value\"}}' 2&gt;&amp;1 || true\n\n## The ONLY way to change it is delete + recreate\nkubectl delete secret locked-secret\n\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: locked-secret\n  namespace: secrets-lab\ntype: Opaque\nimmutable: true\nstringData:\n  token: \"new-immutable-token-xyz789\"\nEOF\n\n## Verify the new value\nkubectl get secret locked-secret -o jsonpath='{.data.token}' | base64 --decode\necho  ## Output: new-immutable-token-xyz789\n\n## Clean up\nkubectl delete secret locked-secret\n</code></pre>"},{"location":"32-Secrets/#09-use-projected-volumes-to-combine-a-secret-and-a-configmap","title":"09. Use Projected Volumes to Combine a Secret and a ConfigMap","text":"<p>Create a Secret and a ConfigMap, then mount both into a single directory in a Pod using a projected volume. Verify all files appear under the same mount path.</p>"},{"location":"32-Secrets/#scenario_8","title":"Scenario:","text":"<p>\u25e6 Your application expects all configuration (sensitive and non-sensitive) in <code>/app/config/</code>. \u25e6 You cannot have two separate mount paths \u2013 the application reads from a single directory. \u25e6 You need to combine a Secret and a ConfigMap into one unified mount.</p> <p>Hint: Use a <code>projected</code> volume with <code>sources</code> containing both <code>secret</code> and <code>configMap</code> entries.</p> Solution <pre><code>## Create the Secret\nkubectl create secret generic app-secret \\\n  --from-literal=db-password=\"pr0j3ct3d!\"\n\n## Create the ConfigMap\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-settings\n  namespace: secrets-lab\ndata:\n  app.env: \"production\"\n  log.level: \"warn\"\nEOF\n\n## Create a Pod with a projected volume combining both\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: projected-demo\n  namespace: secrets-lab\nspec:\n  containers:\n    - name: app\n      image: busybox:1.36\n      command: [\"sh\", \"-c\", \"ls -la /app/config/ &amp;&amp; echo '---' &amp;&amp; for f in /app/config/*; do echo \\\"$f: $(cat $f)\\\"; done &amp;&amp; sleep 3600\"]\n      volumeMounts:\n        - name: all-config\n          mountPath: /app/config\n          readOnly: true\n  volumes:\n    - name: all-config\n      projected:\n        sources:\n          - secret:\n              name: app-secret\n              items:\n                - key: db-password\n                  path: db-password\n          - configMap:\n              name: app-settings\n              items:\n                - key: app.env\n                  path: app.env\n                - key: log.level\n                  path: log.level\n  restartPolicy: Never\nEOF\n\n## Wait and check the logs\nkubectl wait --for=condition=Ready pod/projected-demo --timeout=60s\nkubectl logs projected-demo\n\n## Expected output shows all three files under /app/config/:\n## db-password (from Secret)\n## app.env (from ConfigMap)\n## log.level (from ConfigMap)\n\n## Verify individual files\nkubectl exec projected-demo -- cat /app/config/db-password\n## Output: pr0j3ct3d!\n\nkubectl exec projected-demo -- cat /app/config/app.env\n## Output: production\n\n## Clean up\nkubectl delete pod projected-demo\nkubectl delete secret app-secret\nkubectl delete configmap app-settings\n</code></pre>"},{"location":"32-Secrets/#10-decode-all-values-in-a-secret-using-a-one-liner","title":"10. Decode All Values in a Secret Using a One-Liner","text":"<p>Given a Secret named <code>multi-key-secret</code> with several keys, write a single command that decodes and displays all key-value pairs.</p>"},{"location":"32-Secrets/#scenario_9","title":"Scenario:","text":"<p>\u25e6 You are debugging an application and need to quickly inspect all values in a Secret. \u25e6 Running separate <code>jsonpath</code> + <code>base64 --decode</code> for each key is tedious. \u25e6 You need a single command that outputs all decoded key-value pairs.</p> <p>Hint: Use <code>kubectl get secret -o json</code> piped to a tool that iterates over the <code>data</code> map and decodes each value.</p> Solution <pre><code>## Create a Secret with multiple keys for testing\nkubectl create secret generic multi-key-secret \\\n  --from-literal=key1=\"value-one\" \\\n  --from-literal=key2=\"value-two\" \\\n  --from-literal=key3=\"value-three\" \\\n  --from-literal=api-token=\"tok-abc123\"\n\n## Method 1: Using python3 (available on most systems)\nkubectl get secret multi-key-secret -o json | \\\n  python3 -c \"\nimport json, sys, base64\ndata = json.load(sys.stdin)['data']\nfor k, v in sorted(data.items()):\n    print(f'{k}: {base64.b64decode(v).decode()}')\n\"\n\n## Method 2: Using kubectl jsonpath + bash loop\nfor key in $(kubectl get secret multi-key-secret -o jsonpath='{.data}' | python3 -c \"import json,sys; [print(k) for k in json.load(sys.stdin)]\"); do\n  value=$(kubectl get secret multi-key-secret -o jsonpath=\"{.data.$key}\" | base64 --decode)\n  echo \"$key: $value\"\ndone\n\n## Method 3: Using go-template\nkubectl get secret multi-key-secret -o go-template='{{range $k, $v := .data}}{{$k}}: {{$v | base64decode}}\n{{end}}'\n\n## Expected output (any method):\n## api-token: tok-abc123\n## key1: value-one\n## key2: value-two\n## key3: value-three\n\n## Clean up\nkubectl delete secret multi-key-secret\n</code></pre>"},{"location":"32-Secrets/#11-create-a-secret-with-special-characters-in-values","title":"11. Create a Secret with Special Characters in Values","text":"<p>Create a Secret containing values with special characters: backslashes, quotes, dollar signs, newlines, and unicode characters. Verify they survive the encoding/decoding round-trip.</p>"},{"location":"32-Secrets/#scenario_10","title":"Scenario:","text":"<p>\u25e6 A database password generated by your security tool contains complex special characters. \u25e6 You need to ensure these characters are not corrupted during Secret creation and retrieval. \u25e6 Shell escaping and base64 encoding can sometimes mangle special characters.</p> <p>Hint: Use <code>--from-file</code> with a file containing the exact value to avoid shell escaping issues, or use <code>stringData</code> in a YAML manifest.</p> Solution <pre><code>## Method 1: Using --from-file to avoid shell escaping entirely\nprintf 'P@$$w0rd\\n\"quotes\"\\tand\\\\backslash' &gt; /tmp/special-chars.txt\n\nkubectl create secret generic special-secret \\\n  --from-file=complex-value=/tmp/special-chars.txt\n\n## Verify the round-trip\nkubectl get secret special-secret -o jsonpath='{.data.complex-value}' | base64 --decode\n## Output should exactly match the original value\n\n## Method 2: Using stringData in a YAML manifest for more control\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: special-yaml-secret\n  namespace: secrets-lab\ntype: Opaque\nstringData:\n  password: 'P@$$w0rd!\"quotes\"&amp;&lt;xml&gt;'\n  multiline: |\n    line1: value1\n    line2: value2\n    line3: \"quoted value\"\n  json-config: '{\"key\": \"value with \\\"quotes\\\" and $pecial chars\"}'\nEOF\n\n## Verify each key\nkubectl get secret special-yaml-secret -o jsonpath='{.data.password}' | base64 --decode\necho\n## Output: P@$$w0rd!\"quotes\"&amp;&lt;xml&gt;\n\nkubectl get secret special-yaml-secret -o jsonpath='{.data.multiline}' | base64 --decode\n## Output should preserve the multi-line format\n\nkubectl get secret special-yaml-secret -o jsonpath='{.data.json-config}' | base64 --decode\necho\n## Output: {\"key\": \"value with \\\"quotes\\\" and $pecial chars\"}\n\n## Clean up\nrm /tmp/special-chars.txt\nkubectl delete secret special-secret\nkubectl delete secret special-yaml-secret\n</code></pre>"},{"location":"32-Secrets/#12-write-a-script-that-rotates-a-secret-and-triggers-a-rolling-restart","title":"12. Write a Script That Rotates a Secret and Triggers a Rolling Restart","text":"<p>Write a shell script that generates a new random password, updates a Secret, and triggers a rolling restart of the consuming Deployment. Verify the Pods pick up the new credential.</p>"},{"location":"32-Secrets/#scenario_11","title":"Scenario:","text":"<p>\u25e6 Your security policy requires rotating database passwords every 30 days. \u25e6 The rotation must be automated and must not cause downtime. \u25e6 After rotation, all Pods must pick up the new password via a rolling restart.</p> <p>Hint: Use <code>openssl rand</code> to generate a password, <code>kubectl create secret --dry-run=client -o yaml | kubectl apply -f -</code> to update, and <code>kubectl rollout restart</code> to trigger the restart.</p> Solution <pre><code>## Create the initial Secret and Deployment\nkubectl create secret generic rotatable-db-creds \\\n  --from-literal=username=app-user \\\n  --from-literal=password=\"initial-password-v1\"\n\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: db-consumer\n  namespace: secrets-lab\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: db-consumer\n  template:\n    metadata:\n      labels:\n        app: db-consumer\n    spec:\n      containers:\n        - name: app\n          image: busybox:1.36\n          command: [\"sh\", \"-c\", \"while true; do echo \\\"password=$DB_PASSWORD\\\"; sleep 30; done\"]\n          env:\n            - name: DB_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: rotatable-db-creds\n                  key: password\nEOF\n\n## Wait for the Deployment to be ready\nkubectl rollout status deployment/db-consumer --timeout=60s\n\n## Verify the current password\nkubectl logs deployment/db-consumer | head -1\n## Output: password=initial-password-v1\n\n## --- The rotation script ---\n## Generate a new random password\nNEW_PASSWORD=$(openssl rand -base64 24)\necho \"New password: $NEW_PASSWORD\"\n\n## Update the Secret using the dry-run + apply pattern\nkubectl create secret generic rotatable-db-creds \\\n  --from-literal=username=app-user \\\n  --from-literal=password=\"$NEW_PASSWORD\" \\\n  --dry-run=client -o yaml | kubectl apply -f -\n\n## Verify the Secret was updated\nkubectl get secret rotatable-db-creds -o jsonpath='{.data.password}' | base64 --decode\necho  ## Should match $NEW_PASSWORD\n\n## Trigger a rolling restart so Pods pick up the new env var value\nkubectl rollout restart deployment/db-consumer\n\n## Wait for the rollout to complete\nkubectl rollout status deployment/db-consumer --timeout=120s\n\n## Verify the new Pods have the updated password\nsleep 5\nkubectl logs deployment/db-consumer | head -1\n## Output: password=&lt;new-random-password&gt;\n\n## Clean up\nkubectl delete deployment db-consumer\nkubectl delete secret rotatable-db-creds\n</code></pre>"},{"location":"32-Secrets/#finalize-cleanup","title":"Finalize &amp; Cleanup","text":"<ul> <li>To remove all resources created by this lab, delete the <code>secrets-lab</code> namespace:</li> </ul> <pre><code>## Delete the entire namespace (this removes ALL resources within it)\nkubectl delete namespace secrets-lab\n</code></pre> <ul> <li>Reset your kubectl context to the default namespace:</li> </ul> <pre><code>## Switch back to the default namespace\nkubectl config set-context --current --namespace=default\n</code></pre> <ul> <li>(Optional) Remove any local temporary files created during the lab:</li> </ul> <pre><code>## Clean up any leftover temporary files\nrm -f /tmp/username.txt /tmp/password.txt /tmp/app-secrets.env\nrm -f /tmp/tls.crt /tmp/tls.key\nrm -f /tmp/exercise-tls.crt /tmp/exercise-tls.key\nrm -f /tmp/special-chars.txt\n</code></pre>"},{"location":"32-Secrets/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Secret not found:</li> </ul> <p>Verify the Secret exists in the correct namespace:</p> <pre><code>## List Secrets in the current namespace\nkubectl get secrets -n secrets-lab\n\n## Check if you are in the right namespace\nkubectl config view --minify -o jsonpath='{.contexts[0].context.namespace}'\n</code></pre> <p></p> <ul> <li>Base64 decode errors:</li> </ul> <p>Ensure values were encoded without trailing newlines:</p> <pre><code>## Correct: no trailing newline\necho -n \"myvalue\" | base64\n\n## Incorrect: includes trailing newline (will cause issues)\necho \"myvalue\" | base64\n</code></pre> <p></p> <ul> <li>Pod cannot access Secret:</li> </ul> <p>Check RBAC permissions and ensure the Secret name matches:</p> <pre><code>## Verify the Secret exists\nkubectl get secret &lt;secret-name&gt; -n secrets-lab\n\n## Check Pod events for errors\nkubectl describe pod &lt;pod-name&gt; -n secrets-lab | grep -A10 \"Events:\"\n\n## Common error: \"secret not found\" -- verify the name matches exactly\nkubectl get pod &lt;pod-name&gt; -n secrets-lab -o yaml | grep secretName\n</code></pre> <p></p> <ul> <li>Volume-mounted Secret not updating:</li> </ul> <p>The kubelet syncs Secret updates with a configurable period (default: up to ~1 minute plus cache propagation delay). If using subPath mounts, the file is never updated:</p> <pre><code>## Check kubelet sync period (on the node)\n## Default is --sync-frequency=1m\n\n## IMPORTANT: subPath volume mounts do NOT receive automatic updates\n## Avoid using subPath with Secrets if you need auto-rotation\n</code></pre> <p></p> <ul> <li>Immutable Secret cannot be updated:</li> </ul> <p>This is expected behavior. Delete and recreate the Secret:</p> <pre><code>## Delete the immutable Secret\nkubectl delete secret &lt;secret-name&gt; -n secrets-lab\n\n## Recreate it with updated values\nkubectl apply -f &lt;manifest.yaml&gt;\n</code></pre> <p></p> <ul> <li>TLS Secret creation fails:</li> </ul> <p>Ensure the certificate and key files are valid and match:</p> <pre><code>## Verify the certificate\nopenssl x509 -in /tmp/tls.crt -text -noout\n\n## Verify the key\nopenssl rsa -in /tmp/tls.key -check\n\n## Verify the cert and key match (modulus should be identical)\nopenssl x509 -noout -modulus -in /tmp/tls.crt | openssl md5\nopenssl rsa -noout -modulus -in /tmp/tls.key | openssl md5\n</code></pre> <p></p> <ul> <li>imagePullSecrets not working:</li> </ul> <p>Verify the Secret type and the registry URL:</p> <pre><code>## The Secret type must be kubernetes.io/dockerconfigjson\nkubectl get secret &lt;secret-name&gt; -o jsonpath='{.type}'\n\n## Decode and verify the registry URL matches your image registry\nkubectl get secret &lt;secret-name&gt; -o jsonpath='{.data.\\.dockerconfigjson}' | base64 --decode | python3 -m json.tool\n</code></pre>"},{"location":"32-Secrets/#next-steps","title":"Next Steps","text":"<ul> <li>Explore External Secrets Operator to sync Secrets from external secret managers (AWS Secrets Manager, HashiCorp Vault, Azure Key Vault, GCP Secret Manager).</li> <li>Try Sealed Secrets by Bitnami to safely store encrypted Secrets in Git repositories.</li> <li>Learn about HashiCorp Vault and the Vault Agent Injector for dynamic secret management in Kubernetes.</li> <li>Set up cert-manager to automate TLS certificate issuance and renewal with Let\u2019s Encrypt.</li> <li>Implement Kubernetes RBAC policies to restrict Secret access to only the ServiceAccounts and users that need it.</li> <li>Enable Kubernetes Audit Logging to monitor who accesses Secrets in your cluster.</li> <li>Explore SOPS (Secrets OPerationS) for encrypting Secret manifests before committing to Git.</li> <li>Try the CSI Secrets Store Driver to mount Secrets from external stores as volumes without Kubernetes Secret objects.</li> </ul>"},{"location":"33-NetworkPolicies/","title":"NetworkPolicies - Pod-Level Firewall Rules","text":"<ul> <li>In this lab we will learn how to use Kubernetes NetworkPolicies to control traffic flow between pods, namespaces, and external endpoints - effectively creating firewall rules at the pod level.</li> </ul>"},{"location":"33-NetworkPolicies/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What NetworkPolicies are and how they work</li> <li>Default behavior: all pods can talk to all pods (open by default)</li> <li>How to create ingress and egress rules</li> <li>How to isolate namespaces from each other</li> <li>How to allow traffic only from specific pods using <code>podSelector</code></li> <li>How to allow traffic only from specific namespaces using <code>namespaceSelector</code></li> <li>How to restrict egress to specific CIDR blocks</li> <li>How to implement a default-deny policy</li> <li>Testing network policies with real traffic</li> </ul>"},{"location":"33-NetworkPolicies/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link Network Policies kubernetes.io/docs Declare Network Policy kubernetes.io/docs NetworkPolicy API Reference kubernetes.io/docs"},{"location":"33-NetworkPolicies/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster with a CNI plugin that supports NetworkPolicies (e.g., Calico, Cilium, Weave Net)</li> <li><code>kubectl</code> configured against the cluster</li> </ul> <p>Important: CNI Plugin Required</p> <p>The default Kind/Minikube cluster with the default CNI (kindnet/bridge) does not enforce NetworkPolicies. You need a CNI that supports them:</p> Kind with CalicoMinikube with Calico <pre><code># Create a Kind cluster without default CNI\nkind create cluster --name netpol-lab --config manifests/kind-config.yaml\n\n# Install Calico\nkubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml\n\n# Wait for Calico to be ready\nkubectl wait --for=condition=Ready pods -l k8s-app=calico-node -n kube-system --timeout=120s\n</code></pre> <pre><code>minikube start --cni=calico\n</code></pre>"},{"location":"33-NetworkPolicies/#networkpolicy-overview","title":"NetworkPolicy Overview","text":"<pre><code>graph TB\n    subgraph cluster[\"Kubernetes Cluster\"]\n        subgraph ns_a[\"Namespace: frontend\"]\n            pod_fe[\"Pod: frontend\\nlabel: app=frontend\"]\n        end\n\n        subgraph ns_b[\"Namespace: backend\"]\n            pod_be[\"Pod: backend\\nlabel: app=backend\"]\n            pod_db[\"Pod: database\\nlabel: app=database\"]\n        end\n\n        np[\"NetworkPolicy\\non: backend namespace\\nallow ingress from:\\n  app=frontend\"]\n    end\n\n    pod_fe -- \"\u2705 Allowed\" --&gt; pod_be\n    pod_be -- \"\u2705 Allowed\" --&gt; pod_db\n    pod_fe -. \"\u274c Denied\" .-&gt; pod_db\n\n    np -.-&gt; pod_be\n    np -.-&gt; pod_db</code></pre> Concept Description Ingress rule Controls incoming traffic to selected pods Egress rule Controls outgoing traffic from selected pods podSelector Selects which pods the policy applies to (by labels) Default deny When a policy selects a pod, all non-matching traffic is denied"},{"location":"33-NetworkPolicies/#01-setup-create-namespaces-and-test-pods","title":"01. Setup: Create namespaces and test pods","text":"<pre><code># Clean up\nkubectl delete namespace netpol-frontend netpol-backend --ignore-not-found\n\n# Create namespaces with labels (needed for namespaceSelector)\nkubectl create namespace netpol-frontend\nkubectl label namespace netpol-frontend role=frontend\n\nkubectl create namespace netpol-backend\nkubectl label namespace netpol-backend role=backend\n</code></pre> <p>Deploy test workloads:</p> <pre><code># manifests/test-pods.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web\n  namespace: netpol-backend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:alpine\n          ports:\n            - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: web\n  namespace: netpol-backend\nspec:\n  selector:\n    app: web\n  ports:\n    - port: 80\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: client\n  namespace: netpol-frontend\n  labels:\n    app: client\nspec:\n  containers:\n    - name: curl\n      image: curlimages/curl:latest\n      command: [\"sleep\", \"3600\"]\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: rogue\n  namespace: netpol-frontend\n  labels:\n    app: rogue\nspec:\n  containers:\n    - name: curl\n      image: curlimages/curl:latest\n      command: [\"sleep\", \"3600\"]\n</code></pre> <pre><code>kubectl apply -f manifests/test-pods.yaml\n\n# Wait for pods to be ready\nkubectl wait --for=condition=Ready pod/client -n netpol-frontend --timeout=60s\nkubectl wait --for=condition=Ready pod/rogue -n netpol-frontend --timeout=60s\nkubectl wait --for=condition=Ready -l app=web pod -n netpol-backend --timeout=60s\n</code></pre>"},{"location":"33-NetworkPolicies/#02-verify-default-behavior-all-open","title":"02. Verify: Default behavior (all-open)","text":"<p>Without any NetworkPolicy, all pods can communicate:</p> <pre><code># client can reach the web service\nkubectl exec client -n netpol-frontend -- \\\n  curl -s --max-time 3 web.netpol-backend.svc.cluster.local\n# Expected: nginx welcome page HTML\n\n# rogue can also reach it\nkubectl exec rogue -n netpol-frontend -- \\\n  curl -s --max-time 3 web.netpol-backend.svc.cluster.local\n# Expected: nginx welcome page HTML\n</code></pre>"},{"location":"33-NetworkPolicies/#03-default-deny-all-ingress","title":"03. Default Deny All Ingress","text":"<p>Apply a default deny policy - blocks ALL incoming traffic to pods in <code>netpol-backend</code>:</p> <pre><code># manifests/default-deny-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-ingress\n  namespace: netpol-backend\nspec:\n  podSelector: {}    # Selects ALL pods in this namespace\n  policyTypes:\n    - Ingress        # No ingress rules = deny all incoming traffic\n</code></pre> <pre><code>kubectl apply -f manifests/default-deny-ingress.yaml\n</code></pre> <p>Test - both pods should now be blocked:</p> <pre><code># Both should time out\nkubectl exec client -n netpol-frontend -- \\\n  curl -s --max-time 3 web.netpol-backend.svc.cluster.local\n# Expected: timeout / connection refused\n\nkubectl exec rogue -n netpol-frontend -- \\\n  curl -s --max-time 3 web.netpol-backend.svc.cluster.local\n# Expected: timeout / connection refused\n</code></pre>"},{"location":"33-NetworkPolicies/#04-allow-traffic-from-specific-pods-podselector","title":"04. Allow traffic from specific pods (podSelector)","text":"<p>Allow only pods with label <code>app=client</code> from the <code>netpol-frontend</code> namespace:</p> <pre><code># manifests/allow-client-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-client-ingress\n  namespace: netpol-backend\nspec:\n  podSelector:\n    matchLabels:\n      app: web\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              role: frontend\n          podSelector:\n            matchLabels:\n              app: client\n      ports:\n        - protocol: TCP\n          port: 80\n</code></pre> <pre><code>kubectl apply -f manifests/allow-client-ingress.yaml\n</code></pre> <p>AND vs OR in NetworkPolicy selectors</p> <p>When <code>namespaceSelector</code> and <code>podSelector</code> are in the same <code>from</code> entry (no dash between them), they are combined with AND - both must match. If they were separate entries (each with its own dash), they would be OR.</p> <p>Test:</p> <pre><code># client should work (matches app=client in frontend namespace)\nkubectl exec client -n netpol-frontend -- \\\n  curl -s --max-time 3 web.netpol-backend.svc.cluster.local\n# Expected: nginx welcome page HTML \u2705\n\n# rogue should still be blocked (has app=rogue, not app=client)\nkubectl exec rogue -n netpol-frontend -- \\\n  curl -s --max-time 3 web.netpol-backend.svc.cluster.local\n# Expected: timeout \u274c\n</code></pre>"},{"location":"33-NetworkPolicies/#05-egress-policy-restrict-outgoing-traffic","title":"05. Egress policy - Restrict outgoing traffic","text":"<p>Restrict pods in <code>netpol-backend</code> to only communicate with DNS and internal cluster IPs:</p> <pre><code># manifests/restrict-egress.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: restrict-egress\n  namespace: netpol-backend\nspec:\n  podSelector:\n    matchLabels:\n      app: web\n  policyTypes:\n    - Egress\n  egress:\n    # Allow DNS resolution\n    - to: []\n      ports:\n        - protocol: UDP\n          port: 53\n        - protocol: TCP\n          port: 53\n    # Allow traffic to pods within the same namespace\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              role: backend\n</code></pre> <pre><code>kubectl apply -f manifests/restrict-egress.yaml\n</code></pre>"},{"location":"33-NetworkPolicies/#06-default-deny-all-ingress-egress","title":"06. Default Deny All (Ingress + Egress)","text":"<p>The most restrictive baseline - deny everything, then allowlist:</p> <pre><code># manifests/default-deny-all.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: netpol-backend\nspec:\n  podSelector: {}\n  policyTypes:\n    - Ingress\n    - Egress\n</code></pre> <pre><code># Apply the policy (removes all the previous specific policies first)\nkubectl delete networkpolicy --all -n netpol-backend\nkubectl apply -f manifests/default-deny-all.yaml\n</code></pre> <p>Production Pattern</p> <p>The recommended production approach:</p> <ol> <li>Start with a default-deny-all policy in each namespace</li> <li>Add specific allow policies for each required communication path</li> <li>This is a \u201cwhitelist\u201d approach - explicit is better than implicit</li> </ol>"},{"location":"33-NetworkPolicies/#07-inspect-and-debug-networkpolicies","title":"07. Inspect and debug NetworkPolicies","text":"<pre><code># List all network policies in a namespace\nkubectl get networkpolicy -n netpol-backend\n\n# Describe a specific policy to see its rules\nkubectl describe networkpolicy allow-client-ingress -n netpol-backend\n\n# Check which pods are selected by a policy\nkubectl get pods -n netpol-backend -l app=web\n</code></pre>"},{"location":"33-NetworkPolicies/#08-cleanup","title":"08. Cleanup","text":"<pre><code>kubectl delete namespace netpol-frontend netpol-backend\n</code></pre>"},{"location":"33-NetworkPolicies/#summary","title":"Summary","text":"Concept Key Takeaway Default behavior Without NetworkPolicies, all pods can communicate freely Default deny <code>podSelector: {}</code> with no rules = deny all Ingress rules Control who can send traffic TO your pods Egress rules Control where your pods can send traffic podSelector Match traffic sources/destinations by pod labels namespaceSelector Match traffic sources/destinations by namespace labels AND vs OR Same <code>from</code> entry = AND; separate <code>from</code> entries = OR Best practice Default deny + explicit allow (whitelist approach)"},{"location":"33-NetworkPolicies/#exercises","title":"Exercises","text":"<p>The following exercises will test your understanding of Kubernetes NetworkPolicies. Try to solve each exercise on your own before revealing the solution.</p>"},{"location":"33-NetworkPolicies/#01-allow-ingress-only-on-a-specific-port","title":"01. Allow Ingress Only on a Specific Port","text":"<p>Create a NetworkPolicy that allows ingress to pods labeled <code>app=api</code> only on port 8080 (TCP), denying traffic on all other ports.</p>"},{"location":"33-NetworkPolicies/#scenario","title":"Scenario:","text":"<p>\u25e6 Your API server listens on port 8080 but also has a debug port 9090 that should never be accessible. \u25e6 You need to ensure only port 8080 is reachable from other pods.</p> <p>Hint: Use <code>spec.ingress.ports</code> to specify the allowed port. All other ports will be denied once a policy selects the pod.</p> Solution <pre><code>## Create the namespace\nkubectl create namespace netpol-exercise --dry-run=client -o yaml | kubectl apply -f -\n\n## Deploy an API pod that listens on two ports\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: api-server\n  namespace: netpol-exercise\n  labels:\n    app: api\nspec:\n  containers:\n    - name: api\n      image: nginx:alpine\n      ports:\n        - containerPort: 80\nEOF\n\n## Deploy a client pod\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-client\n  namespace: netpol-exercise\n  labels:\n    app: client\nspec:\n  containers:\n    - name: curl\n      image: curlimages/curl:latest\n      command: [\"sleep\", \"3600\"]\nEOF\n\n## Wait for pods\nkubectl wait --for=condition=Ready pod/api-server -n netpol-exercise --timeout=60s\nkubectl wait --for=condition=Ready pod/test-client -n netpol-exercise --timeout=60s\n\n## Apply the NetworkPolicy allowing only port 8080\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-api-port-only\n  namespace: netpol-exercise\nspec:\n  podSelector:\n    matchLabels:\n      app: api\n  policyTypes:\n    - Ingress\n  ingress:\n    - ports:\n        - protocol: TCP\n          port: 8080\nEOF\n\n## Test: port 8080 would be allowed (if it were listening)\n## Port 80 should be denied by the policy\nkubectl exec test-client -n netpol-exercise -- \\\n  curl -s --max-time 3 api-server:80 2&gt;&amp;1 || echo \"Blocked as expected\"\n\n## Clean up\nkubectl delete namespace netpol-exercise\n</code></pre>"},{"location":"33-NetworkPolicies/#02-allow-traffic-between-specific-namespaces-only","title":"02. Allow Traffic Between Specific Namespaces Only","text":"<p>Create two namespaces (<code>team-a</code> and <code>team-b</code>) and a NetworkPolicy that allows pods in <code>team-b</code> to receive ingress only from pods in <code>team-a</code>.</p>"},{"location":"33-NetworkPolicies/#scenario_1","title":"Scenario:","text":"<p>\u25e6 Team A runs a frontend that needs to call Team B\u2019s backend service. \u25e6 No other namespaces should be able to reach Team B\u2019s pods.</p> <p>Hint: Label the namespaces and use <code>namespaceSelector</code> in the ingress rule.</p> Solution <pre><code>## Create namespaces with labels\nkubectl create namespace team-a\nkubectl label namespace team-a team=a\nkubectl create namespace team-b\nkubectl label namespace team-b team=b\n\n## Deploy a backend in team-b\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: backend\n  namespace: team-b\n  labels:\n    app: backend\nspec:\n  containers:\n    - name: nginx\n      image: nginx:alpine\n      ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend\n  namespace: team-b\nspec:\n  selector:\n    app: backend\n  ports:\n    - port: 80\nEOF\n\n## Deploy clients in team-a and default namespace\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: frontend\n  namespace: team-a\n  labels:\n    app: frontend\nspec:\n  containers:\n    - name: curl\n      image: curlimages/curl:latest\n      command: [\"sleep\", \"3600\"]\nEOF\n\n## Wait for pods\nkubectl wait --for=condition=Ready pod/backend -n team-b --timeout=60s\nkubectl wait --for=condition=Ready pod/frontend -n team-a --timeout=60s\n\n## Apply default deny + allow from team-a only\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all-ingress\n  namespace: team-b\nspec:\n  podSelector: {}\n  policyTypes:\n    - Ingress\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-team-a\n  namespace: team-b\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n    - Ingress\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              team: a\n      ports:\n        - protocol: TCP\n          port: 80\nEOF\n\n## Test: frontend in team-a can reach backend\nkubectl exec frontend -n team-a -- \\\n  curl -s --max-time 3 backend.team-b.svc.cluster.local\n## Expected: nginx welcome page \u2705\n\n## Clean up\nkubectl delete namespace team-a team-b\n</code></pre>"},{"location":"33-NetworkPolicies/#03-create-an-egress-policy-that-only-allows-dns-and-https","title":"03. Create an Egress Policy That Only Allows DNS and HTTPS","text":"<p>Create a NetworkPolicy for pods labeled <code>app=secure-app</code> that only allows egress to DNS (UDP/TCP port 53) and HTTPS (TCP port 443).</p>"},{"location":"33-NetworkPolicies/#scenario_2","title":"Scenario:","text":"<p>\u25e6 Your application needs to resolve DNS names and make HTTPS API calls. \u25e6 All other outbound traffic (HTTP, SSH, database ports) must be blocked.</p> <p>Hint: Use multiple entries in <code>spec.egress</code> with specific port rules.</p> Solution <pre><code>## Create namespace\nkubectl create namespace egress-test\n\n## Deploy the pod\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secure-app\n  namespace: egress-test\n  labels:\n    app: secure-app\nspec:\n  containers:\n    - name: curl\n      image: curlimages/curl:latest\n      command: [\"sleep\", \"3600\"]\nEOF\n\nkubectl wait --for=condition=Ready pod/secure-app -n egress-test --timeout=60s\n\n## Apply the egress policy\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: restrict-egress-dns-https\n  namespace: egress-test\nspec:\n  podSelector:\n    matchLabels:\n      app: secure-app\n  policyTypes:\n    - Egress\n  egress:\n    # Allow DNS resolution\n    - ports:\n        - protocol: UDP\n          port: 53\n        - protocol: TCP\n          port: 53\n    # Allow HTTPS traffic\n    - ports:\n        - protocol: TCP\n          port: 443\nEOF\n\n## Test: HTTPS should work (port 443)\nkubectl exec secure-app -n egress-test -- \\\n  curl -s --max-time 5 -o /dev/null -w \"%{http_code}\" https://kubernetes.default.svc:443 -k 2&gt;&amp;1 || echo \"May vary by cluster\"\n\n## Test: HTTP on port 80 should be blocked\nkubectl exec secure-app -n egress-test -- \\\n  curl -s --max-time 3 http://kubernetes.default.svc:80 2&gt;&amp;1 || echo \"Blocked as expected\"\n\n## Clean up\nkubectl delete namespace egress-test\n</code></pre>"},{"location":"33-NetworkPolicies/#04-implement-a-zero-trust-network-model","title":"04. Implement a Zero-Trust Network Model","text":"<p>Implement a complete zero-trust model for a three-tier application: frontend (port 80), backend (port 8080), and database (port 5432). Each tier can only communicate with its adjacent tier.</p>"},{"location":"33-NetworkPolicies/#scenario_3","title":"Scenario:","text":"<p>\u25e6 Frontend pods can receive traffic from anywhere but can only talk to backend pods. \u25e6 Backend pods can only receive traffic from frontend pods and can only talk to database pods. \u25e6 Database pods can only receive traffic from backend pods and cannot make any outbound connections.</p> <p>Hint: Start with default-deny-all in the namespace, then add specific allow policies for each tier.</p> Solution <pre><code>## Create namespace\nkubectl create namespace zero-trust\n\n## Deploy all three tiers\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: frontend\n  namespace: zero-trust\n  labels:\n    tier: frontend\nspec:\n  containers:\n    - name: nginx\n      image: nginx:alpine\n      ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: backend\n  namespace: zero-trust\n  labels:\n    tier: backend\nspec:\n  containers:\n    - name: nginx\n      image: nginx:alpine\n      ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: database\n  namespace: zero-trust\n  labels:\n    tier: database\nspec:\n  containers:\n    - name: nginx\n      image: nginx:alpine\n      ports:\n        - containerPort: 80\nEOF\n\n## Wait for pods\nkubectl wait --for=condition=Ready pod -l tier -n zero-trust --timeout=60s\n\n## Step 1: Default deny all traffic\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: zero-trust\nspec:\n  podSelector: {}\n  policyTypes:\n    - Ingress\n    - Egress\nEOF\n\n## Step 2: Frontend can receive from anywhere, egress to backend + DNS\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: frontend-policy\n  namespace: zero-trust\nspec:\n  podSelector:\n    matchLabels:\n      tier: frontend\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    - {}\n  egress:\n    - to:\n        - podSelector:\n            matchLabels:\n              tier: backend\n    - ports:\n        - protocol: UDP\n          port: 53\n        - protocol: TCP\n          port: 53\nEOF\n\n## Step 3: Backend receives from frontend, egress to database + DNS\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: backend-policy\n  namespace: zero-trust\nspec:\n  podSelector:\n    matchLabels:\n      tier: backend\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    - from:\n        - podSelector:\n            matchLabels:\n              tier: frontend\n  egress:\n    - to:\n        - podSelector:\n            matchLabels:\n              tier: database\n    - ports:\n        - protocol: UDP\n          port: 53\n        - protocol: TCP\n          port: 53\nEOF\n\n## Step 4: Database receives from backend only, no egress\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: database-policy\n  namespace: zero-trust\nspec:\n  podSelector:\n    matchLabels:\n      tier: database\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    - from:\n        - podSelector:\n            matchLabels:\n              tier: backend\n  egress: []\nEOF\n\n## Verify policies\nkubectl get networkpolicy -n zero-trust\n\n## Clean up\nkubectl delete namespace zero-trust\n</code></pre>"},{"location":"33-NetworkPolicies/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>NetworkPolicy has no effect:</li> </ul> <p>Verify your CNI plugin supports NetworkPolicies:</p> <pre><code>## Check which CNI is installed\nkubectl get pods -n kube-system | grep -E \"calico|cilium|weave\"\n\n## If using Kind with default CNI (kindnet), NetworkPolicies are NOT enforced\n## Reinstall with Calico:\n## kind delete cluster &amp;&amp; kind create cluster --config manifests/kind-config.yaml\n## kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml\n</code></pre> <p></p> <ul> <li>Pods cannot resolve DNS after applying deny policy:</li> </ul> <p>Default-deny policies block DNS (port 53). Always include a DNS allow rule in egress policies:</p> <pre><code>## Add DNS egress to your policy:\n## egress:\n##   - ports:\n##       - protocol: UDP\n##         port: 53\n##       - protocol: TCP\n##         port: 53\n</code></pre> <p></p> <ul> <li>Traffic still getting through after applying deny policy:</li> </ul> <p>Check that the policy selects the correct pods and is in the correct namespace:</p> <pre><code>## Verify the policy is applied\nkubectl get networkpolicy -n &lt;namespace&gt;\n\n## Describe the policy to see which pods it selects\nkubectl describe networkpolicy &lt;policy-name&gt; -n &lt;namespace&gt;\n\n## Verify pod labels match\nkubectl get pods -n &lt;namespace&gt; --show-labels\n</code></pre> <p></p> <ul> <li>Cannot determine if AND or OR logic is being used:</li> </ul> <p>Remember: items in the same <code>from</code> entry (same <code>-</code>) are ANDed; separate <code>from</code> entries (separate <code>-</code>) are ORed:</p> <pre><code>## AND (both must match):\ningress:\n  - from:\n      - namespaceSelector:\n          matchLabels:\n            role: frontend\n        podSelector:            # No dash - same entry = AND\n          matchLabels:\n            app: client\n\n## OR (either can match):\ningress:\n  - from:\n      - namespaceSelector:\n          matchLabels:\n            role: frontend\n      - podSelector:            # Dash - separate entry = OR\n          matchLabels:\n            app: client\n</code></pre> <p></p> <ul> <li>Testing connectivity between pods:</li> </ul> <p>Use <code>curl</code> or <code>wget</code> with timeouts to test connectivity:</p> <pre><code>## Using curl with timeout\nkubectl exec &lt;pod&gt; -n &lt;namespace&gt; -- curl -s --max-time 3 &lt;service&gt;.&lt;namespace&gt;.svc.cluster.local\n\n## Using wget (if curl not available)\nkubectl exec &lt;pod&gt; -n &lt;namespace&gt; -- wget -qO- --timeout=3 &lt;service&gt;.&lt;namespace&gt;.svc.cluster.local\n\n## Using nc (netcat) for specific ports\nkubectl exec &lt;pod&gt; -n &lt;namespace&gt; -- nc -zv -w 3 &lt;service&gt; &lt;port&gt;\n</code></pre>"},{"location":"33-NetworkPolicies/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Cilium NetworkPolicies for advanced L7 (HTTP, gRPC) filtering and DNS-aware policies.</li> <li>Learn about Calico NetworkPolicies for enterprise-grade network security with global policies and FQDN-based rules.</li> <li>Try Network Policy Editor - a visual tool for building and visualizing NetworkPolicies.</li> <li>Implement Kubernetes Security Best Practices combining NetworkPolicies with RBAC, Pod Security Standards, and Secrets management.</li> <li>Explore service mesh solutions (Istio, Linkerd) for mTLS and L7 traffic management on top of NetworkPolicies.</li> </ul>"},{"location":"34-crictl/","title":"crictl - Container Runtime Interface CLI","text":"<ul> <li>Welcome to the <code>crictl</code> hands-on lab! In this tutorial, you will learn how to use <code>crictl</code>, the command-line interface for CRI-compatible container runtimes.</li> <li><code>crictl</code> is an essential debugging and inspection tool that operates at the container runtime level, giving you visibility into what is happening beneath the Kubernetes API layer.</li> <li>Unlike <code>kubectl</code>, which communicates with the Kubernetes API server, <code>crictl</code> talks directly to the container runtime (such as <code>containerd</code> or <code>CRI-O</code>) on a specific node.</li> </ul> <p>Node-Level Tool</p> <p><code>crictl</code> runs directly on Kubernetes nodes, not from your local workstation. All commands in this lab assume you have SSH access to a Kubernetes node (control plane or worker). You must run these commands as <code>root</code> or with <code>sudo</code> privileges.</p>"},{"location":"34-crictl/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What the Container Runtime Interface (CRI) is and why it exists</li> <li>How <code>crictl</code> fits into the Kubernetes architecture</li> <li>How to install and configure <code>crictl</code> on a Kubernetes node</li> <li>How to list, inspect, and filter pods and containers at the runtime level</li> <li>How to view container logs directly from the runtime</li> <li>How to execute commands inside running containers</li> <li>How to manage container images at the runtime level</li> <li>How to monitor resource usage with runtime-level stats</li> <li>How to query runtime information for debugging</li> <li>How to manually create pod sandboxes and containers (advanced debugging)</li> <li>When to use <code>crictl</code> versus <code>kubectl</code> and other container CLI tools</li> </ul>"},{"location":"34-crictl/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link crictl Official Repository github.com/kubernetes-sigs/cri-tools CRI Specification kubernetes.io/docs/concepts/architecture/cri crictl User Guide github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md Kubernetes Container Runtimes kubernetes.io/docs/setup/production-environment/container-runtimes containerd Documentation containerd.io/docs CRI-O Documentation cri-o.io Debugging Kubernetes Nodes kubernetes.io/docs/tasks/debug/debug-cluster/crictl Migrating from Docker to containerd kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim OCI Runtime Specification github.com/opencontainers/runtime-spec"},{"location":"34-crictl/#introduction","title":"Introduction","text":""},{"location":"34-crictl/#what-is-cri-container-runtime-interface","title":"What is CRI (Container Runtime Interface)?","text":"<ul> <li>The Container Runtime Interface (CRI) is a plugin interface that allows the kubelet to use a wide variety of container runtimes without needing to recompile the kubelet itself.</li> <li>CRI defines a set of gRPC services that a container runtime must implement so that the kubelet can manage pods and containers on a node.</li> <li>Before CRI existed, Kubernetes was tightly coupled to Docker. CRI decoupled Kubernetes from any specific runtime, enabling alternatives like <code>containerd</code> and <code>CRI-O</code>.</li> </ul> <p>The CRI specification defines two main gRPC services:</p> Service Responsibility <code>RuntimeService</code> Manages pod sandboxes and containers: creating, starting, stopping, removing, and listing them. <code>ImageService</code> Manages container images: pulling, listing, inspecting, and removing images on the node."},{"location":"34-crictl/#why-crictl-exists","title":"Why crictl Exists","text":"<ul> <li><code>crictl</code> (pronounced \u201ccry-cuttle\u201d) is a CLI tool for CRI-compatible container runtimes.</li> <li>It provides a way to inspect and debug the container runtime directly on a Kubernetes node.</li> <li>It was created as part of the <code>cri-tools</code> project by the Kubernetes SIG Node team.</li> <li><code>crictl</code> is the recommended replacement for <code>docker</code> CLI commands when your cluster uses <code>containerd</code> or <code>CRI-O</code> instead of Docker.</li> </ul> <p>Common use cases for <code>crictl</code>:</p> <ul> <li>Debugging container issues that are not visible through <code>kubectl</code></li> <li>Inspecting the state of containers and pods at the runtime level</li> <li>Viewing logs when the Kubernetes API server is unavailable</li> <li>Checking image availability on a specific node</li> <li>Investigating pod sandbox networking issues</li> <li>Monitoring resource usage at the runtime level</li> <li>Troubleshooting CrashLoopBackOff and ImagePullBackOff errors from the node perspective</li> </ul>"},{"location":"34-crictl/#crictl-vs-docker-cli-vs-nerdctl-vs-ctr","title":"crictl vs docker CLI vs nerdctl vs ctr","text":"<p>Since the removal of <code>dockershim</code> in Kubernetes 1.24, multiple CLI tools exist for interacting with container runtimes. Here is how they compare:</p> Feature <code>crictl</code> <code>docker</code> <code>nerdctl</code> <code>ctr</code> Purpose CRI debugging Docker Engine management containerd management (Docker-compatible) Low-level containerd client Target Runtime Any CRI runtime Docker Engine only containerd only containerd only Kubernetes-aware Yes (pods, sandboxes) No Yes (with nerdctl ps -a) No Pod support Yes (native) No Limited No Image management Pull, list, remove Full (build, push, pull) Full (build, push, pull) Pull, list, remove Container creation Manual (debugging only) Full lifecycle Full lifecycle Full lifecycle Build images No Yes Yes No Compose support No Yes (docker compose) Yes (nerdctl compose) No Recommended for K8s Yes (node debugging) No (deprecated in K8s) Yes (development) No (too low-level) Installed by default Often (kubeadm nodes) No (unless Docker runtime) No Yes (with containerd) <p>Rule of Thumb</p> <ul> <li>Use <code>kubectl</code> for cluster-level operations (from your workstation).</li> <li>Use <code>crictl</code> for node-level debugging (SSH into the node).</li> <li>Use <code>nerdctl</code> if you need a Docker-compatible CLI for <code>containerd</code>.</li> <li>Use <code>ctr</code> only for very low-level containerd operations.</li> </ul>"},{"location":"34-crictl/#cri-architecture","title":"CRI Architecture","text":"<p>The following diagram shows how <code>crictl</code> fits into the Kubernetes container runtime architecture:</p> <pre><code>graph TB\n    subgraph \"User / Operator\"\n        kubectl[\"kubectl&lt;br/&gt;(cluster-level)\"]\n        crictl[\"crictl&lt;br/&gt;(node-level)\"]\n    end\n\n    subgraph \"Control Plane\"\n        api[\"API Server\"]\n    end\n\n    subgraph \"Kubernetes Node\"\n        kubelet[\"kubelet\"]\n        cri_shim[\"CRI gRPC Interface&lt;br/&gt;(unix socket)\"]\n\n        subgraph \"Container Runtime\"\n            containerd[\"containerd / CRI-O\"]\n            oci[\"OCI Runtime&lt;br/&gt;(runc / crun / kata)\"]\n        end\n\n        subgraph \"Workloads\"\n            pod1[\"Pod Sandbox 1\"]\n            c1a[\"Container A\"]\n            c1b[\"Container B\"]\n            pod2[\"Pod Sandbox 2\"]\n            c2a[\"Container C\"]\n        end\n    end\n\n    kubectl --&gt; api\n    api --&gt; kubelet\n    kubelet --&gt;|\"CRI gRPC calls\"| cri_shim\n    crictl --&gt;|\"CRI gRPC calls\"| cri_shim\n    cri_shim --&gt; containerd\n    containerd --&gt; oci\n    oci --&gt; pod1\n    oci --&gt; pod2\n    pod1 --- c1a\n    pod1 --- c1b\n    pod2 --- c2a\n\n    style crictl fill:#f9a825,stroke:#f57f17,color:#000\n    style kubectl fill:#42a5f5,stroke:#1565c0,color:#000\n    style kubelet fill:#66bb6a,stroke:#2e7d32,color:#000\n    style containerd fill:#ab47bc,stroke:#6a1b9a,color:#fff</code></pre> <p>Key points from the architecture:</p> <ol> <li>kubectl communicates with the API Server over HTTPS (cluster-level).</li> <li>The API Server instructs the kubelet on each node.</li> <li>The kubelet communicates with the container runtime via CRI gRPC over a Unix socket.</li> <li>crictl connects to the same Unix socket, bypassing the API Server entirely.</li> <li>The container runtime (<code>containerd</code> or <code>CRI-O</code>) delegates actual container execution to an OCI runtime like <code>runc</code>.</li> </ol>"},{"location":"34-crictl/#when-to-use-crictl-vs-kubectl","title":"When to Use crictl vs kubectl","text":"Scenario Use <code>kubectl</code> Use <code>crictl</code> Deploy, scale, or manage workloads Yes No View pod logs (API server is healthy) Yes Optional View pod logs (API server is down or unreachable) No Yes Inspect container state on a specific node Limited Yes Debug networking at the pod sandbox level No Yes Check which images are cached on a node No Yes Monitor per-container resource usage on a node No Yes Investigate why a container keeps crashing Partially Yes (more detail) Create test pods and containers Yes Yes (manual) Manage cluster resources (Services, Ingress, etc.) Yes No"},{"location":"34-crictl/#configuration-file","title":"Configuration File","text":"<p><code>crictl</code> uses a configuration file at <code>/etc/crictl.yaml</code> to determine which runtime endpoint to connect to. This avoids having to pass the <code>--runtime-endpoint</code> flag with every command.</p> <pre><code>## /etc/crictl.yaml\n## Configuration file for crictl\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\npull-image-on-create: false\ndisable-pull-on-run: false\n</code></pre> Field Description <code>runtime-endpoint</code> The Unix socket path for the container runtime\u2019s CRI service. <code>image-endpoint</code> The Unix socket path for the image service (often the same as <code>runtime-endpoint</code>). <code>timeout</code> Timeout in seconds for CRI gRPC calls. <code>debug</code> When <code>true</code>, enables verbose debug output for all commands. <code>pull-image-on-create</code> When <code>true</code>, automatically pulls the image when creating a container. <code>disable-pull-on-run</code> When <code>true</code>, disables automatic image pulling when running a container. <p>Common runtime endpoint paths:</p> Runtime Socket Path <code>containerd</code> <code>unix:///run/containerd/containerd.sock</code> <code>CRI-O</code> <code>unix:///var/run/crio/crio.sock</code> <code>Docker</code> (via cri-dockerd) <code>unix:///run/cri-dockerd.sock</code>"},{"location":"34-crictl/#prerequisites","title":"Prerequisites","text":"<p>Before starting this lab, ensure you have:</p> <ul> <li>A running Kubernetes cluster (single-node or multi-node)</li> <li>SSH access to at least one Kubernetes node (control plane or worker)</li> <li>Root or sudo privileges on the node</li> <li>A container runtime installed on the node (<code>containerd</code> or <code>CRI-O</code>)</li> <li>Basic familiarity with <code>kubectl</code> and Kubernetes concepts (pods, containers, namespaces)</li> <li>Some workloads already running on the cluster (for inspection)</li> </ul> <p>This Lab Requires Node Access</p> <p>Unlike most Kubernetes labs where you run commands from your workstation using <code>kubectl</code>, this lab requires you to SSH into a Kubernetes node and run commands directly on it. If you are using a managed Kubernetes service (EKS, GKE, AKS), you will need to SSH into a worker node or use a node shell utility like <code>kubectl debug node/&lt;node-name&gt; -it --image=ubuntu</code>.</p> <p>To SSH into a node (example):</p> <pre><code>## If you know the node IP address\nssh user@&lt;node-ip&gt;\n\n## Using kubectl to get node IPs first\nkubectl get nodes -o wide\n\n## Alternative: use kubectl debug to get a shell on a node\n## (requires Kubernetes 1.18+ with ephemeral containers enabled)\nkubectl debug node/&lt;node-name&gt; -it --image=ubuntu\n</code></pre>"},{"location":"34-crictl/#lab","title":"Lab","text":""},{"location":"34-crictl/#step-01-install-crictl","title":"Step 01 - Install crictl","text":"<ul> <li><code>crictl</code> is distributed as a standalone binary from the <code>cri-tools</code> project.</li> <li>On many Kubernetes distributions (kubeadm, k3s, etc.), <code>crictl</code> is already installed. Check first before installing.</li> </ul>"},{"location":"34-crictl/#check-if-crictl-is-already-installed","title":"Check if crictl is Already Installed","text":"<pre><code>## Check if crictl is available on the node\nwhich crictl\n\n## If installed, check the version\ncrictl --version\n\n## Expected output (example):\n## crictl version v1.29.0\n</code></pre>"},{"location":"34-crictl/#install-crictl-if-not-present","title":"Install crictl (if not present)","text":"Linux (amd64)Linux (arm64) <pre><code>## Set the desired version\nVERSION=\"v1.29.0\"\n\n## Download the crictl tarball\ncurl -L \"https://github.com/kubernetes-sigs/cri-tools/releases/download/${VERSION}/crictl-${VERSION}-linux-amd64.tar.gz\" \\\n  -o crictl-${VERSION}-linux-amd64.tar.gz\n\n## Extract the binary to /usr/local/bin\nsudo tar zxvf crictl-${VERSION}-linux-amd64.tar.gz -C /usr/local/bin\n\n## Verify the installation\ncrictl --version\n\n## Clean up the tarball\nrm -f crictl-${VERSION}-linux-amd64.tar.gz\n</code></pre> <pre><code>## Set the desired version\nVERSION=\"v1.29.0\"\n\n## Download the crictl tarball for arm64\ncurl -L \"https://github.com/kubernetes-sigs/cri-tools/releases/download/${VERSION}/crictl-${VERSION}-linux-arm64.tar.gz\" \\\n  -o crictl-${VERSION}-linux-arm64.tar.gz\n\n## Extract the binary to /usr/local/bin\nsudo tar zxvf crictl-${VERSION}-linux-arm64.tar.gz -C /usr/local/bin\n\n## Verify the installation\ncrictl --version\n\n## Clean up the tarball\nrm -f crictl-${VERSION}-linux-arm64.tar.gz\n</code></pre> <p>Version Compatibility</p> <p>It is recommended to use a <code>crictl</code> version that matches your Kubernetes minor version. For example, use <code>crictl</code> v1.29.x with Kubernetes v1.29.x. Check the compatibility matrix for details.</p>"},{"location":"34-crictl/#step-02-configure-crictl","title":"Step 02 - Configure crictl","text":"<ul> <li>Before using <code>crictl</code>, you need to configure it to connect to the correct container runtime socket.</li> <li>Without configuration, <code>crictl</code> will attempt to auto-detect the runtime, but it is best to be explicit.</li> </ul>"},{"location":"34-crictl/#identify-your-container-runtime","title":"Identify Your Container Runtime","text":"<pre><code>## Check which container runtime the kubelet is using\nps aux | grep kubelet | grep -- --container-runtime-endpoint\n\n## Alternative: check the kubelet configuration\nsudo cat /var/lib/kubelet/config.yaml | grep -i container\n\n## For containerd, check if the socket exists\nls -la /run/containerd/containerd.sock\n\n## For CRI-O, check if the socket exists\nls -la /var/run/crio/crio.sock\n</code></pre>"},{"location":"34-crictl/#create-the-configuration-file","title":"Create the Configuration File","text":"containerdCRI-O <pre><code>## Create the crictl configuration for containerd\nsudo tee /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n\n## Verify the configuration\ncat /etc/crictl.yaml\n</code></pre> <pre><code>## Create the crictl configuration for CRI-O\nsudo tee /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///var/run/crio/crio.sock\nimage-endpoint: unix:///var/run/crio/crio.sock\ntimeout: 10\ndebug: false\nEOF\n\n## Verify the configuration\ncat /etc/crictl.yaml\n</code></pre>"},{"location":"34-crictl/#using-the-runtime-endpoint-flag-alternative","title":"Using the \u2013runtime-endpoint Flag (Alternative)","text":"<pre><code>## Instead of a config file, you can specify the endpoint per command\ncrictl --runtime-endpoint unix:///run/containerd/containerd.sock ps\n\n## Or set it as an environment variable\nexport CONTAINER_RUNTIME_ENDPOINT=unix:///run/containerd/containerd.sock\n\n## Now all crictl commands will use this endpoint\ncrictl ps\n</code></pre>"},{"location":"34-crictl/#enable-debug-mode-optional","title":"Enable Debug Mode (Optional)","text":"<pre><code>## Temporarily enable debug mode for troubleshooting\n## This shows the gRPC calls being made to the runtime\nsudo tee /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: true\nEOF\n\n## Run a command with debug output\ncrictl ps\n\n## Remember to disable debug mode when done\n## (set debug: false in /etc/crictl.yaml)\n</code></pre> <p>Debug Mode</p> <p>Enabling <code>debug: true</code> in <code>/etc/crictl.yaml</code> prints the raw gRPC requests and responses. This is extremely useful when troubleshooting connectivity issues with the container runtime socket.</p>"},{"location":"34-crictl/#test-the-configuration","title":"Test the Configuration","text":"<pre><code>## Verify crictl can communicate with the runtime\ncrictl info\n\n## Expected output: JSON with runtime information including\n## version, storage driver, and runtime conditions\n\n## Quick sanity check: list running containers\ncrictl ps\n\n## List all pods managed by the runtime\ncrictl pods\n</code></pre>"},{"location":"34-crictl/#step-03-listing-pods-with-crictl-pods","title":"Step 03 - Listing Pods with crictl pods","text":"<ul> <li>The <code>crictl pods</code> command lists all pod sandboxes managed by the container runtime.</li> <li>A pod sandbox is the runtime\u2019s representation of a Kubernetes pod. It holds the shared Linux namespaces (network, IPC, PID) that containers within the pod share.</li> </ul>"},{"location":"34-crictl/#basic-pod-listing","title":"Basic Pod Listing","text":"<pre><code>## List all pod sandboxes (running and stopped)\ncrictl pods\n\n## Example output:\n## POD ID         CREATED        STATE   NAME                          NAMESPACE     ATTEMPT  RUNTIME\n## a1b2c3d4e5f6   2 hours ago    Ready   nginx-7d456b8f9c-abcde       default       0        (default)\n## f6e5d4c3b2a1   3 hours ago    Ready   coredns-5dd5756b68-xyz12     kube-system   0        (default)\n</code></pre>"},{"location":"34-crictl/#filtering-pods","title":"Filtering Pods","text":"<pre><code>## Filter pods by name\ncrictl pods --name nginx\n\n## Filter pods by namespace\ncrictl pods --namespace kube-system\n\n## Filter pods by state (Ready or NotReady)\ncrictl pods --state Ready\n\n## Filter pods that are not ready (stopped, failed, etc.)\ncrictl pods --state NotReady\n\n## Filter pods by label\ncrictl pods --label app=nginx\n\n## Filter by multiple labels\ncrictl pods --label app=nginx --label version=v1\n\n## Combine multiple filters\ncrictl pods --namespace default --state Ready --label app=nginx\n</code></pre>"},{"location":"34-crictl/#verbose-pod-listing","title":"Verbose Pod Listing","text":"<pre><code>## Show full pod IDs (not truncated)\ncrictl pods --no-trunc\n\n## Show only pod IDs (useful for scripting)\ncrictl pods --quiet\n\n## Show pods with additional information in verbose mode\ncrictl pods --verbose\n\n## Output in JSON format for programmatic consumption\ncrictl pods -o json\n\n## Output in YAML format\ncrictl pods -o yaml\n\n## Output in table format (default)\ncrictl pods -o table\n</code></pre>"},{"location":"34-crictl/#listing-pods-by-last-n","title":"Listing Pods by Last N","text":"<pre><code>## Show only the last 5 pods created\ncrictl pods --last 5\n\n## Show the most recently created pod\ncrictl pods --last 1\n</code></pre> <p>Pod Sandbox vs Kubernetes Pod</p> <p>Every Kubernetes pod corresponds to a pod sandbox in the container runtime. The sandbox is created first (with its own \u201cpause\u201d container), and then the actual application containers are started inside it. When you see a sandbox in <code>crictl pods</code>, it maps 1:1 to a pod you would see in <code>kubectl get pods</code>.</p>"},{"location":"34-crictl/#step-04-inspecting-pods-with-crictl-inspectp","title":"Step 04 - Inspecting Pods with crictl inspectp","text":"<ul> <li>The <code>crictl inspectp</code> command provides detailed information about a specific pod sandbox.</li> <li>This includes network configuration, DNS settings, Linux namespace paths, labels, annotations, and more.</li> </ul>"},{"location":"34-crictl/#inspect-a-pod","title":"Inspect a Pod","text":"<pre><code>## First, get the pod ID you want to inspect\ncrictl pods\n## Note the POD ID from the output (e.g., a1b2c3d4e5f6)\n\n## Inspect the pod sandbox by its ID\ncrictl inspectp a1b2c3d4e5f6\n\n## You can also use a partial ID (as long as it is unique)\ncrictl inspectp a1b2c3\n</code></pre>"},{"location":"34-crictl/#understanding-the-inspect-output","title":"Understanding the Inspect Output","text":"<p>The output is a JSON document with several important sections:</p> <pre><code>## Inspect a pod and pipe through jq for readability\ncrictl inspectp a1b2c3d4e5f6 | jq .\n\n## Get the pod's network information\n## This shows the pod IP address and network namespace\ncrictl inspectp a1b2c3d4e5f6 | jq '.status.network'\n\n## Example output:\n## {\n##   \"additionalIps\": [],\n##   \"ip\": \"10.244.0.15\"\n## }\n\n## Get the pod's Linux namespace paths\ncrictl inspectp a1b2c3d4e5f6 | jq '.info.runtimeSpec.linux.namespaces'\n\n## Get the pod's labels\ncrictl inspectp a1b2c3d4e5f6 | jq '.status.labels'\n\n## Get the pod's annotations\ncrictl inspectp a1b2c3d4e5f6 | jq '.status.annotations'\n\n## Get the pod's DNS configuration\ncrictl inspectp a1b2c3d4e5f6 | jq '.info.runtimeSpec.linux.resources'\n\n## Get the creation timestamp\ncrictl inspectp a1b2c3d4e5f6 | jq '.status.createdAt'\n\n## Get the pod's state\ncrictl inspectp a1b2c3d4e5f6 | jq '.status.state'\n</code></pre>"},{"location":"34-crictl/#inspect-network-namespace","title":"Inspect Network Namespace","text":"<pre><code>## Get the network namespace path for a pod\n## This is useful for debugging networking issues with nsenter\nNETNS=$(crictl inspectp a1b2c3d4e5f6 | jq -r '.info.runtimeSpec.linux.namespaces[] | select(.type==\"network\") | .path')\necho \"Network namespace: $NETNS\"\n\n## Enter the pod's network namespace to debug networking\n## (requires nsenter installed on the node)\nsudo nsenter --net=$NETNS ip addr show\n\n## Check the routes inside the pod's network namespace\nsudo nsenter --net=$NETNS ip route show\n\n## Check DNS resolution from inside the network namespace\nsudo nsenter --net=$NETNS cat /etc/resolv.conf\n</code></pre> <p>Info Section Requires Runtime Support</p> <p>The <code>.info</code> section in the <code>crictl inspectp</code> output requires the container runtime to support the verbose inspection feature. Some runtimes may return an empty info section. If this happens, the <code>.status</code> section will still contain essential information like the pod ID, state, labels, and creation time.</p>"},{"location":"34-crictl/#step-05-listing-containers-with-crictl-ps","title":"Step 05 - Listing Containers with crictl ps","text":"<ul> <li>The <code>crictl ps</code> command lists containers managed by the container runtime.</li> <li>By default, it shows only running containers. Use the <code>-a</code> flag to include all states.</li> </ul>"},{"location":"34-crictl/#basic-container-listing","title":"Basic Container Listing","text":"<pre><code>## List all running containers\ncrictl ps\n\n## Example output:\n## CONTAINER      IMAGE          CREATED        STATE    NAME       ATTEMPT  POD ID         POD\n## b1c2d3e4f5g6   nginx:1.25     2 hours ago    Running  nginx      0        a1b2c3d4e5f6   nginx-7d456b8f9c-abcde\n## h7i8j9k0l1m2   coredns:1.11   3 hours ago    Running  coredns    0        f6e5d4c3b2a1   coredns-5dd5756b68-xyz12\n\n## List ALL containers (including exited, created, unknown states)\ncrictl ps -a\n\n## Example output showing exited containers:\n## CONTAINER      IMAGE          CREATED        STATE    NAME       ATTEMPT  POD ID         POD\n## b1c2d3e4f5g6   nginx:1.25     2 hours ago    Running  nginx      0        a1b2c3d4e5f6   nginx-...\n## n3o4p5q6r7s8   busybox:1.36   1 hour ago     Exited   init-db    0        t9u0v1w2x3y4   myapp-...\n</code></pre>"},{"location":"34-crictl/#filtering-containers","title":"Filtering Containers","text":"<pre><code>## Filter containers by name\ncrictl ps --name nginx\n\n## Filter containers by state\ncrictl ps --state Running\ncrictl ps --state Exited\ncrictl ps --state Created\ncrictl ps --state Unknown\n\n## Filter containers by image\ncrictl ps --image nginx\n\n## Filter containers by label\ncrictl ps --label io.kubernetes.container.name=nginx\n\n## Filter containers belonging to a specific pod\ncrictl ps --pod a1b2c3d4e5f6\n\n## Combine filters: show exited containers for a specific pod\ncrictl ps -a --state Exited --pod a1b2c3d4e5f6\n\n## Show only the last N containers\ncrictl ps -a --last 10\n</code></pre>"},{"location":"34-crictl/#output-formatting","title":"Output Formatting","text":"<pre><code>## Show full IDs (not truncated)\ncrictl ps --no-trunc\n\n## Show only container IDs (useful for scripting)\ncrictl ps --quiet\n\n## JSON output for programmatic processing\ncrictl ps -o json\n\n## YAML output\ncrictl ps -o yaml\n\n## Show all containers with verbose output\ncrictl ps -a --verbose\n</code></pre>"},{"location":"34-crictl/#counting-containers-by-state","title":"Counting Containers by State","text":"<pre><code>## Count running containers\ncrictl ps --quiet | wc -l\n\n## Count all containers (including stopped)\ncrictl ps -a --quiet | wc -l\n\n## Count exited containers only\ncrictl ps -a --state Exited --quiet | wc -l\n\n## List all containers with their exit codes (exited containers)\ncrictl ps -a --state Exited -o json | jq '.containers[] | {name: .metadata.name, id: .id, exitCode: .state}'\n</code></pre> <p>Container States</p> <p>Containers in CRI can be in one of four states:</p> <ul> <li>Created: The container has been created but not started.</li> <li>Running: The container is currently executing.</li> <li>Exited: The container has finished executing (with an exit code).</li> <li>Unknown: The container state cannot be determined.</li> </ul>"},{"location":"34-crictl/#step-06-inspecting-containers-with-crictl-inspect","title":"Step 06 - Inspecting Containers with crictl inspect","text":"<ul> <li>The <code>crictl inspect</code> command provides detailed information about a specific container.</li> <li>This includes the container\u2019s full configuration, resource limits, mounts, environment variables, process info, and more.</li> </ul>"},{"location":"34-crictl/#inspect-a-container","title":"Inspect a Container","text":"<pre><code>## First, get the container ID\ncrictl ps\n## Note the CONTAINER ID (e.g., b1c2d3e4f5g6)\n\n## Inspect the container\ncrictl inspect b1c2d3e4f5g6\n\n## Inspect with pretty-printed JSON (pipe through jq)\ncrictl inspect b1c2d3e4f5g6 | jq .\n</code></pre>"},{"location":"34-crictl/#extract-specific-information","title":"Extract Specific Information","text":"<pre><code>## Get the container's image reference\ncrictl inspect b1c2d3e4f5g6 | jq '.status.image.image'\n\n## Get the container's PID (process ID on the host)\ncrictl inspect b1c2d3e4f5g6 | jq '.info.pid'\n\n## Get the container's resource limits (CPU and memory)\ncrictl inspect b1c2d3e4f5g6 | jq '.info.runtimeSpec.linux.resources'\n\n## Get CPU limits specifically\ncrictl inspect b1c2d3e4f5g6 | jq '.info.runtimeSpec.linux.resources.cpu'\n\n## Get memory limits specifically\ncrictl inspect b1c2d3e4f5g6 | jq '.info.runtimeSpec.linux.resources.memory'\n\n## Get the container's environment variables\ncrictl inspect b1c2d3e4f5g6 | jq '.info.runtimeSpec.process.env'\n\n## Get the container's mount points\ncrictl inspect b1c2d3e4f5g6 | jq '.info.runtimeSpec.mounts'\n\n## Get the container's startup command and arguments\ncrictl inspect b1c2d3e4f5g6 | jq '.info.runtimeSpec.process.args'\n\n## Get the container's working directory\ncrictl inspect b1c2d3e4f5g6 | jq '.info.runtimeSpec.process.cwd'\n\n## Get the container's state and exit code (for exited containers)\ncrictl inspect b1c2d3e4f5g6 | jq '{state: .status.state, exitCode: .status.exitCode, reason: .status.reason}'\n\n## Get the container's creation and start timestamps\ncrictl inspect b1c2d3e4f5g6 | jq '{created: .status.createdAt, started: .status.startedAt, finished: .status.finishedAt}'\n\n## Get the container's log path\ncrictl inspect b1c2d3e4f5g6 | jq '.status.logPath'\n</code></pre>"},{"location":"34-crictl/#inspecting-exited-containers","title":"Inspecting Exited Containers","text":"<pre><code>## When debugging a CrashLoopBackOff, inspect the exited container\n## to find the exit code and reason\n\n## List exited containers\ncrictl ps -a --state Exited\n\n## Inspect the exited container\ncrictl inspect n3o4p5q6r7s8 | jq '{\n  name: .status.metadata.name,\n  state: .status.state,\n  exitCode: .status.exitCode,\n  reason: .status.reason,\n  message: .status.message,\n  startedAt: .status.startedAt,\n  finishedAt: .status.finishedAt\n}'\n\n## Common exit codes:\n## 0   = Success (normal termination)\n## 1   = General error\n## 126 = Command not executable\n## 127 = Command not found\n## 137 = Killed by SIGKILL (OOMKilled or manual kill)\n## 139 = Segmentation fault (SIGSEGV)\n## 143 = Killed by SIGTERM (graceful shutdown)\n</code></pre> <p>Understanding Exit Code 137</p> <p>Exit code 137 (128 + 9 = SIGKILL) is one of the most common crash indicators. It usually means the container was OOMKilled (ran out of memory) or was killed by the kubelet because it exceeded its memory limit. Check the container\u2019s memory limits and actual usage with <code>crictl inspect</code> and <code>crictl stats</code> to confirm.</p>"},{"location":"34-crictl/#step-07-viewing-container-logs-with-crictl-logs","title":"Step 07 - Viewing Container Logs with crictl logs","text":"<ul> <li>The <code>crictl logs</code> command reads the stdout/stderr logs of a container.</li> <li>It works similarly to <code>kubectl logs</code> but operates directly at the runtime level.</li> <li>This is especially useful when the Kubernetes API server is unavailable.</li> </ul>"},{"location":"34-crictl/#basic-log-viewing","title":"Basic Log Viewing","text":"<pre><code>## View logs for a running container\ncrictl logs b1c2d3e4f5g6\n\n## View logs for an exited container (useful for crash debugging)\n## First find the exited container ID\ncrictl ps -a --state Exited\n## Then view its logs\ncrictl logs n3o4p5q6r7s8\n</code></pre>"},{"location":"34-crictl/#log-options","title":"Log Options","text":"<pre><code>## Follow logs in real-time (like tail -f)\ncrictl logs --follow b1c2d3e4f5g6\n\n## Show only the last N lines\ncrictl logs --tail 50 b1c2d3e4f5g6\n\n## Show only the last 10 lines\ncrictl logs --tail 10 b1c2d3e4f5g6\n\n## Show logs with timestamps\ncrictl logs --timestamps b1c2d3e4f5g6\n\n## Example output with timestamps:\n## 2024-01-15T10:30:45.123456789Z 172.17.0.1 - - [15/Jan/2024:10:30:45 +0000] \"GET / HTTP/1.1\" 200 615\n\n## Show logs since a specific time (RFC3339 format)\ncrictl logs --since \"2024-01-15T10:00:00Z\" b1c2d3e4f5g6\n\n## Show logs from the last 5 minutes\n## (Calculate the timestamp 5 minutes ago)\ncrictl logs --since \"$(date -u -d '5 minutes ago' +%Y-%m-%dT%H:%M:%SZ)\" b1c2d3e4f5g6\n\n## Show logs until a specific time\ncrictl logs --until \"2024-01-15T11:00:00Z\" b1c2d3e4f5g6\n\n## Combine: follow logs with timestamps, showing only the last 20 lines\ncrictl logs --follow --timestamps --tail 20 b1c2d3e4f5g6\n</code></pre>"},{"location":"34-crictl/#viewing-logs-of-previous-container-instances","title":"Viewing Logs of Previous Container Instances","text":"<pre><code>## When a container restarts (CrashLoopBackOff), you may want to see\n## logs from the previous instance\n\n## List all containers (including exited) for a specific pod\ncrictl ps -a --pod a1b2c3d4e5f6\n\n## The output shows ATTEMPT numbers:\n## CONTAINER      IMAGE     CREATED        STATE    NAME     ATTEMPT  POD ID\n## b1c2d3e4f5g6   nginx     1 min ago      Running  nginx    3        a1b2...\n## x1y2z3a4b5c6   nginx     3 min ago      Exited   nginx    2        a1b2...\n## d6e7f8g9h0i1   nginx     5 min ago      Exited   nginx    1        a1b2...\n\n## View logs from the previous (exited) container instance\ncrictl logs x1y2z3a4b5c6\n\n## View logs from the oldest instance\ncrictl logs d6e7f8g9h0i1\n</code></pre> <p>Log File Location</p> <p>Container logs are stored on disk at a path determined by the container runtime. You can find the log path with <code>crictl inspect &lt;container-id&gt; | jq '.status.logPath'</code>. For <code>containerd</code>, logs are typically stored under <code>/var/log/pods/</code>.</p>"},{"location":"34-crictl/#step-08-executing-commands-inside-containers-with-crictl-exec","title":"Step 08 - Executing Commands Inside Containers with crictl exec","text":"<ul> <li>The <code>crictl exec</code> command runs a command inside a running container.</li> <li>It works similarly to <code>kubectl exec</code> but at the runtime level.</li> </ul>"},{"location":"34-crictl/#basic-command-execution","title":"Basic Command Execution","text":"<pre><code>## Execute a single command in a container\ncrictl exec b1c2d3e4f5g6 ls /\n\n## Execute a command with arguments\ncrictl exec b1c2d3e4f5g6 cat /etc/hostname\n\n## Check the container's IP address\ncrictl exec b1c2d3e4f5g6 ip addr show\n\n## View running processes inside the container\ncrictl exec b1c2d3e4f5g6 ps aux\n\n## Check DNS resolution inside the container\ncrictl exec b1c2d3e4f5g6 cat /etc/resolv.conf\n\n## Test network connectivity from inside the container\ncrictl exec b1c2d3e4f5g6 wget -qO- http://kubernetes.default.svc.cluster.local/healthz\n</code></pre>"},{"location":"34-crictl/#interactive-shell","title":"Interactive Shell","text":"<pre><code>## Open an interactive shell inside the container\n## -i = interactive (keep stdin open)\n## -t = allocate a pseudo-TTY\ncrictl exec -it b1c2d3e4f5g6 /bin/sh\n\n## If the container has bash available\ncrictl exec -it b1c2d3e4f5g6 /bin/bash\n\n## Once inside the container, you can run commands interactively:\n## # ls /app\n## # env\n## # curl localhost:8080/health\n## # exit\n</code></pre>"},{"location":"34-crictl/#debugging-with-exec","title":"Debugging with exec","text":"<pre><code>## Check if a specific port is listening inside the container\ncrictl exec b1c2d3e4f5g6 netstat -tlnp\n\n## Check environment variables (useful for debugging misconfiguration)\ncrictl exec b1c2d3e4f5g6 env\n\n## Check file system mounts inside the container\ncrictl exec b1c2d3e4f5g6 mount\n\n## Read application configuration files\ncrictl exec b1c2d3e4f5g6 cat /etc/nginx/nginx.conf\n\n## Check available disk space inside the container\ncrictl exec b1c2d3e4f5g6 df -h\n\n## Test if another service is reachable from this container\ncrictl exec b1c2d3e4f5g6 ping -c 3 google.com\n</code></pre> <p>exec Only Works on Running Containers</p> <p>You cannot use <code>crictl exec</code> on exited or stopped containers. For debugging crashed containers, use <code>crictl logs</code> to view their output, or <code>crictl inspect</code> to examine their exit code and state.</p>"},{"location":"34-crictl/#step-09-image-management","title":"Step 09 - Image Management","text":"<ul> <li><code>crictl</code> provides commands to manage container images on the node.</li> <li>This is useful for verifying that images are available, checking disk usage, and cleaning up unused images.</li> </ul>"},{"location":"34-crictl/#listing-images","title":"Listing Images","text":"<pre><code>## List all images on the node\ncrictl images\n\n## Example output:\n## IMAGE                                     TAG        IMAGE ID       SIZE\n## docker.io/library/nginx                   1.25       a6bd71f48f68   67.3MB\n## registry.k8s.io/coredns/coredns           v1.11.1    cbb01a7bd410   16.2MB\n## registry.k8s.io/etcd                      3.5.10     a0d6d4a97e3c   102MB\n## registry.k8s.io/kube-apiserver            v1.29.0    e7972205b661   128MB\n## registry.k8s.io/kube-proxy                v1.29.0    2d4e6e23bb50   82.5MB\n## registry.k8s.io/pause                     3.9        e6f181688397   744kB\n\n## Show full image IDs (not truncated)\ncrictl images --no-trunc\n\n## Show only image IDs (useful for scripting)\ncrictl images --quiet\n\n## Show images with digests\ncrictl images --digests\n\n## Output in JSON format\ncrictl images -o json\n\n## Output in YAML format\ncrictl images -o yaml\n\n## Filter images by repository name\ncrictl images nginx\n\n## Filter images by full reference\ncrictl images docker.io/library/nginx:1.25\n</code></pre>"},{"location":"34-crictl/#pulling-images","title":"Pulling Images","text":"<pre><code>## Pull an image from a registry\ncrictl pull nginx:latest\n\n## Pull a specific version\ncrictl pull nginx:1.25.3\n\n## Pull from a specific registry\ncrictl pull docker.io/library/alpine:3.19\n\n## Pull from a private registry (requires authentication configured in containerd)\ncrictl pull myregistry.example.com/myapp:v1.0\n\n## Pull and show progress\ncrictl pull --no-trunc busybox:latest\n</code></pre>"},{"location":"34-crictl/#inspecting-images","title":"Inspecting Images","text":"<pre><code>## Inspect a specific image to see its details\ncrictl inspecti nginx:latest\n\n## Pretty-print the image inspection\ncrictl inspecti nginx:latest | jq .\n\n## Get the image size\ncrictl inspecti nginx:latest | jq '.info.size'\n\n## Get the image's environment variables\ncrictl inspecti nginx:latest | jq '.info.imageSpec.config.Env'\n\n## Get the image's entrypoint and command\ncrictl inspecti nginx:latest | jq '{entrypoint: .info.imageSpec.config.Entrypoint, cmd: .info.imageSpec.config.Cmd}'\n\n## Get the image's exposed ports\ncrictl inspecti nginx:latest | jq '.info.imageSpec.config.ExposedPorts'\n\n## Get the image's labels\ncrictl inspecti nginx:latest | jq '.info.imageSpec.config.Labels'\n</code></pre>"},{"location":"34-crictl/#removing-images","title":"Removing Images","text":"<pre><code>## Remove a specific image by tag\ncrictl rmi nginx:latest\n\n## Remove a specific image by image ID\ncrictl rmi a6bd71f48f68\n\n## Remove multiple images at once\ncrictl rmi nginx:latest alpine:3.19 busybox:latest\n\n## Remove all unused images (images not referenced by any container)\ncrictl rmi --prune\n\n## Force remove all images (use with caution!)\n## This will remove images even if containers are using them\ncrictl rmi --all\n</code></pre>"},{"location":"34-crictl/#image-filesystem-information","title":"Image Filesystem Information","text":"<pre><code>## Show image filesystem usage on the node\ncrictl imagefsinfo\n\n## Pretty-print the output\ncrictl imagefsinfo | jq .\n\n## This shows:\n## - Timestamp of the information\n## - Filesystem identifier (device, mountpoint)\n## - Used bytes and inodes\n## - Total bytes and inodes available\n</code></pre> <p>Removing Images in Production</p> <p>Be very careful when removing images on production nodes. If you remove an image that a running pod depends on, the pod may fail to restart (e.g., during a node reboot or pod rescheduling). Always use <code>crictl rmi --prune</code> instead of <code>crictl rmi --all</code> in production environments.</p>"},{"location":"34-crictl/#step-10-stats-and-resource-usage","title":"Step 10 - Stats and Resource Usage","text":"<ul> <li><code>crictl</code> provides commands to monitor resource usage of containers and pods in real-time.</li> <li>This is useful for identifying resource-hungry containers and debugging performance issues.</li> </ul>"},{"location":"34-crictl/#container-stats","title":"Container Stats","text":"<pre><code>## Show resource usage for all running containers\ncrictl stats\n\n## Example output:\n## CONTAINER        CPU %   MEM         DISK        INODES\n## b1c2d3e4f5g6     0.15    25.6MB      12.4kB      15\n## h7i8j9k0l1m2     0.08    18.2MB      8.2kB       12\n\n## Show stats for a specific container\ncrictl stats b1c2d3e4f5g6\n\n## Show stats for all containers (including stopped)\ncrictl stats -a\n\n## Output stats in JSON format\ncrictl stats -o json\n\n## Output stats in YAML format\ncrictl stats -o yaml\n\n## Filter stats by label\ncrictl stats --label io.kubernetes.container.name=nginx\n\n## Show stats by container ID\ncrictl stats --id b1c2d3e4f5g6\n</code></pre>"},{"location":"34-crictl/#pod-stats","title":"Pod Stats","text":"<pre><code>## Show resource usage for all pods (aggregated)\ncrictl statsp\n\n## Example output:\n## POD                                CPU %   MEM\n## a1b2c3d4e5f6                       0.23    43.8MB\n## f6e5d4c3b2a1                       0.08    18.2MB\n\n## Show stats for a specific pod\ncrictl statsp --id a1b2c3d4e5f6\n\n## Filter pod stats by label\ncrictl statsp --label app=nginx\n\n## Output pod stats in JSON format\ncrictl statsp -o json\n</code></pre>"},{"location":"34-crictl/#analyzing-resource-usage","title":"Analyzing Resource Usage","text":"<pre><code>## Find the container using the most memory\n## (using JSON output and jq for sorting)\ncrictl stats -o json | jq -r '.stats | sort_by(.memory.workingSetBytes.value) | reverse | .[] | \"\\(.attributes.id[0:12]) \\(.attributes.labels[\"io.kubernetes.container.name\"]) \\(.memory.workingSetBytes.value / 1048576)MB\"'\n\n## Find the container using the most CPU\ncrictl stats -o json | jq -r '.stats | sort_by(.cpu.usageCoreNanoSeconds.value) | reverse | .[] | \"\\(.attributes.id[0:12]) \\(.attributes.labels[\"io.kubernetes.container.name\"]) CPU: \\(.cpu.usageCoreNanoSeconds.value)\"'\n\n## Monitor stats continuously (watch mode)\n## Use the 'watch' command to refresh stats every 2 seconds\nwatch -n 2 crictl stats\n\n## Monitor a specific container continuously\nwatch -n 2 crictl stats --id b1c2d3e4f5g6\n</code></pre> <p>Comparing with kubectl top</p> <p><code>crictl stats</code> shows the raw resource usage from the container runtime, while <code>kubectl top pods</code> shows metrics from the Kubernetes Metrics Server. The values may differ slightly because they come from different sources and measure different things. <code>crictl stats</code> is node-local and does not require the Metrics Server to be installed.</p>"},{"location":"34-crictl/#step-11-runtime-info-and-debugging","title":"Step 11 - Runtime Info and Debugging","text":"<ul> <li><code>crictl</code> provides commands to query the container runtime itself for version information, configuration, and health status.</li> <li>These commands are essential for diagnosing runtime-level issues.</li> </ul>"},{"location":"34-crictl/#runtime-version","title":"Runtime Version","text":"<pre><code>## Show the CRI version and the container runtime version\ncrictl version\n\n## Example output:\n## Version:  0.1.0\n## RuntimeName:  containerd\n## RuntimeVersion:  v1.7.11\n## RuntimeApiVersion:  v1\n</code></pre>"},{"location":"34-crictl/#runtime-information","title":"Runtime Information","text":"<pre><code>## Show detailed runtime information (configuration, features, status)\ncrictl info\n\n## Pretty-print the runtime info\ncrictl info | jq .\n\n## Get the runtime's storage driver\ncrictl info | jq '.config.containerd.snapshotter'\n\n## Get the runtime's CNI configuration\ncrictl info | jq '.config.cni'\n\n## Get the runtime's cgroup driver\ncrictl info | jq '.config.containerd.runtimes.runc.options.SystemdCgroup'\n\n## Check if the runtime is ready\ncrictl info | jq '.status.conditions'\n\n## Example output for conditions:\n## [\n##   { \"type\": \"RuntimeReady\", \"status\": true, \"reason\": \"\", \"message\": \"\" },\n##   { \"type\": \"NetworkReady\", \"status\": true, \"reason\": \"\", \"message\": \"\" }\n## ]\n</code></pre>"},{"location":"34-crictl/#runtime-status-and-health-check","title":"Runtime Status and Health Check","text":"<pre><code>## Check the runtime status conditions\n## This tells you if the runtime and network plugins are healthy\ncrictl info | jq '.status'\n\n## Check specifically if RuntimeReady is true\ncrictl info | jq '.status.conditions[] | select(.type==\"RuntimeReady\")'\n\n## Check specifically if NetworkReady is true\ncrictl info | jq '.status.conditions[] | select(.type==\"NetworkReady\")'\n</code></pre>"},{"location":"34-crictl/#debugging-common-runtime-issues","title":"Debugging Common Runtime Issues","text":"<pre><code>## Check if the runtime socket exists and is accessible\nls -la /run/containerd/containerd.sock\n\n## Check if the containerd service is running\nsystemctl status containerd\n\n## Check containerd logs for errors\njournalctl -u containerd --since \"10 minutes ago\" --no-pager\n\n## For CRI-O:\nsystemctl status crio\njournalctl -u crio --since \"10 minutes ago\" --no-pager\n\n## Check the kubelet logs for CRI-related errors\njournalctl -u kubelet --since \"10 minutes ago\" --no-pager | grep -i \"cri\\|runtime\\|container\"\n\n## Verify the CRI socket permissions\nstat /run/containerd/containerd.sock\n\n## Test connectivity to the runtime socket directly\ncrictl --runtime-endpoint unix:///run/containerd/containerd.sock version\n</code></pre> <p>RuntimeReady and NetworkReady</p> <p>If <code>crictl info</code> shows <code>RuntimeReady: false</code> or <code>NetworkReady: false</code>, it means the container runtime or the CNI plugin is not functioning correctly. This will prevent new pods from starting on this node. Check the runtime logs and CNI configuration immediately.</p>"},{"location":"34-crictl/#step-12-advanced-debugging-creating-pods-and-containers-manually","title":"Step 12 - Advanced Debugging: Creating Pods and Containers Manually","text":"<ul> <li>For advanced debugging scenarios, <code>crictl</code> allows you to manually create pod sandboxes and containers.</li> <li>This is useful for testing runtime behavior, reproducing issues, and understanding the pod lifecycle.</li> </ul> <p>Manual Pod/Container Creation</p> <p>Pods and containers created manually with <code>crictl</code> are not managed by the kubelet. The kubelet will not know about them, they will not appear in <code>kubectl get pods</code>, and they will not be automatically restarted or garbage collected. Use this only for debugging purposes and clean up afterward.</p>"},{"location":"34-crictl/#create-a-pod-sandbox","title":"Create a Pod Sandbox","text":"<pre><code>## First, create a pod sandbox configuration file\ncat &lt;&lt;EOF &gt; /tmp/pod-sandbox-config.json\n{\n  \"metadata\": {\n    \"name\": \"debug-sandbox\",\n    \"namespace\": \"default\",\n    \"attempt\": 1,\n    \"uid\": \"debug-sandbox-uid-001\"\n  },\n  \"log_directory\": \"/tmp/debug-sandbox-logs\",\n  \"linux\": {}\n}\nEOF\n\n## Create the log directory\nmkdir -p /tmp/debug-sandbox-logs\n\n## Create (run) the pod sandbox\n## The 'runp' command creates and starts a new pod sandbox\nSANDBOX_ID=$(crictl runp /tmp/pod-sandbox-config.json)\necho \"Created sandbox: $SANDBOX_ID\"\n\n## Verify the sandbox was created\ncrictl pods --name debug-sandbox\n\n## Inspect the sandbox\ncrictl inspectp $SANDBOX_ID | jq .\n</code></pre>"},{"location":"34-crictl/#create-a-container-inside-the-sandbox","title":"Create a Container Inside the Sandbox","text":"<pre><code>## Create a container configuration file\ncat &lt;&lt;EOF &gt; /tmp/container-config.json\n{\n  \"metadata\": {\n    \"name\": \"debug-container\"\n  },\n  \"image\": {\n    \"image\": \"docker.io/library/busybox:latest\"\n  },\n  \"command\": [\n    \"/bin/sh\", \"-c\", \"echo 'Hello from crictl debug container!' &amp;&amp; sleep 3600\"\n  ],\n  \"log_path\": \"debug-container.log\",\n  \"linux\": {}\n}\nEOF\n\n## Make sure the image is available locally\ncrictl pull busybox:latest\n\n## Create the container (does NOT start it yet)\n## Arguments: container-config sandbox-config sandbox-id\nCONTAINER_ID=$(crictl create $SANDBOX_ID /tmp/container-config.json /tmp/pod-sandbox-config.json)\necho \"Created container: $CONTAINER_ID\"\n\n## Verify the container was created (state = Created)\ncrictl ps -a --id $CONTAINER_ID\n\n## Start the container\ncrictl start $CONTAINER_ID\n\n## Verify the container is now running\ncrictl ps --id $CONTAINER_ID\n</code></pre>"},{"location":"34-crictl/#interact-with-the-manual-container","title":"Interact with the Manual Container","text":"<pre><code>## View the container logs\ncrictl logs $CONTAINER_ID\n\n## Execute a command inside the container\ncrictl exec $CONTAINER_ID hostname\n\n## Open an interactive shell\ncrictl exec -it $CONTAINER_ID /bin/sh\n</code></pre>"},{"location":"34-crictl/#clean-up-manual-pods-and-containers","title":"Clean Up Manual Pods and Containers","text":"<pre><code>## Stop the container\ncrictl stop $CONTAINER_ID\n\n## Remove the container\ncrictl rm $CONTAINER_ID\n\n## Stop the pod sandbox\ncrictl stopp $SANDBOX_ID\n\n## Remove the pod sandbox\ncrictl rmp $SANDBOX_ID\n\n## Verify everything is cleaned up\ncrictl pods --name debug-sandbox\ncrictl ps -a --id $CONTAINER_ID\n\n## Remove the temporary config files\nrm -f /tmp/pod-sandbox-config.json /tmp/container-config.json\nrm -rf /tmp/debug-sandbox-logs\n</code></pre>"},{"location":"34-crictl/#understanding-the-manual-lifecycle","title":"Understanding the Manual Lifecycle","text":"<p>The manual pod/container lifecycle mirrors what the kubelet does automatically:</p> <pre><code>graph LR\n    A[\"runp&lt;br/&gt;(create sandbox)\"] --&gt; B[\"create&lt;br/&gt;(create container)\"]\n    B --&gt; C[\"start&lt;br/&gt;(start container)\"]\n    C --&gt; D[\"exec / logs&lt;br/&gt;(interact)\"]\n    D --&gt; E[\"stop&lt;br/&gt;(stop container)\"]\n    E --&gt; F[\"rm&lt;br/&gt;(remove container)\"]\n    F --&gt; G[\"stopp&lt;br/&gt;(stop sandbox)\"]\n    G --&gt; H[\"rmp&lt;br/&gt;(remove sandbox)\"]\n\n    style A fill:#42a5f5,stroke:#1565c0,color:#000\n    style B fill:#66bb6a,stroke:#2e7d32,color:#000\n    style C fill:#66bb6a,stroke:#2e7d32,color:#000\n    style D fill:#f9a825,stroke:#f57f17,color:#000\n    style E fill:#ef5350,stroke:#c62828,color:#fff\n    style F fill:#ef5350,stroke:#c62828,color:#fff\n    style G fill:#ef5350,stroke:#c62828,color:#fff\n    style H fill:#ef5350,stroke:#c62828,color:#fff</code></pre> <p>kubelet Garbage Collection</p> <p>In normal Kubernetes operation, the kubelet automatically garbage collects exited containers and unused pod sandboxes. When you create pods and containers manually with <code>crictl</code>, the kubelet may garbage collect them if they are not associated with a known pod in the API server. Always clean up your manual resources promptly.</p>"},{"location":"34-crictl/#crictl-command-reference","title":"crictl Command Reference","text":"<p>Below is a quick-reference table of all major <code>crictl</code> commands:</p> Command Description <code>crictl pods</code> List pod sandboxes <code>crictl inspectp</code> Inspect a pod sandbox <code>crictl runp</code> Create and start a pod sandbox <code>crictl stopp</code> Stop a pod sandbox <code>crictl rmp</code> Remove a pod sandbox <code>crictl ps</code> List containers <code>crictl inspect</code> Inspect a container <code>crictl create</code> Create a container <code>crictl start</code> Start a container <code>crictl stop</code> Stop a container <code>crictl rm</code> Remove a container <code>crictl exec</code> Execute a command in a running container <code>crictl logs</code> View container logs <code>crictl attach</code> Attach to a running container <code>crictl port-forward</code> Forward local port to a pod sandbox <code>crictl images</code> List images <code>crictl inspecti</code> Inspect an image <code>crictl pull</code> Pull an image <code>crictl rmi</code> Remove an image <code>crictl imagefsinfo</code> Show image filesystem info <code>crictl stats</code> Show container resource usage <code>crictl statsp</code> Show pod resource usage <code>crictl info</code> Show runtime information <code>crictl version</code> Show CRI and runtime versions <code>crictl completion</code> Generate shell completion scripts"},{"location":"34-crictl/#exercises","title":"Exercises","text":"<p>The following exercises will test your understanding of <code>crictl</code> concepts. Try to solve each exercise on your own before revealing the solution.</p>"},{"location":"34-crictl/#01-list-all-pods-in-a-specific-namespace","title":"01. List All Pods in a Specific Namespace","text":"<p>List all pod sandboxes running in the <code>kube-system</code> namespace using <code>crictl</code>.</p>"},{"location":"34-crictl/#scenario","title":"Scenario:","text":"<p>\u25e6 You are debugging DNS issues and need to check the state of CoreDNS pods on a specific node. \u25e6 You need to verify that all system-level pods are in the <code>Ready</code> state.</p> <p>Hint: Use <code>crictl pods</code> with the <code>--namespace</code> filter.</p> Solution <pre><code>## List all pods in the kube-system namespace\ncrictl pods --namespace kube-system\n\n## List only Ready pods in kube-system\ncrictl pods --namespace kube-system --state Ready\n\n## List pods in kube-system with specific labels (e.g., CoreDNS)\ncrictl pods --namespace kube-system --label k8s-app=kube-dns\n\n## Show full output in JSON format for detailed inspection\ncrictl pods --namespace kube-system -o json | jq '.items[] | {name: .metadata.name, state: .state, id: .id}'\n</code></pre>"},{"location":"34-crictl/#02-find-all-exited-containers-and-get-their-exit-codes","title":"02. Find All Exited Containers and Get Their Exit Codes","text":"<p>Identify all containers that have exited and determine their exit codes.</p>"},{"location":"34-crictl/#scenario_1","title":"Scenario:","text":"<p>\u25e6 A pod is in <code>CrashLoopBackOff</code> and you need to understand why containers keep failing. \u25e6 Exit codes provide critical information about the nature of the failure.</p> <p>Hint: Use <code>crictl ps -a</code> to show all containers, filter by state <code>Exited</code>, and then use <code>crictl inspect</code> to get the exit codes.</p> Solution <pre><code>## List all exited containers\ncrictl ps -a --state Exited\n\n## Get exit codes for all exited containers using JSON output and jq\ncrictl ps -a --state Exited -o json | jq '.containers[] | {\n  name: .metadata.name,\n  id: .id,\n  podSandboxId: .podSandboxId,\n  state: .state,\n  createdAt: .createdAt,\n  finishedAt: .finishedAt\n}'\n\n## Inspect a specific exited container for its exit code\n## Replace &lt;container-id&gt; with the actual container ID\ncrictl inspect &lt;container-id&gt; | jq '{\n  name: .status.metadata.name,\n  exitCode: .status.exitCode,\n  reason: .status.reason,\n  message: .status.message,\n  finishedAt: .status.finishedAt\n}'\n\n## Quick one-liner: list all exited containers with exit codes\nfor CID in $(crictl ps -a --state Exited -q); do\n  NAME=$(crictl inspect $CID 2&gt;/dev/null | jq -r '.status.metadata.name')\n  EXIT=$(crictl inspect $CID 2&gt;/dev/null | jq -r '.status.exitCode')\n  echo \"Container: $CID Name: $NAME ExitCode: $EXIT\"\ndone\n\n## Common exit codes reference:\n##   0   = Success\n##   1   = General error\n##   137 = OOMKilled (SIGKILL)\n##   139 = Segfault (SIGSEGV)\n##   143 = Graceful termination (SIGTERM)\n</code></pre>"},{"location":"34-crictl/#03-get-container-logs-from-the-last-5-minutes","title":"03. Get Container Logs from the Last 5 Minutes","text":"<p>Retrieve the logs from a specific container, showing only entries from the last 5 minutes, with timestamps.</p>"},{"location":"34-crictl/#scenario_2","title":"Scenario:","text":"<p>\u25e6 An application started misbehaving 5 minutes ago, and you need to see only the recent log entries. \u25e6 Including timestamps helps correlate log events with external incidents.</p> <p>Hint: Use <code>crictl logs</code> with <code>--since</code> and <code>--timestamps</code> flags. You will need to calculate the timestamp for 5 minutes ago.</p> Solution <pre><code>## First, identify the container you want to inspect\ncrictl ps\n\n## Note the container ID from the output\n\n## Calculate the timestamp for 5 minutes ago and get logs\n## On most Linux systems:\nSINCE=$(date -u -d '5 minutes ago' +%Y-%m-%dT%H:%M:%SZ)\ncrictl logs --since \"$SINCE\" --timestamps &lt;container-id&gt;\n\n## On systems without GNU date (e.g., Alpine / BusyBox):\n## Use a specific timestamp instead\ncrictl logs --since \"2024-01-15T10:25:00Z\" --timestamps &lt;container-id&gt;\n\n## Alternative: get the last 100 lines with timestamps\n## (useful when you are not sure of the exact time range)\ncrictl logs --tail 100 --timestamps &lt;container-id&gt;\n\n## Follow logs from now with timestamps (real-time monitoring)\ncrictl logs --follow --timestamps --tail 0 &lt;container-id&gt;\n</code></pre>"},{"location":"34-crictl/#04-execute-a-command-inside-a-running-container","title":"04. Execute a Command Inside a Running Container","text":"<p>Use <code>crictl exec</code> to check the environment variables and network configuration of a running container.</p>"},{"location":"34-crictl/#scenario_3","title":"Scenario:","text":"<p>\u25e6 An application is failing to connect to a database, and you suspect the environment variables are misconfigured. \u25e6 You need to verify both the environment variables and the network connectivity from inside the container.</p> <p>Hint: Use <code>crictl exec</code> with commands like <code>env</code>, <code>cat /etc/resolv.conf</code>, and <code>wget</code> or <code>curl</code>.</p> Solution <pre><code>## First, identify the running container\ncrictl ps\n\n## Check environment variables inside the container\ncrictl exec &lt;container-id&gt; env\n\n## Filter for specific environment variables (e.g., database-related)\ncrictl exec &lt;container-id&gt; env | grep -i db\n\n## Check DNS configuration\ncrictl exec &lt;container-id&gt; cat /etc/resolv.conf\n\n## Check the container's IP address\ncrictl exec &lt;container-id&gt; ip addr show 2&gt;/dev/null || \\\n  crictl exec &lt;container-id&gt; cat /etc/hosts\n\n## Test connectivity to a database service\ncrictl exec &lt;container-id&gt; wget -qO- --timeout=5 http://db-service:5432 2&gt;&amp;1 || \\\n  echo \"Connection test completed (non-zero exit may be expected for non-HTTP services)\"\n\n## Test DNS resolution\ncrictl exec &lt;container-id&gt; nslookup db-service.default.svc.cluster.local 2&gt;/dev/null || \\\n  crictl exec &lt;container-id&gt; cat /etc/hosts\n\n## Check listening ports inside the container\ncrictl exec &lt;container-id&gt; netstat -tlnp 2&gt;/dev/null || \\\n  crictl exec &lt;container-id&gt; ss -tlnp 2&gt;/dev/null || \\\n  echo \"Neither netstat nor ss available in container\"\n\n## Open an interactive shell for more thorough investigation\ncrictl exec -it &lt;container-id&gt; /bin/sh\n</code></pre>"},{"location":"34-crictl/#05-pull-an-image-and-verify-it-is-available","title":"05. Pull an Image and Verify It Is Available","text":"<p>Pull the <code>alpine:3.19</code> image using <code>crictl</code> and verify it is available in the local container runtime image store.</p>"},{"location":"34-crictl/#scenario_4","title":"Scenario:","text":"<p>\u25e6 You need to pre-pull images on a node before deploying pods to reduce startup time. \u25e6 You want to verify that a specific image version is available on the node.</p> <p>Hint: Use <code>crictl pull</code> to download the image and <code>crictl images</code> to verify it is present.</p> Solution <pre><code>## Check if the image is already available\ncrictl images | grep alpine\n\n## Pull the specific image\ncrictl pull alpine:3.19\n\n## Verify the image was pulled successfully\ncrictl images | grep alpine\n\n## Get detailed information about the pulled image\ncrictl inspecti alpine:3.19 | jq '{\n  size: .info.size,\n  entrypoint: .info.imageSpec.config.Entrypoint,\n  cmd: .info.imageSpec.config.Cmd,\n  env: .info.imageSpec.config.Env\n}'\n\n## Verify by pulling with the full registry path\ncrictl pull docker.io/library/alpine:3.19\n\n## List all alpine images with their digests\ncrictl images --digests | grep alpine\n\n## Show image filesystem usage to see the impact of the new image\ncrictl imagefsinfo | jq '.status.usedBytes'\n</code></pre>"},{"location":"34-crictl/#06-find-the-container-runtime-version-and-socket-path","title":"06. Find the Container Runtime Version and Socket Path","text":"<p>Determine the container runtime version, the CRI socket path, and whether the runtime is healthy.</p>"},{"location":"34-crictl/#scenario_5","title":"Scenario:","text":"<p>\u25e6 You are troubleshooting node issues and need to verify that the container runtime is functioning correctly. \u25e6 You need to document the runtime version for compliance and compatibility purposes.</p> <p>Hint: Use <code>crictl version</code>, <code>crictl info</code>, and check the configuration file at <code>/etc/crictl.yaml</code>.</p> Solution <pre><code>## Get the CRI and runtime version\ncrictl version\n\n## Sample output:\n## Version:  0.1.0\n## RuntimeName:  containerd\n## RuntimeVersion:  v1.7.11\n## RuntimeApiVersion:  v1\n\n## Check the configured socket path\ncat /etc/crictl.yaml\n\n## Get detailed runtime information\ncrictl info | jq '{\n  runtime: .status,\n  cniConfig: .config.cni,\n  cgroupDriver: .config.containerd.runtimes.runc.options.SystemdCgroup\n}'\n\n## Check the runtime health (conditions)\ncrictl info | jq '.status.conditions[] | {type, status}'\n\n## Verify the socket file exists and check permissions\nls -la /run/containerd/containerd.sock\n\n## Check the containerd (or CRI-O) service status\nsystemctl status containerd 2&gt;/dev/null || systemctl status crio 2&gt;/dev/null\n\n## Get the runtime's build information\ncontainerd --version 2&gt;/dev/null || crio --version 2&gt;/dev/null\n</code></pre>"},{"location":"34-crictl/#07-get-resource-usage-stats-for-all-running-pods","title":"07. Get Resource Usage Stats for All Running Pods","text":"<p>Display the CPU and memory usage for all running pods on this node.</p>"},{"location":"34-crictl/#scenario_6","title":"Scenario:","text":"<p>\u25e6 You suspect a noisy-neighbor problem where one pod is consuming excessive resources. \u25e6 You need a quick overview of resource consumption on this specific node.</p> <p>Hint: Use <code>crictl statsp</code> to get pod-level stats and <code>crictl stats</code> for container-level stats.</p> Solution <pre><code>## Show resource usage for all pods\ncrictl statsp\n\n## Show resource usage for all containers (more granular)\ncrictl stats\n\n## Get pod stats in JSON format for analysis\ncrictl statsp -o json | jq '.stats[] | {\n  podId: .attributes.id[0:12],\n  podName: .attributes.labels[\"io.kubernetes.pod.name\"],\n  namespace: .attributes.labels[\"io.kubernetes.pod.namespace\"],\n  cpu: .cpu.usageCoreNanoSeconds.value,\n  memory: (.memory.workingSetBytes.value / 1048576)\n}' | head -50\n\n## Get container stats in JSON format for analysis\ncrictl stats -o json | jq '.stats[] | {\n  containerId: .attributes.id[0:12],\n  containerName: .attributes.labels[\"io.kubernetes.container.name\"],\n  cpu: .cpu.usageCoreNanoSeconds.value,\n  memoryMB: (.memory.workingSetBytes.value / 1048576)\n}'\n\n## Monitor stats in real-time with watch\nwatch -n 5 crictl statsp\n\n## Sort containers by memory usage (highest first)\ncrictl stats -o json | jq '[.stats[] | {\n  name: .attributes.labels[\"io.kubernetes.container.name\"],\n  memoryMB: (.memory.workingSetBytes.value / 1048576)\n}] | sort_by(.memoryMB) | reverse'\n</code></pre>"},{"location":"34-crictl/#08-inspect-a-pods-network-configuration","title":"08. Inspect a Pod\u2019s Network Configuration","text":"<p>Find the IP address, DNS settings, and network namespace of a specific pod using <code>crictl</code>.</p>"},{"location":"34-crictl/#scenario_7","title":"Scenario:","text":"<p>\u25e6 A pod cannot communicate with other pods, and you need to verify its network configuration. \u25e6 You need the network namespace path to run low-level debugging tools like <code>tcpdump</code>.</p> <p>Hint: Use <code>crictl inspectp</code> with jq to extract the network information, and <code>nsenter</code> to inspect the network namespace.</p> Solution <pre><code>## First, identify the pod sandbox\ncrictl pods --name &lt;pod-name&gt;\n\n## Note the POD ID from the output\n\n## Get the pod's IP address\ncrictl inspectp &lt;pod-id&gt; | jq '.status.network'\n\n## Expected output:\n## { \"additionalIps\": [], \"ip\": \"10.244.0.15\" }\n\n## Get the pod's full network namespace information\ncrictl inspectp &lt;pod-id&gt; | jq '.info.runtimeSpec.linux.namespaces[] | select(.type==\"network\")'\n\n## Get the network namespace path\nNETNS=$(crictl inspectp &lt;pod-id&gt; | jq -r '.info.runtimeSpec.linux.namespaces[] | select(.type==\"network\") | .path')\necho \"Network namespace: $NETNS\"\n\n## Enter the pod's network namespace and inspect network interfaces\nsudo nsenter --net=$NETNS ip addr show\n\n## Check routes in the pod's network namespace\nsudo nsenter --net=$NETNS ip route show\n\n## Check DNS resolution from the pod's perspective\nsudo nsenter --net=$NETNS cat /etc/resolv.conf\n\n## Run tcpdump in the pod's network namespace (for packet capture)\nsudo nsenter --net=$NETNS tcpdump -i eth0 -c 10\n\n## Check iptables rules in the pod's namespace\nsudo nsenter --net=$NETNS iptables -L -n -v\n\n## Test connectivity from the pod's namespace\nsudo nsenter --net=$NETNS ping -c 3 &lt;target-ip&gt;\n</code></pre>"},{"location":"34-crictl/#09-find-which-container-is-using-the-most-memory","title":"09. Find Which Container Is Using the Most Memory","text":"<p>Identify the container on this node that is consuming the most memory.</p>"},{"location":"34-crictl/#scenario_8","title":"Scenario:","text":"<p>\u25e6 The node is under memory pressure and pods are being evicted. \u25e6 You need to quickly identify the biggest memory consumer to take corrective action.</p> <p>Hint: Use <code>crictl stats</code> with JSON output and jq to sort by memory usage.</p> Solution <pre><code>## Get stats in JSON and sort by memory (highest first)\ncrictl stats -o json | jq '[.stats[] | {\n  id: .attributes.id[0:12],\n  name: .attributes.labels[\"io.kubernetes.container.name\"],\n  pod: .attributes.labels[\"io.kubernetes.pod.name\"],\n  namespace: .attributes.labels[\"io.kubernetes.pod.namespace\"],\n  memoryMB: ((.memory.workingSetBytes.value // 0) / 1048576 | floor)\n}] | sort_by(.memoryMB) | reverse | .[0:10]'\n\n## Quick one-liner to find the top memory consumer\ncrictl stats -o json | jq '[.stats[] | {\n  name: .attributes.labels[\"io.kubernetes.container.name\"],\n  memoryMB: ((.memory.workingSetBytes.value // 0) / 1048576 | floor)\n}] | sort_by(.memoryMB) | reverse | .[0]'\n\n## After identifying the container, inspect its memory limits\ncrictl inspect &lt;container-id&gt; | jq '.info.runtimeSpec.linux.resources.memory'\n\n## Check if the container has an OOMKilled history\n## Look for exited containers with exit code 137\ncrictl ps -a --state Exited -o json | jq '.containers[] | select(.metadata.name == \"&lt;container-name&gt;\")' | head -20\n\n## Get the PID of the top consumer and check its memory map on the host\nPID=$(crictl inspect &lt;container-id&gt; | jq '.info.pid')\ncat /proc/$PID/status | grep -i vmrss\n</code></pre>"},{"location":"34-crictl/#10-debug-a-crashloopbackoff-pod-using-crictl","title":"10. Debug a CrashLoopBackOff Pod Using crictl","text":"<p>A pod is in <code>CrashLoopBackOff</code>. Use <code>crictl</code> to investigate the root cause by examining container exit codes and logs.</p>"},{"location":"34-crictl/#scenario_9","title":"Scenario:","text":"<p>\u25e6 <code>kubectl describe pod</code> shows the pod is in <code>CrashLoopBackOff</code>, but the events do not give enough detail. \u25e6 You need to inspect the actual container exit codes and logs from previous crash instances to understand the failure.</p> <p>Hint: Use <code>crictl ps -a</code> to find all container instances (including exited ones), then use <code>crictl inspect</code> for exit codes and <code>crictl logs</code> for the crash output.</p> Solution <pre><code>## Step 1: Find the pod sandbox for the crashing pod\ncrictl pods --name &lt;pod-name&gt;\n## Note the POD ID\n\n## Step 2: List ALL containers for this pod (including exited ones)\ncrictl ps -a --pod &lt;pod-id&gt;\n\n## You should see multiple instances with increasing ATTEMPT numbers:\n## CONTAINER      IMAGE     CREATED        STATE    NAME     ATTEMPT  POD ID\n## aaa111bbb222   myapp     30s ago        Running  myapp    5        pod-id...\n## ccc333ddd444   myapp     2m ago         Exited   myapp    4        pod-id...\n## eee555fff666   myapp     4m ago         Exited   myapp    3        pod-id...\n\n## Step 3: Inspect an exited container to find the exit code\ncrictl inspect ccc333ddd444 | jq '{\n  name: .status.metadata.name,\n  state: .status.state,\n  exitCode: .status.exitCode,\n  reason: .status.reason,\n  message: .status.message,\n  startedAt: .status.startedAt,\n  finishedAt: .status.finishedAt\n}'\n\n## Step 4: View the logs from the exited (crashed) container\ncrictl logs ccc333ddd444\n\n## Step 5: View logs from the oldest crash instance for comparison\ncrictl logs eee555fff666\n\n## Step 6: Check if it is an OOMKill (exit code 137)\nEXIT_CODE=$(crictl inspect ccc333ddd444 | jq '.status.exitCode')\nif [ \"$EXIT_CODE\" == \"137\" ]; then\n  echo \"Container was OOMKilled!\"\n  echo \"Check memory limits:\"\n  crictl inspect ccc333ddd444 | jq '.info.runtimeSpec.linux.resources.memory'\n  echo \"Check current memory usage of the running instance:\"\n  RUNNING_CID=$(crictl ps --pod &lt;pod-id&gt; -q)\n  crictl stats --id $RUNNING_CID\nfi\n\n## Step 7: Check if it is a command-not-found issue (exit code 127)\nif [ \"$EXIT_CODE\" == \"127\" ]; then\n  echo \"Command not found inside container!\"\n  echo \"Check the entrypoint/command:\"\n  crictl inspect ccc333ddd444 | jq '.info.runtimeSpec.process.args'\nfi\n\n## Step 8: If it is a configuration issue, check env vars\ncrictl inspect ccc333ddd444 | jq '.info.runtimeSpec.process.env'\n</code></pre>"},{"location":"34-crictl/#11-clean-up-unused-images-to-free-disk-space","title":"11. Clean Up Unused Images to Free Disk Space","text":"<p>Remove all container images that are not currently referenced by any container on the node.</p>"},{"location":"34-crictl/#scenario_10","title":"Scenario:","text":"<p>\u25e6 The node\u2019s disk is running low because old images from previous deployments are still cached. \u25e6 You need to safely clean up unused images without affecting running workloads.</p> <p>Hint: Use <code>crictl rmi --prune</code> to remove images not referenced by any container. Use <code>crictl imagefsinfo</code> to check disk usage before and after.</p> Solution <pre><code>## Step 1: Check current disk usage by images\ncrictl imagefsinfo | jq '{\n  usedBytes: .status.usedBytes.value,\n  usedMB: (.status.usedBytes.value / 1048576 | floor),\n  totalBytes: .status.fsId\n}'\n\n## Step 2: List all images to see what is on the node\ncrictl images\n\n## Step 3: Count total images\necho \"Total images: $(crictl images -q | wc -l)\"\n\n## Step 4: Count running containers (to understand which images are in use)\necho \"Running containers: $(crictl ps -q | wc -l)\"\n\n## Step 5: Prune unused images (safe - only removes unreferenced images)\ncrictl rmi --prune\n\n## Step 6: Verify the cleanup\ncrictl images\necho \"Remaining images: $(crictl images -q | wc -l)\"\n\n## Step 7: Check disk usage after cleanup\ncrictl imagefsinfo | jq '{\n  usedBytes: .status.usedBytes.value,\n  usedMB: (.status.usedBytes.value / 1048576 | floor)\n}'\n\n## Note: If you need to remove a specific old image\ncrictl rmi &lt;image-name&gt;:&lt;tag&gt;\n\n## To see which images are in use by running containers:\nfor CID in $(crictl ps -q); do\n  crictl inspect $CID 2&gt;/dev/null | jq -r '.status.image.image'\ndone | sort -u\n</code></pre>"},{"location":"34-crictl/#12-create-a-pod-sandbox-and-container-manually-with-crictl-advanced","title":"12. Create a Pod Sandbox and Container Manually with crictl (Advanced)","text":"<p>Create a pod sandbox and run a container inside it manually using <code>crictl</code>. This simulates what the kubelet does when it creates a pod.</p>"},{"location":"34-crictl/#scenario_11","title":"Scenario:","text":"<p>\u25e6 You want to understand the pod lifecycle at the runtime level. \u25e6 You need to test whether the container runtime can create and run containers independently of the kubelet (to isolate issues).</p> <p>Hint: Create JSON config files for the pod sandbox and container, then use <code>crictl runp</code>, <code>crictl create</code>, and <code>crictl start</code>.</p> Solution <pre><code>## Step 1: Create the pod sandbox configuration\ncat &lt;&lt;EOF &gt; /tmp/test-sandbox.json\n{\n  \"metadata\": {\n    \"name\": \"test-pod\",\n    \"namespace\": \"default\",\n    \"attempt\": 1,\n    \"uid\": \"test-pod-uid-$(date +%s)\"\n  },\n  \"log_directory\": \"/tmp/test-pod-logs\",\n  \"linux\": {}\n}\nEOF\n\n## Create the log directory\nmkdir -p /tmp/test-pod-logs\n\n## Step 2: Create the pod sandbox\nSANDBOX_ID=$(crictl runp /tmp/test-sandbox.json)\necho \"Sandbox ID: $SANDBOX_ID\"\n\n## Step 3: Verify the sandbox is running\ncrictl pods --id $SANDBOX_ID\n\n## Step 4: Create the container configuration\ncat &lt;&lt;EOF &gt; /tmp/test-container.json\n{\n  \"metadata\": {\n    \"name\": \"test-nginx\"\n  },\n  \"image\": {\n    \"image\": \"docker.io/library/nginx:alpine\"\n  },\n  \"log_path\": \"test-nginx.log\",\n  \"linux\": {}\n}\nEOF\n\n## Step 5: Pull the image if not already available\ncrictl pull nginx:alpine\n\n## Step 6: Create the container (returns container ID)\nCONTAINER_ID=$(crictl create $SANDBOX_ID /tmp/test-container.json /tmp/test-sandbox.json)\necho \"Container ID: $CONTAINER_ID\"\n\n## Step 7: Start the container\ncrictl start $CONTAINER_ID\n\n## Step 8: Verify the container is running\ncrictl ps --id $CONTAINER_ID\n\n## Step 9: Test the container\ncrictl exec $CONTAINER_ID nginx -v\ncrictl exec $CONTAINER_ID curl -s http://localhost:80 2&gt;/dev/null || \\\n  crictl exec $CONTAINER_ID wget -qO- http://localhost:80\n\n## Step 10: View container logs\ncrictl logs $CONTAINER_ID\n\n## Step 11: Inspect the container\ncrictl inspect $CONTAINER_ID | jq '{\n  state: .status.state,\n  image: .status.image.image,\n  pid: .info.pid,\n  startedAt: .status.startedAt\n}'\n\n## Step 12: Clean up everything\ncrictl stop $CONTAINER_ID\ncrictl rm $CONTAINER_ID\ncrictl stopp $SANDBOX_ID\ncrictl rmp $SANDBOX_ID\n\n## Verify cleanup\ncrictl pods --id $SANDBOX_ID\ncrictl ps -a --id $CONTAINER_ID\n\n## Remove temp files\nrm -f /tmp/test-sandbox.json /tmp/test-container.json\nrm -rf /tmp/test-pod-logs\n</code></pre>"},{"location":"34-crictl/#finalize-cleanup","title":"Finalize &amp; Cleanup","text":"<ul> <li>If you created any manual pods or containers during this lab, clean them up:</li> </ul> <pre><code>## List all manually created sandboxes (ones not managed by kubelet)\n## These typically have simple names like \"debug-sandbox\" or \"test-pod\"\ncrictl pods\n\n## Stop and remove any manual sandboxes\n## Replace &lt;sandbox-id&gt; with the actual ID\ncrictl stopp &lt;sandbox-id&gt;\ncrictl rmp &lt;sandbox-id&gt;\n\n## Clean up any stopped containers that you created\ncrictl ps -a --state Exited\n\n## Remove stopped containers by ID\ncrictl rm &lt;container-id&gt;\n</code></pre> <ul> <li>If you pulled test images that you no longer need:</li> </ul> <pre><code>## Remove specific test images\ncrictl rmi busybox:latest\ncrictl rmi alpine:3.19\ncrictl rmi nginx:alpine\n\n## Or prune all unused images\ncrictl rmi --prune\n</code></pre> <ul> <li>If you modified the <code>crictl</code> configuration (e.g., enabled debug mode), restore it:</li> </ul> <pre><code>## Restore the standard crictl configuration\nsudo tee /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n</code></pre> <ul> <li>Remove any temporary files created during the lab:</li> </ul> <pre><code>## Clean up temporary config files\nrm -f /tmp/pod-sandbox-config.json\nrm -f /tmp/container-config.json\nrm -f /tmp/test-sandbox.json\nrm -f /tmp/test-container.json\nrm -rf /tmp/debug-sandbox-logs\nrm -rf /tmp/test-pod-logs\n</code></pre>"},{"location":"34-crictl/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>\u201ccrictl: command not found\u201d:</li> </ul> <p>Ensure <code>crictl</code> is installed and in your <code>PATH</code>. Check the installation step or verify the binary location:</p> <pre><code>## Check if crictl exists anywhere on the system\nfind / -name \"crictl\" -type f 2&gt;/dev/null\n\n## If found but not in PATH, add it\nexport PATH=$PATH:/usr/local/bin\n</code></pre> <p></p> <ul> <li>\u201cfailed to connect: connection error: \u2026 /run/containerd/containerd.sock: no such file or directory\u201d:</li> </ul> <p>The container runtime socket does not exist at the configured path. Check which runtime is installed and update the configuration:</p> <pre><code>## Check which sockets exist on the system\nls -la /run/containerd/containerd.sock 2&gt;/dev/null\nls -la /var/run/crio/crio.sock 2&gt;/dev/null\nls -la /run/cri-dockerd.sock 2&gt;/dev/null\n\n## Check if containerd is running\nsystemctl status containerd\n\n## Update /etc/crictl.yaml with the correct socket path\n</code></pre> <p></p> <ul> <li>\u201cpermission denied\u201d when running crictl commands:</li> </ul> <p><code>crictl</code> needs access to the container runtime socket, which typically requires root privileges:</p> <pre><code>## Run with sudo\nsudo crictl ps\n\n## Or check socket permissions\nls -la /run/containerd/containerd.sock\n\n## The socket should be owned by root:root with permissions srw-rw----\n## You can add your user to the appropriate group if needed\n</code></pre> <p></p> <ul> <li>\u201ccrictl pods\u201d returns empty output but pods are running (kubectl shows pods):</li> </ul> <p>This usually means <code>crictl</code> is connected to the wrong runtime socket:</p> <pre><code>## Check which runtime the kubelet is using\nps aux | grep kubelet | grep container-runtime-endpoint\n\n## Verify your crictl configuration matches\ncat /etc/crictl.yaml\n\n## Try specifying the endpoint explicitly\ncrictl --runtime-endpoint unix:///run/containerd/containerd.sock pods\n</code></pre> <p></p> <ul> <li>\u201ccontext deadline exceeded\u201d errors:</li> </ul> <p>The runtime is not responding within the configured timeout. This could indicate the runtime is overloaded or having issues:</p> <pre><code>## Increase the timeout in /etc/crictl.yaml\nsudo sed -i 's/timeout: 10/timeout: 30/' /etc/crictl.yaml\n\n## Or pass a longer timeout on the command line\ncrictl --timeout 30 ps\n\n## Check the runtime's health\nsystemctl status containerd\njournalctl -u containerd --since \"5 minutes ago\" --no-pager\n</code></pre> <p></p> <ul> <li>\u201cjq: command not found\u201d when trying to parse JSON output:</li> </ul> <p>Install <code>jq</code> to parse JSON output from <code>crictl</code>:</p> <pre><code>## Debian/Ubuntu\nsudo apt-get install -y jq\n\n## RHEL/CentOS/Fedora\nsudo yum install -y jq\n\n## Alpine\nsudo apk add jq\n</code></pre> <p></p> <ul> <li>\u201ccrictl info\u201d shows <code>RuntimeReady: false</code>:</li> </ul> <p>The container runtime is not healthy. Check the runtime service and its logs:</p> <pre><code>## Restart the container runtime\nsudo systemctl restart containerd\n\n## Check the runtime logs for errors\njournalctl -u containerd --since \"10 minutes ago\" --no-pager\n\n## Verify the runtime recovers\ncrictl info | jq '.status.conditions'\n</code></pre> <p></p> <ul> <li>\u201ccrictl info\u201d shows <code>NetworkReady: false</code>:</li> </ul> <p>The CNI plugin is not functioning correctly. Check the CNI configuration:</p> <pre><code>## Check CNI configuration files\nls -la /etc/cni/net.d/\n\n## Check CNI binary directory\nls -la /opt/cni/bin/\n\n## Check kubelet logs for CNI errors\njournalctl -u kubelet --since \"10 minutes ago\" --no-pager | grep -i cni\n\n## Restart the kubelet to re-initialize networking\nsudo systemctl restart kubelet\n</code></pre>"},{"location":"34-crictl/#next-steps","title":"Next Steps","text":"<ul> <li>Explore <code>nerdctl</code> for a Docker-compatible CLI experience with <code>containerd</code>: nerdctl on GitHub.</li> <li>Learn about <code>ctr</code>, the low-level <code>containerd</code> CLI tool, for even deeper runtime debugging.</li> <li>Study the CRI specification to understand the full gRPC API that <code>crictl</code> uses: CRI API.</li> <li>Practice using <code>nsenter</code> together with <code>crictl</code> for advanced network debugging inside pod sandboxes.</li> <li>Explore the <code>kubectl debug node/&lt;node-name&gt;</code> command as an alternative way to get a shell on a node without SSH.</li> <li>Learn about Kubernetes node-level troubleshooting: Kubernetes Node Debugging Guide.</li> <li>Set up shell completion for <code>crictl</code> to speed up your debugging workflow: <code>crictl completion bash &gt; /etc/bash_completion.d/crictl</code>.</li> <li>Explore the <code>RuntimeClass</code> feature to understand how Kubernetes supports multiple container runtimes on the same node.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/","title":"kubectl Deep Dive","text":"<ul> <li>Welcome to the <code>kubectl</code> deep-dive hands-on lab! This is not a beginner tutorial - it is a comprehensive, in-depth exploration of everything <code>kubectl</code> can do.</li> <li>You will master advanced output formatting, JSONPath expressions, resource patching strategies, interactive debugging, RBAC inspection, raw API access, plugin management, and performance optimization techniques.</li> <li>By the end of this lab, <code>kubectl</code> will feel like a natural extension of your hands when working with any Kubernetes cluster.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>How <code>kubectl</code> communicates with the Kubernetes API server (the full request lifecycle)</li> <li>kubeconfig file structure: clusters, users, contexts, and merging multiple configs</li> <li>The Kubernetes API resource model (Group, Version, Resource)</li> <li>All output formats: JSON, YAML, wide, name, JSONPath, custom-columns, go-template</li> <li>Advanced JSONPath expressions for filtering, sorting, and extracting data</li> <li>Field selectors, label selectors, and advanced <code>get</code> operations</li> <li>Resource inspection with <code>describe</code>, <code>explain</code>, <code>api-resources</code>, and <code>api-versions</code></li> <li>Declarative vs. imperative resource management (<code>apply</code> vs. <code>create</code> vs. <code>replace</code>)</li> <li>Server-side apply, dry-run modes, and <code>kubectl diff</code></li> <li>All three patching strategies: strategic merge, JSON merge, and JSON patch</li> <li>Interactive debugging: <code>exec</code>, <code>cp</code>, <code>port-forward</code>, <code>attach</code>, and <code>debug</code></li> <li>Operational commands: <code>wait</code>, <code>rollout</code>, and <code>autoscale</code></li> <li>RBAC inspection with <code>auth can-i</code>, <code>auth whoami</code>, and <code>auth reconcile</code></li> <li>Extending kubectl with plugins and krew</li> <li>Raw API access via <code>kubectl proxy</code> and token-based requests</li> <li>Performance tips: watch mode, bash completion, aliases, and resource caching</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link kubectl Official Reference kubernetes.io/docs/reference/kubectl kubectl Cheat Sheet kubernetes.io/docs/reference/kubectl/cheatsheet kubeconfig Documentation kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig JSONPath Support kubernetes.io/docs/reference/kubectl/jsonpath kubectl apply Documentation kubernetes.io/docs/reference/kubectl/generated/kubectl_apply Server-Side Apply kubernetes.io/docs/reference/using-api/server-side-apply kubectl patch Documentation kubernetes.io/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch kubectl debug Documentation kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod RBAC Authorization kubernetes.io/docs/reference/access-authn-authz/rbac Extending kubectl with Plugins kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins Krew Plugin Manager krew.sigs.k8s.io Kubernetes API Concepts kubernetes.io/docs/reference/using-api Managing Resources kubernetes.io/docs/concepts/cluster-administration/manage-deployment"},{"location":"36-kubectl-Deep-Dive/#introduction","title":"Introduction","text":""},{"location":"36-kubectl-Deep-Dive/#how-kubectl-works","title":"How kubectl Works","text":"<ul> <li><code>kubectl</code> is the command-line tool for interacting with Kubernetes clusters.</li> <li>It does not communicate directly with nodes, pods, or containers. Instead, every single <code>kubectl</code> command translates into one or more HTTP requests to the Kubernetes API server.</li> <li>The flow is always the same: <code>kubectl</code> reads your kubeconfig file to determine which cluster to talk to, authenticates with the API server, sends the request, and displays the response.</li> </ul> <p>The full request lifecycle looks like this:</p> <ol> <li>Read kubeconfig - <code>kubectl</code> locates the kubeconfig file (default: <code>~/.kube/config</code> or <code>$KUBECONFIG</code>) and reads the current context.</li> <li>Resolve cluster and credentials - From the current context, <code>kubectl</code> determines the API server URL, the client certificate or token, and the CA bundle.</li> <li>Build HTTP request - The <code>kubectl</code> command is translated into an HTTP verb (<code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>PATCH</code>, <code>DELETE</code>) against a REST endpoint (e.g., <code>/api/v1/namespaces/default/pods</code>).</li> <li>TLS handshake and authentication - <code>kubectl</code> establishes a TLS connection to the API server and presents its credentials.</li> <li>API server processing - The API server authenticates the request, checks authorization (RBAC), runs admission controllers, and reads from or writes to etcd.</li> <li>Response - The API server returns a JSON response, which <code>kubectl</code> formats according to your output flags and displays.</li> </ol> <pre><code>flowchart LR\n    A[\"kubectl&lt;br/&gt;(CLI)\"] --&gt;|\"1. Read config\"| B[\"kubeconfig&lt;br/&gt;(~/.kube/config)\"]\n    B --&gt;|\"2. Cluster + Creds\"| A\n    A --&gt;|\"3. HTTPS REST call\"| C[\"kube-apiserver&lt;br/&gt;(Control Plane)\"]\n    C --&gt;|\"4. AuthN / AuthZ /&lt;br/&gt;Admission\"| C\n    C --&gt;|\"5. Read / Write\"| D[\"etcd&lt;br/&gt;(Data Store)\"]\n    D --&gt;|\"6. Response data\"| C\n    C --&gt;|\"7. JSON response\"| A\n    A --&gt;|\"8. Formatted output\"| E[\"Terminal&lt;br/&gt;(stdout)\"]\n\n    style A fill:#326CE5,stroke:#fff,color:#fff\n    style B fill:#F5A623,stroke:#fff,color:#fff\n    style C fill:#326CE5,stroke:#fff,color:#fff\n    style D fill:#4DB33D,stroke:#fff,color:#fff\n    style E fill:#666,stroke:#fff,color:#fff</code></pre> <p>Every kubectl command is just an API call</p> <p>Understanding this is the single most important insight for mastering <code>kubectl</code>. When you run <code>kubectl get pods</code>, you are sending <code>GET /api/v1/namespaces/default/pods</code> to the API server. When you run <code>kubectl apply -f deployment.yaml</code>, you are sending a <code>PATCH</code> or <code>POST</code> request. This mental model will help you debug every issue you encounter.</p>"},{"location":"36-kubectl-Deep-Dive/#kubeconfig-file-structure","title":"kubeconfig File Structure","text":"<p>The kubeconfig file is a YAML file with three main sections: clusters, users, and contexts. A context binds a cluster to a user (and optionally a namespace).</p> <pre><code>apiVersion: v1\nkind: Config\n\n## The currently active context\ncurrent-context: my-cluster-context\n\n## Cluster definitions - where to connect\nclusters:\n  - name: production-cluster\n    cluster:\n      ## The API server URL\n      server: https://k8s-prod.example.com:6443\n      ## CA certificate to verify the API server's TLS cert\n      certificate-authority-data: LS0tLS1CRUdJTi...base64...\n  - name: staging-cluster\n    cluster:\n      server: https://k8s-staging.example.com:6443\n      certificate-authority-data: LS0tLS1CRUdJTi...base64...\n\n## User credentials - how to authenticate\nusers:\n  - name: admin-user\n    user:\n      ## Client certificate authentication\n      client-certificate-data: LS0tLS1CRUdJTi...base64...\n      client-key-data: LS0tLS1CRUdJTi...base64...\n  - name: dev-user\n    user:\n      ## Token-based authentication\n      token: eyJhbGciOiJSUzI1Ni...\n\n## Contexts bind a cluster + user + optional namespace\ncontexts:\n  - name: my-cluster-context\n    context:\n      cluster: production-cluster\n      user: admin-user\n      namespace: default\n  - name: staging-context\n    context:\n      cluster: staging-cluster\n      user: dev-user\n      namespace: staging\n</code></pre> <p>kubeconfig supports multiple authentication methods</p> <p>Besides client certificates and tokens, kubeconfig supports: exec-based credential plugins (e.g., <code>aws-iam-authenticator</code>, <code>gke-gcloud-auth-plugin</code>), OIDC tokens, username/password (deprecated), and auth provider plugins.</p>"},{"location":"36-kubectl-Deep-Dive/#api-resource-model","title":"API Resource Model","text":"<p>Every Kubernetes resource belongs to an API Group, has a Version, and is identified by its Resource type (GVR). Understanding GVR is essential for advanced <code>kubectl</code> usage.</p> Component Description Examples Group A logical collection of related resources <code>\"\"</code> (core), <code>apps</code>, <code>batch</code>, <code>networking.k8s.io</code> Version The API version within a group <code>v1</code>, <code>v1beta1</code>, <code>v2</code> Resource The actual resource type <code>pods</code>, <code>deployments</code>, <code>services</code>, <code>ingresses</code> <p>The REST path for a resource follows this pattern:</p> <ul> <li>Core group: <code>/api/v1/namespaces/{ns}/{resource}</code></li> <li>Named group: <code>/apis/{group}/{version}/namespaces/{ns}/{resource}</code></li> </ul> <p>For example:</p> <ul> <li>Pods: <code>/api/v1/namespaces/default/pods</code> (core group, so just <code>/api/v1</code>)</li> <li>Deployments: <code>/apis/apps/v1/namespaces/default/deployments</code> (apps group)</li> <li>Ingresses: <code>/apis/networking.k8s.io/v1/namespaces/default/ingresses</code></li> </ul>"},{"location":"36-kubectl-Deep-Dive/#verbosity-levels","title":"Verbosity Levels","text":"<p><code>kubectl</code> supports verbosity levels from <code>-v=0</code> to <code>-v=9</code>. These are invaluable for debugging:</p> Level What it shows <code>-v=0</code> Default output only <code>-v=1</code> Adds the HTTP method used (GET, POST, etc.) <code>-v=2</code> Adds timing information for API calls <code>-v=3</code> Adds extended information about changes <code>-v=4</code> Adds debug-level verbosity <code>-v=5</code> Adds trace-level verbosity <code>-v=6</code> Shows the full HTTP request URL <code>-v=7</code> Shows HTTP request headers <code>-v=8</code> Shows the HTTP request body <code>-v=9</code> Shows the full HTTP response body (untruncated) - everything the API server returns <p>Use -v=6 or higher to debug API calls</p> <p>When something is not working as expected, <code>-v=6</code> is the sweet spot. It shows you the exact URL being called without flooding you with body content. Use <code>-v=9</code> only when you need to see the full response payload.</p> <pre><code>## See the exact API URL being called\nkubectl get pods -v=6\n\n## See full request and response headers\nkubectl get pods -v=7\n\n## See the complete request body (useful for apply/patch debugging)\nkubectl apply -f deployment.yaml -v=8\n\n## See the complete response body (the raw JSON from the API server)\nkubectl get pods -v=9\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (minikube, kind, Docker Desktop, or a remote cluster)</li> <li><code>kubectl</code> installed and configured (version 1.25 or higher recommended)</li> <li>Basic familiarity with Kubernetes concepts (pods, deployments, services, namespaces)</li> <li>Terminal access (bash or zsh)</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#verify-your-environment","title":"Verify your environment","text":"<pre><code>## Check kubectl version (client and server)\nkubectl version\n\n## Check cluster connectivity\nkubectl cluster-info\n\n## Check that you have at least one node ready\nkubectl get nodes\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#lab","title":"Lab","text":""},{"location":"36-kubectl-Deep-Dive/#step-01-kubeconfig-mastery","title":"Step 01 - kubeconfig Mastery","text":"<ul> <li>In this step you will learn to manage multiple cluster configurations, switch contexts, merge kubeconfig files, and use the <code>KUBECONFIG</code> environment variable.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#view-your-current-kubeconfig","title":"View your current kubeconfig","text":"<pre><code>## Display the full kubeconfig (with sensitive data redacted)\nkubectl config view\n\n## Display the full kubeconfig with secrets visible\nkubectl config view --raw\n\n## Show only the current context name\nkubectl config current-context\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#work-with-contexts","title":"Work with contexts","text":"<pre><code>## List all available contexts\nkubectl config get-contexts\n\n## Switch to a different context\nkubectl config use-context &lt;context-name&gt;\n\n## Show details about a specific context\nkubectl config get-contexts &lt;context-name&gt;\n\n## Set a default namespace for the current context\n## This avoids having to pass -n &lt;namespace&gt; on every command\nkubectl config set-context --current --namespace=kubectl-lab\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#create-and-manage-contexts-manually","title":"Create and manage contexts manually","text":"<pre><code>## Add a new cluster entry\nkubectl config set-cluster my-new-cluster \\\n  --server=https://k8s.example.com:6443 \\\n  --certificate-authority=/path/to/ca.crt\n\n## Add a new user entry with token authentication\nkubectl config set-credentials my-user \\\n  --token=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\n\n## Add a new user entry with client certificate authentication\nkubectl config set-credentials cert-user \\\n  --client-certificate=/path/to/client.crt \\\n  --client-key=/path/to/client.key\n\n## Create a new context that binds cluster + user + namespace\nkubectl config set-context my-context \\\n  --cluster=my-new-cluster \\\n  --user=my-user \\\n  --namespace=default\n\n## Delete a context\nkubectl config delete-context my-context\n\n## Delete a cluster entry\nkubectl config delete-cluster my-new-cluster\n\n## Delete a user entry\nkubectl config delete-user my-user\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#merge-multiple-kubeconfig-files","title":"Merge multiple kubeconfig files","text":"<pre><code>## The KUBECONFIG environment variable accepts a colon-separated list of files\n## kubectl merges them at runtime (the first file is the default for writes)\nexport KUBECONFIG=~/.kube/config:~/.kube/cluster-2.config:~/.kube/cluster-3.config\n\n## Verify that contexts from all files are visible\nkubectl config get-contexts\n\n## Permanently merge multiple files into one\n## This creates a single flat file with all clusters, users, and contexts\nKUBECONFIG=~/.kube/config:~/.kube/cluster-2.config kubectl config view \\\n  --flatten &gt; ~/.kube/merged-config\n\n## Use the merged config\nexport KUBECONFIG=~/.kube/merged-config\n</code></pre> <p>Use KUBECONFIG per terminal session</p> <p>You can set <code>KUBECONFIG</code> per shell session to isolate cluster access. This is safer than having all clusters in one file, because you cannot accidentally run commands against the wrong cluster.</p>"},{"location":"36-kubectl-Deep-Dive/#one-off-context-override-without-switching","title":"One-off context override without switching","text":"<pre><code>## Run a command against a different context without switching\nkubectl get pods --context=staging-context\n\n## Run a command in a specific namespace without changing default\nkubectl get pods --namespace=kube-system\n\n## Combine both overrides\nkubectl get pods --context=production-context --namespace=monitoring\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#step-02-output-formatting-mastery","title":"Step 02 - Output Formatting Mastery","text":"<ul> <li><code>kubectl</code> supports many output formats. Mastering them transforms you from someone who reads terminal walls to someone who extracts exactly the data they need.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#set-up-the-lab-namespace-and-resources","title":"Set up the lab namespace and resources","text":"<pre><code>## Create the lab namespace\nkubectl apply -f manifests/namespace.yaml\n\n## Deploy sample resources\nkubectl apply -f manifests/sample-deployment.yaml\n\n## Wait for the deployment to be ready\nkubectl wait --for=condition=available deployment/nginx-lab \\\n  -n kubectl-lab --timeout=120s\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#all-output-formats","title":"All output formats","text":"<pre><code>## Default table output (human-readable)\nkubectl get pods -n kubectl-lab\n\n## Wide output - shows additional columns (node name, IP, etc.)\nkubectl get pods -n kubectl-lab -o wide\n\n## YAML output - the full resource definition as YAML\nkubectl get pods -n kubectl-lab -o yaml\n\n## JSON output - the full resource definition as JSON\nkubectl get pods -n kubectl-lab -o json\n\n## Name-only output - just the resource type/name (great for scripting)\nkubectl get pods -n kubectl-lab -o name\n\n## JSONPath output - extract specific fields using JSONPath expressions\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath='{.items[*].metadata.name}'\n\n## Custom columns - define your own tabular output\nkubectl get pods -n kubectl-lab \\\n  -o custom-columns='NAME:.metadata.name,STATUS:.status.phase,IP:.status.podIP'\n\n## Go template - use Go template syntax for complex formatting\nkubectl get pods -n kubectl-lab \\\n  -o go-template='{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#yaml-and-json-output-details","title":"YAML and JSON output details","text":"<pre><code>## Get a single pod as YAML (shows the complete spec including defaults)\nkubectl get pod -n kubectl-lab -l app=nginx-lab -o yaml | head -80\n\n## Get a single pod as JSON and pipe to jq for pretty formatting\nkubectl get pod -n kubectl-lab -l app=nginx-lab -o json | jq '.items[0].metadata'\n\n## Get just the spec section of a deployment\nkubectl get deployment nginx-lab -n kubectl-lab -o json | jq '.spec'\n\n## Get the status section of all pods\nkubectl get pods -n kubectl-lab -o json | jq '.items[].status.phase'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#name-output-for-scripting","title":"Name output for scripting","text":"<pre><code>## Get just pod names (useful for loops)\nkubectl get pods -n kubectl-lab -o name\n## Output: pod/nginx-lab-xxxx-yyyy\n\n## Use in a loop to describe each pod\nfor pod in $(kubectl get pods -n kubectl-lab -o name); do\n  echo \"=== $pod ===\"\n  kubectl describe \"$pod\" -n kubectl-lab | head -20\ndone\n\n## Delete all pods matching a label (using -o name)\n## (Dry run - remove --dry-run=client to actually delete)\nkubectl get pods -n kubectl-lab -l app=nginx-lab -o name | \\\n  xargs kubectl delete --dry-run=client\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#step-03-jsonpath-deep-dive","title":"Step 03 - JSONPath Deep Dive","text":"<ul> <li>JSONPath is a query language for JSON. kubectl\u2019s JSONPath implementation lets you extract, filter, and format data from API responses with surgical precision.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#jsonpath-syntax-reference","title":"JSONPath syntax reference","text":"Expression Meaning <code>$</code> The root object (implicit in kubectl, can be omitted) <code>.field</code> Child field access <code>[n]</code> Array index (0-based) <code>[*]</code> All elements of an array <code>[start:end]</code> Array slice <code>[?(@.field==x)]</code> Filter expression - select elements where condition is true <code>..field</code> Recursive descent - find field at any depth <code>{\"\\n\"}</code> Newline character (for formatting) <code>{\"\\t\"}</code> Tab character (for formatting) <code>{range}{end}</code> Iterate over array elements"},{"location":"36-kubectl-Deep-Dive/#basic-jsonpath-expressions","title":"Basic JSONPath expressions","text":"<pre><code>## Get all pod names as a space-separated list\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath='{.items[*].metadata.name}'\n\n## Get the first pod's name\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath='{.items[0].metadata.name}'\n\n## Get all pod IPs\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath='{.items[*].status.podIP}'\n\n## Get pod names with newlines between them\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{end}'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#formatted-multi-field-output","title":"Formatted multi-field output","text":"<pre><code>## Get pod name and status on each line\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.phase}{\"\\n\"}{end}'\n\n## Get pod name, node name, and pod IP (tab-separated)\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.nodeName}{\"\\t\"}{.status.podIP}{\"\\n\"}{end}'\n\n## Get container images for all pods\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.containers[*].image}{\"\\n\"}{end}'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#filter-expressions","title":"Filter expressions","text":"<pre><code>## Get pods that are in Running phase\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath='{.items[?(@.status.phase==\"Running\")].metadata.name}'\n\n## Get pods scheduled on a specific node (replace NODE_NAME with an actual node)\nNODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath=\"{.items[?(@.spec.nodeName==\\\"${NODE_NAME}\\\")].metadata.name}\"\n\n## Get containers with memory limit of 128Mi\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath='{.items[*].spec.containers[?(@.resources.limits.memory==\"128Mi\")].name}'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#nested-and-recursive-descent","title":"Nested and recursive descent","text":"<pre><code>## Find all container names across all pods (recursive descent)\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath='{..containers[*].name}'\n\n## Find all image names using recursive descent\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath='{..image}'\n\n## Get all label values for the \"app\" key\nkubectl get pods -n kubectl-lab \\\n  -o jsonpath='{.items[*].metadata.labels.app}'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#real-world-jsonpath-examples","title":"Real-world JSONPath examples","text":"<pre><code>## Get all node names and their kernel versions\nkubectl get nodes \\\n  -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.nodeInfo.kernelVersion}{\"\\n\"}{end}'\n\n## Get all namespaces and their status\nkubectl get namespaces \\\n  -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.phase}{\"\\n\"}{end}'\n\n## Get all PVCs with their capacity and storage class\nkubectl get pvc --all-namespaces \\\n  -o jsonpath='{range .items[*]}{.metadata.namespace}{\"\\t\"}{.metadata.name}{\"\\t\"}{.spec.resources.requests.storage}{\"\\t\"}{.spec.storageClassName}{\"\\n\"}{end}'\n\n## Extract all unique container images running in the cluster\nkubectl get pods --all-namespaces \\\n  -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\\n' | sort -u\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#step-04-custom-columns-and-advanced-get-operations","title":"Step 04 - Custom Columns and Advanced Get Operations","text":"<ul> <li>Custom columns give you full control over tabular output. Combined with field selectors, label selectors, and sort operations, <code>get</code> becomes an incredibly powerful query tool.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#custom-columns","title":"Custom columns","text":"<pre><code>## Basic custom columns - pod name and status\nkubectl get pods -n kubectl-lab \\\n  -o custom-columns='NAME:.metadata.name,STATUS:.status.phase'\n\n## Extended custom columns - name, node, IP, status, restarts\nkubectl get pods -n kubectl-lab \\\n  -o custom-columns='\\\nNAME:.metadata.name,\\\nNODE:.spec.nodeName,\\\nIP:.status.podIP,\\\nSTATUS:.status.phase,\\\nRESTARTS:.status.containerStatuses[0].restartCount'\n\n## Custom columns for deployments - name, replicas, available, image\nkubectl get deployments -n kubectl-lab \\\n  -o custom-columns='\\\nNAME:.metadata.name,\\\nDESIRED:.spec.replicas,\\\nAVAILABLE:.status.availableReplicas,\\\nIMAGE:.spec.template.spec.containers[0].image'\n\n## Custom columns from a file (for reuse)\n## Create a columns file:\n## NAME          NAMESPACE          NODE             STATUS\n## .metadata.name .metadata.namespace .spec.nodeName  .status.phase\n## Then use it:\n## kubectl get pods --all-namespaces -o custom-columns-file=columns.txt\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#label-selectors-l-selector","title":"Label selectors (-l / \u2013selector)","text":"<pre><code>## Select pods with a specific label\nkubectl get pods -n kubectl-lab -l app=nginx-lab\n\n## Select pods with multiple label requirements (AND logic)\nkubectl get pods -n kubectl-lab -l app=nginx-lab,tier=frontend\n\n## Select pods where a label exists (any value)\nkubectl get pods -n kubectl-lab -l 'tier'\n\n## Select pods where a label does NOT exist\nkubectl get pods -n kubectl-lab -l '!tier'\n\n## Select pods with set-based requirements\nkubectl get pods -n kubectl-lab -l 'tier in (frontend, backend)'\nkubectl get pods -n kubectl-lab -l 'environment notin (production)'\nkubectl get pods -n kubectl-lab -l 'version in (v1, v2),tier=frontend'\n\n## Show labels as columns in the output\nkubectl get pods -n kubectl-lab --show-labels\n\n## Show specific labels as extra columns\nkubectl get pods -n kubectl-lab -L app,tier,version\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#field-selectors-field-selector","title":"Field selectors (\u2013field-selector)","text":"<pre><code>## Get pods by their status phase\nkubectl get pods -n kubectl-lab --field-selector status.phase=Running\n\n## Get pods NOT in Running state (find problematic pods)\nkubectl get pods --all-namespaces --field-selector status.phase!=Running\n\n## Get pods on a specific node\nNODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')\nkubectl get pods --all-namespaces --field-selector \"spec.nodeName=${NODE_NAME}\"\n\n## Get a specific pod by name using field selector\nkubectl get pods -n kubectl-lab --field-selector metadata.name=nginx-lab\n\n## Combine field selectors (AND logic)\nkubectl get pods --all-namespaces \\\n  --field-selector 'status.phase=Running,metadata.namespace!=kube-system'\n\n## Get events for a specific namespace\nkubectl get events -n kubectl-lab --field-selector type=Warning\n</code></pre> <p>Field selectors are limited</p> <p>Not all fields support field selectors. The supported fields vary by resource type. Common supported fields are <code>metadata.name</code>, <code>metadata.namespace</code>, <code>spec.nodeName</code>, and <code>status.phase</code> for pods. Use <code>kubectl get pods --field-selector help</code> to see which fields are supported (this will produce an error message listing valid fields).</p>"},{"location":"36-kubectl-Deep-Dive/#sorting","title":"Sorting","text":"<pre><code>## Sort pods by creation timestamp (oldest first)\nkubectl get pods -n kubectl-lab --sort-by='.metadata.creationTimestamp'\n\n## Sort pods by restart count (ascending)\nkubectl get pods -n kubectl-lab \\\n  --sort-by='.status.containerStatuses[0].restartCount'\n\n## Sort nodes by capacity CPU\nkubectl get nodes --sort-by='.status.capacity.cpu'\n\n## Sort events by last timestamp\nkubectl get events -n kubectl-lab --sort-by='.lastTimestamp'\n\n## Sort namespaces by name\nkubectl get namespaces --sort-by='.metadata.name'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#all-namespaces-flag","title":"All-namespaces flag","text":"<pre><code>## List pods across ALL namespaces\nkubectl get pods --all-namespaces\nkubectl get pods -A  ## shorthand\n\n## Combine with selectors and output formatting\nkubectl get pods -A -l tier=frontend -o wide\n\n## Get all services across all namespaces with custom columns\nkubectl get svc -A \\\n  -o custom-columns='NAMESPACE:.metadata.namespace,NAME:.metadata.name,TYPE:.spec.type,CLUSTER-IP:.spec.clusterIP'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#step-05-resource-inspection","title":"Step 05 - Resource Inspection","text":"<ul> <li>Beyond <code>get</code>, kubectl provides powerful commands for understanding resource schemas, discovering API capabilities, and deep-inspecting running resources.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#kubectl-describe","title":"kubectl describe","text":"<pre><code>## Describe a pod - shows events, conditions, container status, volumes, etc.\nkubectl describe pod -l app=nginx-lab -n kubectl-lab\n\n## Describe a deployment - shows rollout status, replica sets, conditions\nkubectl describe deployment nginx-lab -n kubectl-lab\n\n## Describe a node - shows capacity, allocatable, conditions, running pods\nkubectl describe node $(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')\n\n## Describe a namespace\nkubectl describe namespace kubectl-lab\n\n## Describe a service\nkubectl describe service nginx-lab -n kubectl-lab\n</code></pre> <p><code>describe</code> vs <code>get -o yaml</code></p> <p><code>describe</code> gives you a human-readable summary with events and computed status. <code>get -o yaml</code> gives you the raw API object, which is useful for programmatic access and for understanding exactly what is stored in etcd. Use <code>describe</code> for debugging and <code>get -o yaml</code> for full detail.</p>"},{"location":"36-kubectl-Deep-Dive/#kubectl-explain","title":"kubectl explain","text":"<pre><code>## Explain a resource type (top-level fields)\nkubectl explain pod\n\n## Explain a specific field\nkubectl explain pod.spec\n\n## Explain a deeply nested field\nkubectl explain pod.spec.containers\n\n## Explain with full recursive output (shows ALL fields at all levels)\nkubectl explain pod.spec.containers --recursive\n\n## Explain deployment spec\nkubectl explain deployment.spec\n\n## Explain deployment strategy (shows the rollingUpdate fields)\nkubectl explain deployment.spec.strategy\n\n## Explain a CRD resource (if installed)\n## kubectl explain myresource.spec\n\n## Specify an API version if multiple versions exist\nkubectl explain deployment --api-version=apps/v1\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-api-resources","title":"kubectl api-resources","text":"<pre><code>## List ALL available resource types in the cluster\nkubectl api-resources\n\n## Show only namespaced resources\nkubectl api-resources --namespaced=true\n\n## Show only cluster-scoped resources\nkubectl api-resources --namespaced=false\n\n## Show resources in a specific API group\nkubectl api-resources --api-group=apps\nkubectl api-resources --api-group=batch\nkubectl api-resources --api-group=networking.k8s.io\nkubectl api-resources --api-group=rbac.authorization.k8s.io\n\n## Show resources that support a specific verb\nkubectl api-resources --verbs=list\nkubectl api-resources --verbs=create,delete\nkubectl api-resources --verbs=watch\n\n## Show output with more detail (including short names and API group)\nkubectl api-resources -o wide\n\n## Find a resource by short name\nkubectl api-resources | grep -i deploy\n## Shows: deployments  deploy  apps/v1  true  Deployment\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-api-versions","title":"kubectl api-versions","text":"<pre><code>## List all available API versions\nkubectl api-versions\n\n## Filter for a specific group\nkubectl api-versions | grep apps\nkubectl api-versions | grep networking\nkubectl api-versions | grep batch\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#comparing-resources","title":"Comparing resources","text":"<pre><code>## Get the full YAML of a running resource (useful for comparison)\nkubectl get deployment nginx-lab -n kubectl-lab -o yaml &gt; running-deployment.yaml\n\n## Compare a local manifest with what is running in the cluster\n## (This shows what would change if you applied the local file)\nkubectl diff -f manifests/sample-deployment.yaml\n\n## Clean up the temporary file\nrm -f running-deployment.yaml\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#step-06-apply-vs-create-vs-replace","title":"Step 06 - apply vs create vs replace","text":"<ul> <li>Understanding when to use <code>apply</code>, <code>create</code>, or <code>replace</code> is critical for managing resources safely and predictably.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#the-three-approaches","title":"The three approaches","text":"Command Style Behavior When to use <code>kubectl apply</code> Declarative Creates or updates resources by merging with existing state Day-to-day GitOps and config management <code>kubectl create</code> Imperative Creates a resource. Fails if the resource already exists One-time creation, quick prototyping <code>kubectl replace</code> Imperative Replaces the entire resource. Fails if it does not exist Full replacement (all unspecified fields are removed)"},{"location":"36-kubectl-Deep-Dive/#kubectl-create-imperative","title":"kubectl create (imperative)","text":"<pre><code>## Create a namespace imperatively\nkubectl create namespace test-imperative\n\n## Create a deployment imperatively (no manifest file needed)\nkubectl create deployment nginx-test \\\n  --image=nginx:1.25-alpine \\\n  --replicas=2 \\\n  -n test-imperative\n\n## Create a service imperatively\nkubectl create service clusterip nginx-test \\\n  --tcp=80:80 \\\n  -n test-imperative\n\n## Create a configmap from literal values\nkubectl create configmap app-config \\\n  --from-literal=key1=value1 \\\n  --from-literal=key2=value2 \\\n  -n test-imperative\n\n## Create a secret from literal values\nkubectl create secret generic db-secret \\\n  --from-literal=username=admin \\\n  --from-literal=password=secret123 \\\n  -n test-imperative\n\n## Generate YAML without creating the resource (useful for bootstrapping manifests)\nkubectl create deployment nginx-gen \\\n  --image=nginx:1.25-alpine \\\n  --replicas=3 \\\n  --dry-run=client -o yaml &gt; generated-deployment.yaml\n\n## Clean up\nkubectl delete namespace test-imperative\nrm -f generated-deployment.yaml\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-apply-declarative","title":"kubectl apply (declarative)","text":"<pre><code>## Apply a single manifest file\nkubectl apply -f manifests/sample-deployment.yaml\n\n## Apply all manifests in a directory\nkubectl apply -f manifests/\n\n## Apply with a specific namespace override\nkubectl apply -f manifests/sample-deployment.yaml -n kubectl-lab\n\n## Apply from a URL\n## kubectl apply -f https://raw.githubusercontent.com/example/repo/main/manifest.yaml\n\n## Apply and record the command in the annotation (deprecated but still used)\nkubectl apply -f manifests/sample-deployment.yaml --record\n\n## Server-side apply (recommended for CI/CD and controllers)\n## Server-side apply tracks field ownership and prevents conflicts\nkubectl apply -f manifests/sample-deployment.yaml --server-side\n\n## Server-side apply with a custom field manager name\nkubectl apply -f manifests/sample-deployment.yaml \\\n  --server-side \\\n  --field-manager=my-ci-pipeline\n\n## Force-apply to resolve conflicts (use with caution)\nkubectl apply -f manifests/sample-deployment.yaml \\\n  --server-side \\\n  --force-conflicts\n</code></pre> <p>Server-Side Apply vs Client-Side Apply</p> <p>Client-side apply (default) uses the <code>kubectl.kubernetes.io/last-applied-configuration</code> annotation to compute diffs. Server-side apply (SSA) uses field ownership tracking in the API server. SSA is more reliable and is the recommended approach for CI/CD pipelines and controllers. SSA allows multiple managers to own different fields of the same resource without conflicts.</p>"},{"location":"36-kubectl-Deep-Dive/#dry-run-modes","title":"Dry-run modes","text":"<pre><code>## Client-side dry run - validates locally, does NOT contact the API server\n## Catches YAML syntax errors but NOT schema violations\nkubectl apply -f manifests/sample-deployment.yaml --dry-run=client\n\n## Server-side dry run - sends the request to the API server but does NOT persist\n## Catches schema violations, admission webhook rejections, and RBAC issues\nkubectl apply -f manifests/sample-deployment.yaml --dry-run=server\n\n## Combine dry-run with output to see what would be created\nkubectl apply -f manifests/sample-deployment.yaml --dry-run=server -o yaml\n\n## Generate manifests with create and dry-run (great for bootstrapping)\nkubectl create deployment test-gen \\\n  --image=nginx:latest \\\n  --replicas=3 \\\n  --dry-run=client -o yaml\n</code></pre> <p>Always prefer \u2013dry-run=server over \u2013dry-run=client</p> <p>Client-side dry-run only validates YAML syntax. Server-side dry-run sends the request to the API server, which validates the schema, runs admission webhooks, and checks RBAC - without actually creating the resource. This catches many more errors.</p>"},{"location":"36-kubectl-Deep-Dive/#kubectl-diff","title":"kubectl diff","text":"<pre><code>## Show what would change if you applied a manifest\n## (similar to 'git diff' - shows additions, removals, changes)\nkubectl diff -f manifests/sample-deployment.yaml\n\n## Diff all manifests in a directory\nkubectl diff -f manifests/\n\n## Diff with server-side apply\nkubectl diff -f manifests/sample-deployment.yaml --server-side\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-replace","title":"kubectl replace","text":"<pre><code>## Replace replaces the ENTIRE resource (not a merge)\n## First export the current state\nkubectl get deployment nginx-lab -n kubectl-lab -o yaml &gt; /tmp/nginx-lab.yaml\n\n## Modify the file, then replace\n## Any fields not in the file will be REMOVED from the resource\nkubectl replace -f /tmp/nginx-lab.yaml\n\n## Replace with --force (deletes and re-creates the resource)\n## WARNING: this causes downtime!\nkubectl replace -f /tmp/nginx-lab.yaml --force\n\n## Clean up temp file\nrm -f /tmp/nginx-lab.yaml\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#step-07-kubectl-patch","title":"Step 07 - kubectl patch","text":"<ul> <li><code>kubectl patch</code> modifies a resource in-place without replacing the entire object. There are three patch types, each with different merge semantics.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#deploy-the-patch-target","title":"Deploy the patch target","text":"<pre><code>## Deploy the resource we will use for patching exercises\nkubectl apply -f manifests/patch-examples.yaml\n\n## Verify the initial state\nkubectl get deployment patch-target -n kubectl-lab -o yaml | head -40\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#patch-type-1-strategic-merge-patch-default","title":"Patch type 1: Strategic Merge Patch (default)","text":"<ul> <li>This is the default and most commonly used patch type. It is Kubernetes-specific and understands how to merge lists (e.g., containers, volumes) by their merge key (usually <code>name</code>).</li> </ul> <pre><code>## Add an annotation using strategic merge patch\nkubectl patch deployment patch-target -n kubectl-lab \\\n  --type=strategic \\\n  -p '{\"metadata\":{\"annotations\":{\"patched-by\":\"strategic-merge\"}}}'\n\n## Update the replica count\nkubectl patch deployment patch-target -n kubectl-lab \\\n  -p '{\"spec\":{\"replicas\":3}}'\n\n## Add a new container to the pod spec\n## Strategic merge patch uses the container \"name\" as the merge key,\n## so this ADDS a sidecar without removing the existing nginx container\nkubectl patch deployment patch-target -n kubectl-lab \\\n  -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"sidecar\",\"image\":\"busybox:1.36\",\"command\":[\"sh\",\"-c\",\"while true; do echo sidecar; sleep 60; done\"]}]}}}}'\n\n## Update an existing container's image (matched by name)\nkubectl patch deployment patch-target -n kubectl-lab \\\n  -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"nginx\",\"image\":\"nginx:1.25-alpine\"}]}}}}'\n\n## Verify the patch was applied\nkubectl get deployment patch-target -n kubectl-lab -o jsonpath='{.spec.template.spec.containers[*].name}'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#patch-type-2-json-merge-patch-rfc-7386","title":"Patch type 2: JSON Merge Patch (RFC 7386)","text":"<ul> <li>JSON merge patch is simpler but less powerful. It completely replaces lists instead of merging them.</li> </ul> <pre><code>## Reset the deployment first\nkubectl apply -f manifests/patch-examples.yaml\n\n## Add an annotation using JSON merge patch\nkubectl patch deployment patch-target -n kubectl-lab \\\n  --type=merge \\\n  -p '{\"metadata\":{\"annotations\":{\"patched-by\":\"json-merge\"}}}'\n\n## Update the image\nkubectl patch deployment patch-target -n kubectl-lab \\\n  --type=merge \\\n  -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"nginx\",\"image\":\"nginx:1.25-alpine\"}]}}}}'\n</code></pre> <p>JSON Merge Patch replaces arrays entirely</p> <p>If you use <code>--type=merge</code> and specify a containers list with only one container, it will replace the entire containers array. Any containers not in your patch will be removed. This is why strategic merge patch is the default for Kubernetes resources.</p>"},{"location":"36-kubectl-Deep-Dive/#patch-type-3-json-patch-rfc-6902","title":"Patch type 3: JSON Patch (RFC 6902)","text":"<ul> <li>JSON Patch uses an array of operations (<code>add</code>, <code>remove</code>, <code>replace</code>, <code>move</code>, <code>copy</code>, <code>test</code>) with explicit paths. It is the most precise patch type.</li> </ul> <pre><code>## Reset the deployment\nkubectl apply -f manifests/patch-examples.yaml\n\n## Add an annotation using JSON Patch\nkubectl patch deployment patch-target -n kubectl-lab \\\n  --type=json \\\n  -p '[{\"op\":\"add\",\"path\":\"/metadata/annotations/patched-by\",\"value\":\"json-patch\"}]'\n\n## Replace the replica count using JSON Patch\nkubectl patch deployment patch-target -n kubectl-lab \\\n  --type=json \\\n  -p '[{\"op\":\"replace\",\"path\":\"/spec/replicas\",\"value\":4}]'\n\n## Add a new label using JSON Patch\nkubectl patch deployment patch-target -n kubectl-lab \\\n  --type=json \\\n  -p '[{\"op\":\"add\",\"path\":\"/metadata/labels/patched\",\"value\":\"true\"}]'\n\n## Remove an annotation using JSON Patch\nkubectl patch deployment patch-target -n kubectl-lab \\\n  --type=json \\\n  -p '[{\"op\":\"remove\",\"path\":\"/metadata/annotations/patched-by\"}]'\n\n## Multiple operations in a single patch\nkubectl patch deployment patch-target -n kubectl-lab \\\n  --type=json \\\n  -p '[\n    {\"op\":\"replace\",\"path\":\"/spec/replicas\",\"value\":2},\n    {\"op\":\"add\",\"path\":\"/metadata/labels/multi-patched\",\"value\":\"true\"},\n    {\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/0/image\",\"value\":\"nginx:1.25-alpine\"}\n  ]'\n\n## Test operation - succeeds only if the value matches (useful for conditional updates)\n## This will fail if the replicas is not 2\nkubectl patch deployment patch-target -n kubectl-lab \\\n  --type=json \\\n  -p '[\n    {\"op\":\"test\",\"path\":\"/spec/replicas\",\"value\":2},\n    {\"op\":\"replace\",\"path\":\"/spec/replicas\",\"value\":5}\n  ]'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#patch-comparison-summary","title":"Patch comparison summary","text":"Feature Strategic Merge JSON Merge JSON Patch Flag <code>--type=strategic</code> <code>--type=merge</code> <code>--type=json</code> List handling Merges by key (e.g. name) Replaces entire list Operates on specific indices Can delete fields Set to <code>null</code> Set to <code>null</code> Use <code>\"op\":\"remove\"</code> Kubernetes-aware Yes No No Best for Most K8s resources Simple updates Surgical precision"},{"location":"36-kubectl-Deep-Dive/#step-08-exec-cp-port-forward-attach-debug","title":"Step 08 - exec, cp, port-forward, attach, debug","text":"<ul> <li>These commands are your interactive debugging toolkit for troubleshooting running pods.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#deploy-the-multi-container-pod","title":"Deploy the multi-container pod","text":"<pre><code>## Deploy the multi-container pod for debugging exercises\nkubectl apply -f manifests/multi-container-pod.yaml\n\n## Wait for the pod to be ready\nkubectl wait --for=condition=Ready pod/multi-container-pod \\\n  -n kubectl-lab --timeout=120s\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-exec","title":"kubectl exec","text":"<pre><code>## Execute a command in the default container\nkubectl exec multi-container-pod -n kubectl-lab -- ls /\n\n## Execute a command in a specific container\nkubectl exec multi-container-pod -n kubectl-lab -c app -- nginx -v\n\n## Execute a command in the sidecar container\nkubectl exec multi-container-pod -n kubectl-lab -c sidecar -- cat /var/log/app/sidecar.log\n\n## Open an interactive shell in the default container\nkubectl exec -it multi-container-pod -n kubectl-lab -- /bin/sh\n\n## Open an interactive shell in a specific container\nkubectl exec -it multi-container-pod -n kubectl-lab -c sidecar -- /bin/sh\n\n## Run multiple commands using sh -c\nkubectl exec multi-container-pod -n kubectl-lab -- \\\n  sh -c 'echo \"Hostname: $(hostname)\" &amp;&amp; echo \"Date: $(date)\"'\n\n## Check environment variables\nkubectl exec multi-container-pod -n kubectl-lab -- env\n\n## Check network connectivity from inside the pod\nkubectl exec multi-container-pod -n kubectl-lab -- \\\n  sh -c 'wget -qO- --timeout=5 http://nginx-lab.kubectl-lab.svc.cluster.local || echo \"Service not reachable\"'\n\n## Check DNS resolution inside the pod\nkubectl exec multi-container-pod -n kubectl-lab -- \\\n  sh -c 'nslookup kubernetes.default.svc.cluster.local 2&gt;/dev/null || cat /etc/resolv.conf'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-logs","title":"kubectl logs","text":"<pre><code>## View logs from the default container\nkubectl logs multi-container-pod -n kubectl-lab\n\n## View logs from a specific container\nkubectl logs multi-container-pod -n kubectl-lab -c app\nkubectl logs multi-container-pod -n kubectl-lab -c sidecar\n\n## Follow logs in real time (like tail -f)\nkubectl logs -f multi-container-pod -n kubectl-lab -c sidecar\n\n## Show logs from the last 10 lines\nkubectl logs --tail=10 multi-container-pod -n kubectl-lab -c sidecar\n\n## Show logs from the last 30 seconds\nkubectl logs --since=30s multi-container-pod -n kubectl-lab -c sidecar\n\n## Show logs from all containers in a pod\nkubectl logs multi-container-pod -n kubectl-lab --all-containers=true\n\n## Show logs from all pods with a specific label\nkubectl logs -l app=nginx-lab -n kubectl-lab\n\n## Show logs with timestamps\nkubectl logs --timestamps multi-container-pod -n kubectl-lab -c sidecar\n\n## Show previous container logs (if the container crashed and restarted)\nkubectl logs multi-container-pod -n kubectl-lab -c app --previous 2&gt;/dev/null || \\\n  echo \"No previous container logs (container has not restarted)\"\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-cp","title":"kubectl cp","text":"<pre><code>## Copy a file FROM a pod to local machine\nkubectl exec multi-container-pod -n kubectl-lab -c app -- \\\n  sh -c 'echo \"Hello from pod\" &gt; /tmp/test.txt'\nkubectl cp kubectl-lab/multi-container-pod:/tmp/test.txt /tmp/from-pod.txt -c app\ncat /tmp/from-pod.txt\n\n## Copy a file TO a pod from local machine\necho \"Hello from local\" &gt; /tmp/to-pod.txt\nkubectl cp /tmp/to-pod.txt kubectl-lab/multi-container-pod:/tmp/to-pod.txt -c app\nkubectl exec multi-container-pod -n kubectl-lab -c app -- cat /tmp/to-pod.txt\n\n## Copy a directory from a pod\nkubectl cp kubectl-lab/multi-container-pod:/etc/nginx /tmp/nginx-config -c app\n\n## Clean up temp files\nrm -f /tmp/from-pod.txt /tmp/to-pod.txt\nrm -rf /tmp/nginx-config\n</code></pre> <p>kubectl cp requires tar in the container</p> <p><code>kubectl cp</code> uses <code>tar</code> under the hood. If the container does not have <code>tar</code> installed (e.g., distroless images), <code>cp</code> will fail. In that case, use <code>kubectl exec</code> with I/O redirection instead: <code>kubectl exec pod -- cat /path/to/file &gt; local-file</code>.</p>"},{"location":"36-kubectl-Deep-Dive/#kubectl-port-forward","title":"kubectl port-forward","text":"<pre><code>## Forward local port 8080 to pod port 80\nkubectl port-forward multi-container-pod 8080:80 -n kubectl-lab &amp;\n## Test it: curl http://localhost:8080\n## Kill the port-forward: kill %1\n\n## Forward to a service (distributes across pods)\nkubectl port-forward service/nginx-lab 8080:80 -n kubectl-lab &amp;\n## Test it: curl http://localhost:8080\n## Kill the port-forward: kill %1\n\n## Forward to a deployment (picks one pod)\nkubectl port-forward deployment/nginx-lab 8080:80 -n kubectl-lab &amp;\n## Kill the port-forward: kill %1\n\n## Forward multiple ports\nkubectl port-forward multi-container-pod 8080:80 9090:9090 -n kubectl-lab &amp;\n## Kill the port-forward: kill %1\n\n## Bind to all interfaces (not just localhost) - useful in VMs/containers\nkubectl port-forward --address 0.0.0.0 multi-container-pod 8080:80 -n kubectl-lab &amp;\n## Kill the port-forward: kill %1\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-attach","title":"kubectl attach","text":"<pre><code>## Attach to a running container's stdout (read-only by default)\n## This is different from exec - attach connects to the main process\nkubectl attach multi-container-pod -n kubectl-lab -c sidecar\n\n## Attach with stdin enabled (for interactive processes)\n## kubectl attach -it &lt;pod&gt; -c &lt;container&gt; -n &lt;namespace&gt;\n\n## Press Ctrl+C to detach\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-debug","title":"kubectl debug","text":"<pre><code>## Create an ephemeral debug container in a running pod\n## This adds a temporary container with debugging tools\nkubectl debug multi-container-pod -n kubectl-lab \\\n  --image=busybox:1.36 \\\n  -it \\\n  --target=app \\\n  -- /bin/sh\n\n## Debug by creating a copy of the pod (original pod is untouched)\nkubectl debug multi-container-pod -n kubectl-lab \\\n  --image=busybox:1.36 \\\n  --copy-to=debug-pod \\\n  -it \\\n  -- /bin/sh\n\n## Debug a node by creating a privileged pod on it\n## This gives you access to the node's filesystem at /host\nNODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')\nkubectl debug node/${NODE_NAME} -it --image=busybox:1.36\n\n## Clean up debug pods\nkubectl delete pod debug-pod -n kubectl-lab --ignore-not-found\n</code></pre> <p>Ephemeral containers vs copy-to</p> <p>Ephemeral containers (<code>--target=app</code>) run inside the same pod and share namespaces with the target container. This means they can see the same network, PIDs, and filesystem mounts. The <code>--copy-to</code> flag creates a new pod which is a clone - useful when you want to debug without affecting the running pod.</p>"},{"location":"36-kubectl-Deep-Dive/#step-09-wait-rollout-autoscale","title":"Step 09 - wait, rollout, autoscale","text":"<ul> <li>These operational commands help you manage the lifecycle of deployments and wait for conditions.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#kubectl-wait","title":"kubectl wait","text":"<pre><code>## Wait for a deployment to be available\nkubectl wait --for=condition=available deployment/nginx-lab \\\n  -n kubectl-lab --timeout=120s\n\n## Wait for a pod to be ready\nkubectl wait --for=condition=Ready pod -l app=nginx-lab \\\n  -n kubectl-lab --timeout=60s\n\n## Wait for a pod to be deleted\n## (start a deletion in another terminal first)\n## kubectl wait --for=delete pod/&lt;pod-name&gt; -n kubectl-lab --timeout=60s\n\n## Wait for a job to complete\n## kubectl wait --for=condition=complete job/&lt;job-name&gt; -n kubectl-lab --timeout=300s\n\n## Wait for all pods in a namespace to be ready\nkubectl wait --for=condition=Ready pods --all -n kubectl-lab --timeout=120s\n\n## Wait using a JSONPath expression\n## Wait until a deployment has the desired replicas available\nkubectl wait --for=jsonpath='{.status.availableReplicas}'=3 \\\n  deployment/nginx-lab -n kubectl-lab --timeout=120s\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-rollout","title":"kubectl rollout","text":"<pre><code>## Check the rollout status of a deployment\nkubectl rollout status deployment/nginx-lab -n kubectl-lab\n\n## View the rollout history (shows revisions)\nkubectl rollout history deployment/nginx-lab -n kubectl-lab\n\n## View details of a specific revision\nkubectl rollout history deployment/nginx-lab -n kubectl-lab --revision=1\n\n## Trigger a new rollout by updating the image\nkubectl set image deployment/nginx-lab nginx=nginx:1.25-alpine -n kubectl-lab\n\n## Watch the rollout progress\nkubectl rollout status deployment/nginx-lab -n kubectl-lab --watch\n\n## Pause a rollout (prevents further updates from being applied)\nkubectl rollout pause deployment/nginx-lab -n kubectl-lab\n\n## Resume a paused rollout\nkubectl rollout resume deployment/nginx-lab -n kubectl-lab\n\n## Undo the last rollout (rollback to previous revision)\nkubectl rollout undo deployment/nginx-lab -n kubectl-lab\n\n## Rollback to a specific revision\nkubectl rollout undo deployment/nginx-lab -n kubectl-lab --to-revision=1\n\n## Restart all pods in a deployment (rolling restart)\n## This is useful to pick up configmap/secret changes\nkubectl rollout restart deployment/nginx-lab -n kubectl-lab\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-autoscale","title":"kubectl autoscale","text":"<pre><code>## Create a Horizontal Pod Autoscaler (HPA)\n## Scales between 2 and 10 replicas based on CPU utilization\nkubectl autoscale deployment nginx-lab -n kubectl-lab \\\n  --min=2 --max=10 --cpu-percent=50\n\n## Check the HPA status\nkubectl get hpa -n kubectl-lab\n\n## Describe the HPA for detailed metrics and conditions\nkubectl describe hpa nginx-lab -n kubectl-lab\n\n## Delete the HPA when done\nkubectl delete hpa nginx-lab -n kubectl-lab\n</code></pre> <p>HPA requires metrics-server</p> <p>The Horizontal Pod Autoscaler needs a metrics source. For CPU/memory-based autoscaling, you need <code>metrics-server</code> installed in the cluster. For custom metrics, you need a custom metrics adapter (e.g., Prometheus Adapter).</p>"},{"location":"36-kubectl-Deep-Dive/#step-10-kubectl-auth","title":"Step 10 - kubectl auth","text":"<ul> <li>The <code>auth</code> subcommand lets you inspect and debug RBAC (Role-Based Access Control) permissions.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#deploy-rbac-resources","title":"Deploy RBAC resources","text":"<pre><code>## Deploy the RBAC demo resources\nkubectl apply -f manifests/rbac-demo.yaml\n\n## Verify the resources were created\nkubectl get serviceaccount,role,rolebinding -n kubectl-lab -l app=rbac-demo\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-auth-can-i","title":"kubectl auth can-i","text":"<pre><code>## Check if your current user can perform an action\nkubectl auth can-i create pods -n kubectl-lab\nkubectl auth can-i delete deployments -n kubectl-lab\nkubectl auth can-i get secrets -n kubectl-lab\nkubectl auth can-i create clusterroles\n\n## Check what a specific service account can do\nkubectl auth can-i list pods \\\n  --as=system:serviceaccount:kubectl-lab:lab-viewer \\\n  -n kubectl-lab\n\n## The lab-viewer service account should be able to list pods\nkubectl auth can-i get pods \\\n  --as=system:serviceaccount:kubectl-lab:lab-viewer \\\n  -n kubectl-lab\n\n## The lab-viewer service account should NOT be able to create pods\nkubectl auth can-i create pods \\\n  --as=system:serviceaccount:kubectl-lab:lab-viewer \\\n  -n kubectl-lab\n\n## The lab-restricted service account has NO permissions\nkubectl auth can-i list pods \\\n  --as=system:serviceaccount:kubectl-lab:lab-restricted \\\n  -n kubectl-lab\n\n## Check all permissions (list everything the user can do)\nkubectl auth can-i --list -n kubectl-lab\n\n## Check all permissions for a service account\nkubectl auth can-i --list \\\n  --as=system:serviceaccount:kubectl-lab:lab-viewer \\\n  -n kubectl-lab\n\n## Check access to a specific resource by name\nkubectl auth can-i get pods/nginx-lab \\\n  --as=system:serviceaccount:kubectl-lab:lab-viewer \\\n  -n kubectl-lab\n\n## Check access to subresources (like pod logs)\nkubectl auth can-i get pods/log \\\n  --as=system:serviceaccount:kubectl-lab:lab-viewer \\\n  -n kubectl-lab\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-auth-whoami","title":"kubectl auth whoami","text":"<pre><code>## Show the current user information (requires K8s 1.27+)\nkubectl auth whoami 2&gt;/dev/null || echo \"whoami requires Kubernetes 1.27+\"\n\n## Show whoami output as YAML\nkubectl auth whoami -o yaml 2&gt;/dev/null || echo \"whoami requires Kubernetes 1.27+\"\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#inspecting-rbac-resources-directly","title":"Inspecting RBAC resources directly","text":"<pre><code>## List all roles in the namespace\nkubectl get roles -n kubectl-lab\n\n## List all role bindings in the namespace\nkubectl get rolebindings -n kubectl-lab\n\n## Get the full YAML of a role (to see its rules)\nkubectl get role pod-reader -n kubectl-lab -o yaml\n\n## List cluster-level roles (there are many built-in ones)\nkubectl get clusterroles | head -20\n\n## Show details of the built-in admin ClusterRole\nkubectl get clusterrole admin -o yaml\n\n## List all cluster role bindings\nkubectl get clusterrolebindings | head -20\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#step-11-kubectl-plugins-and-krew","title":"Step 11 - kubectl Plugins and Krew","text":"<ul> <li><code>kubectl</code> can be extended with plugins. Any executable in your <code>PATH</code> named <code>kubectl-*</code> becomes a kubectl subcommand.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#how-plugins-work","title":"How plugins work","text":"<pre><code>## kubectl discovers plugins automatically\n## Any executable named \"kubectl-&lt;name&gt;\" in your PATH becomes \"kubectl &lt;name&gt;\"\n\n## List all installed plugins\nkubectl plugin list\n\n## Example: create a simple plugin\n## (This creates a script that shows pod count per namespace)\ncat &gt; /tmp/kubectl-pod-count &lt;&lt; 'SCRIPT'\n#!/bin/bash\n## kubectl pod-count - Shows the number of pods in each namespace\necho \"NAMESPACE            POD_COUNT\"\necho \"-------------------  ---------\"\nkubectl get pods --all-namespaces --no-headers 2&gt;/dev/null | \\\n  awk '{count[$1]++} END {for (ns in count) printf \"%-20s %d\\n\", ns, count[ns]}' | \\\n  sort\nSCRIPT\nchmod +x /tmp/kubectl-pod-count\n\n## Add to PATH and test it\nexport PATH=$PATH:/tmp\nkubectl pod-count\n\n## Clean up\nrm -f /tmp/kubectl-pod-count\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#installing-krew-kubectl-plugin-manager","title":"Installing Krew (kubectl plugin manager)","text":"macOSLinux <pre><code>## Install Krew using Homebrew\nbrew install krew\n\n## Add krew to your PATH (add to ~/.bashrc or ~/.zshrc for persistence)\nexport PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n</code></pre> <pre><code>## Install Krew using the official installer\n(\n  set -x; cd \"$(mktemp -d)\" &amp;&amp;\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/aarch64/arm64/')\" &amp;&amp;\n  KREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\n  tar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n  ./\"${KREW}\" install krew\n)\n\n## Add krew to your PATH (add to ~/.bashrc or ~/.zshrc for persistence)\nexport PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#using-krew","title":"Using Krew","text":"<pre><code>## Update the krew plugin index\nkubectl krew update\n\n## Search for plugins\nkubectl krew search\n\n## Search for a specific plugin\nkubectl krew search ctx\nkubectl krew search ns\n\n## Install popular plugins\nkubectl krew install ctx        ## Switch contexts easily\nkubectl krew install ns         ## Switch namespaces easily\nkubectl krew install neat       ## Remove clutter from kubectl output\nkubectl krew install tree       ## Show resource ownership tree\nkubectl krew install images     ## Show container images in use\nkubectl krew install access-matrix  ## Show RBAC access matrix\nkubectl krew install who-can    ## Show who can perform an action\n\n## Use installed plugins\nkubectl ctx                     ## List and switch contexts\nkubectl ns                      ## List and switch namespaces\nkubectl neat get pod -n kubectl-lab -l app=nginx-lab -o yaml  ## Clean YAML output\nkubectl tree deployment nginx-lab -n kubectl-lab               ## Resource hierarchy\n\n## Show info about a plugin\nkubectl krew info ctx\n\n## Uninstall a plugin\nkubectl krew uninstall ctx\n\n## List installed plugins\nkubectl krew list\n</code></pre> <p>Essential krew plugins</p> <p>The most useful plugins for daily work are: <code>ctx</code> (context switching), <code>ns</code> (namespace switching), <code>neat</code> (clean YAML output), <code>tree</code> (resource hierarchy), <code>images</code> (list container images), <code>who-can</code> (RBAC query), and <code>stern</code> (multi-pod log tailing).</p>"},{"location":"36-kubectl-Deep-Dive/#step-12-kubectl-proxy-raw-api-calls-and-token-based-access","title":"Step 12 - kubectl proxy, Raw API Calls, and Token-Based Access","text":"<ul> <li>Sometimes you need to bypass kubectl and talk directly to the Kubernetes API. This section covers three ways to do that.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#kubectl-proxy","title":"kubectl proxy","text":"<pre><code>## Start the kubectl proxy (runs in background on port 8001)\n## The proxy handles authentication for you\nkubectl proxy --port=8001 &amp;\n\n## Now you can make unauthenticated HTTP requests to the API server\n## List all API versions\ncurl -s http://localhost:8001/api | jq .\n\n## List all available API groups\ncurl -s http://localhost:8001/apis | jq '.groups[].name'\n\n## List pods in the kubectl-lab namespace\ncurl -s http://localhost:8001/api/v1/namespaces/kubectl-lab/pods | jq '.items[].metadata.name'\n\n## Get a specific deployment\ncurl -s http://localhost:8001/apis/apps/v1/namespaces/kubectl-lab/deployments/nginx-lab | jq '.metadata.name'\n\n## List all services in all namespaces\ncurl -s http://localhost:8001/api/v1/services | jq '.items[] | {namespace: .metadata.namespace, name: .metadata.name}'\n\n## List all nodes\ncurl -s http://localhost:8001/api/v1/nodes | jq '.items[].metadata.name'\n\n## Get cluster health endpoints\ncurl -s http://localhost:8001/healthz\ncurl -s http://localhost:8001/readyz\ncurl -s http://localhost:8001/livez\n\n## Stop the proxy\nkill %1 2&gt;/dev/null\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-raw-api-calls","title":"kubectl raw API calls","text":"<pre><code>## kubectl get --raw sends a raw GET request and returns the raw response\n## This is useful for accessing API endpoints that kubectl doesn't have a command for\n\n## Get the API server version\nkubectl get --raw /version | jq .\n\n## List API discovery document\nkubectl get --raw /api/v1 | jq '.resources[].name' | head -20\n\n## Get metrics (requires metrics-server)\nkubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes 2&gt;/dev/null | jq . || \\\n  echo \"metrics-server not installed\"\n\n## Get pod metrics\nkubectl get --raw /apis/metrics.k8s.io/v1beta1/namespaces/kubectl-lab/pods 2&gt;/dev/null | jq . || \\\n  echo \"metrics-server not installed\"\n\n## Health check endpoints\nkubectl get --raw /healthz\nkubectl get --raw /readyz\nkubectl get --raw /livez\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#token-based-access-without-kubectl","title":"Token-based access (without kubectl)","text":"<pre><code>## Get the API server URL from kubeconfig\nAPISERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')\necho \"API Server: ${APISERVER}\"\n\n## Create a service account token for direct API access\nTOKEN=$(kubectl create token lab-viewer -n kubectl-lab --duration=10m 2&gt;/dev/null)\n\n## If the create token command is not available (K8s &lt; 1.24), use:\n## TOKEN=$(kubectl get secret -n kubectl-lab -o jsonpath='{.items[?(@.type==\"kubernetes.io/service-account-token\")].data.token}' | base64 -d)\n\nif [ -n \"${TOKEN}\" ]; then\n  ## Make an API call using the token\n  ## --insecure is used here for lab purposes; in production, use the CA cert\n  curl -s --insecure \\\n    -H \"Authorization: Bearer ${TOKEN}\" \\\n    \"${APISERVER}/api/v1/namespaces/kubectl-lab/pods\" | jq '.items[].metadata.name'\n\n  ## This should fail because lab-viewer can only read pods, not deployments\n  curl -s --insecure \\\n    -H \"Authorization: Bearer ${TOKEN}\" \\\n    \"${APISERVER}/apis/apps/v1/namespaces/kubectl-lab/deployments\" | jq '.message'\nelse\n  echo \"Could not create token. Skipping token-based access exercise.\"\nfi\n</code></pre> <p>TLS certificates in production</p> <p>The examples above use <code>--insecure</code> to skip TLS verification for lab purposes. In production, always use the CA certificate: <code>curl --cacert /path/to/ca.crt -H \"Authorization: Bearer $TOKEN\" https://...</code></p>"},{"location":"36-kubectl-Deep-Dive/#step-13-performance-tips-and-productivity","title":"Step 13 - Performance Tips and Productivity","text":"<ul> <li>This final step covers techniques that make you faster and more efficient with kubectl.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#bashzsh-completion","title":"Bash/Zsh completion","text":"macOS (zsh)Linux (bash) <pre><code>## Enable kubectl completion for zsh\n## Add this to your ~/.zshrc file\nsource &lt;(kubectl completion zsh)\n\n## If you have an alias for kubectl, enable completion for the alias too\nalias k=kubectl\ncompdef __start_kubectl k\n</code></pre> <pre><code>## Enable kubectl completion for bash\n## Add this to your ~/.bashrc file\nsource &lt;(kubectl completion bash)\n\n## If you have an alias for kubectl, enable completion for the alias too\nalias k=kubectl\ncomplete -o default -F __start_kubectl k\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#useful-aliases","title":"Useful aliases","text":"<pre><code>## Add these to your ~/.bashrc or ~/.zshrc\n\n## Basic alias\nalias k='kubectl'\n\n## Get commands\nalias kg='kubectl get'\nalias kgp='kubectl get pods'\nalias kgd='kubectl get deployments'\nalias kgs='kubectl get services'\nalias kgn='kubectl get nodes'\nalias kga='kubectl get all'\n\n## Get with options\nalias kgpw='kubectl get pods -o wide'\nalias kgpa='kubectl get pods --all-namespaces'\n\n## Describe\nalias kd='kubectl describe'\nalias kdp='kubectl describe pod'\nalias kdd='kubectl describe deployment'\n\n## Apply and delete\nalias ka='kubectl apply -f'\nalias kdel='kubectl delete'\n\n## Logs\nalias kl='kubectl logs'\nalias klf='kubectl logs -f'\n\n## Exec\nalias kex='kubectl exec -it'\n\n## Context and namespace\nalias kctx='kubectl config get-contexts'\nalias kns='kubectl config set-context --current --namespace'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#watch-mode","title":"Watch mode","text":"<pre><code>## Watch pods continuously (updates in place)\nkubectl get pods -n kubectl-lab --watch\n\n## Short form\nkubectl get pods -n kubectl-lab -w\n\n## Watch with wide output\nkubectl get pods -n kubectl-lab -o wide -w\n\n## Watch events as they happen\nkubectl get events -n kubectl-lab --watch\n\n## Watch a specific resource\nkubectl get deployment nginx-lab -n kubectl-lab -w\n</code></pre> <p>watch vs \u2013watch</p> <p><code>kubectl get pods --watch</code> uses the Kubernetes watch API, which is efficient - the API server pushes updates. The <code>watch kubectl get pods</code> command (using the Unix <code>watch</code> utility) re-runs the full GET request every 2 seconds, which is less efficient but works with any output format.</p>"},{"location":"36-kubectl-Deep-Dive/#resource-caching-and-fast-lookups","title":"Resource caching and fast lookups","text":"<pre><code>## kubectl caches API discovery information locally\n## Force refresh the discovery cache\nkubectl api-resources --cached=false &gt; /dev/null\n\n## The discovery cache is stored at:\n## ~/.kube/cache/discovery/\n\n## For frequently repeated commands in scripts, use --request-timeout\nkubectl get pods -n kubectl-lab --request-timeout=5s\n\n## Use --chunk-size to paginate large result sets\nkubectl get pods --all-namespaces --chunk-size=100\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#quick-yaml-generation-with-dry-run","title":"Quick YAML generation with \u2013dry-run","text":"<pre><code>## Generate deployment YAML without creating it\nkubectl create deployment quick-nginx \\\n  --image=nginx:alpine \\\n  --replicas=3 \\\n  --dry-run=client -o yaml\n\n## Generate service YAML\nkubectl create service clusterip my-service \\\n  --tcp=80:8080 \\\n  --dry-run=client -o yaml\n\n## Generate job YAML\nkubectl create job my-job \\\n  --image=busybox \\\n  --dry-run=client -o yaml -- echo \"Hello\"\n\n## Generate cronjob YAML\nkubectl create cronjob my-cronjob \\\n  --image=busybox \\\n  --schedule=\"0 * * * *\" \\\n  --dry-run=client -o yaml -- echo \"Hello\"\n\n## Generate configmap YAML\nkubectl create configmap my-config \\\n  --from-literal=key=value \\\n  --dry-run=client -o yaml\n\n## Generate secret YAML\nkubectl create secret generic my-secret \\\n  --from-literal=password=s3cr3t \\\n  --dry-run=client -o yaml\n\n## Generate namespace YAML\nkubectl create namespace my-ns \\\n  --dry-run=client -o yaml\n\n## Generate serviceaccount YAML\nkubectl create serviceaccount my-sa \\\n  --dry-run=client -o yaml\n\n## Generate role YAML\nkubectl create role my-role \\\n  --verb=get,list \\\n  --resource=pods \\\n  --dry-run=client -o yaml\n\n## Generate rolebinding YAML\nkubectl create rolebinding my-binding \\\n  --role=my-role \\\n  --serviceaccount=default:my-sa \\\n  --dry-run=client -o yaml\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#kubectl-top-resource-usage","title":"kubectl top (resource usage)","text":"<pre><code>## Show resource usage for nodes (requires metrics-server)\nkubectl top nodes 2&gt;/dev/null || echo \"metrics-server not installed\"\n\n## Show resource usage for pods\nkubectl top pods -n kubectl-lab 2&gt;/dev/null || echo \"metrics-server not installed\"\n\n## Sort by CPU usage\nkubectl top pods -n kubectl-lab --sort-by=cpu 2&gt;/dev/null || echo \"metrics-server not installed\"\n\n## Sort by memory usage\nkubectl top pods -n kubectl-lab --sort-by=memory 2&gt;/dev/null || echo \"metrics-server not installed\"\n\n## Show container-level resource usage\nkubectl top pods -n kubectl-lab --containers 2&gt;/dev/null || echo \"metrics-server not installed\"\n\n## Show resource usage across all namespaces\nkubectl top pods -A --sort-by=cpu 2&gt;/dev/null || echo \"metrics-server not installed\"\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#exercises","title":"Exercises","text":"<p>The following exercises will test your deep understanding of <code>kubectl</code>. Try to solve each exercise on your own before revealing the solution.</p>"},{"location":"36-kubectl-Deep-Dive/#01-get-all-non-running-pods","title":"01. Get All Non-Running Pods","text":"<p>Get all pods across all namespaces that are NOT in <code>Running</code> state using a field selector.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario","title":"Scenario:","text":"<p>\u25e6 You are troubleshooting a cluster and need to quickly find all pods that are not healthy. \u25e6 Field selectors let you filter server-side, which is more efficient than client-side filtering.</p> <p>Hint: Use <code>--field-selector</code> with the <code>status.phase</code> field and the <code>!=</code> operator.</p> Solution <pre><code>## Get all pods that are NOT in Running state across all namespaces\n## The field selector status.phase!=Running filters on the API server side\nkubectl get pods --all-namespaces \\\n  --field-selector status.phase!=Running\n\n## For more detail, add wide output\nkubectl get pods --all-namespaces \\\n  --field-selector status.phase!=Running \\\n  -o wide\n\n## Combine with other phases to be more specific\n## Find only Failed pods\nkubectl get pods --all-namespaces \\\n  --field-selector status.phase=Failed\n\n## Find Pending pods (often indicates scheduling issues)\nkubectl get pods --all-namespaces \\\n  --field-selector status.phase=Pending\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#02-extract-all-container-images","title":"02. Extract All Container Images","text":"<p>Extract all unique container images running in the cluster using JSONPath.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_1","title":"Scenario:","text":"<p>\u25e6 You need to audit which container images are deployed across the cluster for security scanning. \u25e6 JSONPath combined with shell tools gives you a powerful extraction pipeline.</p> <p>Hint: Use <code>-o jsonpath</code> to extract <code>.spec.containers[*].image</code> from all pods.</p> Solution <pre><code>## Method 1: JSONPath with tr and sort\n## Get all container images from all pods, deduplicate and sort\nkubectl get pods --all-namespaces \\\n  -o jsonpath='{.items[*].spec.containers[*].image}' | \\\n  tr ' ' '\\n' | sort -u\n\n## Method 2: JSONPath range for cleaner output\nkubectl get pods --all-namespaces \\\n  -o jsonpath='{range .items[*]}{range .spec.containers[*]}{.image}{\"\\n\"}{end}{end}' | \\\n  sort -u\n\n## Method 3: Include init containers too (for a complete audit)\necho \"=== Regular Containers ===\"\nkubectl get pods --all-namespaces \\\n  -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\\n' | sort -u\n\necho \"\"\necho \"=== Init Containers ===\"\nkubectl get pods --all-namespaces \\\n  -o jsonpath='{.items[*].spec.initContainers[*].image}' | tr ' ' '\\n' | sort -u\n\n## Method 4: custom-columns approach\nkubectl get pods --all-namespaces \\\n  -o custom-columns='IMAGE:.spec.containers[*].image' --no-headers | \\\n  tr ',' '\\n' | sort -u\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#03-custom-columns-pod-dashboard","title":"03. Custom Columns: Pod Dashboard","text":"<p>Use custom-columns to display pod name, node, status, and restart count in a clean table.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_2","title":"Scenario:","text":"<p>\u25e6 You want a quick dashboard view of pod health without the noise of full <code>get -o wide</code> output. \u25e6 Custom columns let you select exactly the fields you care about.</p> <p>Hint: Use <code>-o custom-columns</code> with JSONPath expressions for each column.</p> Solution <pre><code>## Custom columns showing name, node, status, restart count, and age\nkubectl get pods -n kubectl-lab \\\n  -o custom-columns='\\\nNAME:.metadata.name,\\\nNODE:.spec.nodeName,\\\nSTATUS:.status.phase,\\\nRESTARTS:.status.containerStatuses[0].restartCount,\\\nIP:.status.podIP'\n\n## Extended version with container image\nkubectl get pods -n kubectl-lab \\\n  -o custom-columns='\\\nNAME:.metadata.name,\\\nNODE:.spec.nodeName,\\\nSTATUS:.status.phase,\\\nRESTARTS:.status.containerStatuses[0].restartCount,\\\nIMAGE:.spec.containers[0].image,\\\nIP:.status.podIP'\n\n## All namespaces with namespace column\nkubectl get pods --all-namespaces \\\n  -o custom-columns='\\\nNAMESPACE:.metadata.namespace,\\\nNAME:.metadata.name,\\\nSTATUS:.status.phase,\\\nRESTARTS:.status.containerStatuses[0].restartCount,\\\nNODE:.spec.nodeName'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#04-switch-kubeconfig-contexts","title":"04. Switch Kubeconfig Contexts","text":"<p>Switch between multiple kubeconfig contexts and verify which cluster is active after each switch.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_3","title":"Scenario:","text":"<p>\u25e6 You manage multiple clusters (dev, staging, production) and need to switch between them safely. \u25e6 Always verify the active context before running commands to avoid accidental changes to the wrong cluster.</p> <p>Hint: Use <code>kubectl config get-contexts</code>, <code>kubectl config use-context</code>, and <code>kubectl config current-context</code>.</p> Solution <pre><code>## Step 1: List all available contexts\nkubectl config get-contexts\n\n## Step 2: Note the current context (the one marked with *)\nkubectl config current-context\n\n## Step 3: Switch to a different context (replace with your actual context name)\n## kubectl config use-context &lt;other-context-name&gt;\n\n## Step 4: Verify the switch\nkubectl config current-context\n\n## Step 5: Verify the cluster you are now connected to\nkubectl cluster-info\n\n## Step 6: Switch back to the original context\n## kubectl config use-context &lt;original-context-name&gt;\n\n## Step 7: Use --context flag for one-off commands without switching\n## kubectl get pods --context=&lt;other-context-name&gt;\n\n## Pro tip: use kubectl config rename-context for clearer names\n## kubectl config rename-context old-name new-name\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#05-preview-changes-with-kubectl-diff","title":"05. Preview Changes with kubectl diff","text":"<p>Use <code>kubectl diff</code> to preview what changes would be applied to the cluster before actually applying a manifest.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_4","title":"Scenario:","text":"<p>\u25e6 Before applying changes to production, you want to see exactly what will change. \u25e6 <code>kubectl diff</code> shows a unified diff between the live cluster state and the local manifest.</p> <p>Hint: Modify a field in the local manifest (e.g., replica count) and run <code>kubectl diff -f &lt;file&gt;</code>.</p> Solution <pre><code>## Step 1: Make sure the deployment is applied\nkubectl apply -f manifests/sample-deployment.yaml\n\n## Step 2: Create a modified version of the manifest\ncp manifests/sample-deployment.yaml /tmp/modified-deployment.yaml\n\n## Step 3: Change the replica count in the modified file (from 3 to 5)\nsed -i.bak 's/replicas: 3/replicas: 5/' /tmp/modified-deployment.yaml\n\n## Step 4: Use kubectl diff to preview the changes\n## Lines prefixed with - are the current state, + are the proposed changes\nkubectl diff -f /tmp/modified-deployment.yaml\n\n## Step 5: If the diff looks good, apply it\n## kubectl apply -f /tmp/modified-deployment.yaml\n\n## Step 6: You can also diff an entire directory\nkubectl diff -f manifests/\n\n## Clean up\nrm -f /tmp/modified-deployment.yaml /tmp/modified-deployment.yaml.bak\n\n## Restore original state\nkubectl apply -f manifests/sample-deployment.yaml\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#06-patch-a-deployment-with-strategic-merge-patch","title":"06. Patch a Deployment with Strategic Merge Patch","text":"<p>Use <code>kubectl patch</code> with a strategic merge patch to add an annotation and update the deployment\u2019s labels without affecting other fields.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_5","title":"Scenario:","text":"<p>\u25e6 You need to add monitoring annotations to existing deployments without redeploying. \u25e6 Strategic merge patch is the safest option because it merges intelligently.</p> <p>Hint: Use <code>kubectl patch</code> with <code>--type=strategic</code> (or omit <code>--type</code> as it is the default) and a JSON payload.</p> Solution <pre><code>## Add annotations to the deployment\nkubectl patch deployment nginx-lab -n kubectl-lab \\\n  -p '{\n    \"metadata\": {\n      \"annotations\": {\n        \"monitoring.example.com/enabled\": \"true\",\n        \"monitoring.example.com/port\": \"80\"\n      }\n    }\n  }'\n\n## Verify the annotations were added\nkubectl get deployment nginx-lab -n kubectl-lab \\\n  -o jsonpath='{.metadata.annotations}' | jq .\n\n## Add a new label without removing existing ones\nkubectl patch deployment nginx-lab -n kubectl-lab \\\n  -p '{\"metadata\":{\"labels\":{\"patched\":\"true\"}}}'\n\n## Verify the labels (all original labels should still be present)\nkubectl get deployment nginx-lab -n kubectl-lab \\\n  -o jsonpath='{.metadata.labels}' | jq .\n\n## Clean up the patch - remove the added annotation\nkubectl patch deployment nginx-lab -n kubectl-lab \\\n  --type=json \\\n  -p '[{\"op\":\"remove\",\"path\":\"/metadata/annotations/monitoring.example.com~1enabled\"},\n       {\"op\":\"remove\",\"path\":\"/metadata/annotations/monitoring.example.com~1port\"}]'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#07-debug-with-an-ephemeral-container","title":"07. Debug with an Ephemeral Container","text":"<p>Use <code>kubectl debug</code> to attach an ephemeral container to a running pod and inspect its network connectivity and filesystem.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_6","title":"Scenario:","text":"<p>\u25e6 A pod is experiencing network issues, but its container does not have debugging tools installed. \u25e6 Ephemeral containers let you inject a debugging container into the running pod without restarting it.</p> <p>Hint: Use <code>kubectl debug &lt;pod&gt; --image=busybox --target=&lt;container&gt; -it -- /bin/sh</code>.</p> Solution <pre><code>## Get the name of a running pod\nPOD_NAME=$(kubectl get pods -n kubectl-lab -l app=nginx-lab \\\n  -o jsonpath='{.items[0].metadata.name}')\necho \"Debugging pod: ${POD_NAME}\"\n\n## Attach an ephemeral container to the running pod\n## --target=nginx shares the PID namespace with the nginx container\nkubectl debug \"${POD_NAME}\" -n kubectl-lab \\\n  --image=busybox:1.36 \\\n  --target=nginx \\\n  -it -- /bin/sh\n\n## Inside the ephemeral container, you can:\n## - Check network: wget -qO- http://localhost\n## - Check DNS:     nslookup kubernetes.default.svc.cluster.local\n## - Check processes: ps aux (if PID sharing is enabled)\n## - Check filesystem: ls /proc/1/root/ (access target container's filesystem)\n## - Exit: type 'exit'\n\n## Alternative: create a copy of the pod for debugging\nkubectl debug \"${POD_NAME}\" -n kubectl-lab \\\n  --image=busybox:1.36 \\\n  --copy-to=debug-copy-pod \\\n  -it -- /bin/sh\n\n## Clean up the debug copy pod\nkubectl delete pod debug-copy-pod -n kubectl-lab --ignore-not-found\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#08-server-side-dry-run-validation","title":"08. Server-Side Dry Run Validation","text":"<p>Use <code>--dry-run=server</code> to validate a manifest against the API server without actually creating the resource.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_7","title":"Scenario:","text":"<p>\u25e6 You have a YAML manifest and want to ensure it is valid against the cluster\u2019s schema and admission webhooks before applying it. \u25e6 Server-side dry run catches more errors than client-side dry run.</p> <p>Hint: Use <code>kubectl apply -f &lt;file&gt; --dry-run=server</code> and observe the output for validation errors.</p> Solution <pre><code>## Step 1: Validate a correct manifest (should succeed)\nkubectl apply -f manifests/sample-deployment.yaml --dry-run=server\n## Output: deployment.apps/nginx-lab configured (server dry run)\n\n## Step 2: Create an intentionally broken manifest\ncat &gt; /tmp/broken-manifest.yaml &lt;&lt; 'EOF'\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: broken-deploy\n  namespace: kubectl-lab\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: broken\n  template:\n    metadata:\n      labels:\n        app: broken\n    spec:\n      containers:\n        - name: app\n          image: nginx:latest\n          ports:\n            - containerPort: \"eighty\"  ## Invalid: must be integer\nEOF\n\n## Step 3: Validate the broken manifest with server-side dry run\n## This should return an error about the invalid port\nkubectl apply -f /tmp/broken-manifest.yaml --dry-run=server 2&gt;&amp;1 || true\n\n## Step 4: Compare with client-side dry run (may not catch the error)\nkubectl apply -f /tmp/broken-manifest.yaml --dry-run=client 2&gt;&amp;1 || true\n\n## Step 5: Fix the manifest and validate again\nsed 's/\"eighty\"/80/' /tmp/broken-manifest.yaml | kubectl apply --dry-run=server -f -\n\n## Clean up\nrm -f /tmp/broken-manifest.yaml\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#09-find-api-resources-by-verb","title":"09. Find API Resources by Verb","text":"<p>Find all Kubernetes API resources that support the \u201clist\u201d verb.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_8","title":"Scenario:","text":"<p>\u25e6 You are building automation and need to know which resources can be listed programmatically. \u25e6 Different resources support different verbs (get, list, create, update, delete, watch, patch).</p> <p>Hint: Use <code>kubectl api-resources</code> with the <code>--verbs</code> flag.</p> Solution <pre><code>## List all resources that support the \"list\" verb\nkubectl api-resources --verbs=list\n\n## List resources that support both \"list\" and \"watch\" (for informer-based controllers)\nkubectl api-resources --verbs=list,watch\n\n## List resources that support \"create\" (can be created)\nkubectl api-resources --verbs=create\n\n## List resources that can be deleted\nkubectl api-resources --verbs=delete\n\n## Find resources that support the \"patch\" verb\nkubectl api-resources --verbs=patch\n\n## Combine with namespace filter to show only namespaced resources that support list\nkubectl api-resources --verbs=list --namespaced=true\n\n## Count resources by API group\nkubectl api-resources --verbs=list -o name | cut -d. -f2- | sort | uniq -c | sort -rn\n\n## Find resources that DON'T support create (read-only resources)\n## Compare the full list with create-supporting resources\ndiff &lt;(kubectl api-resources -o name | sort) \\\n     &lt;(kubectl api-resources --verbs=create -o name | sort) | \\\n     grep '^&lt;' | sed 's/&lt; //'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#10-check-service-account-permissions","title":"10. Check Service Account Permissions","text":"<p>Use <code>kubectl auth can-i</code> to check what a specific service account is allowed to do.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_9","title":"Scenario:","text":"<p>\u25e6 A team member reports that their application (running as a service account) cannot access certain resources. \u25e6 You need to diagnose the exact RBAC permissions.</p> <p>Hint: Use <code>kubectl auth can-i</code> with the <code>--as=system:serviceaccount:&lt;namespace&gt;:&lt;name&gt;</code> flag and <code>--list</code>.</p> Solution <pre><code>## Make sure RBAC resources are deployed\nkubectl apply -f manifests/rbac-demo.yaml\n\n## Check if lab-viewer can list pods (should be YES)\nkubectl auth can-i list pods \\\n  --as=system:serviceaccount:kubectl-lab:lab-viewer \\\n  -n kubectl-lab\n\n## Check if lab-viewer can create pods (should be NO)\nkubectl auth can-i create pods \\\n  --as=system:serviceaccount:kubectl-lab:lab-viewer \\\n  -n kubectl-lab\n\n## Check if lab-viewer can read pod logs (should be YES, per the Role definition)\nkubectl auth can-i get pods/log \\\n  --as=system:serviceaccount:kubectl-lab:lab-viewer \\\n  -n kubectl-lab\n\n## Check if lab-viewer can delete deployments (should be NO)\nkubectl auth can-i delete deployments \\\n  --as=system:serviceaccount:kubectl-lab:lab-viewer \\\n  -n kubectl-lab\n\n## List ALL permissions for the lab-viewer service account\nkubectl auth can-i --list \\\n  --as=system:serviceaccount:kubectl-lab:lab-viewer \\\n  -n kubectl-lab\n\n## Check the restricted service account (should have NO permissions)\nkubectl auth can-i --list \\\n  --as=system:serviceaccount:kubectl-lab:lab-restricted \\\n  -n kubectl-lab\n\n## Check if the restricted account can even list pods (should be NO)\nkubectl auth can-i list pods \\\n  --as=system:serviceaccount:kubectl-lab:lab-restricted \\\n  -n kubectl-lab\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#11-generate-a-csv-with-go-templates","title":"11. Generate a CSV with Go Templates","text":"<p>Use go-template output formatting to generate a CSV of pod information (name, namespace, status, IP, node).</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_10","title":"Scenario:","text":"<p>\u25e6 You need to export pod information to a spreadsheet or feed it into another tool. \u25e6 Go templates give you complete control over output formatting.</p> <p>Hint: Use <code>-o go-template</code> with <code>{{range .items}}</code> and <code>{{\"\\n\"}}</code> for newlines.</p> Solution <pre><code>## Generate a CSV header and data for all pods in the namespace\necho \"NAME,NAMESPACE,STATUS,POD_IP,NODE\"\nkubectl get pods -n kubectl-lab \\\n  -o go-template='{{range .items}}{{.metadata.name}},{{.metadata.namespace}},{{.status.phase}},{{.status.podIP}},{{.spec.nodeName}}{{\"\\n\"}}{{end}}'\n\n## Generate CSV for all namespaces\necho \"NAME,NAMESPACE,STATUS,POD_IP,NODE\"\nkubectl get pods --all-namespaces \\\n  -o go-template='{{range .items}}{{.metadata.name}},{{.metadata.namespace}},{{.status.phase}},{{.status.podIP}},{{.spec.nodeName}}{{\"\\n\"}}{{end}}'\n\n## Generate CSV with container information\necho \"POD,CONTAINER,IMAGE\"\nkubectl get pods -n kubectl-lab \\\n  -o go-template='{{range .items}}{{$pod := .metadata.name}}{{range .spec.containers}}{{$pod}},{{.name}},{{.image}}{{\"\\n\"}}{{end}}{{end}}'\n\n## Save to a file\necho \"NAME,NAMESPACE,STATUS,POD_IP,NODE\" &gt; /tmp/pods.csv\nkubectl get pods --all-namespaces \\\n  -o go-template='{{range .items}}{{.metadata.name}},{{.metadata.namespace}},{{.status.phase}},{{.status.podIP}},{{.spec.nodeName}}{{\"\\n\"}}{{end}}' &gt;&gt; /tmp/pods.csv\n\necho \"CSV saved to /tmp/pods.csv\"\ncat /tmp/pods.csv\n\n## Clean up\nrm -f /tmp/pods.csv\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#12-wait-for-a-deployment-rollout","title":"12. Wait for a Deployment Rollout","text":"<p>Use <code>kubectl wait</code> to block until a deployment is fully rolled out with all replicas available.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_11","title":"Scenario:","text":"<p>\u25e6 In a CI/CD pipeline, you need to wait for a deployment to be fully ready before proceeding with integration tests. \u25e6 <code>kubectl wait</code> is designed exactly for this purpose - it exits with code 0 when the condition is met.</p> <p>Hint: Use <code>kubectl wait --for=condition=available</code> or <code>kubectl rollout status</code>.</p> Solution <pre><code>## Method 1: kubectl wait with condition\n## Wait for the deployment to report the Available condition as True\nkubectl wait --for=condition=available deployment/nginx-lab \\\n  -n kubectl-lab --timeout=120s\n\n## Method 2: kubectl rollout status (blocks until complete)\nkubectl rollout status deployment/nginx-lab -n kubectl-lab\n\n## Method 3: Wait for a specific number of ready replicas using JSONPath\nkubectl wait --for=jsonpath='{.status.readyReplicas}'=3 \\\n  deployment/nginx-lab -n kubectl-lab --timeout=120s\n\n## Simulate a rollout and wait for it\n## Step 1: Trigger a rolling update\nkubectl set image deployment/nginx-lab nginx=nginx:1.25-alpine -n kubectl-lab\n\n## Step 2: Wait for the rollout to complete\nkubectl rollout status deployment/nginx-lab -n kubectl-lab --timeout=120s\necho \"Rollout complete! Deployment is ready.\"\n\n## Method 4: Wait for all pods to be Ready\nkubectl wait --for=condition=Ready pod -l app=nginx-lab \\\n  -n kubectl-lab --timeout=120s\n\n## Use in a script (checking exit code)\nif kubectl wait --for=condition=available deployment/nginx-lab \\\n  -n kubectl-lab --timeout=60s; then\n  echo \"Deployment is ready, proceeding with tests...\"\nelse\n  echo \"Deployment failed to become ready within timeout!\"\n  exit 1\nfi\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#13-access-the-api-via-kubectl-proxy","title":"13. Access the API via kubectl proxy","text":"<p>Use <code>kubectl proxy</code> to access the Kubernetes API server via a local HTTP proxy without authentication.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_12","title":"Scenario:","text":"<p>\u25e6 You want to explore the Kubernetes API using <code>curl</code> or a web browser for debugging or learning. \u25e6 <code>kubectl proxy</code> handles all the authentication so you can make simple HTTP requests.</p> <p>Hint: Start the proxy with <code>kubectl proxy &amp;</code>, then use <code>curl http://localhost:8001/api/...</code>.</p> Solution <pre><code>## Start the kubectl proxy in the background on port 8001\nkubectl proxy --port=8001 &amp;\nPROXY_PID=$!\necho \"Proxy started with PID: ${PROXY_PID}\"\n\n## Wait a moment for the proxy to start\nsleep 2\n\n## List the core API versions\ncurl -s http://localhost:8001/api | jq '.versions'\n\n## List all pods in the kubectl-lab namespace\ncurl -s http://localhost:8001/api/v1/namespaces/kubectl-lab/pods | \\\n  jq '.items[].metadata.name'\n\n## Get a specific deployment\ncurl -s http://localhost:8001/apis/apps/v1/namespaces/kubectl-lab/deployments/nginx-lab | \\\n  jq '{name: .metadata.name, replicas: .spec.replicas, availableReplicas: .status.availableReplicas}'\n\n## List all namespaces\ncurl -s http://localhost:8001/api/v1/namespaces | jq '.items[].metadata.name'\n\n## Check the API server health\necho \"Health: $(curl -s http://localhost:8001/healthz)\"\necho \"Ready:  $(curl -s http://localhost:8001/readyz)\"\n\n## List all API groups\ncurl -s http://localhost:8001/apis | jq '.groups[].name'\n\n## Stop the proxy\nkill ${PROXY_PID}\necho \"Proxy stopped.\"\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#14-create-a-custom-kubectl-alias-script","title":"14. Create a Custom kubectl Alias Script","text":"<p>Create a shell function or alias that shows pods alongside their resource requests and limits.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_13","title":"Scenario:","text":"<p>\u25e6 You frequently need to check pod resource allocation to debug scheduling and OOM issues. \u25e6 A custom alias saves you from typing complex JSONPath expressions every time.</p> <p>Hint: Create a shell function that uses <code>-o custom-columns</code> or <code>-o go-template</code> with resource fields.</p> Solution <pre><code>## Method 1: Shell function using custom-columns\nkpod_resources() {\n  local ns=\"${1:---all-namespaces}\"\n  if [ \"$ns\" != \"--all-namespaces\" ] &amp;&amp; [ \"$ns\" != \"-A\" ]; then\n    ns=\"-n $ns\"\n  fi\n  kubectl get pods ${ns} \\\n    -o custom-columns='\\\nNAMESPACE:.metadata.namespace,\\\nNAME:.metadata.name,\\\nCPU_REQ:.spec.containers[0].resources.requests.cpu,\\\nCPU_LIM:.spec.containers[0].resources.limits.cpu,\\\nMEM_REQ:.spec.containers[0].resources.requests.memory,\\\nMEM_LIM:.spec.containers[0].resources.limits.memory,\\\nSTATUS:.status.phase'\n}\n\n## Use it for a specific namespace\nkpod_resources kubectl-lab\n\n## Use it for all namespaces\nkpod_resources --all-namespaces\n\n## Method 2: Shell function using go-template for CSV-style output\nkpod_csv() {\n  echo \"NAMESPACE,POD,CONTAINER,CPU_REQ,CPU_LIM,MEM_REQ,MEM_LIM\"\n  kubectl get pods \"${@}\" \\\n    -o go-template='{{range .items}}{{$ns := .metadata.namespace}}{{$pod := .metadata.name}}{{range .spec.containers}}{{$ns}},{{$pod}},{{.name}},{{if .resources.requests.cpu}}{{.resources.requests.cpu}}{{else}}&lt;none&gt;{{end}},{{if .resources.limits.cpu}}{{.resources.limits.cpu}}{{else}}&lt;none&gt;{{end}},{{if .resources.requests.memory}}{{.resources.requests.memory}}{{else}}&lt;none&gt;{{end}},{{if .resources.limits.memory}}{{.resources.limits.memory}}{{else}}&lt;none&gt;{{end}}{{\"\\n\"}}{{end}}{{end}}'\n}\n\n## Use it\nkpod_csv -n kubectl-lab\n\n## To make these permanent, add them to ~/.bashrc or ~/.zshrc\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#15-find-resource-hungry-pods-with-kubectl-top","title":"15. Find Resource-Hungry Pods with kubectl top","text":"<p>Use <code>kubectl top</code> with <code>--sort-by</code> to find the most resource-hungry pods in the cluster.</p>"},{"location":"36-kubectl-Deep-Dive/#scenario_14","title":"Scenario:","text":"<p>\u25e6 The cluster is running low on resources and you need to identify which pods are consuming the most CPU and memory. \u25e6 <code>kubectl top</code> provides real-time resource consumption data (requires metrics-server).</p> <p>Hint: Use <code>kubectl top pods --sort-by=cpu</code> or <code>--sort-by=memory</code> with <code>--all-namespaces</code>.</p> Solution <pre><code>## NOTE: These commands require metrics-server to be installed in the cluster\n## Install metrics-server if not present:\n## kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n## Find top CPU consumers across all namespaces\nkubectl top pods --all-namespaces --sort-by=cpu 2&gt;/dev/null | head -20 || \\\n  echo \"metrics-server not installed. Install it first.\"\n\n## Find top memory consumers across all namespaces\nkubectl top pods --all-namespaces --sort-by=memory 2&gt;/dev/null | head -20 || \\\n  echo \"metrics-server not installed. Install it first.\"\n\n## Find top CPU consumers in a specific namespace\nkubectl top pods -n kubectl-lab --sort-by=cpu 2&gt;/dev/null || \\\n  echo \"metrics-server not installed.\"\n\n## Show per-container resource usage\nkubectl top pods -n kubectl-lab --containers --sort-by=cpu 2&gt;/dev/null || \\\n  echo \"metrics-server not installed.\"\n\n## Node resource usage (to check overall cluster capacity)\nkubectl top nodes --sort-by=cpu 2&gt;/dev/null || \\\n  echo \"metrics-server not installed.\"\n\n## Combine with other commands for a complete picture\n## Show pods sorted by CPU alongside their resource requests\necho \"=== Top CPU Pods ===\"\nkubectl top pods -n kubectl-lab --sort-by=cpu 2&gt;/dev/null || echo \"metrics-server required\"\necho \"\"\necho \"=== Pod Resource Requests ===\"\nkubectl get pods -n kubectl-lab \\\n  -o custom-columns='\\\nNAME:.metadata.name,\\\nCPU_REQ:.spec.containers[0].resources.requests.cpu,\\\nMEM_REQ:.spec.containers[0].resources.requests.memory'\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#finalize-cleanup","title":"Finalize &amp; Cleanup","text":"<ul> <li>To remove all resources created by this lab, run the following commands:</li> </ul> <pre><code>## Delete all lab resources by removing the namespace\n## This deletes everything inside it (pods, deployments, services, roles, etc.)\nkubectl delete namespace kubectl-lab\n\n## Verify the namespace is deleted\nkubectl get namespace kubectl-lab 2&gt;&amp;1 || echo \"Namespace kubectl-lab deleted successfully\"\n</code></pre> <ul> <li>If you created any temporary files during the exercises:</li> </ul> <pre><code>## Clean up any temp files created during the lab\nrm -f /tmp/from-pod.txt /tmp/to-pod.txt /tmp/broken-manifest.yaml\nrm -f /tmp/modified-deployment.yaml /tmp/pods.csv\nrm -rf /tmp/nginx-config\nrm -f /tmp/kubectl-pod-count\n</code></pre> <ul> <li>If you modified your shell configuration for aliases or completions, those changes persist in your <code>~/.bashrc</code> or <code>~/.zshrc</code> file. They are useful to keep.</li> </ul>"},{"location":"36-kubectl-Deep-Dive/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>kubectl: command not found:</li> </ul> <p>Make sure <code>kubectl</code> is installed and in your <code>PATH</code>. Check with:</p> <pre><code>which kubectl\nkubectl version --client\n</code></pre> <p></p> <ul> <li>Unable to connect to the server:</li> </ul> <p>Check that your cluster is running and your kubeconfig is correct:</p> <pre><code>## Check the current context\nkubectl config current-context\n\n## Check the cluster endpoint\nkubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}'\n\n## Try to reach the API server\nkubectl cluster-info\n</code></pre> <p></p> <ul> <li>error: the server doesn\u2019t have a resource type:</li> </ul> <p>The resource might be in a different API group, or the CRD might not be installed:</p> <pre><code>## List all available API resources\nkubectl api-resources | grep -i &lt;resource-name&gt;\n\n## Check if a CRD is installed\nkubectl get crd | grep -i &lt;resource-name&gt;\n</code></pre> <p></p> <ul> <li>pods not starting (Pending, CrashLoopBackOff, ImagePullBackOff):</li> </ul> <pre><code>## Check pod status and events\nkubectl describe pod &lt;pod-name&gt; -n kubectl-lab\n\n## Check pod logs\nkubectl logs &lt;pod-name&gt; -n kubectl-lab\n\n## Check events in the namespace\nkubectl get events -n kubectl-lab --sort-by='.lastTimestamp'\n</code></pre> <p></p> <ul> <li>RBAC permission denied:</li> </ul> <pre><code>## Check what your current user can do\nkubectl auth can-i --list -n kubectl-lab\n\n## Check with verbosity to see the API call\nkubectl get pods -n kubectl-lab -v=6\n</code></pre> <p></p> <ul> <li>JSONPath expressions returning empty results:</li> </ul> <pre><code>## First, check the raw JSON structure to understand the path\nkubectl get pods -n kubectl-lab -o json | jq '.' | head -50\n\n## Verify the field exists at the expected path\nkubectl get pods -n kubectl-lab -o json | jq '.items[0].status.phase'\n</code></pre> <p></p> <ul> <li>metrics-server not available (kubectl top fails):</li> </ul> <pre><code>## Check if metrics-server is installed\nkubectl get deployment metrics-server -n kube-system 2&gt;/dev/null || \\\n  echo \"metrics-server is not installed\"\n\n## Install metrics-server (for lab/dev clusters)\n## kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n</code></pre> <p></p> <ul> <li>kubectl debug fails (ephemeral containers not supported):</li> </ul> <p>Ephemeral containers require Kubernetes 1.25+. Check your cluster version:</p> <pre><code>kubectl version\n</code></pre>"},{"location":"36-kubectl-Deep-Dive/#next-steps","title":"Next Steps","text":"<ul> <li>Practice these commands daily until they become muscle memory. The best way to learn kubectl is to use it constantly.</li> <li>Explore the kubectl reference documentation for commands not covered in this lab.</li> <li>Try writing shell scripts that combine multiple kubectl commands for real operational tasks (e.g., automated health checks, resource reports, cleanup scripts).</li> <li>Install and explore additional krew plugins: <code>stern</code> (multi-pod log tailing), <code>sniff</code> (packet capture), <code>resource-capacity</code> (node capacity planning), <code>ktop</code> (terminal-based resource dashboard).</li> <li>Learn about Kubernetes client libraries (client-go, Python kubernetes client) for programmatic access to the same API that kubectl uses.</li> <li>Study the Kubernetes API conventions to understand why the API works the way it does.</li> <li>Explore kubectl plugins development to build your own custom commands for your organization.</li> </ul>"},{"location":"37-ResourceQuotas/","title":"ResourceQuotas &amp; LimitRanges - Multi-Tenant Resource Control","text":"<ul> <li>In this lab we will learn how to use ResourceQuotas and LimitRanges to control and limit resource consumption per namespace, ensuring fair sharing in multi-tenant Kubernetes clusters.</li> </ul>"},{"location":"37-ResourceQuotas/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What ResourceQuotas and LimitRanges are</li> <li>How to set CPU, memory, and object count limits per namespace</li> <li>How to enforce default resource requests/limits for every container</li> <li>How to prevent namespace resource starvation</li> <li>How ResourceQuotas and LimitRanges work together</li> <li>How to monitor quota usage</li> <li>Best practices for multi-tenant clusters</li> </ul>"},{"location":"37-ResourceQuotas/#official-documentation-references","title":"Official Documentation &amp; References","text":"Resource Link Resource Quotas kubernetes.io/docs Limit Ranges kubernetes.io/docs Managing Resources for Containers kubernetes.io/docs Configure Quotas for API Objects kubernetes.io/docs"},{"location":"37-ResourceQuotas/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster (<code>kubectl cluster-info</code> should work)</li> <li><code>kubectl</code> configured against the cluster</li> </ul>"},{"location":"37-ResourceQuotas/#overview","title":"Overview","text":"<pre><code>graph TB\n    subgraph cluster[\"Kubernetes Cluster\"]\n        subgraph ns_a[\"Namespace: team-alpha\"]\n            rq_a[\"ResourceQuota\\nCPU: 2 cores\\nMemory: 4Gi\\nPods: 10\"]\n            lr_a[\"LimitRange\\nDefault CPU: 200m\\nDefault Mem: 256Mi\"]\n            pods_a[\"Pods (within limits)\"]\n        end\n\n        subgraph ns_b[\"Namespace: team-beta\"]\n            rq_b[\"ResourceQuota\\nCPU: 4 cores\\nMemory: 8Gi\\nPods: 20\"]\n            lr_b[\"LimitRange\\nDefault CPU: 500m\\nDefault Mem: 512Mi\"]\n            pods_b[\"Pods (within limits)\"]\n        end\n    end\n\n    rq_a --&gt; pods_a\n    lr_a --&gt; pods_a\n    rq_b --&gt; pods_b\n    lr_b --&gt; pods_b</code></pre> Resource Scope Purpose ResourceQuota Namespace Caps the total resources a namespace can consume LimitRange Namespace Sets per-container default and max resource constraints"},{"location":"37-ResourceQuotas/#01-create-namespaces","title":"01. Create namespaces","text":"<pre><code># Clean up\nkubectl delete namespace quota-lab --ignore-not-found\n\n# Create lab namespace\nkubectl create namespace quota-lab\n</code></pre>"},{"location":"37-ResourceQuotas/#02-create-a-resourcequota","title":"02. Create a ResourceQuota","text":"<p>A ResourceQuota limits the total resources consumed by all pods in a namespace:</p> <pre><code># manifests/resource-quota.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: quota-lab\nspec:\n  hard:\n    # Compute resources\n    requests.cpu: \"2\"          # Total CPU requests across all pods\n    requests.memory: 4Gi       # Total memory requests across all pods\n    limits.cpu: \"4\"            # Total CPU limits across all pods\n    limits.memory: 8Gi         # Total memory limits across all pods\n    # Object count limits\n    pods: \"10\"                 # Maximum number of pods\n    services: \"5\"              # Maximum number of services\n    configmaps: \"10\"           # Maximum number of configmaps\n    secrets: \"10\"              # Maximum number of secrets\n    persistentvolumeclaims: \"5\"\n</code></pre> <pre><code>kubectl apply -f manifests/resource-quota.yaml\n\n# Check the quota\nkubectl get resourcequota -n quota-lab\nkubectl describe resourcequota compute-quota -n quota-lab\n</code></pre> <p>Expected output:</p> <pre><code>Name:                    compute-quota\nNamespace:               quota-lab\nResource                 Used  Hard\n--------                 ----  ----\nconfigmaps               1     10\nlimits.cpu               0     4\nlimits.memory            0     8Gi\npersistentvolumeclaims   0     5\npods                     0     10\nrequests.cpu             0     2\nrequests.memory          0     4Gi\nsecrets                  0     10\nservices                 0     5\n</code></pre>"},{"location":"37-ResourceQuotas/#03-deploy-a-pod-without-resource-requests-will-fail","title":"03. Deploy a pod without resource requests (will fail!)","text":"<p>Important</p> <p>When a ResourceQuota is active in a namespace, every container must specify resource requests and limits. Pods without them will be rejected.</p> <pre><code># This will FAIL because no resource requests/limits are specified\nkubectl run test-no-limits --image=nginx -n quota-lab\n# Error: pods \"test-no-limits\" is forbidden: failed quota: compute-quota:\n# must specify limits.cpu, limits.memory, requests.cpu, requests.memory\n</code></pre>"},{"location":"37-ResourceQuotas/#04-deploy-a-pod-with-resource-requests","title":"04. Deploy a pod with resource requests","text":"<pre><code># manifests/pod-with-resources.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: web-server\n  namespace: quota-lab\nspec:\n  containers:\n    - name: nginx\n      image: nginx:alpine\n      resources:\n        requests:\n          cpu: 250m\n          memory: 128Mi\n        limits:\n          cpu: 500m\n          memory: 256Mi\n</code></pre> <pre><code>kubectl apply -f manifests/pod-with-resources.yaml\n\n# Check quota usage - resources are now counted\nkubectl describe resourcequota compute-quota -n quota-lab\n</code></pre> <p>Expected:</p> <pre><code>Resource                 Used    Hard\n--------                 ----    ----\nlimits.cpu               500m    4\nlimits.memory            256Mi   8Gi\npods                     1       10\nrequests.cpu             250m    2\nrequests.memory          128Mi   4Gi\n</code></pre>"},{"location":"37-ResourceQuotas/#05-exceed-the-quota-will-fail","title":"05. Exceed the quota (will fail!)","text":"<p>Try to deploy more pods than the quota allows:</p> <pre><code># manifests/deployment-exceeds-quota.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hungry-app\n  namespace: quota-lab\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: hungry\n  template:\n    metadata:\n      labels:\n        app: hungry\n    spec:\n      containers:\n        - name: app\n          image: nginx:alpine\n          resources:\n            requests:\n              cpu: 500m         # 5 replicas \u00d7 500m = 2.5 CPU &gt; 2 CPU quota\n              memory: 1Gi       # 5 replicas \u00d7 1Gi = 5Gi &gt; 4Gi quota\n            limits:\n              cpu: \"1\"\n              memory: 2Gi\n</code></pre> <pre><code>kubectl apply -f manifests/deployment-exceeds-quota.yaml\n\n# Check how many pods were actually created\nkubectl get pods -n quota-lab -l app=hungry\n# Only some pods will be created - the rest are blocked by quota\n\n# Check events for the ReplicaSet to see the quota error\nkubectl get events -n quota-lab --field-selector reason=FailedCreate --sort-by='.lastTimestamp'\n# \"exceeded quota: compute-quota\"\n</code></pre>"},{"location":"37-ResourceQuotas/#06-create-a-limitrange","title":"06. Create a LimitRange","text":"<p>A LimitRange sets per-container defaults and constraints - so pods without explicit resources still get reasonable limits:</p> <pre><code># manifests/limit-range.yaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: default-limits\n  namespace: quota-lab\nspec:\n  limits:\n    - type: Container\n      default:           # Default limits (if not specified)\n        cpu: 300m\n        memory: 256Mi\n      defaultRequest:    # Default requests (if not specified)\n        cpu: 100m\n        memory: 128Mi\n      max:               # Maximum allowed per container\n        cpu: \"1\"\n        memory: 1Gi\n      min:               # Minimum required per container\n        cpu: 50m\n        memory: 64Mi\n    - type: Pod\n      max:               # Maximum total resources per pod\n        cpu: \"2\"\n        memory: 2Gi\n</code></pre> <pre><code># Clean up previous resources\nkubectl delete deployment hungry-app -n quota-lab --ignore-not-found\nkubectl delete pod web-server -n quota-lab --ignore-not-found\n\nkubectl apply -f manifests/limit-range.yaml\n\n# Inspect the LimitRange\nkubectl describe limitrange default-limits -n quota-lab\n</code></pre>"},{"location":"37-ResourceQuotas/#07-deploy-a-pod-without-resource-specs-gets-defaults-from-limitrange","title":"07. Deploy a pod without resource specs (gets defaults from LimitRange)","text":"<pre><code># Now this works! LimitRange injects default requests/limits\nkubectl run auto-limited --image=nginx:alpine -n quota-lab\n\n# Wait for it\nkubectl wait --for=condition=Ready pod/auto-limited -n quota-lab --timeout=60s\n\n# Check the pod - it has resources injected automatically\nkubectl get pod auto-limited -n quota-lab -o jsonpath='{.spec.containers[0].resources}' | python3 -m json.tool\n</code></pre> <p>Expected output:</p> <pre><code>{\n    \"limits\": {\n        \"cpu\": \"300m\",\n        \"memory\": \"256Mi\"\n    },\n    \"requests\": {\n        \"cpu\": \"100m\",\n        \"memory\": \"128Mi\"\n    }\n}\n</code></pre>"},{"location":"37-ResourceQuotas/#08-try-to-exceed-per-container-max-will-fail","title":"08. Try to exceed per-container max (will fail!)","text":"<pre><code># manifests/pod-exceeds-limit.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: greedy-pod\n  namespace: quota-lab\nspec:\n  containers:\n    - name: app\n      image: nginx:alpine\n      resources:\n        requests:\n          cpu: 100m\n          memory: 128Mi\n        limits:\n          cpu: \"2\"           # Exceeds LimitRange max of 1 CPU\n          memory: 2Gi        # Exceeds LimitRange max of 1Gi\n</code></pre> <pre><code>kubectl apply -f manifests/pod-exceeds-limit.yaml\n# Error: pods \"greedy-pod\" is forbidden:\n# [maximum cpu usage per Container is 1, but limit is 2,\n#  maximum memory usage per Container is 1Gi, but limit is 2Gi]\n</code></pre>"},{"location":"37-ResourceQuotas/#09-monitor-quota-usage","title":"09. Monitor quota usage","text":"<pre><code># View quota usage across all namespaces\nkubectl get resourcequota --all-namespaces\n\n# Detailed usage for our namespace\nkubectl describe resourcequota compute-quota -n quota-lab\n\n# View in JSON/YAML for programmatic access\nkubectl get resourcequota compute-quota -n quota-lab -o yaml\n\n# Check if a specific resource is approaching limits\nkubectl get resourcequota compute-quota -n quota-lab \\\n  -o jsonpath='{range .status.used}{@}{\"\\n\"}{end}'\n</code></pre>"},{"location":"37-ResourceQuotas/#10-scope-based-quotas-optional-advanced-topic","title":"10. Scope-based quotas (optional advanced topic)","text":"<p>ResourceQuotas can be scoped to specific priority classes or pod states:</p> <pre><code># manifests/scoped-quota.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: best-effort-quota\n  namespace: quota-lab\nspec:\n  hard:\n    pods: \"5\"\n  scopeSelector:\n    matchExpressions:\n      - operator: In\n        scopeName: PriorityClass\n        values: [\"low-priority\"]\n</code></pre> <pre><code>kubectl apply -f manifests/scoped-quota.yaml\n</code></pre>"},{"location":"37-ResourceQuotas/#11-cleanup","title":"11. Cleanup","text":"<pre><code>kubectl delete namespace quota-lab\n</code></pre>"},{"location":"37-ResourceQuotas/#summary","title":"Summary","text":"Concept Key Takeaway ResourceQuota Caps total namespace resource consumption (CPU, memory, objects) LimitRange Sets per-container defaults, min, and max Together LimitRange provides defaults \u2192 ResourceQuota enforces totals No resources = rejected With a ResourceQuota, pods must declare resources Defaults injection LimitRange automatically adds requests/limits to naked pods Object count quotas Limit pods, services, secrets, PVCs, etc. per namespace Best practice Always use both ResourceQuota + LimitRange in multi-tenant clusters"},{"location":"37-ResourceQuotas/#exercises","title":"Exercises","text":"<p>The following exercises will test your understanding of ResourceQuotas and LimitRanges. Try to solve each exercise on your own before revealing the solution.</p>"},{"location":"37-ResourceQuotas/#01-create-a-quota-that-limits-only-object-counts","title":"01. Create a Quota That Limits Only Object Counts","text":"<p>Create a ResourceQuota named <code>object-count-quota</code> that limits the namespace to a maximum of 3 pods, 2 services, and 2 configmaps - without any CPU or memory restrictions.</p>"},{"location":"37-ResourceQuotas/#scenario","title":"Scenario:","text":"<p>\u25e6 You want to prevent teams from accidentally creating too many resources. \u25e6 CPU and memory are managed by the LimitRange, so the quota only needs to count objects.</p> <p>Hint: Use the <code>pods</code>, <code>services</code>, and <code>configmaps</code> fields under <code>spec.hard</code> without any <code>requests.*</code> or <code>limits.*</code> fields.</p> Solution <pre><code>## Create namespace\nkubectl create namespace count-quota-test\n\n## Apply the object-count-only quota\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: object-count-quota\n  namespace: count-quota-test\nspec:\n  hard:\n    pods: \"3\"\n    services: \"2\"\n    configmaps: \"2\"\nEOF\n\n## Verify\nkubectl describe resourcequota object-count-quota -n count-quota-test\n\n## Create pods (no resource requests needed since no CPU/memory quota)\nkubectl run pod1 --image=nginx:alpine -n count-quota-test\nkubectl run pod2 --image=nginx:alpine -n count-quota-test\nkubectl run pod3 --image=nginx:alpine -n count-quota-test\n\n## This 4th pod should be rejected\nkubectl run pod4 --image=nginx:alpine -n count-quota-test 2&gt;&amp;1 || echo \"Rejected: quota exceeded\"\n\n## Check usage\nkubectl describe resourcequota object-count-quota -n count-quota-test\n\n## Clean up\nkubectl delete namespace count-quota-test\n</code></pre>"},{"location":"37-ResourceQuotas/#02-use-limitrange-to-set-max-resource-per-pod","title":"02. Use LimitRange to Set Max Resource Per Pod","text":"<p>Create a LimitRange that sets the maximum total CPU per pod to 1 core and maximum total memory per pod to 1Gi. Deploy a pod with two containers that together exceed these limits.</p>"},{"location":"37-ResourceQuotas/#scenario_1","title":"Scenario:","text":"<p>\u25e6 You want to prevent any single pod from consuming too many resources. \u25e6 The limit applies to the sum of all containers in the pod, not per container.</p> <p>Hint: Use <code>type: Pod</code> in the LimitRange with <code>max</code> settings.</p> Solution <pre><code>## Create namespace\nkubectl create namespace pod-limit-test\n\n## Create the pod-level LimitRange\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: pod-max-limits\n  namespace: pod-limit-test\nspec:\n  limits:\n    - type: Pod\n      max:\n        cpu: \"1\"\n        memory: 1Gi\n    - type: Container\n      defaultRequest:\n        cpu: 100m\n        memory: 128Mi\n      default:\n        cpu: 200m\n        memory: 256Mi\nEOF\n\n## This pod should work (total: 400m CPU, 512Mi memory)\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: within-limits\n  namespace: pod-limit-test\nspec:\n  containers:\n    - name: app1\n      image: nginx:alpine\n      resources:\n        requests:\n          cpu: 100m\n          memory: 128Mi\n        limits:\n          cpu: 200m\n          memory: 256Mi\n    - name: app2\n      image: nginx:alpine\n      resources:\n        requests:\n          cpu: 100m\n          memory: 128Mi\n        limits:\n          cpu: 200m\n          memory: 256Mi\nEOF\necho \"Pod within-limits created successfully\"\n\n## This pod should be REJECTED (total: 1200m CPU &gt; 1 core max)\ncat &lt;&lt;'EOF' | kubectl apply -f - 2&gt;&amp;1 || echo \"Rejected: exceeds pod max\"\napiVersion: v1\nkind: Pod\nmetadata:\n  name: exceeds-limits\n  namespace: pod-limit-test\nspec:\n  containers:\n    - name: app1\n      image: nginx:alpine\n      resources:\n        requests:\n          cpu: 400m\n          memory: 256Mi\n        limits:\n          cpu: 600m\n          memory: 512Mi\n    - name: app2\n      image: nginx:alpine\n      resources:\n        requests:\n          cpu: 400m\n          memory: 256Mi\n        limits:\n          cpu: 600m\n          memory: 512Mi\nEOF\n\n## Clean up\nkubectl delete namespace pod-limit-test\n</code></pre>"},{"location":"37-ResourceQuotas/#03-monitor-quota-usage-and-set-up-alerts","title":"03. Monitor Quota Usage and Set Up Alerts","text":"<p>Write a script that checks quota usage across all namespaces and warns when any resource exceeds 80% utilization.</p>"},{"location":"37-ResourceQuotas/#scenario_2","title":"Scenario:","text":"<p>\u25e6 You are responsible for cluster operations and need early warning when namespaces approach their quotas. \u25e6 You want a simple script (no Prometheus needed) to check current usage.</p> <p>Hint: Use <code>kubectl get resourcequota -A -o json</code> and parse the <code>status.used</code> vs <code>status.hard</code> fields.</p> Solution <pre><code>## Create a test namespace with quota for demonstration\nkubectl create namespace quota-monitor-test\n\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: test-quota\n  namespace: quota-monitor-test\nspec:\n  hard:\n    pods: \"5\"\n    requests.cpu: \"1\"\n    requests.memory: 1Gi\nEOF\n\n## Create some pods to use quota\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: consumer1\n  namespace: quota-monitor-test\nspec:\n  containers:\n    - name: app\n      image: nginx:alpine\n      resources:\n        requests:\n          cpu: 400m\n          memory: 512Mi\n        limits:\n          cpu: 400m\n          memory: 512Mi\nEOF\n\n## The monitoring script\ncat &lt;&lt;'SCRIPT'\n#!/bin/bash\n## quota-monitor.sh - Check ResourceQuota usage across all namespaces\nTHRESHOLD=80\n\necho \"=== ResourceQuota Usage Report ===\"\necho \"\"\n\nkubectl get resourcequota -A -o json | python3 -c \"\nimport json, sys\n\ndata = json.load(sys.stdin)\nthreshold = $THRESHOLD\n\nfor item in data.get('items', []):\n    ns = item['metadata']['namespace']\n    name = item['metadata']['name']\n    hard = item.get('status', {}).get('hard', {})\n    used = item.get('status', {}).get('used', {})\n\n    for resource in hard:\n        h = hard[resource]\n        u = used.get(resource, '0')\n\n        # Parse values (simplified - handles integers and 'Mi/Gi' suffixes)\n        def parse_val(v):\n            v = str(v)\n            if v.endswith('Gi'): return float(v[:-2]) * 1024\n            if v.endswith('Mi'): return float(v[:-2])\n            if v.endswith('m'): return float(v[:-1])\n            try: return float(v)\n            except: return 0\n\n        hard_val = parse_val(h)\n        used_val = parse_val(u)\n\n        if hard_val &gt; 0:\n            pct = (used_val / hard_val) * 100\n            status = '\u26a0\ufe0f  WARNING' if pct &gt;= threshold else '\u2705'\n            print(f'{status} {ns}/{name}: {resource} = {u}/{h} ({pct:.0f}%)')\n\"\nSCRIPT\n\n## Clean up\nkubectl delete namespace quota-monitor-test\n</code></pre>"},{"location":"37-ResourceQuotas/#04-create-quotas-for-different-priority-classes","title":"04. Create Quotas for Different Priority Classes","text":"<p>Create two PriorityClasses (<code>high-priority</code> and <code>low-priority</code>) and ResourceQuotas that limit how many pods of each priority class can run.</p>"},{"location":"37-ResourceQuotas/#scenario_3","title":"Scenario:","text":"<p>\u25e6 Critical services use <code>high-priority</code> and should get up to 5 pods. \u25e6 Background jobs use <code>low-priority</code> and should be limited to 3 pods. \u25e6 This prevents low-priority workloads from consuming the namespace\u2019s pod quota.</p> <p>Hint: Use <code>scopeSelector</code> with <code>PriorityClass</code> scope in the ResourceQuota.</p> Solution <pre><code>## Create namespace\nkubectl create namespace priority-quota-test\n\n## Create PriorityClasses\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 1000\nglobalDefault: false\ndescription: \"High priority for critical services\"\n---\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: low-priority\nvalue: 100\nglobalDefault: false\ndescription: \"Low priority for background jobs\"\nEOF\n\n## Create scoped quotas\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: high-priority-quota\n  namespace: priority-quota-test\nspec:\n  hard:\n    pods: \"5\"\n  scopeSelector:\n    matchExpressions:\n      - operator: In\n        scopeName: PriorityClass\n        values: [\"high-priority\"]\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: low-priority-quota\n  namespace: priority-quota-test\nspec:\n  hard:\n    pods: \"3\"\n  scopeSelector:\n    matchExpressions:\n      - operator: In\n        scopeName: PriorityClass\n        values: [\"low-priority\"]\nEOF\n\n## Verify quotas\nkubectl describe resourcequota -n priority-quota-test\n\n## Deploy high-priority pods\nfor i in 1 2 3; do\n  kubectl run high-$i --image=nginx:alpine --priority-class-name=high-priority -n priority-quota-test\ndone\n\n## Deploy low-priority pods\nfor i in 1 2 3; do\n  kubectl run low-$i --image=nginx:alpine --priority-class-name=low-priority -n priority-quota-test\ndone\n\n## This 4th low-priority pod should be rejected\nkubectl run low-4 --image=nginx:alpine --priority-class-name=low-priority -n priority-quota-test 2&gt;&amp;1 || echo \"Rejected: low-priority quota exceeded\"\n\n## Check quota usage\nkubectl describe resourcequota -n priority-quota-test\n\n## Clean up\nkubectl delete namespace priority-quota-test\nkubectl delete priorityclass high-priority low-priority\n</code></pre>"},{"location":"37-ResourceQuotas/#05-verify-limitrange-defaults-are-applied-automatically","title":"05. Verify LimitRange Defaults Are Applied Automatically","text":"<p>Create a LimitRange, deploy a pod without any resource specifications, and verify the pod received the default values from the LimitRange.</p>"},{"location":"37-ResourceQuotas/#scenario_4","title":"Scenario:","text":"<p>\u25e6 New developers on your team forget to set resource requests/limits. \u25e6 The LimitRange ensures every container gets reasonable defaults. \u25e6 You need to prove the injection happens automatically.</p> <p>Hint: Deploy a pod with <code>kubectl run</code> (no resources) and inspect the pod\u2019s YAML to see injected values.</p> Solution <pre><code>## Create namespace with LimitRange\nkubectl create namespace defaults-test\n\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: container-defaults\n  namespace: defaults-test\nspec:\n  limits:\n    - type: Container\n      default:\n        cpu: 500m\n        memory: 256Mi\n      defaultRequest:\n        cpu: 200m\n        memory: 128Mi\nEOF\n\n## Deploy a pod WITHOUT any resource specifications\nkubectl run naked-pod --image=nginx:alpine -n defaults-test\n\n## Wait for it\nkubectl wait --for=condition=Ready pod/naked-pod -n defaults-test --timeout=60s\n\n## Inspect the pod - LimitRange should have injected defaults\nkubectl get pod naked-pod -n defaults-test -o jsonpath='{.spec.containers[0].resources}' | python3 -m json.tool\n\n## Expected output:\n## {\n##     \"limits\": {\n##         \"cpu\": \"500m\",\n##         \"memory\": \"256Mi\"\n##     },\n##     \"requests\": {\n##         \"cpu\": \"200m\",\n##         \"memory\": \"128Mi\"\n##     }\n## }\n\n## Verify LimitRange is the source\nkubectl describe limitrange container-defaults -n defaults-test\n\n## Clean up\nkubectl delete namespace defaults-test\n</code></pre>"},{"location":"37-ResourceQuotas/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Pod rejected with \u201cmust specify limits/requests\u201d:</li> </ul> <p>A ResourceQuota is active and requires all pods to declare resources. Add resource requests/limits or create a LimitRange to inject defaults:</p> <pre><code>## Check if a ResourceQuota exists\nkubectl get resourcequota -n &lt;namespace&gt;\n\n## Quick fix: Add a LimitRange to inject defaults\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: default-limits\n  namespace: &lt;namespace&gt;\nspec:\n  limits:\n    - type: Container\n      default:\n        cpu: 200m\n        memory: 128Mi\n      defaultRequest:\n        cpu: 100m\n        memory: 64Mi\nEOF\n</code></pre> <p></p> <ul> <li>Deployment stuck - ReplicaSet cannot create pods:</li> </ul> <p>Check the ReplicaSet events for quota errors:</p> <pre><code>## Get the ReplicaSet name\nkubectl get rs -n &lt;namespace&gt;\n\n## Check events for quota errors\nkubectl describe rs &lt;replicaset-name&gt; -n &lt;namespace&gt; | grep -A5 \"Events:\"\n\n## Or check events directly\nkubectl get events -n &lt;namespace&gt; --field-selector reason=FailedCreate --sort-by='.lastTimestamp'\n</code></pre> <p></p> <ul> <li>Pod rejected with \u201cexceeds maximum\u201d from LimitRange:</li> </ul> <p>The container\u2019s requests or limits exceed the LimitRange maximum. Reduce the resource values:</p> <pre><code>## Check the LimitRange constraints\nkubectl describe limitrange -n &lt;namespace&gt;\n\n## Show min/max for Container and Pod types\nkubectl get limitrange -n &lt;namespace&gt; -o yaml\n</code></pre> <p></p> <ul> <li>Quota shows \u201cUsed\u201d but pods are not running:</li> </ul> <p>Quota counts even non-running pods. Check for failed or pending pods:</p> <pre><code>## List all pods (including non-running)\nkubectl get pods -n &lt;namespace&gt; --field-selector=status.phase!=Running\n\n## Delete failed pods to reclaim quota\nkubectl delete pods --field-selector=status.phase=Failed -n &lt;namespace&gt;\n</code></pre> <p></p> <ul> <li>Cannot determine remaining quota capacity:</li> </ul> <p>Compare <code>Used</code> vs <code>Hard</code> values to see remaining capacity:</p> <pre><code>## Detailed quota view\nkubectl describe resourcequota -n &lt;namespace&gt;\n\n## JSON output for programmatic access\nkubectl get resourcequota -n &lt;namespace&gt; -o json | python3 -c \"\nimport json, sys\ndata = json.load(sys.stdin)\nfor item in data.get('items', data.get('status', {}).get('hard', {}) and [data]):\n    status = item.get('status', {})\n    hard = status.get('hard', {})\n    used = status.get('used', {})\n    print(f\\\"Quota: {item['metadata']['name']}\\\")\n    for k in hard:\n        print(f'  {k}: {used.get(k, \\\"0\\\")} / {hard[k]}')\n\"\n</code></pre>"},{"location":"37-ResourceQuotas/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Vertical Pod Autoscaler (VPA) to automatically adjust resource requests based on actual usage.</li> <li>Learn about Horizontal Pod Autoscaler (HPA) to scale replicas based on resource utilization.</li> <li>Study KEDA (Lab 30) for event-driven autoscaling that works alongside quotas.</li> <li>Explore Hierarchical Quotas for managing quotas in complex multi-tenant setups with namespace hierarchies.</li> <li>Try kube-resource-report for visualizing resource usage across namespaces.</li> <li>Implement PodDisruptionBudgets (Lab 17) alongside quotas to ensure availability during maintenance.</li> </ul>"},{"location":"Tasks/","title":"KubernetesLabs Tasks","text":"<p>\u2022 Welcome to the KubernetesLabs Tasks section. \u2022 Each folder below contains a hands-on task/exercise that you can complete independently to practice specific Kubernetes skills. \u2022 Follow the README file in each task for detailed instructions and solutions.</p>"},{"location":"Tasks/#task-index","title":"Task Index","text":""},{"location":"Tasks/#core-kubernetes","title":"Core Kubernetes","text":"Kubernetes CLI Tasks Collection of comprehensive Kubernetes exercises covering CLI commands, pod debugging, deployments, services, configmaps, secrets, and more. Kubernetes Service Tasks Exercises for Services, Networking, and Service Discovery Kubernetes Scheduling Tasks Node Affinity, Pod Affinity, Anti-Affinity, Taints, Tolerations, and Topology Spread Constraints"},{"location":"Tasks/#tools-ecosystem","title":"Tools &amp; Ecosystem","text":"Kubernetes Helm Tasks Helm chart creation, packaging, templating, repositories, and deployment best practices Kubernetes ArgoCD Tasks ArgoCD installation, CLI usage, application deployment, GitOps workflows, App of Apps, sync waves, and fleet management Kubernetes Kubebuilder Tasks Kubebuilder operator development, CRD creation, reconciliation loops, webhooks, and testing Kubernetes KEDA Tasks KEDA event-driven autoscaling, ScaledObjects, ScaledJobs, TriggerAuthentication, and scaling patterns Kubernetes Harbor + ArgoCD Airgap Tasks Harbor registry, Nginx Ingress, fully offline/airgap ArgoCD deployment, image mirroring, Helm chart GitOps, and end-to-end pipeline <p>Happy learning and hacking with Kubernetes!</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/","title":"Kubernetes ArgoCD Tasks","text":"<ul> <li>Hands-on Kubernetes exercises covering ArgoCD installation, CLI usage, application deployment, GitOps workflows, and the App of Apps pattern.</li> <li>Each task includes a description, scenario, and a detailed solution with step-by-step instructions.</li> <li>Practice these tasks to master ArgoCD from initial installation to advanced multi-app orchestration.</li> </ul>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#table-of-contents","title":"Table of Contents","text":"<ul> <li>01. Install ArgoCD via Helm</li> <li>02. Expose ArgoCD via Ingress</li> <li>03. Login with the ArgoCD CLI</li> <li>04. Deploy Your First Application via CLI</li> <li>05. Inspect Application Status and Health</li> <li>06. Manually Trigger a Sync</li> <li>07. Diff Live State Against Git</li> <li>08. Enable Auto-Sync with Self-Heal and Auto-Prune</li> <li>09. Test Self-Healing</li> <li>10. View Deployment History</li> <li>11. Rollback an Application</li> <li>12. Deploy a Helm Chart via ArgoCD</li> <li>13. Deploy from Kustomize via ArgoCD</li> <li>14. Connect a Private Repository</li> <li>15. The App of Apps Pattern</li> <li>16. Use Sync Waves for Ordered Deployment</li> <li>17. Manage Projects</li> <li>18. Use Resource Hooks (PreSync / PostSync)</li> <li>19. Troubleshoot a Failed Sync</li> <li>20. Cleanup and Uninstall ArgoCD</li> <li>21. Chain CLI Commands for Release Workflows</li> </ul>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#01-install-argocd-via-helm","title":"01. Install ArgoCD via Helm","text":"<p>Install ArgoCD on a Kubernetes cluster using the official Argo Helm chart.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario","title":"Scenario:","text":"<p>\u25e6 Your team has adopted GitOps and needs a central delivery platform for all Kubernetes workloads.   \u25e6 You\u2019ve chosen ArgoCD and need to install it on the cluster from scratch using Helm.</p> <p>Hint: <code>helm repo add</code>, <code>helm upgrade --install</code>, <code>kubectl get pods -n argocd</code></p> Solution <pre><code># 1. Add the Argo Helm repository\nhelm repo add argo https://argoproj.github.io/argo-helm\nhelm repo update argo\n\n# 2. Install ArgoCD in the argocd namespace (insecure mode: TLS terminated at Ingress)\nhelm upgrade --install argocd argo/argo-cd \\\n    --namespace argocd \\\n    --create-namespace \\\n    --set server.insecure=true \\\n    --wait\n\n# 3. Verify all pods are Running\nkubectl get pods -n argocd\n\n# Expected output (all pods Running):\n# NAME                                                READY   STATUS\n# argocd-application-controller-0                    1/1     Running\n# argocd-dex-server-xxxx                             1/1     Running\n# argocd-redis-xxxx                                  1/1     Running\n# argocd-repo-server-xxxx                            1/1     Running\n# argocd-server-xxxx                                 1/1     Running\n\n# 4. Retrieve the initial admin password\nkubectl -n argocd get secret argocd-initial-admin-secret \\\n    -o jsonpath=\"{.data.password}\" | base64 -d; echo\n\n# Save this password - you'll need it for the CLI and Web UI\n\n# 5. Verify the ArgoCD CRDs were installed\nkubectl get crd | grep argoproj\n# Expected: applications.argoproj.io, appprojects.argoproj.io, ...\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#02-expose-argocd-via-ingress","title":"02. Expose ArgoCD via Ingress","text":"<p>Expose the ArgoCD API server using an Nginx Ingress so it is accessible via a hostname instead of port-forwarding.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_1","title":"Scenario:","text":"<p>\u25e6 Port-forwarding is fine for development but your team needs a stable URL to access the ArgoCD UI and CLI.   \u25e6 You will create an Ingress pointing <code>argocd.local</code> at the ArgoCD server.</p> <p>Prerequisites: Nginx Ingress Controller installed on the cluster.</p> <p>Hint: <code>argocd-ingress.yaml</code>, <code>/etc/hosts</code>, <code>kubectl apply</code></p> Solution <pre><code># 1. Create the Ingress manifest\ncat &gt; argocd-ingress.yaml &lt;&lt; 'EOF'\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argocd-server-ingress\n  namespace: argocd\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTP\"\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: argocd.local\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: argocd-server\n                port:\n                  number: 80\nEOF\n\n# 2. Apply the Ingress\nkubectl apply -f argocd-ingress.yaml\n\n# 3. Verify the Ingress was created\nkubectl get ingress -n argocd\n\n# 4. Get the Ingress IP (use node IP for Kind/Minikube)\nINGRESS_IP=$(kubectl get nodes \\\n    -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n\necho \"Ingress IP: ${INGRESS_IP}\"\n\n# 5. Add the hostname to /etc/hosts\necho \"${INGRESS_IP}  argocd.local\" | sudo tee -a /etc/hosts\n\n# 6. Verify connectivity\ncurl -s -o /dev/null -w \"%{http_code}\" http://argocd.local\n# Expected: 200\n\n# Open in browser\nopen http://argocd.local\n\n# Fallback: port-forward if Ingress is not available\nkubectl port-forward svc/argocd-server -n argocd 8080:80 &amp;\nopen http://localhost:8080\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#03-login-with-the-argocd-cli","title":"03. Login with the ArgoCD CLI","text":"<p>Install the ArgoCD CLI and authenticate to the server.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_2","title":"Scenario:","text":"<p>\u25e6 You will use the ArgoCD CLI to manage applications, repositories, and sync policies from the terminal.   \u25e6 Before any CLI operations, you must authenticate to the ArgoCD server.</p> <p>Hint: <code>brew install argocd</code>, <code>argocd login</code>, <code>argocd account update-password</code></p> Solution <pre><code># \u2500\u2500 Step 1: Install the ArgoCD CLI \u2500\u2500\n\n# macOS\nbrew install argocd\n\n# Linux\ncurl -sSL -o argocd-linux-amd64 \\\n    https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\nrm argocd-linux-amd64\n\n# Verify installation\nargocd version --client\n\n# \u2500\u2500 Step 2: Retrieve the admin password \u2500\u2500\n\nARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret \\\n    -o jsonpath=\"{.data.password}\" | base64 -d)\necho \"Admin password: ${ARGOCD_PASSWORD}\"\n\n# \u2500\u2500 Step 3: Login via Ingress \u2500\u2500\n\nargocd login argocd.local \\\n    --username admin \\\n    --password \"${ARGOCD_PASSWORD}\" \\\n    --insecure\n\n# Login via port-forward (fallback)\nargocd login localhost:8080 \\\n    --username admin \\\n    --password \"${ARGOCD_PASSWORD}\" \\\n    --insecure\n\n# \u2500\u2500 Step 4: Verify login \u2500\u2500\n\nargocd account get-user-info\n# Output shows the currently logged-in user\n\nargocd cluster list\n# Should show: https://kubernetes.default.svc  in-cluster  ...\n\n# \u2500\u2500 Step 5: Change the admin password (recommended) \u2500\u2500\n\nargocd account update-password \\\n    --current-password \"${ARGOCD_PASSWORD}\" \\\n    --new-password \"MySecurePassword123!\"\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#04-deploy-your-first-application-via-cli","title":"04. Deploy Your First Application via CLI","text":"<p>Create and deploy the classic ArgoCD guestbook example application using the CLI.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_3","title":"Scenario:","text":"<p>\u25e6 You want to deploy a demo application to validate that ArgoCD can fetch from Git and deploy to the cluster.   \u25e6 You will use the public <code>argocd-example-apps</code> repository and the <code>guestbook</code> path.</p> <p>Hint: <code>argocd app create</code>, <code>argocd app sync</code>, <code>kubectl port-forward</code></p> Solution <pre><code># 1. Create the application\nargocd app create guestbook \\\n  --repo https://github.com/argoproj/argocd-example-apps.git \\\n  --path guestbook \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace guestbook \\\n  --sync-option CreateNamespace=true\n\n# 2. Verify the application was created\nargocd app list\n\n# Output shows:\n# NAME       CLUSTER     NAMESPACE  STATUS     HEALTH   SYNCPOLICY  ...\n# guestbook  in-cluster  guestbook  OutOfSync  Missing  &lt;none&gt;\n\n# 3. Manually trigger the first sync (deploy to cluster)\nargocd app sync guestbook\n\n# 4. Wait for the application to become Healthy + Synced\nargocd app wait guestbook --health --sync --timeout 120\n\n# 5. Verify Kubernetes resources were created\nkubectl get all -n guestbook\n\n# Expected:\n# pod/guestbook-ui-xxxx    Running\n# service/guestbook-ui\n# deployment/guestbook-ui\n\n# 6. Access the application\nkubectl port-forward svc/guestbook-ui -n guestbook 8081:80 &amp;\nopen http://localhost:8081\n\n# Cleanup the port-forward\nkill %1\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#05-inspect-application-status-and-health","title":"05. Inspect Application Status and Health","text":"<p>Use the CLI to inspect the full status, health, and resource tree of a deployed application.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_4","title":"Scenario:","text":"<p>\u25e6 A team member deployed an application and you need to understand its current state without touching the cluster directly.   \u25e6 You want to see what Kubernetes resources ArgoCD is managing.</p> <p>Hint: <code>argocd app get</code>, <code>--refresh</code>, <code>--output tree</code></p> Solution <pre><code># 1. Get a summary of the application\nargocd app get guestbook\n\n# Output includes:\n# Name:               guestbook\n# Project:            default\n# Server:             https://kubernetes.default.svc\n# Namespace:          guestbook\n# URL:                http://argocd.local/applications/guestbook\n# Repo:               https://github.com/argoproj/argocd-example-apps.git\n# Target:             HEAD\n# Path:               guestbook\n# SyncWindow:         Sync Allowed\n# Sync Policy:        &lt;none&gt;\n# Sync Status:        Synced to HEAD\n# Health Status:      Healthy\n#\n# GROUP  KIND        NAMESPACE  NAME          STATUS  HEALTH   HOOK  MESSAGE\n#        Service     guestbook  guestbook-ui  Synced  Healthy\n# apps   Deployment  guestbook  guestbook-ui  Synced  Healthy        ...\n\n# 2. Force-refresh from Git before displaying\nargocd app get guestbook --refresh\n\n# 3. Display as a resource tree\nargocd app get guestbook --output tree\n\n# 4. Output as JSON for scripting\nargocd app get guestbook -o json\n\n# 5. Get JSON and parse with jq\nargocd app get guestbook -o json | \\\n  jq '{name: .metadata.name, sync: .status.sync.status, health: .status.health.status}'\n\n# 6. Watch live updates\nwatch argocd app get guestbook\n\n# 7. Get all applications and their health at once\nargocd app list -o wide\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#06-manually-trigger-a-sync","title":"06. Manually Trigger a Sync","text":"<p>Practice manually triggering a sync and understand all the sync options available.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_5","title":"Scenario:","text":"<p>\u25e6 You pushed a change to Git and want to immediately deploy it without waiting for the 3-minute poll interval.   \u25e6 You also want to understand force sync, dry-run, and selective resource sync options.</p> <p>Hint: <code>argocd app sync</code>, <code>--dry-run</code>, <code>--force</code>, <code>--resource</code></p> Solution <pre><code># 1. Basic sync - apply the Git state to the cluster\nargocd app sync guestbook\n\n# 2. Sync and wait for completion with a timeout\nargocd app sync guestbook --timeout 120\n\n# 3. Dry-run - preview what would change without applying\nargocd app sync guestbook --dry-run\n\n# 4. Force sync - replace resources even if spec is unchanged\nargocd app sync guestbook --force\n\n# 5. Sync with pruning - delete resources removed from Git\nargocd app sync guestbook --prune\n\n# 6. Sync a specific resource only (avoids re-applying unchanged resources)\nargocd app sync guestbook \\\n  --resource apps:Deployment:guestbook-ui\n\n# 7. Sync only resources matching a label\nargocd app sync guestbook \\\n  --label app=guestbook-ui\n\n# 8. Sync with apply-out-of-sync-only (skip already-synced resources)\nargocd app sync guestbook --apply-out-of-sync-only\n\n# 9. Sync multiple applications at once\nargocd app sync guestbook app-of-apps efk-stack\n\n# 10. Check the sync status after sync\nargocd app get guestbook | grep -E \"Sync|Health\"\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#07-diff-live-state-against-git","title":"07. Diff Live State Against Git","text":"<p>Use <code>argocd app diff</code> to see exactly what has drifted between the live cluster state and Git.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_6","title":"Scenario:","text":"<p>\u25e6 A developer manually patched a running Deployment with <code>kubectl edit</code> and your monitoring shows the application is <code>OutOfSync</code>.   \u25e6 Before syncing to fix the drift, you want to see exactly what changed.</p> <p>Hint: <code>argocd app diff</code>, <code>kubectl scale</code>, drift detection</p> Solution <pre><code># 1. Deliberately introduce drift - manually scale the deployment\nkubectl scale deployment guestbook-ui --replicas=5 -n guestbook\n\n# 2. Wait for ArgoCD to detect the drift\nsleep 10\nargocd app get guestbook | grep -E \"Sync|Health\"\n# Sync Status: OutOfSync\n\n# 3. Show the diff - what changed in live vs Git\nargocd app diff guestbook\n\n# Output highlights the replica count change:\n# ===== apps/Deployment guestbook/guestbook-ui ======\n# 10       - replicas: 5    (live)\n# 10       + replicas: 1    (desired from Git)\n\n# 4. Diff against a specific Git revision\nargocd app diff guestbook --revision HEAD~1\n\n# 5. Diff only a specific resource\nargocd app diff guestbook \\\n  --resource apps:Deployment:guestbook-ui\n\n# 6. Use in CI - exit non-zero if drift is detected\nif ! argocd app diff guestbook --exit-code; then\n  echo \"DRIFT DETECTED - syncing...\"\n  argocd app sync guestbook\nfi\n\n# 7. Restore the desired state from Git\nargocd app sync guestbook\nkubectl get deployment guestbook-ui -n guestbook\n# READY should be back to 1/1\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#08-enable-auto-sync-with-self-heal-and-auto-prune","title":"08. Enable Auto-Sync with Self-Heal and Auto-Prune","text":"<p>Configure automated sync so ArgoCD continuously reconciles the cluster state with Git.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_7","title":"Scenario:","text":"<p>\u25e6 Your team pushes application changes directly to Git and expects them to be deployed automatically.   \u25e6 You also want ArgoCD to clean up resources removed from Git and heal any manual drift.</p> <p>Hint: <code>argocd app set --sync-policy automated</code>, <code>--self-heal</code>, <code>--auto-prune</code></p> Solution <pre><code># 1. Enable automated sync (ArgoCD polls Git every ~3 minutes)\nargocd app set guestbook --sync-policy automated\n\n# 2. Verify the sync policy was applied\nargocd app get guestbook | grep \"Sync Policy\"\n# Sync Policy: Automated\n\n# 3. Add self-heal: restores Git state if cluster is manually modified\nargocd app set guestbook --self-heal\n\n# 4. Add auto-prune: deletes resources removed from Git\nargocd app set guestbook --auto-prune\n\n# 5. Verify all options are active\nargocd app get guestbook | grep -E \"Sync Policy|Prune|Self Heal\"\n\n# 6. Test auto-sync: manually break the state\nkubectl scale deployment guestbook-ui --replicas=5 -n guestbook\necho \"Waiting for ArgoCD self-heal...\"\nsleep 30\nkubectl get deployment guestbook-ui -n guestbook\n# READY should be restored to 1/1 by ArgoCD\n\n# 7. Configure using app manifest equivalents (declarative approach)\n# The equivalent spec in an Application YAML:\n# spec:\n#   syncPolicy:\n#     automated:\n#       prune: true\n#       selfHeal: true\n#     syncOptions:\n#       - CreateNamespace=true\n\n# 8. Disable auto-sync (switch back to manual)\nargocd app set guestbook --sync-policy none\nargocd app get guestbook | grep \"Sync Policy\"\n# Sync Policy: &lt;none&gt;\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#09-test-self-healing","title":"09. Test Self-Healing","text":"<p>Validate that ArgoCD self-healing works by deliberately introducing drift and observing automatic recovery.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_8","title":"Scenario:","text":"<p>\u25e6 A runbook says to test ArgoCD self-healing quarterly.   \u25e6 You need to break the cluster state and confirm ArgoCD repairs it within the reconciliation window.</p> <p>Hint: <code>kubectl scale</code>, <code>kubectl delete</code>, <code>watch argocd app get</code></p> Solution <pre><code># 0. Ensure auto-sync + self-heal are enabled\nargocd app set guestbook --sync-policy automated --self-heal --auto-prune\n\n# \u2500\u2500 Test 1: Scale drift \u2500\u2500\n\n# Break it\nkubectl scale deployment guestbook-ui --replicas=10 -n guestbook\necho \"Breaking: scaled to 10 replicas\"\n\n# Watch ArgoCD detect and fix it (up to ~30s)\nwatch -n 5 \"kubectl get deployment guestbook-ui -n guestbook &amp;&amp; argocd app get guestbook | grep -E 'Status|Health'\"\n\n# After ~30 seconds, replicas will return to the value in Git\nkubectl get deployment guestbook-ui -n guestbook\n# DESIRED should match Git (e.g., 1)\n\n# \u2500\u2500 Test 2: Delete a managed resource \u2500\u2500\n\n# Delete the service\nkubectl delete service guestbook-ui -n guestbook\necho \"Deleted the guestbook-ui service\"\n\n# ArgoCD detects the missing resource and recreates it\nsleep 30\nkubectl get service guestbook-ui -n guestbook\n# Service should be recreated\n\n# \u2500\u2500 Test 3: Manual label change \u2500\u2500\n\n# Add a label not in Git\nkubectl label deployment guestbook-ui -n guestbook manual-change=true\n\n# ArgoCD will detect and revert this within the next sync cycle\nsleep 60\nkubectl get deployment guestbook-ui -n guestbook --show-labels | grep manual-change\n# Label should be gone\n\n# \u2500\u2500 Summary \u2500\u2500\n\nargocd app get guestbook\n# Health Status: Healthy\n# Sync Status: Synced\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#10-view-deployment-history","title":"10. View Deployment History","text":"<p>Use <code>argocd app history</code> to inspect the deployment history of an application.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_9","title":"Scenario:","text":"<p>\u25e6 You need to audit which Git commits were deployed over the past month.   \u25e6 You want to identify the revision ID to use for a rollback.</p> <p>Hint: <code>argocd app history</code>, <code>-o json</code>, <code>jq</code></p> Solution <pre><code># 1. Create some history by triggering multiple syncs\nargocd app sync guestbook\nargocd app sync guestbook\nargocd app sync guestbook\n\n# 2. View the deployment history\nargocd app history guestbook\n\n# Output shows each deployment:\n# ID  DATE                           REVISION\n# 0   2026-02-22 10:00:00 +0000 UTC  HEAD (abc1234)\n# 1   2026-02-22 10:05:00 +0000 UTC  HEAD (abc1234)\n# 2   2026-02-22 10:10:00 +0000 UTC  HEAD (abc1234)\n\n# 3. Output as JSON for scripting\nargocd app history guestbook -o json\n\n# 4. Extract key fields with jq\nargocd app history guestbook -o json | \\\n  jq '.[] | {id: .id, date: .deployedAt, revision: .revision}'\n\n# 5. Find the most recent deployment\nargocd app history guestbook -o json | jq '.[-1]'\n\n# 6. Find deployments by Git commit SHA\nargocd app history guestbook -o json | \\\n  jq '.[] | select(.revision | contains(\"abc1234\"))'\n\n# 7. Save history to file for an audit log\nargocd app history guestbook -o json &gt; guestbook-deploy-history.json\ncat guestbook-deploy-history.json\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#11-rollback-an-application","title":"11. Rollback an Application","text":"<p>Rollback an application to a previously deployed revision using the ArgoCD CLI.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_10","title":"Scenario:","text":"<p>\u25e6 A recent deployment introduced a regression.   \u25e6 You need to immediately revert to the last known-good revision to restore service.</p> <p>Hint: <code>argocd app history</code>, <code>argocd app rollback</code>, <code>argocd app set --sync-policy</code></p> Solution <pre><code># 1. Inspect the deployment history to choose a target revision\nargocd app history guestbook\n\n# Note the ID of the revision you want to roll back to.\n# In this example, we'll rollback to revision ID 0.\n\n# 2. Perform the rollback\nargocd app rollback guestbook 0\n\n# ArgoCD rolls back the cluster state to the snapshot from revision 0.\n# NOTE: Rollback disables automated sync on the app to prevent\n#       ArgoCD from immediately re-syncing forward again.\n\n# 3. Wait for the rollback to complete\nargocd app wait guestbook --health --timeout 120\n\n# 4. Verify the status\nargocd app get guestbook\n\n# 5. Verify the Kubernetes resources reflect the rolled-back state\nkubectl get all -n guestbook\n\n# 6. Check history - rollback is recorded as a new entry\nargocd app history guestbook\n\n# 7. Re-enable auto-sync after the incident is resolved\nargocd app set guestbook \\\n  --sync-policy automated \\\n  --self-heal \\\n  --auto-prune\n\n# 8. Confirm the app is back to Synced + Healthy\nargocd app get guestbook | grep -E \"Sync|Health\"\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#12-deploy-a-helm-chart-via-argocd","title":"12. Deploy a Helm Chart via ArgoCD","text":"<p>Use ArgoCD to deploy a Helm chart from a chart repository, with custom values managed in Git.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_11","title":"Scenario:","text":"<p>\u25e6 You want ArgoCD to own the lifecycle of a Helm release, including upgrades and drift detection.   \u25e6 Custom <code>values.yaml</code> overrides are stored in Git so changes go through GitOps.</p> <p>Hint: <code>argocd app create --helm-chart</code>, <code>--helm-set</code>, <code>--revision</code></p> Solution <pre><code># \u2500\u2500 Option A: Deploy a Helm chart from an OCI / chart registry \u2500\u2500\n\nargocd app create nginx-helm \\\n  --repo https://charts.bitnami.com/bitnami \\\n  --helm-chart nginx \\\n  --revision 15.1.0 \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace nginx-helm \\\n  --sync-option CreateNamespace=true \\\n  --helm-set service.type=ClusterIP \\\n  --helm-set replicaCount=2\n\n# Sync and wait\nargocd app sync nginx-helm\nargocd app wait nginx-helm --health --timeout 120\n\n# \u2500\u2500 Option B: Deploy a Helm chart stored in a Git repository \u2500\u2500\n\n# Store values overrides in Git, e.g.:\n#   my-repo/nginx/values.yaml\n#   apiVersion: argoproj.io/v1alpha1  \u2190 not needed, ArgoCD auto-detects Helm\n\nargocd app create nginx-git-helm \\\n  --repo https://github.com/my-org/my-charts.git \\\n  --path nginx \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace nginx-git-helm \\\n  --sync-option CreateNamespace=true\n\n# \u2500\u2500 Update Helm values through CLI (without changing Git) \u2500\u2500\n\nargocd app set nginx-helm \\\n  --helm-set replicaCount=3 \\\n  --helm-set image.tag=1.25.0\n\nargocd app sync nginx-helm\n\n# \u2500\u2500 Verify \u2500\u2500\n\nargocd app get nginx-helm | grep -E \"Sync|Health|Revision\"\nkubectl get deployment -n nginx-helm\n\n# \u2500\u2500 Cleanup \u2500\u2500\n\nargocd app delete nginx-helm --yes\nkubectl delete namespace nginx-helm\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#13-deploy-from-kustomize-via-argocd","title":"13. Deploy from Kustomize via ArgoCD","text":"<p>Use ArgoCD to deploy a Kustomize-based application, showing how ArgoCD auto-detects the tool.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_12","title":"Scenario:","text":"<p>\u25e6 Your team uses Kustomize overlays to manage configuration across environments (base + overlays).   \u25e6 You want ArgoCD to render and deploy the Kustomize manifests for a specific overlay.</p> <p>Hint: ArgoCD auto-detects Kustomize from <code>kustomization.yaml</code>. Point <code>--path</code> to the overlay directory.</p> Solution <pre><code># 1. Create a minimal Kustomize app structure in your Git repo\n#    Structure:\n#    kustomize-demo/\n#    \u251c\u2500\u2500 base/\n#    \u2502   \u251c\u2500\u2500 deployment.yaml\n#    \u2502   \u251c\u2500\u2500 service.yaml\n#    \u2502   \u2514\u2500\u2500 kustomization.yaml\n#    \u2514\u2500\u2500 overlays/\n#        \u2514\u2500\u2500 dev/\n#            \u251c\u2500\u2500 replica-patch.yaml\n#            \u2514\u2500\u2500 kustomization.yaml\n\n# 2. Create the application in ArgoCD pointing at a Kustomize overlay\nargocd app create kustomize-demo \\\n  --repo https://github.com/argoproj/argocd-example-apps.git \\\n  --path kustomize-guestbook \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace kustomize-demo \\\n  --sync-option CreateNamespace=true\n\n# ArgoCD detects kustomization.yaml and uses `kustomize build` to render manifests\n\n# 3. Sync the application\nargocd app sync kustomize-demo\nargocd app wait kustomize-demo --health --timeout 120\n\n# 4. View rendered manifests (what kustomize build produced)\nargocd app manifests kustomize-demo\n\n# 5. Verify resources\nkubectl get all -n kustomize-demo\n\n# 6. Apply a Kustomize image override via CLI\nargocd app set kustomize-demo \\\n  --kustomize-image gcr.io/argoproj/argocd-example-apps/guestbook-ui:v0.2\n\nargocd app sync kustomize-demo\n\n# 7. Cleanup\nargocd app delete kustomize-demo --yes\nkubectl delete namespace kustomize-demo\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#14-connect-a-private-repository","title":"14. Connect a Private Repository","text":"<p>Add a private Git repository to ArgoCD using an HTTPS token or SSH key.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_13","title":"Scenario:","text":"<p>\u25e6 Your application manifests live in a private GitHub repository.   \u25e6 ArgoCD needs credentials to clone the repository in order to deploy from it.</p> <p>Hint: <code>argocd repo add</code>, <code>--username</code>, <code>--password</code>, <code>--ssh-private-key-path</code></p> Solution <pre><code># \u2500\u2500 Option A: Connect via HTTPS Personal Access Token (PAT) \u2500\u2500\n\n# Create a GitHub PAT with 'repo' scope at https://github.com/settings/tokens\n\nargocd repo add https://github.com/my-org/private-repo.git \\\n    --username git \\\n    --password &lt;YOUR_PAT_HERE&gt;\n\n# \u2500\u2500 Option B: Connect via SSH Key \u2500\u2500\n\n# Generate a deploy key (no passphrase)\nssh-keygen -t ed25519 -C \"argocd-deploy-key\" -f ~/.ssh/argocd-deploy-key -N \"\"\n\n# Add the public key to GitHub repo:\n# GitHub Repo \u2192 Settings \u2192 Deploy Keys \u2192 Add Deploy Key\n# Paste the contents of ~/.ssh/argocd-deploy-key.pub\n\n# Add the private key to ArgoCD\nargocd repo add git@github.com:my-org/private-repo.git \\\n    --ssh-private-key-path ~/.ssh/argocd-deploy-key\n\n# \u2500\u2500 Option C: Add a private Helm chart repository \u2500\u2500\n\nargocd repo add https://my-private-charts.example.com \\\n    --type helm \\\n    --name private-charts \\\n    --username admin \\\n    --password &lt;PASSWORD&gt;\n\n# \u2500\u2500 Verify the connection \u2500\u2500\n\nargocd repo list\n\n# Expected output shows STATUS: Successful\n# SERVER                                        TYPE  STATUS      MESSAGE\n# https://github.com/my-org/private-repo.git   git   Successful\n\n# \u2500\u2500 Use the private repo in an application \u2500\u2500\n\nargocd app create my-private-app \\\n  --repo https://github.com/my-org/private-repo.git \\\n  --path manifests \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace my-app\n\n# \u2500\u2500 Remove a repository \u2500\u2500\n\nargocd repo rm https://github.com/my-org/private-repo.git\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#15-the-app-of-apps-pattern","title":"15. The App of Apps Pattern","text":"<p>Use a single root Application to manage a directory of child Application manifests declaratively.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_14","title":"Scenario:","text":"<p>\u25e6 You have many microservices and want a single GitOps entry point.   \u25e6 Adding or removing an app is as simple as committing or deleting a YAML file in Git.   \u25e6 The App of Apps pattern makes fleet management fully declarative.</p> <p>Hint: <code>argocd app create</code> pointing at a directory of Application YAMLs, <code>argocd app list</code></p> Solution <pre><code># \u2500\u2500 Step 1: Create child Application manifests and commit them to Git \u2500\u2500\n\n# apps/guestbook.yaml\ncat &gt; /tmp/guestbook.yaml &lt;&lt; 'EOF'\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: guestbook\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/argoproj/argocd-example-apps.git\n    targetRevision: HEAD\n    path: guestbook\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: guestbook\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\nEOF\n\n# apps/nginx.yaml\ncat &gt; /tmp/nginx.yaml &lt;&lt; 'EOF'\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: nginx-demo\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/argoproj/argocd-example-apps.git\n    targetRevision: HEAD\n    path: nginx-ingress\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: nginx-demo\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\nEOF\n\n# Commit both files to the apps/ directory of your Git repository\n\n# \u2500\u2500 Step 2: Create the root App of Apps \u2500\u2500\n\nargocd app create app-of-apps \\\n  --repo https://github.com/my-org/my-gitops-repo.git \\\n  --path apps \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace argocd \\\n  --sync-policy automated \\\n  --auto-prune \\\n  --self-heal\n\n# \u2500\u2500 Step 3: Sync the root app \u2500\u2500\n\nargocd app sync app-of-apps\n\n# ArgoCD discovers all YAML files in apps/ and creates child Applications\n\n# \u2500\u2500 Step 4: Verify all child apps were created \u2500\u2500\n\nargocd app list\n# Expected:\n# NAME          CLUSTER     NAMESPACE  STATUS  HEALTH   SYNCPOLICY\n# app-of-apps   in-cluster  argocd     Synced  Healthy  Auto-Prune\n# guestbook     in-cluster  guestbook  Synced  Healthy  Auto-Prune\n# nginx-demo    in-cluster  nginx-demo Synced  Healthy  Auto-Prune\n\n# \u2500\u2500 Step 5: Add a new application (GitOps way) \u2500\u2500\n\n# Commit a new YAML file to the apps/ directory in Git.\n# ArgoCD detects the change and automatically creates the child Application.\n# No kubectl or argocd commands needed!\n\n# \u2500\u2500 Step 6: Remove an application (GitOps way) \u2500\u2500\n\n# Delete the YAML file from apps/ in Git and commit.\n# With auto-prune enabled, ArgoCD deletes the Application and its resources.\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#16-use-sync-waves-for-ordered-deployment","title":"16. Use Sync Waves for Ordered Deployment","text":"<p>Control the order in which resources are synced during a deployment using sync wave annotations.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_15","title":"Scenario:","text":"<p>\u25e6 You have a database, a backend API, and a frontend that must start in order.   \u25e6 Sync waves let you define phases so that each component waits for the previous one to become healthy.</p> <p>Hint: <code>argocd.argoproj.io/sync-wave</code> annotation, wave numbers</p> Solution <pre><code># Sync waves are set as annotations on Kubernetes resources in Git.\n# Resources in lower waves deploy and become healthy before higher waves start.\n\n# \u2500\u2500 Example: 3-tier app with ordered deployment \u2500\u2500\n\n# wave 0: Namespace and ConfigMaps (no dependencies)\ncat &gt; /tmp/namespace.yaml &lt;&lt; 'EOF'\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-app\n  annotations:\n    argocd.argoproj.io/sync-wave: \"0\"\nEOF\n\n# wave 1: Database (must be healthy before the API starts)\ncat &gt; /tmp/database-deployment.yaml &lt;&lt; 'EOF'\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  namespace: my-app\n  annotations:\n    argocd.argoproj.io/sync-wave: \"1\"\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n        - name: postgres\n          image: postgres:15\n          env:\n            - name: POSTGRES_PASSWORD\n              value: \"mysecretpassword\"\nEOF\n\n# wave 2: Backend API (waits for database to be healthy)\ncat &gt; /tmp/api-deployment.yaml &lt;&lt; 'EOF'\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend-api\n  namespace: my-app\n  annotations:\n    argocd.argoproj.io/sync-wave: \"2\"\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: backend-api\n  template:\n    metadata:\n      labels:\n        app: backend-api\n    spec:\n      containers:\n        - name: api\n          image: my-api:latest\nEOF\n\n# wave 3: Frontend (waits for the API to be healthy)\ncat &gt; /tmp/frontend-deployment.yaml &lt;&lt; 'EOF'\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: my-app\n  annotations:\n    argocd.argoproj.io/sync-wave: \"3\"\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n        - name: frontend\n          image: my-frontend:latest\nEOF\n\n# Commit all files to a Git path, then create the app:\nargocd app create my-app \\\n  --repo https://github.com/my-org/my-repo.git \\\n  --path manifests \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace my-app \\\n  --sync-option CreateNamespace=true\n\nargocd app sync my-app\n\n# Watch the wave-by-wave deployment progress\nwatch argocd app get my-app\n\n# Wave execution order:\n# Wave 0: Namespace created\n# Wave 1: postgres Deployment reaches Healthy\n# Wave 2: backend-api Deployment reaches Healthy\n# Wave 3: frontend Deployment reaches Healthy\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#17-manage-projects","title":"17. Manage Projects","text":"<p>Create an ArgoCD Project to restrict what repositories, clusters, and namespaces an application can use.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_16","title":"Scenario:","text":"<p>\u25e6 Your cluster hosts applications for multiple teams (frontend, backend, ops).   \u25e6 You want to prevent the frontend team from accidentally deploying to the <code>kube-system</code> namespace.   \u25e6 ArgoCD Projects provide RBAC-level isolation between teams.</p> <p>Hint: <code>argocd proj create</code>, <code>--src-repos</code>, <code>--dest</code>, <code>argocd proj list</code></p> Solution <pre><code># 1. Create a project for the frontend team\nargocd proj create frontend \\\n  --description \"Frontend team applications\" \\\n  --src-repos \"https://github.com/my-org/frontend-repo.git\" \\\n  --dest \"https://kubernetes.default.svc,frontend-*\" \\\n  --dest \"https://kubernetes.default.svc,staging\"\n\n# --src-repos: only this repo is allowed as a source\n# --dest:      only these patterns are allowed as destinations (cluster,namespace)\n\n# 2. Verify the project was created\nargocd proj list\n\n# 3. View project details\nargocd proj get frontend\n\n# 4. Add additional allowed source repositories\nargocd proj add-source frontend \\\n  \"https://github.com/my-org/shared-charts.git\"\n\n# 5. Add allowed destinations\nargocd proj add-destination frontend \\\n  https://kubernetes.default.svc production-frontend\n\n# 6. Set cluster-scope resource DENY list (prevent modification of cluster-level resources)\nargocd proj deny-cluster-resource frontend \"*\" \"*\"\nargocd proj allow-cluster-resource frontend \"\" \"Namespace\"\n\n# 7. Assign an application to the project\nargocd app create frontend-app \\\n  --project frontend \\\n  --repo https://github.com/my-org/frontend-repo.git \\\n  --path manifests \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace frontend-prod\n\n# 8. Attempting to use a disallowed repo will fail with a permission error\nargocd app create bad-app \\\n  --project frontend \\\n  --repo https://github.com/other-org/other-repo.git \\\n  --path manifests \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace kube-system\n# Error: application destination {... kube-system} is not permitted in project 'frontend'\n\n# 9. Cleanup\nargocd app delete frontend-app --yes 2&gt;/dev/null || true\nargocd proj delete frontend\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#18-use-resource-hooks-presync-postsync","title":"18. Use Resource Hooks (PreSync / PostSync)","text":"<p>Use ArgoCD resource hooks to run Jobs before or after a sync operation - e.g., database migrations or smoke tests.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_17","title":"Scenario:","text":"<p>\u25e6 Your application requires a database migration to run before the new version starts.   \u25e6 After deployment you want a smoke test to verify the application is responding correctly.</p> <p>Hint: <code>argocd.argoproj.io/hook</code> annotation, <code>PreSync</code>, <code>PostSync</code>, <code>argocd.argoproj.io/hook-delete-policy</code></p> Solution <pre><code># Hooks are standard Kubernetes Jobs with special ArgoCD annotations.\n# They are stored in your Git repository alongside the application manifests.\n\n# \u2500\u2500 PreSync Hook: Run database migration before sync \u2500\u2500\n\ncat &gt; /tmp/pre-sync-migration.yaml &lt;&lt; 'EOF'\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migration\n  namespace: my-app\n  annotations:\n    argocd.argoproj.io/hook: PreSync\n    argocd.argoproj.io/hook-delete-policy: HookSucceeded\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n        - name: migrate\n          image: my-app:latest\n          command: [\"./migrate.sh\"]\n          env:\n            - name: DB_HOST\n              value: postgres.my-app.svc\nEOF\n\n# \u2500\u2500 PostSync Hook: Run smoke test after sync \u2500\u2500\n\ncat &gt; /tmp/post-sync-smoke-test.yaml &lt;&lt; 'EOF'\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: smoke-test\n  namespace: my-app\n  annotations:\n    argocd.argoproj.io/hook: PostSync\n    argocd.argoproj.io/hook-delete-policy: HookSucceeded\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n        - name: smoke-test\n          image: curlimages/curl:latest\n          command:\n            - sh\n            - -c\n            - |\n              echo \"Running smoke test...\"\n              until curl -sf http://my-app.my-app.svc/health; do\n                echo \"Service not ready, retrying in 5s...\"\n                sleep 5\n              done\n              echo \"Smoke test passed!\"\nEOF\n\n# \u2500\u2500 Sync Wave + Hook combination \u2500\u2500\n# Use sync waves to order hooks relative to other resources:\n#   annotations:\n#     argocd.argoproj.io/hook: PreSync\n#     argocd.argoproj.io/sync-wave: \"-5\"   # Run early within PreSync phase\n\n# \u2500\u2500 Hook Delete Policies \u2500\u2500\n# HookSucceeded:       Delete job when it succeeds (default)\n# HookFailed:          Delete job when it fails\n# BeforeHookCreation:  Delete previous run before creating a new one\n\n# \u2500\u2500 Commit the hook files to Git and sync \u2500\u2500\n\nargocd app sync my-app\n\n# Watch hooks execute\nkubectl get jobs -n my-app -w\n\n# Check hook logs\nkubectl logs job/db-migration -n my-app\nkubectl logs job/smoke-test -n my-app\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#19-troubleshoot-a-failed-sync","title":"19. Troubleshoot a Failed Sync","text":"<p>Diagnose and fix a sync failure using CLI commands and <code>kubectl</code>.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_18","title":"Scenario:","text":"<p>\u25e6 An application is stuck in <code>OutOfSync</code> or <code>Degraded</code> state.   \u25e6 You need to identify the root cause and resolve it.</p> <p>Hint: <code>argocd app get</code>, <code>argocd app diff</code>, <code>kubectl describe</code>, <code>kubectl logs</code></p> Solution <pre><code># \u2500\u2500 Step 1: Get the high-level status \u2500\u2500\n\nargocd app get &lt;app-name&gt;\n# Look for degraded resources or error messages in the resource list\n\n# \u2500\u2500 Step 2: Show the diff to understand what ArgoCD is trying to apply \u2500\u2500\n\nargocd app diff &lt;app-name&gt;\n\n# \u2500\u2500 Step 3: Get the rendered manifests \u2500\u2500\n\nargocd app manifests &lt;app-name&gt;\n# Validate the manifest looks correct\n\n# \u2500\u2500 Step 4: Check ArgoCD conditions and events \u2500\u2500\n\nkubectl describe application &lt;app-name&gt; -n argocd\n# Look at Conditions and Events sections\n\n# \u2500\u2500 Step 5: Check the ArgoCD application controller logs \u2500\u2500\n\nkubectl logs -n argocd \\\n  -l app.kubernetes.io/name=argocd-application-controller \\\n  --tail=100 | grep -i \"error\\|failed\\|&lt;app-name&gt;\"\n\n# \u2500\u2500 Step 6: Check the repo-server logs (manifest rendering issues) \u2500\u2500\n\nkubectl logs -n argocd \\\n  -l app.kubernetes.io/name=argocd-repo-server \\\n  --tail=50 | grep -i \"error\\|failed\"\n\n# \u2500\u2500 Step 7: Force-refresh and retry sync \u2500\u2500\n\nargocd app get &lt;app-name&gt; --refresh\nargocd app sync &lt;app-name&gt; --force\n\n# \u2500\u2500 Step 8: Common issues and fixes \u2500\u2500\n\n# Issue: Repository error (auth failure)\nargocd repo list               # Check STATUS column\nargocd repo get &lt;repo-url&gt;     # Check detailed status\n\n# Issue: Out of sync but diff shows no changes (stuck sync)\nargocd app sync &lt;app-name&gt; --force --replace\n\n# Issue: Hook is stuck running\nkubectl get jobs -n &lt;namespace&gt;\nkubectl delete job &lt;stuck-job-name&gt; -n &lt;namespace&gt;\nargocd app sync &lt;app-name&gt;\n\n# Issue: Resource exists with different owner (e.g., fields managed by another controller)\nargocd app sync &lt;app-name&gt; --server-side-apply\n\n# Issue: Namespace doesn't exist\nargocd app set &lt;app-name&gt; --sync-option CreateNamespace=true\nargocd app sync &lt;app-name&gt;\n\n# \u2500\u2500 Step 9: App of Apps - child apps not created \u2500\u2500\n\nargocd app get app-of-apps            # Check root is Synced\nargocd repo list                      # Confirm repo is accessible\nargocd app manifests app-of-apps      # Confirm apps/ dir renders correctly\nkubectl get applications -n argocd    # Check all Application CRs\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#20-cleanup-and-uninstall-argocd","title":"20. Cleanup and Uninstall ArgoCD","text":"<p>Safely remove all ArgoCD applications and uninstall ArgoCD from the cluster.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_19","title":"Scenario:","text":"<p>\u25e6 You\u2019ve finished a demo or training environment and need to tear everything down cleanly.   \u25e6 Resources must be deleted in the correct order to avoid orphaned namespaces or finalizer deadlocks.</p> <p>Hint: <code>argocd app delete --cascade</code>, <code>helm uninstall</code>, finalizer removal</p> Solution <pre><code># \u2500\u2500 Step 1: Delete all managed applications (cascade removes K8s resources too) \u2500\u2500\n\n# List all applications first\nargocd app list\n\n# Delete individual apps with cascade\nargocd app delete guestbook --yes\nargocd app delete app-of-apps --yes\n\n# Or delete ALL applications in one command\nargocd app list -o name | xargs -I {} argocd app delete {} --yes\n\n# \u2500\u2500 Step 2: Verify managed namespaces were cleaned up \u2500\u2500\n\nkubectl get namespace | grep -E \"guestbook|efk|nginx\"\n\n# \u2500\u2500 Step 3: Remove connected repositories \u2500\u2500\n\nargocd repo list | awk 'NR&gt;1 {print $1}' | xargs -I {} argocd repo rm {}\n\n# \u2500\u2500 Step 4: Remove custom Projects (if any were created) \u2500\u2500\n\nargocd proj list | awk 'NR&gt;1 &amp;&amp; $1 != \"default\" {print $1}' | \\\n  xargs -I {} argocd proj delete {}\n\n# \u2500\u2500 Step 5: If apps are stuck due to finalizers, remove them manually \u2500\u2500\n\n# List all Application CRs\nkubectl get applications -n argocd\n\n# Remove a stuck application's finalizer\nkubectl patch application &lt;app-name&gt; -n argocd \\\n  -p '{\"metadata\":{\"finalizers\":[]}}' \\\n  --type merge\n\n# \u2500\u2500 Step 6: Uninstall ArgoCD via Helm \u2500\u2500\n\nhelm uninstall argocd --namespace argocd\n\n# \u2500\u2500 Step 7: Delete the ArgoCD namespace and CRDs \u2500\u2500\n\nkubectl delete namespace argocd\n\n# Delete ArgoCD CRDs\nkubectl get crd | grep argoproj.io | awk '{print $1}' | \\\n  xargs kubectl delete crd\n\n# \u2500\u2500 Step 8: Verify everything is gone \u2500\u2500\n\nkubectl get all -n argocd        # Should return \"No resources found\"\nkubectl get crd | grep argoproj  # Should return nothing\nhelm list --all-namespaces       # argocd should not appear\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#21-chain-cli-commands-for-release-workflows","title":"21. Chain CLI Commands for Release Workflows","text":"<p>Practice common multi-step ArgoCD CLI workflows for day-to-day GitOps operations.</p>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#scenario_20","title":"Scenario:","text":"<p>\u25e6 You want repeatable, scriptable workflows for deploying, validating, and rolling back GitOps applications.   \u25e6 These one-liners and scripts model real-world CI/CD integration patterns.</p> <p>Hint: Chain <code>argocd</code> and <code>kubectl</code> commands with <code>&amp;&amp;</code>, <code>||</code>, and loops.</p> Solution <pre><code># \u2500\u2500 Workflow 1: Install ArgoCD, login, and deploy guestbook in one sequence \u2500\u2500\n\nhelm upgrade --install argocd argo/argo-cd \\\n    --namespace argocd --create-namespace \\\n    --set server.insecure=true --wait &amp;&amp; \\\nPASS=$(kubectl -n argocd get secret argocd-initial-admin-secret \\\n    -o jsonpath=\"{.data.password}\" | base64 -d) &amp;&amp; \\\nargocd login argocd.local \\\n    --username admin --password \"$PASS\" --insecure &amp;&amp; \\\nargocd app create guestbook \\\n    --repo https://github.com/argoproj/argocd-example-apps.git \\\n    --path guestbook \\\n    --dest-server https://kubernetes.default.svc \\\n    --dest-namespace guestbook \\\n    --sync-option CreateNamespace=true &amp;&amp; \\\nargocd app sync guestbook &amp;&amp; \\\nargocd app wait guestbook --health --timeout 120 &amp;&amp; \\\necho \"Guestbook deployed successfully!\"\n\n# \u2500\u2500 Workflow 2: Deploy and auto-heal setup \u2500\u2500\n\nargocd app create guestbook \\\n    --repo https://github.com/argoproj/argocd-example-apps.git \\\n    --path guestbook \\\n    --dest-server https://kubernetes.default.svc \\\n    --dest-namespace guestbook \\\n    --sync-policy automated \\\n    --auto-prune \\\n    --self-heal \\\n    --sync-option CreateNamespace=true &amp;&amp; \\\nargocd app wait guestbook --health &amp;&amp; \\\nargocd app get guestbook\n\n# \u2500\u2500 Workflow 3: Health-check and rollback on failure \u2500\u2500\n\nargocd app sync guestbook --timeout 120 &amp;&amp; \\\nargocd app wait guestbook --health --timeout 60 || \\\n( echo \"Deployment failed - rolling back.\" &amp;&amp; argocd app rollback guestbook 0 )\n\n# \u2500\u2500 Workflow 4: Check all apps and alert on degraded \u2500\u2500\n\nDEGRADED=$(argocd app list -o json | jq -r \\\n  '.[] | select(.status.health.status != \"Healthy\") | .metadata.name')\nif [ -n \"$DEGRADED\" ]; then\n  echo \"ALERT: Degraded applications detected:\"\n  echo \"$DEGRADED\"\nelse\n  echo \"All applications are Healthy.\"\nfi\n\n# \u2500\u2500 Workflow 5: Force-refresh and sync all out-of-sync apps \u2500\u2500\n\nargocd app list -o json | \\\n  jq -r '.[] | select(.status.sync.status == \"OutOfSync\") | .metadata.name' | \\\n  xargs -I {} bash -c 'argocd app get {} --refresh &amp;&amp; argocd app sync {}'\n\n# \u2500\u2500 Workflow 6: Deploy App of Apps and wait for all children \u2500\u2500\n\nargocd app create app-of-apps \\\n    --repo https://github.com/my-org/my-gitops-repo.git \\\n    --path apps \\\n    --dest-server https://kubernetes.default.svc \\\n    --dest-namespace argocd \\\n    --sync-policy automated &amp;&amp; \\\nargocd app sync app-of-apps &amp;&amp; \\\nsleep 10 &amp;&amp; \\\nargocd app list\n\n# \u2500\u2500 Workflow 7: Export all application definitions for backup \u2500\u2500\n\nmkdir -p argocd-backup\nargocd app list -o name | while read APP; do\n  argocd app get \"$APP\" -o json &gt; \"argocd-backup/${APP}.json\"\n  echo \"Backed up: ${APP}\"\ndone\nls -la argocd-backup/\n\n# \u2500\u2500 Workflow 8: Multi-environment deployment with different values \u2500\u2500\n\nfor ENV in dev staging prod; do\n  argocd app create \"guestbook-${ENV}\" \\\n    --repo https://github.com/argoproj/argocd-example-apps.git \\\n    --path guestbook \\\n    --dest-server https://kubernetes.default.svc \\\n    --dest-namespace \"guestbook-${ENV}\" \\\n    --sync-policy automated \\\n    --auto-prune \\\n    --self-heal \\\n    --sync-option CreateNamespace=true\n  echo \"Created: guestbook-${ENV}\"\ndone\nargocd app list\n\n# \u2500\u2500 Workflow 9: Full teardown \u2500\u2500\n\nargocd app list -o name | xargs -I {} argocd app delete {} --yes &amp;&amp; \\\nhelm uninstall argocd -n argocd &amp;&amp; \\\nkubectl delete namespace argocd &amp;&amp; \\\necho \"ArgoCD fully removed.\"\n\n# Cleanup multi-env apps\nfor ENV in dev staging prod; do\n  kubectl delete namespace \"guestbook-${ENV}\" 2&gt;/dev/null || true\ndone\nrm -rf argocd-backup/\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#diagram-argocd-gitops-workflow","title":"Diagram: ArgoCD GitOps Workflow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        ArgoCD GitOps Flow                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                     \u2502\n\u2502   Developer \u2500\u2500\u25ba git push \u2500\u2500\u25ba Git Repository (Source of Truth)      \u2502\n\u2502                                    \u2502                                \u2502\n\u2502                        ArgoCD polls every ~3 min                    \u2502\n\u2502                                    \u2502                                \u2502\n\u2502           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502           \u2502            ArgoCD Control Plane                \u2502        \u2502\n\u2502           \u2502                                               \u2502        \u2502\n\u2502           \u2502  API Server \u25c4\u2500\u2500 argocd CLI / Web UI           \u2502        \u2502\n\u2502           \u2502       \u2502                                        \u2502        \u2502\n\u2502           \u2502  App Controller \u2500\u2500\u25ba compare desired vs live   \u2502        \u2502\n\u2502           \u2502       \u2502                                        \u2502        \u2502\n\u2502           \u2502  Repo Server \u2500\u2500\u25ba renders Helm/Kustomize/YAML  \u2502        \u2502\n\u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502                                    \u2502                                \u2502\n\u2502                              sync / heal                            \u2502\n\u2502                                    \u2502                                \u2502\n\u2502           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502           \u2502           Kubernetes Cluster                   \u2502        \u2502\n\u2502           \u2502                                               \u2502        \u2502\n\u2502           \u2502  Namespace: guestbook  \u2500\u2500\u25ba Deployment, Svc    \u2502        \u2502\n\u2502           \u2502  Namespace: efk        \u2500\u2500\u25ba Elasticsearch,\u2026    \u2502        \u2502\n\u2502           \u2502  Namespace: argocd     \u2500\u2500\u25ba App of Apps        \u2502        \u2502\n\u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502                                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"Tasks/Kubernetes-ArgoCD-Tasks/#quick-reference-essential-argocd-cli-commands","title":"Quick Reference: Essential ArgoCD CLI Commands","text":"Command Description <code>argocd login &lt;server&gt;</code> Authenticate to ArgoCD server <code>argocd account update-password</code> Change the admin password <code>argocd cluster list</code> List connected clusters <code>argocd repo add &lt;url&gt;</code> Connect a Git or Helm repository <code>argocd repo list</code> List all connected repositories <code>argocd app create &lt;name&gt;</code> Create a new application <code>argocd app list</code> List all applications and their status <code>argocd app get &lt;name&gt;</code> Get detailed status and resource tree <code>argocd app get &lt;name&gt; --refresh</code> Force-refresh from Git before displaying <code>argocd app sync &lt;name&gt;</code> Manually trigger a sync <code>argocd app sync &lt;name&gt; --dry-run</code> Preview a sync without applying <code>argocd app diff &lt;name&gt;</code> Show diff between Git and live state <code>argocd app set &lt;name&gt; --sync-policy automated</code> Enable automated sync <code>argocd app set &lt;name&gt; --self-heal --auto-prune</code> Enable self-heal and auto-prune <code>argocd app wait &lt;name&gt; --health</code> Wait for application to become Healthy <code>argocd app history &lt;name&gt;</code> Show deployment history <code>argocd app rollback &lt;name&gt; &lt;revision-id&gt;</code> Rollback to a previous revision <code>argocd app manifests &lt;name&gt;</code> Show rendered Kubernetes manifests <code>argocd app delete &lt;name&gt; --yes</code> Delete application (cascades to K8s resources) <code>argocd proj create &lt;name&gt;</code> Create a new project <code>argocd proj list</code> List all projects <code>argocd context</code> List all saved server contexts <code>argocd context &lt;name&gt;</code> Switch to a different ArgoCD server context"},{"location":"Tasks/Kubernetes-CLI-Tasks/","title":"Kubernetes CLI Tasks","text":"<ul> <li>Hands-on Kubernetes exercises covering essential CLI commands, debugging techniques, and advanced orchestration concepts.</li> <li>Each task includes a description and a detailed solution with step-by-step instructions.</li> <li>Practice these tasks to master Kubernetes from basic operations to advanced deployment scenarios.</li> </ul>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#table-of-contents","title":"Table of Contents","text":"<ul> <li>01. Kubernetes Pod Workflow</li> <li>02. Pod Debugging Challenge</li> <li>03. Imperative to Declarative</li> <li>04. Scaling Deployments</li> <li>05. Rolling Updates and Rollbacks</li> <li>06. ConfigMaps and Environment Variables</li> <li>07. Secrets Management</li> <li>08. Persistent Storage with PVCs</li> <li>09. Multi-Container Pods</li> <li>10. Jobs and CronJobs</li> <li>11. Namespaces and Isolation</li> <li>12. Resource Limits and Quotas</li> <li>13. Liveness and Readiness Probes</li> <li>14. Node Selection and Affinity</li> </ul>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#01-kubernetes-pod-workflow","title":"01. Kubernetes Pod Workflow","text":"<p>Start an <code>nginx</code> pod, verify it\u2019s running, execute a command inside it to check the version, and then delete it.</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario","title":"Scenario:","text":"<p>\u25e6 As a developer, you need to quickly verify a container image or run a temporary workload without creating a full deployment.   \u25e6 This workflow allows you to spin up pods, interact with them, and clean them up efficiently.</p> <p>Hint: <code>kubectl run</code>, <code>kubectl get</code>, <code>kubectl exec</code>, <code>kubectl delete</code></p> Solution <pre><code># 1. Run an nginx pod\nkubectl run nginx-pod --image=nginx:alpine\n\n# 2. Verify it is running\nkubectl get pods\n\n# 3. Execute a command inside the pod\nkubectl exec nginx-pod -- nginx -v\n\n# 4. Delete the pod\nkubectl delete pod nginx-pod\n</code></pre>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#02-pod-debugging-challenge","title":"02. Pod Debugging Challenge","text":"<p>Run a pod that is destined to fail (e.g., using a non-existent image), inspect its status, find the error reason, and then fix it (by creating a correct one).</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario_1","title":"Scenario:","text":"<p>\u25e6 Your application pod is stuck in <code>ImagePullBackOff</code> or <code>CrashLoopBackOff</code>.   \u25e6 You need to diagnose the issue using Kubernetes inspection tools to understand why it\u2019s failing.</p> <p>Hint: <code>kubectl run</code>, <code>kubectl get</code>, <code>kubectl describe</code>, <code>kubectl logs</code></p> Solution <pre><code># 1. Run a pod with a wrong image\nkubectl run bad-pod --image=nginx:wrongtag\n\n# 2. Check status (should show ErrImagePull or ImagePullBackOff)\nkubectl get pods\n\n# 3. Describe the pod to see events\nkubectl describe pod bad-pod\n\n# 4. Delete the bad pod\nkubectl delete pod bad-pod\n\n# 5. Run a correct pod\nkubectl run good-pod --image=nginx:alpine\n</code></pre>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#03-imperative-to-declarative","title":"03. Imperative to Declarative","text":"<p>Create a pod using an imperative command, export its configuration to a YAML file, delete the pod, and recreate it using the YAML file.</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario_2","title":"Scenario:","text":"<p>\u25e6 You want to move from ad-hoc CLI commands to Infrastructure as Code (IaC).   \u25e6 Generating YAML from existing resources or dry-runs is a quick way to scaffold your manifests.</p> <p>Hint: <code>kubectl run --dry-run=client -o yaml</code></p> Solution <pre><code># 1. Generate YAML for a pod\nkubectl run my-pod --image=redis:alpine --dry-run=client -o yaml &gt; my-pod.yaml\n\n# 2. Create the pod from YAML\nkubectl apply -f my-pod.yaml\n\n# 3. Verify it exists\nkubectl get pods\n\n# 4. Delete the pod using the file\nkubectl delete -f my-pod.yaml\n</code></pre>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#04-scaling-deployments","title":"04. Scaling Deployments","text":"<p>Create a deployment with 2 replicas, verify them, and then scale it up to 5 replicas.</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario_3","title":"Scenario:","text":"<p>\u25e6 Your application is receiving high traffic and you need to increase capacity.   \u25e6 Kubernetes Deployments make scaling stateless applications trivial.</p> <p>Hint: <code>kubectl create deployment</code>, <code>kubectl scale</code></p> Solution <pre><code># 1. Create a deployment\nkubectl create deployment my-dep --image=nginx:alpine --replicas=2\n\n# 2. Verify replicas\nkubectl get pods\n\n# 3. Scale up\nkubectl scale deployment my-dep --replicas=5\n\n# 4. Verify scaling\nkubectl get pods\n\n# Cleanup\nkubectl delete deployment my-dep\n</code></pre>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#05-rolling-updates-and-rollbacks","title":"05. Rolling Updates and Rollbacks","text":"<p>Update the image of a deployment to a new version, watch the rollout status, and then rollback to the previous version.</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario_4","title":"Scenario:","text":"<p>\u25e6 You deployed a new version of your app, but it has a bug.   \u25e6 You need to quickly revert to the last stable version without downtime.</p> <p>Hint: <code>kubectl set image</code>, <code>kubectl rollout status</code>, <code>kubectl rollout undo</code></p> Solution <pre><code># 1. Create deployment with nginx:1.21\nkubectl create deployment web-app --image=nginx:1.21 --replicas=3\n\n# 2. Update image to nginx:1.22\nkubectl set image deployment/web-app nginx=nginx:1.22\n\n# 3. Watch rollout\nkubectl rollout status deployment/web-app\n\n# 4. Rollback to previous version\nkubectl rollout undo deployment/web-app\n\n# Cleanup\nkubectl delete deployment web-app\n</code></pre>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#06-configmaps-and-environment-variables","title":"06. ConfigMaps and Environment Variables","text":"<p>Create a ConfigMap with some data and inject it into a pod as environment variables.</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario_5","title":"Scenario:","text":"<p>\u25e6 You need to configure your application (e.g., DB host, API URL) without hardcoding values in the image.   \u25e6 ConfigMaps decouple configuration artifacts from image content.</p> <p>Hint: <code>kubectl create configmap</code>, <code>envFrom</code> in YAML</p> Solution <pre><code># 1. Create a ConfigMap\nkubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod\n\n# 2. Create a pod that uses it (using dry-run to generate yaml first is easier)\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: config-pod\nspec:\n  containers:\n  - name: test-container\n    image: busybox\n    command: [ \"sh\", \"-c\", \"env\" ]\n    envFrom:\n    - configMapRef:\n        name: app-config\nEOF\n\n# 3. Check logs to see env vars\nkubectl logs config-pod | grep APP_\n\n# Cleanup\nkubectl delete pod config-pod\nkubectl delete cm app-config\n</code></pre>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#07-secrets-management","title":"07. Secrets Management","text":"<p>Create a Secret and mount it as a volume in a pod.</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario_6","title":"Scenario:","text":"<p>\u25e6 Your application needs sensitive data like passwords or API keys.   \u25e6 Secrets store this data securely and can be mounted as files or env vars.</p> <p>Hint: <code>kubectl create secret</code>, <code>volumeMounts</code></p> Solution <pre><code># 1. Create a generic secret\nkubectl create secret generic my-secret --from-literal=password=s3cr3t\n\n# 2. Create a pod mounting the secret\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox\n    command: [\"sleep\", \"3600\"]\n    volumeMounts:\n    - name: secret-volume\n      mountPath: \"/etc/secret-volume\"\n      readOnly: true\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: my-secret\nEOF\n\n# 3. Verify secret file exists\nkubectl exec secret-pod -- cat /etc/secret-volume/password\n\n# Cleanup\nkubectl delete pod secret-pod\nkubectl delete secret my-secret\n</code></pre>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#08-persistent-storage-with-pvcs","title":"08. Persistent Storage with PVCs","text":"<p>Create a PersistentVolumeClaim (PVC) and mount it to a pod to persist data.</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario_7","title":"Scenario:","text":"<p>\u25e6 You are running a database or stateful app that needs to save data even if the pod restarts.   \u25e6 PVCs request storage from the cluster\u2019s storage provisioner.</p> <p>Hint: <code>PersistentVolumeClaim</code>, <code>volumes</code></p> Solution <pre><code># 1. Create a PVC\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\nEOF\n\n# 2. Create a pod using the PVC\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pvc-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox\n    command: [\"sleep\", \"3600\"]\n    volumeMounts:\n    - mountPath: \"/data\"\n      name: my-storage\n  volumes:\n  - name: my-storage\n    persistentVolumeClaim:\n      claimName: my-pvc\nEOF\n\n# 3. Write data\nkubectl exec pvc-pod -- sh -c \"echo 'Hello Storage' &gt; /data/test.txt\"\n\n# 4. Delete pod and recreate (data should persist - exercise for reader)\nkubectl delete pod pvc-pod\n# Re-apply pod yaml and check file\n\n# Cleanup\nkubectl delete pvc my-pvc\n</code></pre>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#09-multi-container-pods","title":"09. Multi-Container Pods","text":"<p>Create a pod with two containers: a main application and a sidecar helper.</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario_8","title":"Scenario:","text":"<p>\u25e6 You need a helper process (like a log shipper or proxy) to run alongside your main application in the same network namespace.   \u25e6 Multi-container pods share storage and network.</p> <p>Hint: <code>containers</code> array in Pod spec</p> Solution <pre><code># 1. Create multi-container pod\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: multi-container-pod\nspec:\n  containers:\n  - name: main-app\n    image: busybox\n    command: [\"sh\", \"-c\", \"while true; do echo 'Main App' &gt; /shared/index.html; sleep 5; done\"]\n    volumeMounts:\n    - name: shared-data\n      mountPath: /shared\n  - name: sidecar\n    image: busybox\n    command: [\"sh\", \"-c\", \"while true; do cat /shared/index.html; sleep 5; done\"]\n    volumeMounts:\n    - name: shared-data\n      mountPath: /shared\n  volumes:\n  - name: shared-data\n    emptyDir: {}\nEOF\n\n# 2. Check logs of sidecar\nkubectl logs multi-container-pod -c sidecar\n\n# Cleanup\nkubectl delete pod multi-container-pod\n</code></pre>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#10-jobs-and-cronjobs","title":"10. Jobs and CronJobs","text":"<p>Create a Job that runs to completion, and a CronJob that runs every minute.</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario_9","title":"Scenario:","text":"<p>\u25e6 You have a batch process (database migration, report generation) or a periodic task.   \u25e6 Jobs ensure a task finishes successfully; CronJobs schedule them.</p> <p>Hint: <code>kubectl create job</code>, <code>kubectl create cronjob</code></p> Solution <pre><code># 1. Create a Job\nkubectl create job my-job --image=busybox -- echo \"Job Completed\"\n\n# 2. Check job status\nkubectl get jobs\nkubectl logs job/my-job\n\n# 3. Create a CronJob\nkubectl create cronjob my-cron --image=busybox --schedule=\"*/1 * * * *\" -- echo \"Cron Run\"\n\n# 4. Wait for a run and check jobs created by cron\nkubectl get jobs --watch\n\n# Cleanup\nkubectl delete job my-job\nkubectl delete cronjob my-cron\n</code></pre>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#11-namespaces-and-isolation","title":"11. Namespaces and Isolation","text":"<p>Create a new namespace and run a pod inside it.</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario_10","title":"Scenario:","text":"<p>\u25e6 You want to separate development resources from production.   \u25e6 Namespaces provide a scope for names and can be used to divide cluster resources.</p> <p>Hint: <code>kubectl create namespace</code>, <code>kubectl run -n</code></p> Solution <pre><code># 1. Create namespace\nkubectl create ns dev\n\n# 2. Run pod in namespace\nkubectl run dev-pod --image=nginx:alpine -n dev\n\n# 3. Verify it's not in default\nkubectl get pods\nkubectl get pods -n dev\n\n# Cleanup\nkubectl delete ns dev\n</code></pre>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#12-resource-limits-and-quotas","title":"12. Resource Limits and Quotas","text":"<p>Create a pod with CPU and Memory requests and limits.</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario_11","title":"Scenario:","text":"<p>\u25e6 You need to ensure fair resource usage and prevent one container from starving others.   \u25e6 Requests guarantee resources; limits cap them.</p> <p>Hint: <code>resources.requests</code>, <code>resources.limits</code></p> Solution <pre><code># 1. Create pod with limits\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: resource-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:alpine\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\nEOF\n\n# 2. Describe to see limits\nkubectl describe pod resource-pod\n\n# Cleanup\nkubectl delete pod resource-pod\n</code></pre>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#13-liveness-and-readiness-probes","title":"13. Liveness and Readiness Probes","text":"<p>Add a liveness probe to a pod to restart it if it freezes, and a readiness probe to control traffic flow.</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario_12","title":"Scenario:","text":"<p>\u25e6 Your app might deadlock or take time to start up.   \u25e6 Liveness probes restart unhealthy pods; Readiness probes remove them from Service endpoints until ready.</p> <p>Hint: <code>livenessProbe</code>, <code>readinessProbe</code></p> Solution <pre><code># 1. Create pod with probes\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: probe-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:alpine\n    livenessProbe:\n      httpGet:\n        path: /\n        port: 80\n      initialDelaySeconds: 3\n      periodSeconds: 3\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      initialDelaySeconds: 5\n      periodSeconds: 5\nEOF\n\n# 2. Describe to see probe status\nkubectl describe pod probe-pod\n\n# Cleanup\nkubectl delete pod probe-pod\n</code></pre>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#14-node-selection-and-affinity","title":"14. Node Selection and Affinity","text":"<p>Schedule a pod on a specific node using a node selector (requires a node label).</p>"},{"location":"Tasks/Kubernetes-CLI-Tasks/#scenario_13","title":"Scenario:","text":"<p>\u25e6 You have specialized hardware (GPU, SSD) on specific nodes.   \u25e6 You need to ensure your pod lands on the correct node.</p> <p>Hint: <code>kubectl label nodes</code>, <code>nodeSelector</code></p> Solution <pre><code># 1. Label a node (use your node name, e.g., minikube or docker-desktop)\n# Get node name\nNODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')\nkubectl label node $NODE_NAME disk=ssd\n\n# 2. Create pod with nodeSelector\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ssd-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:alpine\n  nodeSelector:\n    disk: ssd\nEOF\n\n# 3. Verify it's running\nkubectl get pod ssd-pod -o wide\n\n# Cleanup\nkubectl delete pod ssd-pod\nkubectl label node $NODE_NAME disk-\n</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/","title":"Kubernetes Harbor + ArgoCD Airgap Tasks","text":"<ul> <li>Hands-on Kubernetes exercises covering Harbor registry installation, Nginx Ingress setup, fully offline/airgap ArgoCD deployment, Helm chart creation, and GitOps application delivery.</li> <li>Each task includes a description, scenario, explanation, and a detailed solution with step-by-step instructions and scripts.</li> <li>Practice these tasks to master end-to-end airgapped GitOps workflows using Harbor as a private container registry and ArgoCD as the deployment engine.</li> </ul>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#table-of-contents","title":"Table of Contents","text":"<ul> <li>01. Install Nginx Ingress Controller + Harbor Registry</li> <li>02. Configure Harbor with Ingress (harbor.local)</li> <li>03. Prepare ArgoCD for Full Offline/Airgap Install</li> <li>04. Create a Git Repository with a Helm Chart</li> <li>05. Deploy ArgoCD (Offline Install Using Harbor)</li> <li>06. Create an ArgoCD Application to Deploy the Helm Chart</li> <li>Full Install Script (All Steps)</li> </ul>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph LR\n    subgraph Airgap GitOps Architecture\n        subgraph Online[\"Online Machine\"]\n            OPS[\"docker pull\\ndocker tag\\ndocker push\"]\n        end\n\n        subgraph Harbor[\"Harbor Registry\\nharbor.local\"]\n            PROJECTS[\"Projects:\\n- argocd\\n- library\\n- helm-charts\"]\n        end\n\n        subgraph K8s[\"Kubernetes Cluster\"]\n            ARGOCD[\"ArgoCD (airgap)\\nAll images from\\nHarbor registry\"]\n            APP[\"Application\\n(Helm Chart)\\nfrom Git repo\"]\n            ARGOCD --&gt; APP\n        end\n\n        subgraph Ingress[\"Nginx Ingress Controller\"]\n            PORT[\"harbor.local:80\"]\n        end\n\n        subgraph Git[\"Git Repository\\n(local/remote)\"]\n            CONTENTS[\"Contains:\\n- Helm chart (my-app/)\\n- ArgoCD Application manifest\"]\n        end\n\n        Online -- \"pull/push\" --&gt; Harbor\n        K8s -- \"pull\" --&gt; Harbor\n        Ingress --- Harbor\n    end</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#01-install-nginx-ingress-controller-harbor-registry","title":"01. Install Nginx Ingress Controller + Harbor Registry","text":"<p>Install the Nginx Ingress Controller and Harbor container registry on a Kubernetes cluster from the internet.</p>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#scenario","title":"Scenario:","text":"<ul> <li>You are setting up a private container registry environment.</li> <li>Harbor will serve as the local OCI registry for all container images and Helm charts.</li> <li>The Nginx Ingress Controller is required to expose Harbor (and later ArgoCD) via hostnames.</li> </ul>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#explanation","title":"Explanation:","text":"<ul> <li>Nginx Ingress Controller acts as a reverse proxy that routes HTTP/HTTPS traffic to services inside the cluster based on hostname and path rules.</li> <li>Harbor is an open-source container registry that supports image management, vulnerability scanning, RBAC, and Helm chart hosting. It is the backbone of an airgap deployment - all images are pre-loaded here.</li> <li>We install both from the internet first, then use Harbor to serve all images for the offline ArgoCD install.</li> </ul> <p>Prerequisites: A running Kubernetes cluster (Kind, Minikube, or cloud-based), <code>helm</code>, <code>kubectl</code>, <code>docker</code> installed.</p> <p>Hint: <code>helm repo add</code>, <code>helm upgrade --install</code>, <code>kubectl get pods</code></p> Solution <pre><code>#!/bin/bash\n# =============================================================================\n# Step 01 - Install Nginx Ingress Controller + Harbor Registry\n# =============================================================================\nset -e\n\n# \u2500\u2500 Color definitions \u2500\u2500\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'\nCYAN='\\033[0;36m'\nNC='\\033[0m'\n\ninfo()    { echo -e \"${BLUE}[INFO]${NC}    $*\"; }\nsuccess() { echo -e \"${GREEN}[OK]${NC}      $*\"; }\nwarn()    { echo -e \"${YELLOW}[WARN]${NC}    $*\"; }\nerror()   { echo -e \"${RED}[ERROR]${NC}   $*\" &gt;&amp;2; exit 1; }\nheader()  { echo -e \"\\n${CYAN}=== $* ===${NC}\"; }\n\n# \u2500\u2500 1. Add Helm repositories \u2500\u2500\nheader \"Adding Helm Repositories\"\n\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo add harbor https://helm.goharbor.io\nhelm repo update\nsuccess \"Helm repositories added and updated\"\n\n# \u2500\u2500 2. Install Nginx Ingress Controller \u2500\u2500\nheader \"Installing Nginx Ingress Controller\"\n\nhelm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \\\n    --namespace ingress-nginx \\\n    --create-namespace \\\n    --set controller.service.type=NodePort \\\n    --set controller.service.nodePorts.http=30080 \\\n    --set controller.service.nodePorts.https=30443 \\\n    --set controller.admissionWebhooks.enabled=false \\\n    --wait --timeout 5m\n\n# Verify Ingress Controller pods\nkubectl get pods -n ingress-nginx\nsuccess \"Nginx Ingress Controller installed\"\n\n# \u2500\u2500 3. Get the Ingress IP/Node IP \u2500\u2500\nheader \"Detecting Cluster Node IP\"\n\nNODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\ninfo \"Node IP: ${NODE_IP}\"\n\n# \u2500\u2500 4. Install Harbor \u2500\u2500\nheader \"Installing Harbor Registry\"\n\nhelm upgrade --install harbor harbor/harbor \\\n    --namespace harbor \\\n    --create-namespace \\\n    --set expose.type=ingress \\\n    --set expose.ingress.className=nginx \\\n    --set expose.ingress.hosts.core=harbor.local \\\n    --set expose.tls.enabled=false \\\n    --set externalURL=http://harbor.local \\\n    --set harborAdminPassword=Harbor12345 \\\n    --set persistence.enabled=false \\\n    --wait --timeout 10m\n\n# Verify Harbor pods\nkubectl get pods -n harbor\nsuccess \"Harbor registry installed\"\n\n# \u2500\u2500 5. Add harbor.local to /etc/hosts \u2500\u2500\nheader \"Configuring /etc/hosts\"\n\nif ! grep -q \"harbor.local\" /etc/hosts; then\n    echo \"${NODE_IP}  harbor.local\" | sudo tee -a /etc/hosts\n    success \"Added harbor.local to /etc/hosts\"\nelse\n    warn \"harbor.local already exists in /etc/hosts\"\nfi\n\n# \u2500\u2500 6. Verify Harbor is accessible \u2500\u2500\nheader \"Verifying Harbor Access\"\n\n# Wait for Ingress to be ready\nsleep 10\n\nHTTP_CODE=$(curl -s -o /dev/null -w \"%{http_code}\" http://harbor.local/api/v2.0/health 2&gt;/dev/null || echo \"000\")\nif [ \"${HTTP_CODE}\" = \"200\" ]; then\n    success \"Harbor is healthy (HTTP ${HTTP_CODE})\"\nelse\n    warn \"Harbor returned HTTP ${HTTP_CODE} - it may still be starting up\"\n    info \"Try: curl http://harbor.local/api/v2.0/health\"\nfi\n\necho \"\"\ninfo \"Harbor UI:       http://harbor.local\"\ninfo \"Harbor Admin:    admin / Harbor12345\"\ninfo \"Ingress Node IP: ${NODE_IP}\"\nsuccess \"Step 01 complete!\"\n</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#key-concepts","title":"Key Concepts:","text":"Component Purpose Nginx Ingress Controller Routes HTTP traffic to services based on hostname/path rules Harbor Private container registry + Helm chart repository NodePort Exposes Ingress on fixed ports (30080/30443) on each node <code>expose.type=ingress</code> Tells Harbor to create Ingress resources for external access <code>persistence.enabled=false</code> Uses emptyDir for lab purposes (data lost on pod restart)"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#02-configure-harbor-with-ingress-harborlocal","title":"02. Configure Harbor with Ingress (harbor.local)","text":"<p>Verify Harbor is accessible via <code>harbor.local</code>, create projects, and configure Docker to trust the insecure registry.</p>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#scenario_1","title":"Scenario:","text":"<ul> <li>Harbor is installed but you need to verify the Ingress route works correctly.</li> <li>You need to create Harbor projects to organize images for the airgap deployment.</li> <li>Docker must be configured to allow pushing/pulling from the insecure (HTTP) registry.</li> </ul>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#explanation_1","title":"Explanation:","text":"<ul> <li>Harbor Projects are logical groupings for container images (similar to Docker Hub organizations).</li> <li>We create an <code>argocd</code> project to hold all ArgoCD-related images and a <code>library</code> project for general images.</li> <li>Since we use HTTP (not HTTPS), Docker needs <code>harbor.local</code> added to its insecure registries list.</li> </ul> <p>Hint: <code>curl</code>, Harbor API, Docker <code>daemon.json</code>, <code>docker login</code></p> Solution <pre><code>#!/bin/bash\n# =============================================================================\n# Step 02 - Configure Harbor with Ingress (harbor.local)\n# =============================================================================\nset -e\n\nRED='\\033[0;31m'; GREEN='\\033[0;32m'; YELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'; CYAN='\\033[0;36m'; NC='\\033[0m'\ninfo()    { echo -e \"${BLUE}[INFO]${NC}    $*\"; }\nsuccess() { echo -e \"${GREEN}[OK]${NC}      $*\"; }\nwarn()    { echo -e \"${YELLOW}[WARN]${NC}    $*\"; }\nerror()   { echo -e \"${RED}[ERROR]${NC}   $*\" &gt;&amp;2; exit 1; }\nheader()  { echo -e \"\\n${CYAN}=== $* ===${NC}\"; }\n\nHARBOR_URL=\"http://harbor.local\"\nHARBOR_USER=\"admin\"\nHARBOR_PASS=\"Harbor12345\"\n\n# \u2500\u2500 1. Verify Harbor health \u2500\u2500\nheader \"Verifying Harbor Health\"\n\nHTTP_CODE=$(curl -s -o /dev/null -w \"%{http_code}\" ${HARBOR_URL}/api/v2.0/health)\nif [ \"${HTTP_CODE}\" != \"200\" ]; then\n    error \"Harbor is not healthy (HTTP ${HTTP_CODE}). Check pods: kubectl get pods -n harbor\"\nfi\nsuccess \"Harbor is healthy\"\n\n# \u2500\u2500 2. Verify Ingress routing \u2500\u2500\nheader \"Verifying Ingress Configuration\"\n\nkubectl get ingress -n harbor\ninfo \"Harbor Ingress rules:\"\nkubectl describe ingress -n harbor | grep -E \"Host|Path|Backend\"\n\n# \u2500\u2500 3. Create Harbor projects via API \u2500\u2500\nheader \"Creating Harbor Projects\"\n\ncreate_project() {\n    local project_name=$1\n    local response\n    response=$(curl -s -o /dev/null -w \"%{http_code}\" \\\n        -X POST \"${HARBOR_URL}/api/v2.0/projects\" \\\n        -H \"Content-Type: application/json\" \\\n        -u \"${HARBOR_USER}:${HARBOR_PASS}\" \\\n        -d \"{\\\"project_name\\\": \\\"${project_name}\\\", \\\"public\\\": true}\")\n\n    if [ \"${response}\" = \"201\" ]; then\n        success \"Created project: ${project_name}\"\n    elif [ \"${response}\" = \"409\" ]; then\n        warn \"Project already exists: ${project_name}\"\n    else\n        error \"Failed to create project ${project_name} (HTTP ${response})\"\n    fi\n}\n\ncreate_project \"argocd\"\ncreate_project \"library\"\n\n# \u2500\u2500 4. Verify projects \u2500\u2500\nheader \"Listing Harbor Projects\"\n\ncurl -s -u \"${HARBOR_USER}:${HARBOR_PASS}\" \\\n    \"${HARBOR_URL}/api/v2.0/projects\" | \\\n    python3 -m json.tool 2&gt;/dev/null | grep -E '\"name\"|\"project_id\"' || \\\ncurl -s -u \"${HARBOR_USER}:${HARBOR_PASS}\" \\\n    \"${HARBOR_URL}/api/v2.0/projects\" | grep -o '\"name\":\"[^\"]*\"'\n\n# \u2500\u2500 5. Configure Docker for insecure registry \u2500\u2500\nheader \"Configuring Docker for Insecure Registry\"\n\nDOCKER_DAEMON=\"/etc/docker/daemon.json\"\n\ninfo \"Docker must trust harbor.local as an insecure registry (HTTP).\"\ninfo \"\"\ninfo \"Add the following to ${DOCKER_DAEMON}:\"\necho \"\"\necho '  {\n  \"insecure-registries\": [\"harbor.local\"]\n}'\necho \"\"\n\n# Attempt to configure automatically (requires sudo)\nif [ -f \"${DOCKER_DAEMON}\" ]; then\n    if grep -q \"harbor.local\" \"${DOCKER_DAEMON}\"; then\n        success \"harbor.local already in insecure-registries\"\n    else\n        warn \"Please add harbor.local to insecure-registries manually\"\n        info \"Then restart Docker: sudo systemctl restart docker\"\n    fi\nelse\n    info \"Creating ${DOCKER_DAEMON} with insecure registry config\"\n    sudo mkdir -p /etc/docker\n    echo '{\"insecure-registries\": [\"harbor.local\"]}' | sudo tee \"${DOCKER_DAEMON}\"\n    sudo systemctl restart docker 2&gt;/dev/null || warn \"Restart Docker manually\"\n    success \"Docker configured\"\nfi\n\n# \u2500\u2500 6. Docker login to Harbor \u2500\u2500\nheader \"Logging in to Harbor\"\n\ndocker login harbor.local \\\n    -u \"${HARBOR_USER}\" \\\n    -p \"${HARBOR_PASS}\" &amp;&amp; \\\n    success \"Docker login successful\" || \\\n    warn \"Docker login failed - ensure Docker is configured for insecure registries\"\n\n# \u2500\u2500 7. Test push/pull \u2500\u2500\nheader \"Testing Push/Pull\"\n\ndocker pull busybox:latest\ndocker tag busybox:latest harbor.local/library/busybox:latest\ndocker push harbor.local/library/busybox:latest &amp;&amp; \\\n    success \"Test push successful!\" || \\\n    warn \"Test push failed - check Docker insecure registry config\"\n\necho \"\"\ninfo \"Harbor URL:      ${HARBOR_URL}\"\ninfo \"Projects:        argocd, library\"\ninfo \"Credentials:     ${HARBOR_USER} / ${HARBOR_PASS}\"\nsuccess \"Step 02 complete!\"\n</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#ingress-verification","title":"Ingress Verification:","text":"<pre><code># Check the Ingress resource created by Harbor\nkubectl get ingress -n harbor -o wide\n\n# Expected output:\n# NAME                    CLASS   HOSTS          ADDRESS        PORTS   AGE\n# harbor-ingress          nginx   harbor.local   10.x.x.x      80      5m\n\n# Test with curl\ncurl http://harbor.local/api/v2.0/systeminfo\n# Returns Harbor version and system information\n</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#03-prepare-argocd-for-full-offlineairgap-install","title":"03. Prepare ArgoCD for Full Offline/Airgap Install","text":"<p>Identify, pull, tag, and push all required ArgoCD container images to the Harbor registry for a fully airgapped installation.</p>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#scenario_2","title":"Scenario:","text":"<ul> <li>Your production cluster has no internet access (airgap environment).</li> <li>All container images must be pre-loaded into the private Harbor registry before deploying ArgoCD.</li> <li>You need to identify every image the ArgoCD Helm chart will use and mirror them to Harbor.</li> </ul>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#explanation_2","title":"Explanation:","text":"<ul> <li>An airgap installation means no external network access - every container image must already exist in a local registry.</li> <li>The ArgoCD Helm chart deploys multiple components (server, controller, repo-server, redis, dex, notifications), each with its own container image.</li> <li>We use <code>helm template</code> to render all manifests and extract image references, then mirror them to Harbor.</li> <li>The ArgoCD Helm chart version and image tags are tightly coupled - always use matching versions.</li> </ul> <p>Hint: <code>helm template</code>, <code>grep image:</code>, <code>docker pull/tag/push</code>, Harbor API</p> Solution"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#complete-list-of-argocd-images-v213","title":"Complete List of ArgoCD Images (v2.13)","text":"<p>The following images are required for a full ArgoCD offline installation:</p> Component Image ArgoCD Server <code>quay.io/argoproj/argocd:v2.13.3</code> ArgoCD Application Controller <code>quay.io/argoproj/argocd:v2.13.3</code> ArgoCD Repo Server <code>quay.io/argoproj/argocd:v2.13.3</code> ArgoCD Notifications <code>quay.io/argoproj/argocd:v2.13.3</code> ArgoCD ApplicationSet <code>quay.io/argoproj/argocd:v2.13.3</code> Redis (HA Cache) <code>redis:7.4.2-alpine</code> Dex (OIDC Provider) <code>ghcr.io/dexidp/dex:v2.41.1</code> <p>Note: ArgoCD uses a single image (<code>quay.io/argoproj/argocd</code>) for multiple components - the entrypoint command differs per component. The exact image tags may vary based on the Helm chart version. Always verify with <code>helm template</code>.</p> <pre><code>#!/bin/bash\n# =============================================================================\n# Step 03 - Prepare ArgoCD for Full Offline/Airgap Install\n# =============================================================================\nset -e\n\nRED='\\033[0;31m'; GREEN='\\033[0;32m'; YELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'; CYAN='\\033[0;36m'; NC='\\033[0m'\ninfo()    { echo -e \"${BLUE}[INFO]${NC}    $*\"; }\nsuccess() { echo -e \"${GREEN}[OK]${NC}      $*\"; }\nwarn()    { echo -e \"${YELLOW}[WARN]${NC}    $*\"; }\nerror()   { echo -e \"${RED}[ERROR]${NC}   $*\" &gt;&amp;2; exit 1; }\nheader()  { echo -e \"\\n${CYAN}=== $* ===${NC}\"; }\n\nHARBOR_URL=\"harbor.local\"\nHARBOR_USER=\"admin\"\nHARBOR_PASS=\"Harbor12345\"\nARGOCD_CHART_VERSION=\"7.7.12\"\n\n# \u2500\u2500 1. Add ArgoCD Helm repository \u2500\u2500\nheader \"Adding ArgoCD Helm Repository\"\n\nhelm repo add argo https://argoproj.github.io/argo-helm\nhelm repo update argo\nsuccess \"ArgoCD Helm repo added\"\n\n# \u2500\u2500 2. Discover all required images using helm template \u2500\u2500\nheader \"Discovering Required Images\"\n\ninfo \"Rendering ArgoCD Helm chart to extract image references...\"\n\nIMAGES=$(helm template argocd argo/argo-cd \\\n    --version \"${ARGOCD_CHART_VERSION}\" \\\n    --namespace argocd 2&gt;/dev/null | \\\n    grep -E \"image:\" | \\\n    sed 's/.*image: *\"\\?\\([^\"]*\\)\"\\?.*/\\1/' | \\\n    sort -u)\n\necho \"\"\ninfo \"Required images for ArgoCD ${ARGOCD_CHART_VERSION}:\"\necho \"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\"\necho \"${IMAGES}\" | while read -r img; do\n    echo \"  ${img}\"\ndone\necho \"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\"\n\n# \u2500\u2500 3. Define the image mapping (source \u2192 Harbor target) \u2500\u2500\nheader \"Building Image Mirror Map\"\n\n# ArgoCD images - all use the same base image with different entrypoints\ndeclare -A IMAGE_MAP\n\nwhile IFS= read -r img; do\n    # Extract the image name and tag\n    # Convert quay.io/argoproj/argocd:v2.13.3 \u2192 harbor.local/argocd/argocd:v2.13.3\n    # Convert redis:7.4.2-alpine \u2192 harbor.local/argocd/redis:7.4.2-alpine\n    # Convert ghcr.io/dexidp/dex:v2.41.1 \u2192 harbor.local/argocd/dex:v2.41.1\n\n    local_name=$(echo \"${img}\" | rev | cut -d'/' -f1 | rev)  # e.g. argocd:v2.13.3\n    IMAGE_MAP[\"${img}\"]=\"${HARBOR_URL}/argocd/${local_name}\"\ndone &lt;&lt;&lt; \"${IMAGES}\"\n\ninfo \"Image mirror mapping:\"\nfor src in \"${!IMAGE_MAP[@]}\"; do\n    echo \"  ${src}\"\n    echo \"    \u2192 ${IMAGE_MAP[$src]}\"\ndone\n\n# \u2500\u2500 4. Pull all images from the internet \u2500\u2500\nheader \"Pulling Images from Internet\"\n\nfor src in \"${!IMAGE_MAP[@]}\"; do\n    info \"Pulling: ${src}\"\n    docker pull \"${src}\" || error \"Failed to pull ${src}\"\n    success \"Pulled: ${src}\"\ndone\n\n# \u2500\u2500 5. Tag images for Harbor \u2500\u2500\nheader \"Tagging Images for Harbor\"\n\nfor src in \"${!IMAGE_MAP[@]}\"; do\n    dst=\"${IMAGE_MAP[$src]}\"\n    info \"Tagging: ${src} \u2192 ${dst}\"\n    docker tag \"${src}\" \"${dst}\"\n    success \"Tagged: ${dst}\"\ndone\n\n# \u2500\u2500 6. Push images to Harbor \u2500\u2500\nheader \"Pushing Images to Harbor\"\n\ndocker login \"${HARBOR_URL}\" -u \"${HARBOR_USER}\" -p \"${HARBOR_PASS}\" || \\\n    error \"Docker login to Harbor failed\"\n\nfor src in \"${!IMAGE_MAP[@]}\"; do\n    dst=\"${IMAGE_MAP[$src]}\"\n    info \"Pushing: ${dst}\"\n    docker push \"${dst}\" || error \"Failed to push ${dst}\"\n    success \"Pushed: ${dst}\"\ndone\n\n# \u2500\u2500 7. Verify images in Harbor \u2500\u2500\nheader \"Verifying Images in Harbor\"\n\ninfo \"Images in 'argocd' project:\"\ncurl -s -u \"${HARBOR_USER}:${HARBOR_PASS}\" \\\n    \"http://${HARBOR_URL}/api/v2.0/projects/argocd/repositories\" | \\\n    python3 -m json.tool 2&gt;/dev/null || \\\ncurl -s -u \"${HARBOR_USER}:${HARBOR_PASS}\" \\\n    \"http://${HARBOR_URL}/api/v2.0/projects/argocd/repositories\"\n\n# \u2500\u2500 8. Save the ArgoCD Helm chart locally for offline use \u2500\u2500\nheader \"Saving ArgoCD Helm Chart for Offline Use\"\n\nmkdir -p /tmp/argocd-airgap\nhelm pull argo/argo-cd \\\n    --version \"${ARGOCD_CHART_VERSION}\" \\\n    --destination /tmp/argocd-airgap/\n\nls -la /tmp/argocd-airgap/\nsuccess \"Helm chart saved to /tmp/argocd-airgap/\"\n\n# \u2500\u2500 9. Push Helm chart to Harbor OCI registry (optional) \u2500\u2500\nheader \"Pushing Helm Chart to Harbor OCI Registry\"\n\nhelm push /tmp/argocd-airgap/argo-cd-${ARGOCD_CHART_VERSION}.tgz \\\n    oci://${HARBOR_URL}/argocd 2&gt;/dev/null &amp;&amp; \\\n    success \"Helm chart pushed to Harbor OCI\" || \\\n    warn \"OCI push skipped (Harbor may need OCI enabled or use chartmuseum)\"\n\necho \"\"\necho \"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\"\ninfo \"Summary of mirrored images:\"\necho \"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\"\nfor src in \"${!IMAGE_MAP[@]}\"; do\n    echo \"  ${IMAGE_MAP[$src]}\"\ndone\necho \"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\"\necho \"\"\nsuccess \"Step 03 complete! All ArgoCD images are in Harbor.\"\n</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#quick-reference-image-discovery-commands","title":"Quick Reference: Image Discovery Commands","text":"<pre><code># Method 1: helm template (recommended - shows exact images)\nhelm template argocd argo/argo-cd --version 7.7.12 | grep \"image:\" | sort -u\n\n# Method 2: helm show values (shows configurable image fields)\nhelm show values argo/argo-cd --version 7.7.12 | grep -A2 \"repository:\"\n\n# Method 3: After install - check running pods\nkubectl get pods -n argocd -o jsonpath='{range .items[*]}{range .spec.containers[*]}{.image}{\"\\n\"}{end}{end}' | sort -u\n\n# Verify images in Harbor via API\ncurl -s -u admin:Harbor12345 http://harbor.local/api/v2.0/projects/argocd/repositories | python3 -m json.tool\n</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#04-create-a-git-repository-with-a-helm-chart","title":"04. Create a Git Repository with a Helm Chart","text":"<p>Create a local Git repository containing a sample Helm chart that ArgoCD will deploy.</p>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#scenario_3","title":"Scenario:","text":"<ul> <li>ArgoCD follows the GitOps model - the Git repository is the single source of truth.</li> <li>You need a Helm chart in a Git repository that ArgoCD can monitor and deploy.</li> <li>The chart deploys a simple nginx-based web application with configurable replicas and a custom welcome page.</li> </ul>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#explanation_3","title":"Explanation:","text":"<ul> <li>GitOps means the desired state of the cluster is declared in Git. ArgoCD watches the repo and syncs changes automatically.</li> <li>The Helm chart contains templates for a Deployment, Service, and ConfigMap.</li> <li>We use a local bare Git repository for the lab (simulating a remote Git server). In production, this would be GitHub, GitLab, or Gitea.</li> <li>ArgoCD can detect Helm charts automatically by the presence of <code>Chart.yaml</code>.</li> </ul> <p>Hint: <code>git init --bare</code>, <code>helm create</code>, <code>git push</code></p> Solution <pre><code>#!/bin/bash\n# =============================================================================\n# Step 04 - Create a Git Repository with a Helm Chart\n# =============================================================================\nset -e\n\nRED='\\033[0;31m'; GREEN='\\033[0;32m'; YELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'; CYAN='\\033[0;36m'; NC='\\033[0m'\ninfo()    { echo -e \"${BLUE}[INFO]${NC}    $*\"; }\nsuccess() { echo -e \"${GREEN}[OK]${NC}      $*\"; }\nwarn()    { echo -e \"${YELLOW}[WARN]${NC}    $*\"; }\nerror()   { echo -e \"${RED}[ERROR]${NC}   $*\" &gt;&amp;2; exit 1; }\nheader()  { echo -e \"\\n${CYAN}=== $* ===${NC}\"; }\n\nREPO_BASE=\"/tmp/gitops-lab\"\nBARE_REPO=\"${REPO_BASE}/helm-apps.git\"\nWORK_DIR=\"${REPO_BASE}/helm-apps-workspace\"\nCHART_NAME=\"my-web-app\"\n\n# \u2500\u2500 1. Create a bare Git repository (simulates remote server) \u2500\u2500\nheader \"Creating Bare Git Repository\"\n\nrm -rf \"${REPO_BASE}\"\nmkdir -p \"${REPO_BASE}\"\n\ngit init --bare \"${BARE_REPO}\"\nsuccess \"Bare repo created at ${BARE_REPO}\"\n\n# \u2500\u2500 2. Clone the bare repo into a working directory \u2500\u2500\nheader \"Cloning Working Directory\"\n\ngit clone \"${BARE_REPO}\" \"${WORK_DIR}\"\ncd \"${WORK_DIR}\"\nsuccess \"Working directory: ${WORK_DIR}\"\n\n# \u2500\u2500 3. Scaffold the Helm chart \u2500\u2500\nheader \"Creating Helm Chart: ${CHART_NAME}\"\n\nhelm create \"${CHART_NAME}\"\n\n# \u2500\u2500 4. Customize Chart.yaml \u2500\u2500\ncat &gt; \"${CHART_NAME}/Chart.yaml\" &lt;&lt; 'EOF'\napiVersion: v2\nname: my-web-app\ndescription: A simple web application deployed via ArgoCD GitOps\ntype: application\nversion: 0.1.0\nappVersion: \"1.25.0\"\nmaintainers:\n  - name: platform-team\n    email: platform@example.com\nEOF\nsuccess \"Chart.yaml customized\"\n\n# \u2500\u2500 5. Customize values.yaml \u2500\u2500\ncat &gt; \"${CHART_NAME}/values.yaml\" &lt;&lt; 'EOF'\nreplicaCount: 2\n\nimage:\n  repository: nginx\n  pullPolicy: IfNotPresent\n  tag: \"1.25-alpine\"\n\nimagePullSecrets: []\nnameOverride: \"\"\nfullnameOverride: \"\"\n\nserviceAccount:\n  create: true\n  automount: true\n  annotations: {}\n  name: \"\"\n\npodAnnotations: {}\npodLabels: {}\npodSecurityContext: {}\nsecurityContext: {}\n\nservice:\n  type: ClusterIP\n  port: 80\n\ningress:\n  enabled: false\n\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 50m\n    memory: 64Mi\n\nautoscaling:\n  enabled: false\n\nvolumes: []\nvolumeMounts: []\nnodeSelector: {}\ntolerations: []\naffinity: {}\n\n# Custom welcome page\nwelcomePage:\n  title: \"GitOps Demo App\"\n  message: \"Deployed by ArgoCD from Harbor airgap registry!\"\n  backgroundColor: \"#1a1a2e\"\n  textColor: \"#e94560\"\nEOF\nsuccess \"values.yaml customized\"\n\n# \u2500\u2500 6. Create a custom ConfigMap template for the welcome page \u2500\u2500\ncat &gt; \"${CHART_NAME}/templates/configmap.yaml\" &lt;&lt; 'TEMPLATE'\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"my-web-app.fullname\" . }}-html\n  labels:\n    {{- include \"my-web-app.labels\" . | nindent 4 }}\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n      &lt;title&gt;{{ .Values.welcomePage.title }}&lt;/title&gt;\n      &lt;style&gt;\n        body {\n          font-family: 'Segoe UI', Arial, sans-serif;\n          display: flex;\n          justify-content: center;\n          align-items: center;\n          min-height: 100vh;\n          margin: 0;\n          background: {{ .Values.welcomePage.backgroundColor }};\n          color: {{ .Values.welcomePage.textColor }};\n        }\n        .container { text-align: center; }\n        h1 { font-size: 2.5em; margin-bottom: 0.5em; }\n        .info { font-size: 1.2em; margin: 8px 0; color: #eee; }\n        .badge {\n          display: inline-block;\n          background: {{ .Values.welcomePage.textColor }};\n          color: white;\n          padding: 5px 15px;\n          border-radius: 20px;\n          margin: 5px;\n          font-size: 0.9em;\n        }\n      &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n      &lt;div class=\"container\"&gt;\n        &lt;h1&gt;{{ .Values.welcomePage.title }}&lt;/h1&gt;\n        &lt;p class=\"info\"&gt;{{ .Values.welcomePage.message }}&lt;/p&gt;\n        &lt;p class=\"info\"&gt;\n          &lt;span class=\"badge\"&gt;Release: {{ .Release.Name }}&lt;/span&gt;\n          &lt;span class=\"badge\"&gt;Namespace: {{ .Release.Namespace }}&lt;/span&gt;\n        &lt;/p&gt;\n        &lt;p class=\"info\"&gt;\n          &lt;span class=\"badge\"&gt;Chart: {{ .Chart.Name }}-{{ .Chart.Version }}&lt;/span&gt;\n          &lt;span class=\"badge\"&gt;App: {{ .Chart.AppVersion }}&lt;/span&gt;\n        &lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\nTEMPLATE\nsuccess \"ConfigMap template created\"\n\n# \u2500\u2500 7. Update the Deployment to mount the ConfigMap \u2500\u2500\ncat &gt; \"${CHART_NAME}/templates/deployment.yaml\" &lt;&lt; 'TEMPLATE'\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"my-web-app.fullname\" . }}\n  labels:\n    {{- include \"my-web-app.labels\" . | nindent 4 }}\nspec:\n  {{- if not .Values.autoscaling.enabled }}\n  replicas: {{ .Values.replicaCount }}\n  {{- end }}\n  selector:\n    matchLabels:\n      {{- include \"my-web-app.selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      annotations:\n        checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}\n      {{- with .Values.podAnnotations }}\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      labels:\n        {{- include \"my-web-app.labels\" . | nindent 8 }}\n        {{- with .Values.podLabels }}\n        {{- toYaml . | nindent 8 }}\n        {{- end }}\n    spec:\n      {{- with .Values.imagePullSecrets }}\n      imagePullSecrets:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      serviceAccountName: {{ include \"my-web-app.serviceAccountName\" . }}\n      {{- with .Values.podSecurityContext }}\n      securityContext:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      containers:\n        - name: {{ .Chart.Name }}\n          {{- with .Values.securityContext }}\n          securityContext:\n            {{- toYaml . | nindent 12 }}\n          {{- end }}\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n          imagePullPolicy: {{ .Values.image.pullPolicy }}\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          {{- with .Values.resources }}\n          resources:\n            {{- toYaml . | nindent 12 }}\n          {{- end }}\n          volumeMounts:\n            - name: html\n              mountPath: /usr/share/nginx/html\n              readOnly: true\n            {{- with .Values.volumeMounts }}\n            {{- toYaml . | nindent 12 }}\n            {{- end }}\n      volumes:\n        - name: html\n          configMap:\n            name: {{ include \"my-web-app.fullname\" . }}-html\n        {{- with .Values.volumes }}\n        {{- toYaml . | nindent 8 }}\n        {{- end }}\n      {{- with .Values.nodeSelector }}\n      nodeSelector:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.affinity }}\n      affinity:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.tolerations }}\n      tolerations:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\nTEMPLATE\nsuccess \"Deployment template updated with ConfigMap volume mount\"\n\n# \u2500\u2500 8. Validate the chart \u2500\u2500\nheader \"Validating Helm Chart\"\n\nhelm lint \"${CHART_NAME}/\"\nhelm template test-release \"${CHART_NAME}/\" &gt; /dev/null\nsuccess \"Chart passes lint and template rendering\"\n\n# \u2500\u2500 9. Commit and push to the bare repository \u2500\u2500\nheader \"Committing to Git Repository\"\n\ngit add .\ngit commit -m \"Add ${CHART_NAME} Helm chart for GitOps deployment\"\ngit push origin master 2&gt;/dev/null || git push origin main\n\nsuccess \"Chart pushed to Git repository\"\n\n# \u2500\u2500 10. Verify the repository contents \u2500\u2500\nheader \"Repository Contents\"\n\necho \"\"\nfind \"${CHART_NAME}\" -type f | sort | while read -r f; do\n    echo \"  ${f}\"\ndone\necho \"\"\n\ninfo \"Bare repo:     ${BARE_REPO}\"\ninfo \"Working dir:   ${WORK_DIR}\"\ninfo \"Chart path:    ${CHART_NAME}/\"\nsuccess \"Step 04 complete!\"\n</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#repository-structure","title":"Repository Structure:","text":"<pre><code>helm-apps/\n\u2514\u2500\u2500 my-web-app/\n    \u251c\u2500\u2500 Chart.yaml              # Chart metadata\n    \u251c\u2500\u2500 values.yaml             # Default values\n    \u251c\u2500\u2500 charts/                 # Dependencies (empty)\n    \u251c\u2500\u2500 templates/\n    \u2502   \u251c\u2500\u2500 _helpers.tpl        # Named templates\n    \u2502   \u251c\u2500\u2500 configmap.yaml      # Custom HTML welcome page\n    \u2502   \u251c\u2500\u2500 deployment.yaml     # Deployment with ConfigMap mount\n    \u2502   \u251c\u2500\u2500 service.yaml        # ClusterIP service\n    \u2502   \u251c\u2500\u2500 serviceaccount.yaml # Service account\n    \u2502   \u251c\u2500\u2500 hpa.yaml            # HPA (disabled by default)\n    \u2502   \u251c\u2500\u2500 NOTES.txt           # Post-install notes\n    \u2502   \u2514\u2500\u2500 tests/\n    \u2502       \u2514\u2500\u2500 test-connection.yaml\n    \u2514\u2500\u2500 .helmignore\n</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#05-deploy-argocd-offline-install-using-harbor","title":"05. Deploy ArgoCD (Offline Install Using Harbor)","text":"<p>Deploy ArgoCD using only images from the Harbor registry - a fully airgapped installation.</p>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#scenario_4","title":"Scenario:","text":"<ul> <li>The cluster has no internet access (simulated by overriding all image references).</li> <li>All ArgoCD images are served from <code>harbor.local/argocd/</code>.</li> <li>The Helm chart is installed from the locally saved <code>.tgz</code> file (not fetched from the internet).</li> <li>ArgoCD is exposed via Ingress on <code>argocd.local</code>.</li> </ul>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#explanation_4","title":"Explanation:","text":"<ul> <li>The Helm <code>--set</code> flags override every image reference to point at Harbor instead of the public registries (quay.io, ghcr.io, docker.io).</li> <li><code>global.image.repository</code> overrides the main ArgoCD image for all components.</li> <li>Individual overrides are needed for Redis and Dex since they use different base images.</li> <li>The <code>--set server.insecure=true</code> flag disables TLS on the ArgoCD server (TLS is terminated at the Ingress level).</li> </ul> <p>Hint: <code>helm install</code>, <code>--set global.image.repository</code>, <code>--set redis.image.repository</code></p> Solution <pre><code>#!/bin/bash\n# =============================================================================\n# Step 05 - Deploy ArgoCD (Offline Install Using Harbor)\n# =============================================================================\nset -e\n\nRED='\\033[0;31m'; GREEN='\\033[0;32m'; YELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'; CYAN='\\033[0;36m'; NC='\\033[0m'\ninfo()    { echo -e \"${BLUE}[INFO]${NC}    $*\"; }\nsuccess() { echo -e \"${GREEN}[OK]${NC}      $*\"; }\nwarn()    { echo -e \"${YELLOW}[WARN]${NC}    $*\"; }\nerror()   { echo -e \"${RED}[ERROR]${NC}   $*\" &gt;&amp;2; exit 1; }\nheader()  { echo -e \"\\n${CYAN}=== $* ===${NC}\"; }\n\nHARBOR_URL=\"harbor.local\"\nARGOCD_CHART=\"/tmp/argocd-airgap/argo-cd-7.7.12.tgz\"\nNODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\n\n# \u2500\u2500 1. Verify the local Helm chart exists \u2500\u2500\nheader \"Verifying Local ArgoCD Helm Chart\"\n\nif [ ! -f \"${ARGOCD_CHART}\" ]; then\n    warn \"Local chart not found at ${ARGOCD_CHART}\"\n    info \"Falling back to Helm repository (ensure argo repo is added)\"\n    ARGOCD_CHART=\"argo/argo-cd\"\n    CHART_VERSION_FLAG=\"--version 7.7.12\"\nelse\n    CHART_VERSION_FLAG=\"\"\n    success \"Found local chart: ${ARGOCD_CHART}\"\nfi\n\n# \u2500\u2500 2. Create ArgoCD values file for offline install \u2500\u2500\nheader \"Creating Airgap Values File\"\n\ncat &gt; /tmp/argocd-airgap-values.yaml &lt;&lt; EOF\n# =============================================================================\n# ArgoCD Airgap Values - All images from Harbor (${HARBOR_URL})\n# =============================================================================\n\nglobal:\n  image:\n    repository: ${HARBOR_URL}/argocd/argocd\n    tag: \"v2.13.3\"\n\nserver:\n  insecure: true\n  ingress:\n    enabled: true\n    ingressClassName: nginx\n    hostname: argocd.local\n    annotations:\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTP\"\n\nredis:\n  image:\n    repository: ${HARBOR_URL}/argocd/redis\n    tag: \"7.4.2-alpine\"\n\ndex:\n  image:\n    repository: ${HARBOR_URL}/argocd/dex\n    tag: \"v2.41.1\"\nEOF\n\ninfo \"Values file created at /tmp/argocd-airgap-values.yaml\"\necho \"\"\ncat /tmp/argocd-airgap-values.yaml\necho \"\"\n\n# \u2500\u2500 3. Install ArgoCD with airgap values \u2500\u2500\nheader \"Installing ArgoCD (Offline / Airgap Mode)\"\n\nhelm upgrade --install argocd ${ARGOCD_CHART} \\\n    ${CHART_VERSION_FLAG} \\\n    --namespace argocd \\\n    --create-namespace \\\n    -f /tmp/argocd-airgap-values.yaml \\\n    --wait --timeout 10m\n\nsuccess \"ArgoCD installed in airgap mode\"\n\n# \u2500\u2500 4. Verify all pods are running and using Harbor images \u2500\u2500\nheader \"Verifying ArgoCD Pods\"\n\nkubectl get pods -n argocd\n\necho \"\"\ninfo \"Container images in use:\"\nkubectl get pods -n argocd -o jsonpath='{range .items[*]}{range .spec.containers[*]}{.image}{\"\\n\"}{end}{end}' | sort -u\n\n# Verify all images come from Harbor\nNON_HARBOR=$(kubectl get pods -n argocd \\\n    -o jsonpath='{range .items[*]}{range .spec.containers[*]}{.image}{\"\\n\"}{end}{end}' | \\\n    grep -v \"${HARBOR_URL}\" || true)\n\nif [ -z \"${NON_HARBOR}\" ]; then\n    success \"All images are served from Harbor (${HARBOR_URL})\"\nelse\n    warn \"Some images are NOT from Harbor:\"\n    echo \"${NON_HARBOR}\"\nfi\n\n# \u2500\u2500 5. Configure argocd.local in /etc/hosts \u2500\u2500\nheader \"Configuring argocd.local\"\n\nif ! grep -q \"argocd.local\" /etc/hosts; then\n    echo \"${NODE_IP}  argocd.local\" | sudo tee -a /etc/hosts\n    success \"Added argocd.local to /etc/hosts\"\nelse\n    warn \"argocd.local already in /etc/hosts\"\nfi\n\n# \u2500\u2500 6. Verify Ingress \u2500\u2500\nheader \"Verifying ArgoCD Ingress\"\n\nkubectl get ingress -n argocd\n\nsleep 5\nHTTP_CODE=$(curl -s -o /dev/null -w \"%{http_code}\" http://argocd.local 2&gt;/dev/null || echo \"000\")\ninfo \"ArgoCD UI HTTP response: ${HTTP_CODE}\"\n\n# \u2500\u2500 7. Retrieve admin password \u2500\u2500\nheader \"ArgoCD Admin Credentials\"\n\nARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret \\\n    -o jsonpath=\"{.data.password}\" | base64 -d)\n\necho \"\"\ninfo \"ArgoCD URL:      http://argocd.local\"\ninfo \"Username:        admin\"\ninfo \"Password:        ${ARGOCD_PASSWORD}\"\n\n# \u2500\u2500 8. Login with ArgoCD CLI (if installed) \u2500\u2500\nheader \"ArgoCD CLI Login\"\n\nif command -v argocd &amp;&gt; /dev/null; then\n    argocd login argocd.local \\\n        --username admin \\\n        --password \"${ARGOCD_PASSWORD}\" \\\n        --insecure &amp;&amp; \\\n        success \"ArgoCD CLI login successful\" || \\\n        warn \"CLI login failed - try: argocd login argocd.local --insecure\"\nelse\n    info \"ArgoCD CLI not installed. Install with:\"\n    info \"  brew install argocd (macOS)\"\n    info \"  curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\"\nfi\n\necho \"\"\nsuccess \"Step 05 complete! ArgoCD is running in full airgap mode.\"\n</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#verification-commands","title":"Verification Commands:","text":"<pre><code># Check all pods are Running\nkubectl get pods -n argocd -o wide\n\n# Confirm images come from Harbor\nkubectl get pods -n argocd -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{range .spec.containers[*]}{.image}{\"\\n\"}{end}{end}'\n\n# Expected output (all images from harbor.local):\n# argocd-application-controller-0       harbor.local/argocd/argocd:v2.13.3\n# argocd-dex-server-xxxx                harbor.local/argocd/dex:v2.41.1\n# argocd-redis-xxxx                     harbor.local/argocd/redis:7.4.2-alpine\n# argocd-repo-server-xxxx               harbor.local/argocd/argocd:v2.13.3\n# argocd-server-xxxx                    harbor.local/argocd/argocd:v2.13.3\n\n# Test the UI\ncurl -s -o /dev/null -w \"%{http_code}\" http://argocd.local\n# Expected: 200\n</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#06-create-an-argocd-application-to-deploy-the-helm-chart","title":"06. Create an ArgoCD Application to Deploy the Helm Chart","text":"<p>Create an ArgoCD Application manifest that points to the Git repository and deploys the Helm chart with automated sync.</p>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#scenario_5","title":"Scenario:","text":"<ul> <li>The Git repository (from Step 04) contains a Helm chart.</li> <li>ArgoCD should watch this repository, render the Helm chart, and deploy it to the cluster.</li> <li>Auto-sync with self-heal ensures the cluster always matches the Git state.</li> <li>Any change pushed to Git is automatically deployed.</li> </ul>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#explanation_5","title":"Explanation:","text":"<ul> <li>An ArgoCD Application is a Custom Resource (CR) that defines: which Git repo to watch, which path contains the manifests, and where to deploy them.</li> <li>Setting <code>syncPolicy.automated</code> enables auto-sync - ArgoCD polls Git and applies changes without manual intervention.</li> <li><code>selfHeal: true</code> reverts any manual cluster changes back to the Git-defined state.</li> <li><code>prune: true</code> deletes resources removed from Git.</li> <li>For Helm charts, ArgoCD auto-detects <code>Chart.yaml</code> and uses <code>helm template</code> to render manifests.</li> </ul> <p>Hint: <code>argocd app create</code>, <code>kubectl apply -f application.yaml</code>, <code>argocd app sync</code></p> Solution <pre><code>#!/bin/bash\n# =============================================================================\n# Step 06 - Create an ArgoCD Application to Deploy the Helm Chart\n# =============================================================================\nset -e\n\nRED='\\033[0;31m'; GREEN='\\033[0;32m'; YELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'; CYAN='\\033[0;36m'; NC='\\033[0m'\ninfo()    { echo -e \"${BLUE}[INFO]${NC}    $*\"; }\nsuccess() { echo -e \"${GREEN}[OK]${NC}      $*\"; }\nwarn()    { echo -e \"${YELLOW}[WARN]${NC}    $*\"; }\nerror()   { echo -e \"${RED}[ERROR]${NC}   $*\" &gt;&amp;2; exit 1; }\nheader()  { echo -e \"\\n${CYAN}=== $* ===${NC}\"; }\n\nBARE_REPO=\"/tmp/gitops-lab/helm-apps.git\"\nAPP_NAME=\"my-web-app\"\nAPP_NAMESPACE=\"my-web-app\"\n\n# \u2500\u2500 1. Register the Git repository with ArgoCD \u2500\u2500\nheader \"Registering Git Repository with ArgoCD\"\n\n# For a local bare repo, ArgoCD needs the path to be accessible from inside the cluster.\n# Option A: Use a Git server (Gitea, GitLab)\n# Option B: Mount the bare repo as a volume (for local testing)\n# Option C: Use argocd-repo-server to serve local repos\n\n# For this lab, we'll use the repo-server to access local repos\n# by copying the bare repo to a PVC or using a ConfigMap.\n# Simplest approach: patch the repo-server to mount the host path.\n\ninfo \"For production: use a Git server (GitHub, GitLab, Gitea).\"\ninfo \"For this lab: we configure ArgoCD to use a local repo path.\"\n\n# Create the Application manifest\nheader \"Creating ArgoCD Application Manifest\"\n\ncat &gt; /tmp/argocd-app-my-web-app.yaml &lt;&lt; 'EOF'\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-web-app\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n\n  source:\n    # \u2500\u2500 Replace with your actual Git repository URL \u2500\u2500\n    # For GitHub/GitLab:\n    #   repoURL: https://github.com/&lt;your-org&gt;/helm-apps.git\n    # For local Gitea:\n    #   repoURL: http://gitea.local:3000/&lt;user&gt;/helm-apps.git\n    repoURL: https://github.com/&lt;your-org&gt;/helm-apps.git\n    targetRevision: HEAD\n    path: my-web-app\n\n    # Helm-specific configuration\n    helm:\n      # Override values for this specific deployment\n      valuesObject:\n        replicaCount: 3\n        welcomePage:\n          title: \"Airgap GitOps App\"\n          message: \"Deployed by ArgoCD from Harbor registry!\"\n          backgroundColor: \"#0f3460\"\n          textColor: \"#e94560\"\n\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: my-web-app\n\n  syncPolicy:\n    automated:\n      prune: true       # Delete resources removed from Git\n      selfHeal: true    # Revert manual cluster changes\n    syncOptions:\n      - CreateNamespace=true\n      - ApplyOutOfSyncOnly=true\n    retry:\n      limit: 5\n      backoff:\n        duration: 5s\n        factor: 2\n        maxDuration: 3m\nEOF\n\nsuccess \"Application manifest created\"\necho \"\"\ncat /tmp/argocd-app-my-web-app.yaml\necho \"\"\n\n# \u2500\u2500 2. Apply the Application manifest \u2500\u2500\nheader \"Deploying ArgoCD Application\"\n\nkubectl apply -f /tmp/argocd-app-my-web-app.yaml\nsuccess \"Application created in ArgoCD\"\n\n# \u2500\u2500 3. Wait for the application to sync \u2500\u2500\nheader \"Waiting for Application Sync\"\n\ninfo \"Waiting for ArgoCD to sync the application...\"\n\nif command -v argocd &amp;&gt; /dev/null; then\n    argocd app wait \"${APP_NAME}\" --health --sync --timeout 120 &amp;&amp; \\\n        success \"Application is Synced and Healthy\" || \\\n        warn \"Sync is still in progress - check the ArgoCD UI\"\nelse\n    # Wait using kubectl\n    for i in $(seq 1 30); do\n        HEALTH=$(kubectl get application \"${APP_NAME}\" -n argocd \\\n            -o jsonpath='{.status.health.status}' 2&gt;/dev/null || echo \"Unknown\")\n        SYNC=$(kubectl get application \"${APP_NAME}\" -n argocd \\\n            -o jsonpath='{.status.sync.status}' 2&gt;/dev/null || echo \"Unknown\")\n\n        info \"Attempt ${i}/30 - Health: ${HEALTH}, Sync: ${SYNC}\"\n\n        if [ \"${HEALTH}\" = \"Healthy\" ] &amp;&amp; [ \"${SYNC}\" = \"Synced\" ]; then\n            success \"Application is Synced and Healthy!\"\n            break\n        fi\n        sleep 5\n    done\nfi\n\n# \u2500\u2500 4. Verify the deployed resources \u2500\u2500\nheader \"Verifying Deployed Resources\"\n\nkubectl get all -n \"${APP_NAMESPACE}\"\n\necho \"\"\ninfo \"Pods:\"\nkubectl get pods -n \"${APP_NAMESPACE}\" -o wide\n\necho \"\"\ninfo \"Services:\"\nkubectl get svc -n \"${APP_NAMESPACE}\"\n\n# \u2500\u2500 5. Verify via ArgoCD \u2500\u2500\nheader \"ArgoCD Application Status\"\n\nif command -v argocd &amp;&gt; /dev/null; then\n    argocd app get \"${APP_NAME}\"\nelse\n    kubectl get application \"${APP_NAME}\" -n argocd \\\n        -o jsonpath='{.status.sync.status}' &amp;&amp; echo \"\"\n    kubectl get application \"${APP_NAME}\" -n argocd \\\n        -o jsonpath='{.status.health.status}' &amp;&amp; echo \"\"\nfi\n\n# \u2500\u2500 6. Access the application \u2500\u2500\nheader \"Accessing the Application\"\n\ninfo \"Port-forward to access the app:\"\ninfo \"  kubectl port-forward svc/${APP_NAME} -n ${APP_NAMESPACE} 8081:80\"\ninfo \"  open http://localhost:8081\"\necho \"\"\ninfo \"Or create an Ingress for http://my-web-app.local\"\n\n# \u2500\u2500 7. Test GitOps - push a change and watch auto-sync \u2500\u2500\nheader \"Testing GitOps Workflow\"\n\ninfo \"To test auto-sync, modify the Helm chart in Git:\"\ninfo \"\"\ninfo \"  cd /tmp/gitops-lab/helm-apps-workspace\"\ninfo \"  # Edit my-web-app/values.yaml (change replicaCount to 5)\"\ninfo \"  git add . &amp;&amp; git commit -m 'Scale to 5 replicas' &amp;&amp; git push\"\ninfo \"\"\ninfo \"ArgoCD will detect the change and automatically sync within ~3 minutes.\"\ninfo \"Or trigger manually: argocd app sync ${APP_NAME}\"\n\necho \"\"\nsuccess \"Step 06 complete! GitOps pipeline is fully operational.\"\n</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#alternative-create-the-application-via-cli","title":"Alternative: Create the Application via CLI","text":"<pre><code># Using the ArgoCD CLI instead of a manifest file\nargocd app create my-web-app \\\n    --repo https://github.com/&lt;your-org&gt;/helm-apps.git \\\n    --path my-web-app \\\n    --dest-server https://kubernetes.default.svc \\\n    --dest-namespace my-web-app \\\n    --sync-policy automated \\\n    --auto-prune \\\n    --self-heal \\\n    --sync-option CreateNamespace=true \\\n    --helm-set replicaCount=3 \\\n    --helm-set welcomePage.title=\"Airgap GitOps App\"\n\n# Sync and wait\nargocd app sync my-web-app\nargocd app wait my-web-app --health --timeout 120\n\n# Verify\nargocd app get my-web-app\nkubectl get all -n my-web-app\n</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#application-lifecycle-diagram","title":"Application Lifecycle Diagram:","text":"<pre><code>graph LR\n    subgraph Developer\n        DEV[git push]\n    end\n    subgraph Git_Repo [Git Repo]\n        REPO[helm-apps/\\nmy-web-app/]\n    end\n    subgraph ArgoCD [ArgoCD airgap mode]\n        ARGO[Detects change\\nRenders Helm\\nApplies to K8s]\n    end\n    subgraph Kubernetes\n        K8S[Namespace: my-web-app\\nDeployment (3)\\nService (CIP)\\nConfigMap (HTML)\\nServiceAccount]\n    end\n\n    DEV --&gt; REPO\n    ARGO -- poll ~3min --&gt; REPO\n    ARGO --&gt; K8S</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#full-install-script-all-steps","title":"Full Install Script (All Steps)","text":"<p>A single script that runs all six steps end-to-end. Each step is modular and can be run independently or as part of this combined installer.</p> Solution <pre><code>#!/bin/bash\n# =============================================================================\n#  Harbor + ArgoCD Airgap Full Installer\n#  Runs all 6 steps: Ingress, Harbor, Image Mirror, Git Repo, ArgoCD, App\n# =============================================================================\nset -e\n\n# \u2500\u2500 Configuration \u2500\u2500\nHARBOR_URL=\"harbor.local\"\nHARBOR_USER=\"admin\"\nHARBOR_PASS=\"Harbor12345\"\nARGOCD_CHART_VERSION=\"7.7.12\"\nREPO_BASE=\"/tmp/gitops-lab\"\nBARE_REPO=\"${REPO_BASE}/helm-apps.git\"\nWORK_DIR=\"${REPO_BASE}/helm-apps-workspace\"\nCHART_NAME=\"my-web-app\"\n\n# \u2500\u2500 Color definitions \u2500\u2500\nRED='\\033[0;31m'; GREEN='\\033[0;32m'; YELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'; CYAN='\\033[0;36m'; BOLD='\\033[1m'; NC='\\033[0m'\n\ninfo()    { echo -e \"${BLUE}[INFO]${NC}    $*\"; }\nsuccess() { echo -e \"${GREEN}[OK]${NC}      $*\"; }\nwarn()    { echo -e \"${YELLOW}[WARN]${NC}    $*\"; }\nerror()   { echo -e \"${RED}[ERROR]${NC}   $*\" &gt;&amp;2; exit 1; }\nheader()  { echo -e \"\\n${CYAN}=== $* ===${NC}\"; }\nbanner()  { echo -e \"\\n${BOLD}${CYAN}\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557${NC}\"; \\\n            echo -e \"${BOLD}${CYAN}\u2551  $*${NC}\"; \\\n            echo -e \"${BOLD}${CYAN}\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d${NC}\"; }\n\nwait_for_pods() {\n    local namespace=$1\n    local timeout=${2:-300}\n    local start=$(date +%s)\n\n    info \"Waiting for all pods in ${namespace} to be Ready (timeout: ${timeout}s)...\"\n    while true; do\n        local not_ready=$(kubectl get pods -n \"${namespace}\" --no-headers 2&gt;/dev/null | \\\n            grep -v \"Running\\|Completed\" | wc -l | tr -d ' ')\n\n        if [ \"${not_ready}\" = \"0\" ] &amp;&amp; [ \"$(kubectl get pods -n ${namespace} --no-headers 2&gt;/dev/null | wc -l | tr -d ' ')\" -gt 0 ]; then\n            success \"All pods in ${namespace} are Ready\"\n            return 0\n        fi\n\n        local elapsed=$(( $(date +%s) - start ))\n        if [ ${elapsed} -ge ${timeout} ]; then\n            warn \"Timeout waiting for pods in ${namespace}\"\n            kubectl get pods -n \"${namespace}\"\n            return 1\n        fi\n\n        sleep 5\n    done\n}\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nbanner \"STEP 1/6: Install Nginx Ingress Controller + Harbor\"\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nheader \"Adding Helm Repositories\"\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx 2&gt;/dev/null || true\nhelm repo add harbor https://helm.goharbor.io 2&gt;/dev/null || true\nhelm repo add argo https://argoproj.github.io/argo-helm 2&gt;/dev/null || true\nhelm repo update\n\nheader \"Installing Nginx Ingress Controller\"\nhelm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \\\n    --namespace ingress-nginx \\\n    --create-namespace \\\n    --set controller.service.type=NodePort \\\n    --set controller.service.nodePorts.http=30080 \\\n    --set controller.service.nodePorts.https=30443 \\\n    --set controller.admissionWebhooks.enabled=false \\\n    --wait --timeout 5m\nsuccess \"Nginx Ingress Controller installed\"\n\nNODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}')\ninfo \"Node IP: ${NODE_IP}\"\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nbanner \"STEP 2/6: Install and Configure Harbor (harbor.local)\"\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nheader \"Installing Harbor\"\nhelm upgrade --install harbor harbor/harbor \\\n    --namespace harbor \\\n    --create-namespace \\\n    --set expose.type=ingress \\\n    --set expose.ingress.className=nginx \\\n    --set expose.ingress.hosts.core=harbor.local \\\n    --set expose.tls.enabled=false \\\n    --set externalURL=http://harbor.local \\\n    --set harborAdminPassword=\"${HARBOR_PASS}\" \\\n    --set persistence.enabled=false \\\n    --wait --timeout 10m\nsuccess \"Harbor installed\"\n\n# Configure /etc/hosts\nif ! grep -q \"harbor.local\" /etc/hosts; then\n    echo \"${NODE_IP}  harbor.local\" | sudo tee -a /etc/hosts\nfi\nif ! grep -q \"argocd.local\" /etc/hosts; then\n    echo \"${NODE_IP}  argocd.local\" | sudo tee -a /etc/hosts\nfi\n\n# Wait for Harbor to be healthy\nheader \"Waiting for Harbor Health\"\nfor i in $(seq 1 30); do\n    HTTP_CODE=$(curl -s -o /dev/null -w \"%{http_code}\" http://harbor.local/api/v2.0/health 2&gt;/dev/null || echo \"000\")\n    if [ \"${HTTP_CODE}\" = \"200\" ]; then\n        success \"Harbor is healthy\"\n        break\n    fi\n    info \"Attempt ${i}/30 - HTTP ${HTTP_CODE}\"\n    sleep 10\ndone\n\n# Create Harbor projects\nheader \"Creating Harbor Projects\"\nfor project in argocd library; do\n    response=$(curl -s -o /dev/null -w \"%{http_code}\" \\\n        -X POST \"http://harbor.local/api/v2.0/projects\" \\\n        -H \"Content-Type: application/json\" \\\n        -u \"${HARBOR_USER}:${HARBOR_PASS}\" \\\n        -d \"{\\\"project_name\\\": \\\"${project}\\\", \\\"public\\\": true}\")\n    if [ \"${response}\" = \"201\" ] || [ \"${response}\" = \"409\" ]; then\n        success \"Project ${project} ready\"\n    else\n        warn \"Project ${project} returned HTTP ${response}\"\n    fi\ndone\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nbanner \"STEP 3/6: Mirror ArgoCD Images to Harbor\"\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nheader \"Discovering Required Images\"\nIMAGES=$(helm template argocd argo/argo-cd \\\n    --version \"${ARGOCD_CHART_VERSION}\" \\\n    --namespace argocd 2&gt;/dev/null | \\\n    grep -E \"image:\" | \\\n    sed 's/.*image: *\"\\?\\([^\"]*\\)\"\\?.*/\\1/' | \\\n    sort -u)\n\ninfo \"Images to mirror:\"\necho \"${IMAGES}\"\n\nheader \"Pulling, Tagging, and Pushing Images\"\ndocker login \"${HARBOR_URL}\" -u \"${HARBOR_USER}\" -p \"${HARBOR_PASS}\" 2&gt;/dev/null || \\\n    warn \"Docker login failed - configure insecure registries first\"\n\nwhile IFS= read -r img; do\n    [ -z \"${img}\" ] &amp;&amp; continue\n    local_name=$(echo \"${img}\" | rev | cut -d'/' -f1 | rev)\n    target=\"${HARBOR_URL}/argocd/${local_name}\"\n\n    info \"Mirroring: ${img} \u2192 ${target}\"\n    docker pull \"${img}\" 2&gt;/dev/null &amp;&amp; \\\n    docker tag \"${img}\" \"${target}\" &amp;&amp; \\\n    docker push \"${target}\" 2&gt;/dev/null &amp;&amp; \\\n    success \"Mirrored: ${target}\" || \\\n    warn \"Failed to mirror: ${img}\"\ndone &lt;&lt;&lt; \"${IMAGES}\"\n\n# Save Helm chart locally\nheader \"Saving ArgoCD Helm Chart Locally\"\nmkdir -p /tmp/argocd-airgap\nhelm pull argo/argo-cd --version \"${ARGOCD_CHART_VERSION}\" --destination /tmp/argocd-airgap/ 2&gt;/dev/null || true\nsuccess \"Chart saved\"\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nbanner \"STEP 4/6: Create Git Repository with Helm Chart\"\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nheader \"Setting Up Git Repository\"\nrm -rf \"${REPO_BASE}\"\nmkdir -p \"${REPO_BASE}\"\n\ngit init --bare \"${BARE_REPO}\"\ngit clone \"${BARE_REPO}\" \"${WORK_DIR}\"\n\ncd \"${WORK_DIR}\"\nhelm create \"${CHART_NAME}\"\n\n# Customize Chart.yaml\ncat &gt; \"${CHART_NAME}/Chart.yaml\" &lt;&lt; 'EOF'\napiVersion: v2\nname: my-web-app\ndescription: A simple web application deployed via ArgoCD GitOps\ntype: application\nversion: 0.1.0\nappVersion: \"1.25.0\"\nEOF\n\n# Customize values.yaml\ncat &gt; \"${CHART_NAME}/values.yaml\" &lt;&lt; 'EOF'\nreplicaCount: 2\nimage:\n  repository: nginx\n  pullPolicy: IfNotPresent\n  tag: \"1.25-alpine\"\nimagePullSecrets: []\nnameOverride: \"\"\nfullnameOverride: \"\"\nserviceAccount:\n  create: true\n  automount: true\n  annotations: {}\n  name: \"\"\npodAnnotations: {}\npodLabels: {}\npodSecurityContext: {}\nsecurityContext: {}\nservice:\n  type: ClusterIP\n  port: 80\ningress:\n  enabled: false\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 50m\n    memory: 64Mi\nautoscaling:\n  enabled: false\nvolumes: []\nvolumeMounts: []\nnodeSelector: {}\ntolerations: []\naffinity: {}\nwelcomePage:\n  title: \"GitOps Demo App\"\n  message: \"Deployed by ArgoCD from Harbor airgap registry!\"\n  backgroundColor: \"#1a1a2e\"\n  textColor: \"#e94560\"\nEOF\n\n# Create ConfigMap template\ncat &gt; \"${CHART_NAME}/templates/configmap.yaml\" &lt;&lt; 'TEMPLATE'\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"my-web-app.fullname\" . }}-html\n  labels:\n    {{- include \"my-web-app.labels\" . | nindent 4 }}\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;&lt;title&gt;{{ .Values.welcomePage.title }}&lt;/title&gt;\n    &lt;style&gt;\n      body { font-family: Arial, sans-serif; display: flex; justify-content: center;\n             align-items: center; min-height: 100vh; margin: 0;\n             background: {{ .Values.welcomePage.backgroundColor }};\n             color: {{ .Values.welcomePage.textColor }}; }\n      .container { text-align: center; }\n      h1 { font-size: 2.5em; }\n      .info { font-size: 1.2em; margin: 8px 0; color: #eee; }\n      .badge { display: inline-block; background: {{ .Values.welcomePage.textColor }};\n               color: white; padding: 5px 15px; border-radius: 20px; margin: 5px; }\n    &lt;/style&gt;&lt;/head&gt;\n    &lt;body&gt;&lt;div class=\"container\"&gt;\n      &lt;h1&gt;{{ .Values.welcomePage.title }}&lt;/h1&gt;\n      &lt;p class=\"info\"&gt;{{ .Values.welcomePage.message }}&lt;/p&gt;\n      &lt;p class=\"info\"&gt;\n        &lt;span class=\"badge\"&gt;Release: {{ .Release.Name }}&lt;/span&gt;\n        &lt;span class=\"badge\"&gt;Namespace: {{ .Release.Namespace }}&lt;/span&gt;\n        &lt;span class=\"badge\"&gt;Chart: {{ .Chart.Name }}-{{ .Chart.Version }}&lt;/span&gt;\n      &lt;/p&gt;\n    &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\nTEMPLATE\n\n# Update Deployment to mount ConfigMap\ncat &gt; \"${CHART_NAME}/templates/deployment.yaml\" &lt;&lt; 'TEMPLATE'\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"my-web-app.fullname\" . }}\n  labels:\n    {{- include \"my-web-app.labels\" . | nindent 4 }}\nspec:\n  {{- if not .Values.autoscaling.enabled }}\n  replicas: {{ .Values.replicaCount }}\n  {{- end }}\n  selector:\n    matchLabels:\n      {{- include \"my-web-app.selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      annotations:\n        checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}\n      {{- with .Values.podAnnotations }}\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      labels:\n        {{- include \"my-web-app.labels\" . | nindent 8 }}\n        {{- with .Values.podLabels }}\n        {{- toYaml . | nindent 8 }}\n        {{- end }}\n    spec:\n      {{- with .Values.imagePullSecrets }}\n      imagePullSecrets:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      serviceAccountName: {{ include \"my-web-app.serviceAccountName\" . }}\n      containers:\n        - name: {{ .Chart.Name }}\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n          imagePullPolicy: {{ .Values.image.pullPolicy }}\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          livenessProbe:\n            httpGet:\n              path: /\n              port: http\n          readinessProbe:\n            httpGet:\n              path: /\n              port: http\n          {{- with .Values.resources }}\n          resources:\n            {{- toYaml . | nindent 12 }}\n          {{- end }}\n          volumeMounts:\n            - name: html\n              mountPath: /usr/share/nginx/html\n              readOnly: true\n      volumes:\n        - name: html\n          configMap:\n            name: {{ include \"my-web-app.fullname\" . }}-html\n      {{- with .Values.nodeSelector }}\n      nodeSelector:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.tolerations }}\n      tolerations:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\nTEMPLATE\n\n# Validate and push\nhelm lint \"${CHART_NAME}/\"\ngit add .\ngit commit -m \"Add my-web-app Helm chart\"\ngit push origin master 2&gt;/dev/null || git push origin main\nsuccess \"Git repository ready\"\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nbanner \"STEP 5/6: Deploy ArgoCD (Offline/Airgap Mode)\"\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nheader \"Installing ArgoCD from Harbor\"\n\nARGOCD_CHART_FILE=\"/tmp/argocd-airgap/argo-cd-${ARGOCD_CHART_VERSION}.tgz\"\nif [ -f \"${ARGOCD_CHART_FILE}\" ]; then\n    CHART_SRC=\"${ARGOCD_CHART_FILE}\"\nelse\n    CHART_SRC=\"argo/argo-cd\"\nfi\n\ncat &gt; /tmp/argocd-airgap-values.yaml &lt;&lt; EOF\nglobal:\n  image:\n    repository: ${HARBOR_URL}/argocd/argocd\n    tag: \"v2.13.3\"\nserver:\n  insecure: true\n  ingress:\n    enabled: true\n    ingressClassName: nginx\n    hostname: argocd.local\n    annotations:\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTP\"\nredis:\n  image:\n    repository: ${HARBOR_URL}/argocd/redis\n    tag: \"7.4.2-alpine\"\ndex:\n  image:\n    repository: ${HARBOR_URL}/argocd/dex\n    tag: \"v2.41.1\"\nEOF\n\nhelm upgrade --install argocd ${CHART_SRC} \\\n    --version \"${ARGOCD_CHART_VERSION}\" \\\n    --namespace argocd \\\n    --create-namespace \\\n    -f /tmp/argocd-airgap-values.yaml \\\n    --wait --timeout 10m\n\nsuccess \"ArgoCD installed in airgap mode\"\n\n# Verify images\ninfo \"Container images in use:\"\nkubectl get pods -n argocd -o jsonpath='{range .items[*]}{range .spec.containers[*]}{.image}{\"\\n\"}{end}{end}' | sort -u\n\nARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret \\\n    -o jsonpath=\"{.data.password}\" | base64 -d)\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nbanner \"STEP 6/6: Create ArgoCD Application\"\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nheader \"Creating ArgoCD Application for my-web-app\"\n\ncat &gt; /tmp/argocd-app-my-web-app.yaml &lt;&lt; 'EOF'\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-web-app\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/&lt;your-org&gt;/helm-apps.git\n    targetRevision: HEAD\n    path: my-web-app\n    helm:\n      valuesObject:\n        replicaCount: 3\n        welcomePage:\n          title: \"Airgap GitOps App\"\n          message: \"Deployed by ArgoCD from Harbor registry!\"\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: my-web-app\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\nEOF\n\nkubectl apply -f /tmp/argocd-app-my-web-app.yaml\nsuccess \"ArgoCD Application created\"\n\n# Wait for sync\nheader \"Waiting for Application to Sync\"\nfor i in $(seq 1 30); do\n    HEALTH=$(kubectl get application my-web-app -n argocd \\\n        -o jsonpath='{.status.health.status}' 2&gt;/dev/null || echo \"Unknown\")\n    SYNC=$(kubectl get application my-web-app -n argocd \\\n        -o jsonpath='{.status.sync.status}' 2&gt;/dev/null || echo \"Unknown\")\n\n    if [ \"${HEALTH}\" = \"Healthy\" ] &amp;&amp; [ \"${SYNC}\" = \"Synced\" ]; then\n        success \"Application is Synced and Healthy!\"\n        break\n    fi\n    info \"Attempt ${i}/30 - Health: ${HEALTH}, Sync: ${SYNC}\"\n    sleep 5\ndone\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nbanner \"INSTALLATION COMPLETE\"\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\necho \"\"\necho \"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\"\necho \"\"\ninfo \"Harbor Registry:\"\ninfo \"  URL:        http://harbor.local\"\ninfo \"  Username:   ${HARBOR_USER}\"\ninfo \"  Password:   ${HARBOR_PASS}\"\necho \"\"\ninfo \"ArgoCD:\"\ninfo \"  URL:        http://argocd.local\"\ninfo \"  Username:   admin\"\ninfo \"  Password:   ${ARGOCD_PASSWORD}\"\necho \"\"\ninfo \"Git Repository:\"\ninfo \"  Bare:       ${BARE_REPO}\"\ninfo \"  Workspace:  ${WORK_DIR}\"\necho \"\"\ninfo \"Application:\"\ninfo \"  Name:       my-web-app\"\ninfo \"  Namespace:  my-web-app\"\ninfo \"  Access:     kubectl port-forward svc/my-web-app -n my-web-app 8081:80\"\necho \"\"\necho \"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\"\nsuccess \"All 6 steps completed successfully!\"\n</code></pre>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#quick-reference-key-commands","title":"Quick Reference: Key Commands","text":"Task Command Check Harbor health <code>curl http://harbor.local/api/v2.0/health</code> List Harbor projects <code>curl -u admin:Harbor12345 http://harbor.local/api/v2.0/projects</code> Docker login to Harbor <code>docker login harbor.local -u admin -p Harbor12345</code> Mirror an image to Harbor <code>docker pull IMG &amp;&amp; docker tag IMG harbor.local/proj/IMG &amp;&amp; docker push</code> Discover ArgoCD images <code>helm template argocd argo/argo-cd \\| grep image: \\| sort -u</code> ArgoCD admin password <code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" \\| base64 -d</code> ArgoCD CLI login <code>argocd login argocd.local --insecure --username admin --password PASS</code> Create ArgoCD app <code>kubectl apply -f application.yaml</code> Sync ArgoCD app <code>argocd app sync my-web-app</code> Check app health <code>argocd app get my-web-app</code> Verify airgap images <code>kubectl get pods -n argocd -o jsonpath='{..image}' \\| tr ' ' '\\n' \\| sort -u</code>"},{"location":"Tasks/Kubernetes-Harbor-ArgoCD-Airgap-Tasks/#cleanup","title":"Cleanup","text":"<pre><code># Remove ArgoCD application\nargocd app delete my-web-app --yes 2&gt;/dev/null || \\\n    kubectl delete application my-web-app -n argocd\n\n# Uninstall ArgoCD\nhelm uninstall argocd -n argocd\nkubectl delete namespace argocd\n\n# Uninstall Harbor\nhelm uninstall harbor -n harbor\nkubectl delete namespace harbor\n\n# Uninstall Ingress Controller\nhelm uninstall ingress-nginx -n ingress-nginx\nkubectl delete namespace ingress-nginx\n\n# Remove namespaces\nkubectl delete namespace my-web-app 2&gt;/dev/null || true\n\n# Clean up local files\nrm -rf /tmp/gitops-lab /tmp/argocd-airgap /tmp/argocd-airgap-values.yaml /tmp/argocd-app-my-web-app.yaml\n\n# Remove /etc/hosts entries\nsudo sed -i '' '/harbor.local/d' /etc/hosts\nsudo sed -i '' '/argocd.local/d' /etc/hosts\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/","title":"Kubernetes Helm Chart Tasks","text":"<ul> <li>Hands-on Kubernetes exercises covering Helm chart creation, packaging, deployment, and best practices.</li> <li>Each task includes a description, scenario, and a detailed solution with step-by-step instructions.</li> <li>Practice these tasks to master Helm from basic chart scaffolding to advanced templating and chart repositories.</li> </ul>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#table-of-contents","title":"Table of Contents","text":"<ul> <li>01. Scaffold a Helm Chart</li> <li>02. Explore the Chart Structure</li> <li>03. Deploy an Nginx-Based Chart</li> <li>04. Customize the Welcome Page with Current Date &amp; Time</li> <li>05. Add a Service</li> <li>06. Add Two Ingress Resources with Different Paths</li> <li>07. Add an ExternalName Service</li> <li>08. Values Overrides and Environments</li> <li>09. Template Helpers and Named Templates</li> <li>10. Template Control Flow (if / range / with)</li> <li>11. Chart Dependencies (Subcharts)</li> <li>12. Linting, Dry-Run, and Debugging</li> <li>13. Package and Host a Chart Repository</li> <li>14. Upgrade, Rollback, and Release History</li> <li>15. Hooks (Pre-install / Post-install)</li> <li>16. Use <code>helm status</code> to Inspect a Release</li> <li>17. Extract Values with <code>helm get values</code></li> <li>18. Show Chart Values with <code>helm show values</code></li> <li>19. Search Charts with <code>helm search repo</code></li> <li>20. Update Repositories with <code>helm repo update</code></li> <li>21. Run Chart Tests with <code>helm test</code></li> <li>22. Use <code>helm get all</code> to Retrieve Complete Release Info</li> <li>23. Use <code>helm list</code> with Filters and Formatting</li> <li>24. Chain Multiple Commands for Release Management</li> </ul>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#01-scaffold-a-helm-chart","title":"01. Scaffold a Helm Chart","text":"<p>Create a new Helm chart from scratch using <code>helm create</code> and explore the generated files.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario","title":"Scenario:","text":"<p>\u25e6 You need to package an application for Kubernetes and want a standardized project structure.   \u25e6 <code>helm create</code> generates a best-practice skeleton you can customize.</p> <p>Hint: <code>helm create</code>, <code>tree</code></p> Solution <pre><code># 1. Create a new chart named \"my-nginx-app\"\nhelm create my-nginx-app\n\n# 2. Explore the generated structure\ntree my-nginx-app/\n\n# Output:\n# my-nginx-app/\n# \u251c\u2500\u2500 Chart.yaml          # Chart metadata (name, version, description)\n# \u251c\u2500\u2500 values.yaml         # Default configuration values\n# \u251c\u2500\u2500 charts/             # Dependency charts (subcharts)\n# \u251c\u2500\u2500 templates/          # Kubernetes manifest templates\n# \u2502   \u251c\u2500\u2500 NOTES.txt       # Post-install usage notes\n# \u2502   \u251c\u2500\u2500 _helpers.tpl    # Named template definitions\n# \u2502   \u251c\u2500\u2500 deployment.yaml\n# \u2502   \u251c\u2500\u2500 hpa.yaml\n# \u2502   \u251c\u2500\u2500 ingress.yaml\n# \u2502   \u251c\u2500\u2500 service.yaml\n# \u2502   \u251c\u2500\u2500 serviceaccount.yaml\n# \u2502   \u2514\u2500\u2500 tests/\n# \u2502       \u2514\u2500\u2500 test-connection.yaml\n# \u2514\u2500\u2500 .helmignore         # Files to exclude when packaging\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#02-explore-the-chart-structure","title":"02. Explore the Chart Structure","text":"<p>Inspect <code>Chart.yaml</code> and <code>values.yaml</code> to understand how Helm charts are configured.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_1","title":"Scenario:","text":"<p>\u25e6 Before modifying a chart, you need to understand what each file does.   \u25e6 <code>Chart.yaml</code> defines the chart identity; <code>values.yaml</code> drives all the template rendering.</p> <p>Hint: <code>cat Chart.yaml</code>, <code>cat values.yaml</code></p> Solution <pre><code># 1. Inspect Chart.yaml\ncat my-nginx-app/Chart.yaml\n\n# Key fields:\n# - apiVersion: v2       (Helm 3 chart)\n# - name: my-nginx-app\n# - version: 0.1.0       (chart version - bump this on changes)\n# - appVersion: \"1.16.0\" (the app version being deployed)\n\n# 2. Inspect values.yaml\ncat my-nginx-app/values.yaml\n\n# Key fields:\n# - replicaCount: 1\n# - image.repository: nginx\n# - image.tag: \"\"        (defaults to appVersion from Chart.yaml)\n# - service.type: ClusterIP\n# - service.port: 80\n# - ingress.enabled: false\n\n# 3. See how values are consumed in templates\ngrep -n '{{ .Values' my-nginx-app/templates/deployment.yaml\n\n# 4. Render the templates without deploying (dry-run)\nhelm template my-release my-nginx-app/\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#03-deploy-an-nginx-based-chart","title":"03. Deploy an Nginx-Based Chart","text":"<p>Install the chart to your cluster using the default nginx image, verify the deployment, and access nginx.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_2","title":"Scenario:","text":"<p>\u25e6 You want to deploy a basic nginx web server using Helm to validate the chart works before customizing it.</p> <p>Hint: <code>helm install</code>, <code>kubectl get all</code>, <code>kubectl port-forward</code></p> Solution <pre><code># 1. Install the chart\nhelm install my-nginx my-nginx-app/\n\n# 2. Verify all resources were created\nkubectl get all -l app.kubernetes.io/instance=my-nginx\n\n# 3. Check the release\nhelm list\n\n# 4. Access the application via port-forward\nkubectl port-forward svc/my-nginx-my-nginx-app 8080:80\n\n# 5. In another terminal or browser\ncurl http://localhost:8080\n# Should show the default nginx welcome page\n\n# 6. Uninstall when done\nhelm uninstall my-nginx\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#04-customize-the-welcome-page-with-current-date-time","title":"04. Customize the Welcome Page with Current Date &amp; Time","text":"<p>Create a ConfigMap that generates a custom HTML welcome page showing the current date and time, and mount it into the nginx container.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_3","title":"Scenario:","text":"<p>\u25e6 You want to display dynamic content (deployment timestamp) on the nginx welcome page.   \u25e6 This demonstrates how Helm templates can inject build-time values into application configuration.</p> <p>Hint: <code>now</code>, <code>date</code>, ConfigMap volume mount, <code>{{ .Release }}</code></p> Solution"},{"location":"Tasks/Kubernetes-Helm-Tasks/#step-1-create-the-configmap-template-templatesconfigmap-htmlyaml","title":"Step 1: Create the ConfigMap template [templates/configmap-html.yaml]","text":"<pre><code>cat &gt; my-nginx-app/templates/configmap-html.yaml &lt;&lt; 'TEMPLATE'\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ include \"my-nginx-app.fullname\" . }}-html\n  labels:\n    {{- include \"my-nginx-app.labels\" . | nindent 4 }}\ndata:\n  index.html: |\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n      &lt;title&gt;{{ .Values.welcomePage.title | default \"Welcome\" }}&lt;/title&gt;\n      &lt;style&gt;\n        body {\n          font-family: Arial, sans-serif;\n          display: flex;\n          justify-content: center;\n          align-items: center;\n          min-height: 100vh;\n          margin: 0;\n          background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n          color: white;\n        }\n        .container { text-align: center; }\n        h1 { font-size: 2.5em; }\n        .info { font-size: 1.2em; margin: 10px 0; }\n        .time { font-size: 3em; font-weight: bold; margin: 20px 0; }\n      &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n      &lt;div class=\"container\"&gt;\n        &lt;h1&gt;{{ .Values.welcomePage.title | default \"Welcome to My Nginx App\" }}&lt;/h1&gt;\n        &lt;p class=\"info\"&gt;Release: &lt;strong&gt;{{ .Release.Name }}&lt;/strong&gt;&lt;/p&gt;\n        &lt;p class=\"info\"&gt;Namespace: &lt;strong&gt;{{ .Release.Namespace }}&lt;/strong&gt;&lt;/p&gt;\n        &lt;p class=\"info\"&gt;Chart Version: &lt;strong&gt;{{ .Chart.Version }}&lt;/strong&gt;&lt;/p&gt;\n        &lt;p class=\"info\"&gt;App Version: &lt;strong&gt;{{ .Chart.AppVersion }}&lt;/strong&gt;&lt;/p&gt;\n        &lt;p class=\"time\"&gt;Deployed at: {{ now | date \"2006-01-02 15:04:05 MST\" }}&lt;/p&gt;\n        {{- if .Values.welcomePage.message }}\n        &lt;p class=\"info\"&gt;{{ .Values.welcomePage.message }}&lt;/p&gt;\n        {{- end }}\n      &lt;/div&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\nTEMPLATE\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#step-2-add-welcome-page-values-to-valuesyaml","title":"Step 2: Add welcome page values to <code>values.yaml</code>","text":"<pre><code>cat &gt;&gt; my-nginx-app/values.yaml &lt;&lt; 'EOF'\n\n# Custom welcome page configuration\nwelcomePage:\n  title: \"Welcome to My Nginx App\"\n  message: \"Deployed with Helm!\"\nEOF\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#step-3-update-the-deployment-template-to-mount-the-configmap","title":"Step 3: Update the deployment template to mount the ConfigMap","text":"<p>Edit <code>my-nginx-app/templates/deployment.yaml</code> - add the <code>volumeMounts</code> and <code>volumes</code>:</p> <pre><code># Add volumeMounts under the container spec and volumes under the pod spec.\n# The easiest approach: replace the deployment template entirely.\n# Here we patch the key sections:\n\n# In the container spec, add:\n#   volumeMounts:\n#     - name: html-volume\n#       mountPath: /usr/share/nginx/html\n#       readOnly: true\n\n# In the pod spec (same level as containers), add:\n#   volumes:\n#     - name: html-volume\n#       configMap:\n#         name: {{ include \"my-nginx-app.fullname\" . }}-html\n</code></pre> <p>For a quick approach, replace the entire deployment template:</p> <pre><code>cat &gt; my-nginx-app/templates/deployment.yaml &lt;&lt; 'TEMPLATE'\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"my-nginx-app.fullname\" . }}\n  labels:\n    {{- include \"my-nginx-app.labels\" . | nindent 4 }}\nspec:\n  {{- if not .Values.autoscaling.enabled }}\n  replicas: {{ .Values.replicaCount }}\n  {{- end }}\n  selector:\n    matchLabels:\n      {{- include \"my-nginx-app.selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      annotations:\n        # Force pod restart on ConfigMap changes\n        checksum/html: {{ include (print $.Template.BasePath \"/configmap-html.yaml\") . | sha256sum }}\n      {{- with .Values.podAnnotations }}\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      labels:\n        {{- include \"my-nginx-app.labels\" . | nindent 8 }}\n        {{- with .Values.podLabels }}\n        {{- toYaml . | nindent 8 }}\n        {{- end }}\n    spec:\n      {{- with .Values.imagePullSecrets }}\n      imagePullSecrets:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      serviceAccountName: {{ include \"my-nginx-app.serviceAccountName\" . }}\n      {{- with .Values.podSecurityContext }}\n      securityContext:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      containers:\n        - name: {{ .Chart.Name }}\n          {{- with .Values.securityContext }}\n          securityContext:\n            {{- toYaml . | nindent 12 }}\n          {{- end }}\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n          imagePullPolicy: {{ .Values.image.pullPolicy }}\n          ports:\n            - name: http\n              containerPort: {{ .Values.service.port }}\n              protocol: TCP\n          livenessProbe:\n            {{- toYaml .Values.livenessProbe | nindent 12 }}\n          readinessProbe:\n            {{- toYaml .Values.readinessProbe | nindent 12 }}\n          {{- with .Values.resources }}\n          resources:\n            {{- toYaml . | nindent 12 }}\n          {{- end }}\n          volumeMounts:\n            - name: html-volume\n              mountPath: /usr/share/nginx/html\n              readOnly: true\n            {{- with .Values.volumeMounts }}\n            {{- toYaml . | nindent 12 }}\n            {{- end }}\n      volumes:\n        - name: html-volume\n          configMap:\n            name: {{ include \"my-nginx-app.fullname\" . }}-html\n        {{- with .Values.volumes }}\n        {{- toYaml . | nindent 8 }}\n        {{- end }}\n      {{- with .Values.nodeSelector }}\n      nodeSelector:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.affinity }}\n      affinity:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\n      {{- with .Values.tolerations }}\n      tolerations:\n        {{- toYaml . | nindent 8 }}\n      {{- end }}\nTEMPLATE\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#step-4-install-and-verify","title":"Step 4: Install and verify","text":"<pre><code># Install (or upgrade if already installed)\nhelm upgrade --install my-nginx my-nginx-app/\n\n# Port-forward and check the custom page\nkubectl port-forward svc/my-nginx-my-nginx-app 8080:80\n\n# In another terminal\ncurl http://localhost:8080\n# Should show the custom HTML page with current date/time and release info\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#05-add-a-service","title":"05. Add a Service","text":"<p>Verify the Service template supports ClusterIP, NodePort, and LoadBalancer types via <code>values.yaml</code>.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_4","title":"Scenario:","text":"<p>\u25e6 You want to expose your application with different service types depending on the environment (e.g., ClusterIP for dev, NodePort for minikube, LoadBalancer for cloud).   \u25e6 You need your chart to be flexible enough to deploy with different service types depending on the environment.   \u25e6 The default <code>helm create</code> already includes a service template - you need to understand and test it.</p> <p>Hint: <code>--set service.type=NodePort</code>, <code>helm upgrade</code></p> Solution <pre><code># 1. Check the current service template\ncat my-nginx-app/templates/service.yaml\n\n# 2. The default template already supports configurable type:\n#    type: {{ .Values.service.type }}\n#    port: {{ .Values.service.port }}\n\n# 3. Install with ClusterIP (default)\nhelm upgrade --install my-nginx my-nginx-app/\n\n# 4. Verify\nkubectl get svc -l app.kubernetes.io/instance=my-nginx\n# TYPE should be ClusterIP\n\n# 5. Upgrade to NodePort\nhelm upgrade my-nginx my-nginx-app/ --set service.type=NodePort\n\n# 6. Verify\nkubectl get svc -l app.kubernetes.io/instance=my-nginx\n# TYPE should now be NodePort\n\n# 7. Upgrade to LoadBalancer\nhelm upgrade my-nginx my-nginx-app/ --set service.type=LoadBalancer\n\n# 8. Verify\nkubectl get svc -l app.kubernetes.io/instance=my-nginx\n# TYPE should now be LoadBalancer (EXTERNAL-IP may stay &lt;pending&gt; on local clusters)\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#06-add-two-ingress-resources-with-different-paths","title":"06. Add Two Ingress Resources with Different Paths","text":"<p>Create two Ingress resources: one serving the main app at <code>/</code> and another serving a health/status endpoint at <code>/status</code>.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_5","title":"Scenario:","text":"<p>\u25e6 Your application has a main frontend and a separate status/health page.   \u25e6 You want to route traffic using different URL paths to the same backend, each with its own Ingress resource.   \u25e6 This is useful when different Ingress resources need different annotations (rate limiting, auth, etc.).</p> <p>Prerequisites: An Ingress controller must be installed (e.g., <code>nginx-ingress</code>).</p> <p>Hint: Two separate Ingress templates, <code>pathType: Prefix</code></p> Solution <p>Step 1: Create the main Ingress template [templates/ingress-main.yaml]</p> <pre><code>cat &gt; my-nginx-app/templates/ingress-main.yaml &lt;&lt; 'TEMPLATE'\n{{- if .Values.ingress.main.enabled -}}\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ include \"my-nginx-app.fullname\" . }}-main\n  labels:\n    {{- include \"my-nginx-app.labels\" . | nindent 4 }}\n  {{- with .Values.ingress.main.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  {{- if .Values.ingress.main.className }}\n  ingressClassName: {{ .Values.ingress.main.className }}\n  {{- end }}\n  rules:\n    - host: {{ .Values.ingress.main.host | quote }}\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: {{ include \"my-nginx-app.fullname\" . }}\n                port:\n                  number: {{ .Values.service.port }}\n  {{- if .Values.ingress.main.tls }}\n  tls:\n    {{- toYaml .Values.ingress.main.tls | nindent 4 }}\n  {{- end }}\n{{- end }}\nTEMPLATE\n</code></pre> <p>Step 2: Create the status Ingress template [templates/ingress-status.yaml]</p> <pre><code>cat &gt; my-nginx-app/templates/ingress-status.yaml &lt;&lt; 'TEMPLATE'\n{{- if .Values.ingress.status.enabled -}}\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: {{ include \"my-nginx-app.fullname\" . }}-status\n  labels:\n    {{- include \"my-nginx-app.labels\" . | nindent 4 }}\n  {{- with .Values.ingress.status.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\nspec:\n  {{- if .Values.ingress.status.className }}\n  ingressClassName: {{ .Values.ingress.status.className }}\n  {{- end }}\n  rules:\n    - host: {{ .Values.ingress.status.host | quote }}\n      http:\n        paths:\n          - path: /status\n            pathType: Prefix\n            backend:\n              service:\n                name: {{ include \"my-nginx-app.fullname\" . }}\n                port:\n                  number: {{ .Values.service.port }}\n  {{- if .Values.ingress.status.tls }}\n  tls:\n    {{- toYaml .Values.ingress.status.tls | nindent 4 }}\n  {{- end }}\n{{- end }}\nTEMPLATE\n</code></pre> <p>Step 3: Remove the default ingress template (optional) and update <code>values.yaml</code></p> <pre><code># Remove the default generated ingress template to avoid confusion\nrm my-nginx-app/templates/ingress.yaml\n\n# Add ingress values to values.yaml\ncat &gt;&gt; my-nginx-app/values.yaml &lt;&lt; 'EOF'\n\n# Ingress configuration - two separate Ingress resources\ningress:\n  main:\n    enabled: true\n    className: nginx\n    host: my-nginx.local\n    annotations:\n      nginx.ingress.kubernetes.io/rewrite-target: /\n    tls: []\n  status:\n    enabled: true\n    className: nginx\n    host: my-nginx.local\n    annotations:\n      nginx.ingress.kubernetes.io/rewrite-target: /\n      # Example: different rate limit for status endpoint\n      nginx.ingress.kubernetes.io/limit-rps: \"10\"\n    tls: []\nEOF\n</code></pre> <p>Step 4: Deploy and verify</p> <pre><code># Upgrade the release\nhelm upgrade --install my-nginx my-nginx-app/\n\n# Verify both Ingress resources were created\nkubectl get ingress -l app.kubernetes.io/instance=my-nginx\n\n# Expected output:\n# NAME                        CLASS   HOSTS            ADDRESS   PORTS   AGE\n# my-nginx-my-nginx-app-main    nginx   my-nginx.local             80      5s\n# my-nginx-my-nginx-app-status  nginx   my-nginx.local             80      5s\n\n# Describe each to see the path rules\nkubectl describe ingress my-nginx-my-nginx-app-main\nkubectl describe ingress my-nginx-my-nginx-app-status\n\n# Test (add host entry or use curl with Host header)\n# curl -H \"Host: my-nginx.local\" http://&lt;INGRESS_IP&gt;/\n# curl -H \"Host: my-nginx.local\" http://&lt;INGRESS_IP&gt;/status\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#07-add-an-externalname-service","title":"07. Add an ExternalName Service","text":"<p>Add a second Service of type <code>ExternalName</code> that maps a Kubernetes service name to an external DNS name.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_6","title":"Scenario:","text":"<p>\u25e6 Your application needs to connect to an external API or database (e.g., an RDS instance, a SaaS endpoint).   \u25e6 By using an ExternalName service, you can refer to it by a local name inside the cluster and change the target later without modifying application code.</p> <p>Hint: <code>type: ExternalName</code>, <code>externalName</code></p> Solution <p>Step 1: Create the ExternalName Service template [templates/service-external.yaml]</p> <pre><code>cat &gt; my-nginx-app/templates/service-external.yaml &lt;&lt; 'TEMPLATE'\n{{- if .Values.externalService.enabled -}}\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ include \"my-nginx-app.fullname\" . }}-external\n  labels:\n    {{- include \"my-nginx-app.labels\" . | nindent 4 }}\nspec:\n  type: ExternalName\n  externalName: {{ .Values.externalService.host | quote }}\n  {{- if .Values.externalService.ports }}\n  ports:\n    {{- toYaml .Values.externalService.ports | nindent 4 }}\n  {{- end }}\n{{- end }}\nTEMPLATE\n</code></pre> <p>Step 2: Add values to <code>values.yaml</code></p> <pre><code>cat &gt;&gt; my-nginx-app/values.yaml &lt;&lt; 'EOF'\n\n# ExternalName service - maps a local name to an external DNS\nexternalService:\n  enabled: true\n  host: api.example.com\n  ports:\n    - port: 443\n      protocol: TCP\nEOF\n</code></pre> <p>Step 3: Deploy and verify</p> <pre><code># Upgrade the release\nhelm upgrade --install my-nginx my-nginx-app/\n\n# Verify the ExternalName service\nkubectl get svc -l app.kubernetes.io/instance=my-nginx\n\n# Should show something like:\n# NAME                              TYPE           CLUSTER-IP   EXTERNAL-IP       PORT(S)\n# my-nginx-my-nginx-app             ClusterIP      10.x.x.x    &lt;none&gt;            80/TCP\n# my-nginx-my-nginx-app-external    ExternalName   &lt;none&gt;       api.example.com   443/TCP\n\n# Test DNS resolution from inside the cluster\nkubectl run dns-check --image=busybox --restart=Never \\\n  -- nslookup my-nginx-my-nginx-app-external\nkubectl logs dns-check\n# Should resolve to api.example.com\n\n# Cleanup test pod\nkubectl delete pod dns-check\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#08-values-overrides-and-environments","title":"08. Values Overrides and Environments","text":"<p>Use multiple values files to manage different environments (dev, staging, production).</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_7","title":"Scenario:","text":"<p>\u25e6 You have one chart but need different configurations per environment (replica count, image tag, resource limits).   \u25e6 Helm supports layering multiple <code>-f</code> values files and <code>--set</code> overrides.</p> <p>Hint: <code>helm install -f</code>, <code>--set</code>, multiple values files</p> Solution <pre><code># 1. Create a dev values file\ncat &gt; values-dev.yaml &lt;&lt; 'EOF'\nreplicaCount: 1\nimage:\n  tag: \"alpine\"\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\nwelcomePage:\n  title: \"DEV Environment\"\n  message: \"This is the development instance\"\nEOF\n\n# 2. Create a production values file\ncat &gt; values-prod.yaml &lt;&lt; 'EOF'\nreplicaCount: 3\nimage:\n  tag: \"stable\"\nresources:\n  limits:\n    cpu: 500m\n    memory: 512Mi\n  requests:\n    cpu: 250m\n    memory: 256Mi\nwelcomePage:\n  title: \"PRODUCTION\"\n  message: \"Production instance - handle with care\"\nservice:\n  type: LoadBalancer\nEOF\n\n# 3. Install with dev values\nhelm upgrade --install my-nginx-dev my-nginx-app/ \\\n  -f values-dev.yaml \\\n  --namespace dev --create-namespace\n\n# 4. Install with prod values\nhelm upgrade --install my-nginx-prod my-nginx-app/ \\\n  -f values-prod.yaml \\\n  --namespace prod --create-namespace\n\n# 5. Verify different configurations\nkubectl get deployment -n dev -o wide\nkubectl get deployment -n prod -o wide\n\n# 6. Override a single value on top of a values file\nhelm upgrade my-nginx-dev my-nginx-app/ \\\n  -f values-dev.yaml \\\n  --set replicaCount=2 \\\n  --namespace dev\n\n# Cleanup\nhelm uninstall my-nginx-dev -n dev\nhelm uninstall my-nginx-prod -n prod\nkubectl delete ns dev prod\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#09-template-helpers-and-named-templates","title":"09. Template Helpers and Named Templates","text":"<p>Create a custom named template in <code>_helpers.tpl</code> and use it across multiple templates.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_8","title":"Scenario:","text":"<p>\u25e6 You have repeated logic (e.g., generating labels, resource names) across templates.   \u25e6 Named templates (partials) in <code>_helpers.tpl</code> let you define reusable snippets.</p> <p>Hint: <code>define</code>, <code>include</code>, <code>_helpers.tpl</code></p> Solution <pre><code># 1. Inspect existing helpers\ncat my-nginx-app/templates/_helpers.tpl\n\n# You'll see templates like:\n# {{- define \"my-nginx-app.name\" -}}         \u2192 Chart name\n# {{- define \"my-nginx-app.fullname\" -}}     \u2192 Release-qualified name\n# {{- define \"my-nginx-app.labels\" -}}       \u2192 Standard labels\n# {{- define \"my-nginx-app.selectorLabels\" -}} \u2192 Selector labels\n\n# 2. Add a custom helper - e.g., environment label\ncat &gt;&gt; my-nginx-app/templates/_helpers.tpl &lt;&lt; 'EOF'\n\n{{/*\nCustom: Generate environment-specific annotations\n*/}}\n{{- define \"my-nginx-app.envAnnotations\" -}}\napp.kubernetes.io/environment: {{ .Values.environment | default \"dev\" }}\napp.kubernetes.io/team: {{ .Values.team | default \"platform\" }}\n{{- end }}\nEOF\n\n# 3. Use it in a template (e.g., deployment.yaml metadata.annotations):\n#   annotations:\n#     {{- include \"my-nginx-app.envAnnotations\" . | nindent 4 }}\n\n# 4. Add default values\ncat &gt;&gt; my-nginx-app/values.yaml &lt;&lt; 'EOF'\n\n# Environment metadata\nenvironment: dev\nteam: platform\nEOF\n\n# 5. Test rendering\nhelm template my-nginx my-nginx-app/ | grep -A2 \"environment\"\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#10-template-control-flow-if-range-with","title":"10. Template Control Flow (if / range / with)","text":"<p>Practice Helm template control structures: conditionals, loops, and scoping.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_9","title":"Scenario:","text":"<p>\u25e6 You need to conditionally render resources, iterate over lists, or scope into nested values.   \u25e6 Go template control flow is essential for writing flexible Helm charts.</p> <p>Hint: <code>{{- if }}</code>, <code>{{- range }}</code>, <code>{{- with }}</code></p> Solution <pre><code># 1. Conditional: only create a resource if enabled\n# Already used in ingress templates:\n#   {{- if .Values.ingress.main.enabled -}}\n#   ...\n#   {{- end }}\n\n# 2. Range: iterate over a list\n# Example: Add multiple environment variables from a values list\n# In values.yaml:\ncat &gt;&gt; my-nginx-app/values.yaml &lt;&lt; 'EOF'\n\n# Extra environment variables\nextraEnv:\n  - name: LOG_LEVEL\n    value: \"info\"\n  - name: APP_MODE\n    value: \"production\"\nEOF\n\n# In deployment.yaml, under containers[].env:\n#   {{- range .Values.extraEnv }}\n#   - name: {{ .name }}\n#     value: {{ .value | quote }}\n#   {{- end }}\n\n# 3. With: scope into a map\n# {{- with .Values.nodeSelector }}\n# nodeSelector:\n#   {{- toYaml . | nindent 8 }}\n# {{- end }}\n\n# 4. Test the rendering\nhelm template my-nginx my-nginx-app/ --set extraEnv[0].name=DEBUG,extraEnv[0].value=true\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#11-chart-dependencies-subcharts","title":"11. Chart Dependencies (Subcharts)","text":"<p>Add a dependency (e.g., Redis) as a subchart and configure it through the parent <code>values.yaml</code>.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_10","title":"Scenario:","text":"<p>\u25e6 Your application needs a Redis cache alongside nginx.   \u25e6 Instead of writing Redis manifests from scratch, you depend on an existing chart from a repository.</p> <p>Hint: <code>Chart.yaml</code> dependencies, <code>helm dependency update</code></p> Solution <pre><code># 1. Add dependency to Chart.yaml\ncat &gt;&gt; my-nginx-app/Chart.yaml &lt;&lt; 'EOF'\n\ndependencies:\n  - name: redis\n    version: \"~18.0\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: redis.enabled\nEOF\n\n# 2. Add redis configuration in values.yaml\ncat &gt;&gt; my-nginx-app/values.yaml &lt;&lt; 'EOF'\n\n# Redis subchart configuration\nredis:\n  enabled: false          # Set to true to deploy Redis alongside nginx\n  architecture: standalone\n  auth:\n    enabled: false\nEOF\n\n# 3. Build dependencies (downloads the redis chart into charts/)\nhelm dependency update my-nginx-app/\n\n# 4. Verify\nls my-nginx-app/charts/\n# Should show: redis-18.x.x.tgz\n\n# 5. Install with Redis enabled\nhelm upgrade --install my-nginx my-nginx-app/ --set redis.enabled=true\n\n# 6. Verify Redis pods\nkubectl get pods -l app.kubernetes.io/instance=my-nginx\n\n# Cleanup\nhelm uninstall my-nginx\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#12-linting-dry-run-and-debugging","title":"12. Linting, Dry-Run, and Debugging","text":"<p>Use Helm\u2019s built-in tools to validate, debug, and troubleshoot your chart before deploying.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_11","title":"Scenario:","text":"<p>\u25e6 You modified several templates and want to catch errors before deploying to the cluster.   \u25e6 Helm provides lint, template, dry-run, and debug tools for this purpose.</p> <p>Hint: <code>helm lint</code>, <code>helm template</code>, <code>--dry-run</code>, <code>--debug</code></p> Solution <pre><code># 1. Lint - checks for common errors and best practices\nhelm lint my-nginx-app/\n\n# With values overrides\nhelm lint my-nginx-app/ -f values-dev.yaml\n\n# 2. Template - render manifests locally (no cluster needed)\nhelm template my-release my-nginx-app/ &gt; rendered.yaml\ncat rendered.yaml\n\n# 3. Dry-run - simulates install against the cluster (validates with API server)\nhelm install my-nginx my-nginx-app/ --dry-run\n\n# 4. Dry-run + Debug - shows rendered templates AND computed values\nhelm install my-nginx my-nginx-app/ --dry-run --debug\n\n# 5. Get rendered templates for a deployed release\nhelm get manifest my-nginx\n\n# 6. Get the computed values for a deployed release\nhelm get values my-nginx\n\n# 7. Get all information about a release\nhelm get all my-nginx\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#13-package-and-host-a-chart-repository","title":"13. Package and Host a Chart Repository","text":"<p>Package the chart and host it in a Git-based chart repository using GitHub Pages.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_12","title":"Scenario:","text":"<p>\u25e6 You want to share your Helm chart with your team or the community.   \u25e6 A Helm chart repository is simply a web server hosting <code>index.yaml</code> and <code>.tgz</code> chart packages.   \u25e6 GitHub Pages is a free and easy way to host a chart repo.</p> <p>Hint: <code>helm package</code>, <code>helm repo index</code>, GitHub Pages</p> Solution <pre><code># \u2500\u2500 Step 1: Package the chart \u2500\u2500\n\nhelm package my-nginx-app/\n# Output: my-nginx-app-0.1.0.tgz\n\n# \u2500\u2500 Step 2: Create a chart repository on GitHub \u2500\u2500\n\n# Create a new GitHub repository (e.g., \"helm-charts\")\n# Clone it locally:\ngit clone https://github.com/&lt;your-username&gt;/helm-charts.git\ncd helm-charts\n\n# Create a docs/ directory (GitHub Pages will serve from here)\nmkdir -p docs\n\n# Move the packaged chart\ncp ../my-nginx-app-0.1.0.tgz docs/\n\n# \u2500\u2500 Step 3: Generate the repository index \u2500\u2500\n\nhelm repo index docs/ --url https://&lt;your-username&gt;.github.io/helm-charts/\n\n# Verify the index\ncat docs/index.yaml\n\n# \u2500\u2500 Step 4: Push to GitHub \u2500\u2500\n\ngit add .\ngit commit -m \"Add my-nginx-app chart\"\ngit push origin main\n\n# \u2500\u2500 Step 5: Enable GitHub Pages \u2500\u2500\n\n# Go to: GitHub repo \u2192 Settings \u2192 Pages\n# Set Source: Deploy from branch \u2192 main \u2192 /docs\n# Save and wait for deployment\n\n# \u2500\u2500 Step 6: Add the repo to Helm \u2500\u2500\n\nhelm repo add my-charts https://&lt;your-username&gt;.github.io/helm-charts/\nhelm repo update\n\n# Verify the chart is available\nhelm search repo my-charts\n\n# \u2500\u2500 Step 7: Install from the repository \u2500\u2500\n\nhelm install my-nginx my-charts/my-nginx-app\n</code></pre> <p>Alternative: Use OCI Registry (Helm 3.8+)</p> <pre><code># Push to an OCI-compatible registry (e.g., GitHub Container Registry)\nhelm push my-nginx-app-0.1.0.tgz oci://ghcr.io/&lt;your-username&gt;/charts\n\n# Install from OCI\nhelm install my-nginx oci://ghcr.io/&lt;your-username&gt;/charts/my-nginx-app --version 0.1.0\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#14-upgrade-rollback-and-release-history","title":"14. Upgrade, Rollback, and Release History","text":"<p>Upgrade a release with new values, inspect its history, and rollback to a previous revision.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_13","title":"Scenario:","text":"<p>\u25e6 You deployed version 1 of your chart, then upgraded to version 2 with bad configuration.   \u25e6 You need to quickly rollback to the known-good state.</p> <p>Hint: <code>helm upgrade</code>, <code>helm history</code>, <code>helm rollback</code></p> Solution <pre><code># 1. Initial install (revision 1)\nhelm install my-nginx my-nginx-app/ \\\n  --set welcomePage.title=\"Version 1\"\n\n# 2. Upgrade to revision 2 (change the title)\nhelm upgrade my-nginx my-nginx-app/ \\\n  --set welcomePage.title=\"Version 2 - BROKEN\"\n\n# 3. Check release history\nhelm history my-nginx\n\n# Output:\n# REVISION  STATUS      DESCRIPTION\n# 1         superseded  Install complete\n# 2         deployed    Upgrade complete\n\n# 4. Rollback to revision 1\nhelm rollback my-nginx 1\n\n# 5. Verify history (now shows 3 revisions)\nhelm history my-nginx\n\n# Output:\n# REVISION  STATUS      DESCRIPTION\n# 1         superseded  Install complete\n# 2         superseded  Upgrade complete\n# 3         deployed    Rollback to 1\n\n# 6. Verify the running app shows \"Version 1\" again\nkubectl port-forward svc/my-nginx-my-nginx-app 8080:80\ncurl http://localhost:8080 | grep \"Version\"\n\n# Cleanup\nhelm uninstall my-nginx\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#15-hooks-pre-install-post-install","title":"15. Hooks (Pre-install / Post-install)","text":"<p>Create Helm hooks that run a Job before and after chart installation.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_14","title":"Scenario:","text":"<p>\u25e6 You need to run a database migration before the app starts, or send a notification after deployment.   \u25e6 Helm hooks let you run resources at specific points in the release lifecycle.</p> <p>Hint: <code>helm.sh/hook</code> annotation, <code>pre-install</code>, <code>post-install</code></p> Solution <p>Step 1: Create a pre-install hook [templates/pre-install-job.yaml]</p> <pre><code>cat &gt; my-nginx-app/templates/pre-install-job.yaml &lt;&lt; 'TEMPLATE'\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"my-nginx-app.fullname\" . }}-pre-install\n  labels:\n    {{- include \"my-nginx-app.labels\" . | nindent 4 }}\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n        - name: pre-install\n          image: busybox\n          command:\n            - sh\n            - -c\n            - |\n              echo \"=== Pre-install hook ===\"\n              echo \"Running pre-flight checks...\"\n              echo \"Release: {{ .Release.Name }}\"\n              echo \"Namespace: {{ .Release.Namespace }}\"\n              echo \"Chart: {{ .Chart.Name }}-{{ .Chart.Version }}\"\n              echo \"Pre-install complete!\"\nTEMPLATE\n</code></pre> <p>Step 2: Create a post-install hook [templates/post-install-job.yaml]</p> <pre><code>cat &gt; my-nginx-app/templates/post-install-job.yaml &lt;&lt; 'TEMPLATE'\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"my-nginx-app.fullname\" . }}-post-install\n  labels:\n    {{- include \"my-nginx-app.labels\" . | nindent 4 }}\n  annotations:\n    \"helm.sh/hook\": post-install,post-upgrade\n    \"helm.sh/hook-weight\": \"5\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n        - name: post-install\n          image: busybox\n          command:\n            - sh\n            - -c\n            - |\n              echo \"=== Post-install hook ===\"\n              echo \"Deployment verified!\"\n              echo \"Release: {{ .Release.Name }}\"\n              echo \"Post-install complete!\"\nTEMPLATE\n</code></pre> <p>Step 3: Deploy and observe hooks</p> <pre><code># Install and watch the hooks execute\nhelm install my-nginx my-nginx-app/\n\n# Check jobs (hook jobs auto-delete on success due to hook-delete-policy)\nkubectl get jobs\n\n# If you want to see the logs, remove the hook-delete-policy temporarily\n# and check:\nkubectl logs job/my-nginx-my-nginx-app-pre-install\nkubectl logs job/my-nginx-my-nginx-app-post-install\n\n# Hook execution order:\n# 1. pre-install Job runs\n# 2. Chart resources are created (Deployment, Service, etc.)\n# 3. post-install Job runs\n\n# Cleanup\nhelm uninstall my-nginx\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#diagram-helm-chart-architecture","title":"Diagram: Helm Chart Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Helm Chart: my-nginx-app                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  Chart.yaml \u2500\u2500\u2500\u2500 name, version, dependencies                 \u2502\n\u2502                                                              \u2502\n\u2502  values.yaml \u2500\u2500\u2500 defaults \u25c4\u2500\u2500 values-dev.yaml               \u2502\n\u2502                             \u25c4\u2500\u2500 values-prod.yaml             \u2502\n\u2502                             \u25c4\u2500\u2500 --set overrides              \u2502\n\u2502                                                              \u2502\n\u2502  templates/ \u2500\u252c\u2500\u2500 deployment.yaml \u2500\u2500\u25ba Deployment              \u2502\n\u2502              \u251c\u2500\u2500 service.yaml \u2500\u2500\u2500\u2500\u2500\u25ba Service (ClusterIP)     \u2502\n\u2502              \u251c\u2500\u2500 service-external \u2500\u25ba Service (ExternalName)  \u2502\n\u2502              \u251c\u2500\u2500 ingress-main \u2500\u2500\u2500\u2500\u2500\u25ba Ingress (path: /)       \u2502\n\u2502              \u251c\u2500\u2500 ingress-status \u2500\u2500\u2500\u25ba Ingress (path: /status) \u2502\n\u2502              \u251c\u2500\u2500 configmap-html \u2500\u2500\u2500\u25ba ConfigMap (welcome page)\u2502\n\u2502              \u251c\u2500\u2500 pre-install-job \u2500\u2500\u25ba Hook (pre-install)      \u2502\n\u2502              \u251c\u2500\u2500 post-install-job \u2500\u25ba Hook (post-install)     \u2502\n\u2502              \u251c\u2500\u2500 _helpers.tpl \u2500\u2500\u2500\u2500\u2500\u25ba Named templates         \u2502\n\u2502              \u2514\u2500\u2500 NOTES.txt \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba Post-install message    \u2502\n\u2502                                                              \u2502\n\u2502  charts/ \u2500\u2500\u2500\u2500\u2500\u2500 redis (subchart dependency)                  \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                  helm package \u2500\u2500\u25ba my-nginx-app-0.1.0.tgz\n                            \u2502\n                  GitHub Pages / OCI Registry\n                            \u2502\n                  helm repo add / helm install\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#quick-reference-essential-helm-commands","title":"Quick Reference: Essential Helm Commands","text":"Command Description <code>helm create &lt;name&gt;</code> Scaffold a new chart <code>helm install &lt;release&gt; &lt;chart&gt;</code> Install a chart <code>helm upgrade &lt;release&gt; &lt;chart&gt;</code> Upgrade a release <code>helm upgrade --install</code> Install or upgrade (idempotent) <code>helm uninstall &lt;release&gt;</code> Remove a release <code>helm list</code> List installed releases <code>helm history &lt;release&gt;</code> Show release revision history <code>helm rollback &lt;release&gt; &lt;rev&gt;</code> Rollback to a previous revision <code>helm template &lt;release&gt; &lt;chart&gt;</code> Render templates locally <code>helm lint &lt;chart&gt;</code> Check chart for errors <code>helm package &lt;chart&gt;</code> Package chart into <code>.tgz</code> <code>helm repo index &lt;dir&gt;</code> Generate repository index <code>helm repo add &lt;name&gt; &lt;url&gt;</code> Add a chart repository <code>helm search repo &lt;keyword&gt;</code> Search charts in added repos <code>helm dependency update &lt;chart&gt;</code> Download chart dependencies <code>helm get values &lt;release&gt;</code> Show computed values for a release <code>helm get manifest &lt;release&gt;</code> Show rendered manifests for a release"},{"location":"Tasks/Kubernetes-Helm-Tasks/#16-use-helm-status-to-inspect-a-release","title":"16. Use <code>helm status</code> to Inspect a Release","text":"<p>Use <code>helm status</code> to view detailed information about a deployed release including resource status and NOTES.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_15","title":"Scenario:","text":"<p>\u25e6 A release was deployed by another team member and you need to understand its current state.   \u25e6 You want to see the NOTES.txt output again without reinstalling.   \u25e6 <code>helm status</code> provides a quick overview of the release deployment status and health.</p> <p>Hint: <code>helm status</code>, <code>--revision</code>, <code>-o yaml</code></p> Solution <pre><code># 1. Install a release first\nhelm install my-nginx my-nginx-app/\n\n# 2. Get the status of the release\nhelm status my-nginx\n\n# Output shows:\n# - Last deployment time\n# - Release status (deployed, failed, pending, etc.)\n# - Deployed resources\n# - NOTES.txt content\n\n# 3. Get status in YAML format\nhelm status my-nginx -o yaml\n\n# 4. Get status in JSON format\nhelm status my-nginx -o json\n\n# 5. Get status of a specific revision\nhelm status my-nginx --revision 1\n\n# 6. Check status from a specific namespace\nhelm status my-nginx --namespace production\n\n# Cleanup\nhelm uninstall my-nginx\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#17-extract-values-with-helm-get-values","title":"17. Extract Values with <code>helm get values</code>","text":"<p>Use <code>helm get values</code> to see what values were actually used for a deployed release.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_16","title":"Scenario:","text":"<p>\u25e6 You deployed a chart months ago with custom values and need to remember what overrides were applied.   \u25e6 Multiple team members have upgraded the release and you want to know the current configuration.   \u25e6 You need to replicate the same configuration in another environment.</p> <p>Hint: <code>helm get values</code>, <code>--all</code>, <code>--revision</code></p> Solution <pre><code># 1. Install with custom values\nhelm install my-nginx my-nginx-app/ \\\n  --set replicaCount=3 \\\n  --set welcomePage.title=\"Production App\"\n\n# 2. Get only the user-supplied values\nhelm get values my-nginx\n\n# Output shows only the overrides:\n# replicaCount: 3\n# welcomePage:\n#   title: Production App\n\n# 3. Get ALL values (including defaults from values.yaml)\nhelm get values my-nginx --all\n\n# 4. Get values from a specific revision\nhelm upgrade my-nginx my-nginx-app/ --set replicaCount=5\nhelm get values my-nginx --revision 1\nhelm get values my-nginx --revision 2\n\n# 5. Output as JSON\nhelm get values my-nginx -o json\n\n# 6. Save values to file for reuse\nhelm get values my-nginx &gt; my-nginx-values.yaml\n\n# 7. Use saved values in another deployment\nhelm install my-nginx-copy my-nginx-app/ -f my-nginx-values.yaml\n\n# Cleanup\nhelm uninstall my-nginx my-nginx-copy\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#18-show-chart-values-with-helm-show-values","title":"18. Show Chart Values with <code>helm show values</code>","text":"<p>Use <code>helm show values</code> to inspect the default values of a chart before installing.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_17","title":"Scenario:","text":"<p>\u25e6 You want to install a third-party chart from a repository but need to understand what configuration options are available.   \u25e6 You\u2019re evaluating multiple charts and want to compare their configuration interfaces.   \u25e6 You need to create a custom values file but want to start from the defaults.</p> <p>Hint: <code>helm show values</code>, chart repositories</p> Solution <pre><code># 1. Show default values of a local chart\nhelm show values my-nginx-app/\n\n# 2. Show values from a packaged chart\nhelm package my-nginx-app/\nhelm show values my-nginx-app-0.1.0.tgz\n\n# 3. Add a public chart repository\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n\n# 4. Show default values from a repository chart\nhelm show values bitnami/nginx\n\n# 5. Show values at a specific chart version\nhelm show values bitnami/nginx --version 15.0.0\n\n# 6. Save default values to file for customization\nhelm show values bitnami/nginx &gt; nginx-defaults.yaml\n\n# 7. Compare values between chart versions\nhelm show values bitnami/nginx --version 14.0.0 &gt; nginx-v14-values.yaml\nhelm show values bitnami/nginx --version 15.0.0 &gt; nginx-v15-values.yaml\ndiff nginx-v14-values.yaml nginx-v15-values.yaml\n\n# 8. Show all chart information (Chart.yaml + README + values)\nhelm show all bitnami/nginx\nhelm show chart bitnami/nginx\nhelm show readme bitnami/nginx\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#19-search-charts-with-helm-search-repo","title":"19. Search Charts with <code>helm search repo</code>","text":"<p>Use <code>helm search repo</code> to find charts in added repositories.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_18","title":"Scenario:","text":"<p>\u25e6 You need to deploy PostgreSQL but don\u2019t want to write manifests from scratch.   \u25e6 You want to find and compare available charts for a specific technology.   \u25e6 You need to discover what version of a chart is available.</p> <p>Hint: <code>helm search repo</code>, <code>--versions</code>, <code>--version</code></p> Solution <pre><code># 1. Add popular chart repositories\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo add stable https://charts.helm.sh/stable\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n\n# 2. Update repository index\nhelm repo update\n\n# 3. Search for charts by keyword\nhelm search repo nginx\n\n# 4. Search showing all available versions\nhelm search repo nginx --versions\n\n# 5. Search with version constraint\nhelm search repo nginx --version \"~15.0\"\n\n# 6. Search for development/pre-release versions\nhelm search repo nginx --devel\n\n# 7. Search with regex pattern\nhelm search repo 'nginx.*'\n\n# 8. Search across all repositories with output formatting\nhelm search repo postgresql -o json\nhelm search repo postgresql -o yaml\n\n# 9. Search and filter with grep\nhelm search repo database | grep -i postgres\n\n# 10. List all charts from a specific repository\nhelm search repo bitnami/\n\n# 11. Show detailed output\nhelm search repo nginx --max-col-width 0\n\n# 12. Install a found chart\nhelm search repo bitnami/redis --versions | head -5\nhelm install my-redis bitnami/redis --version 18.0.0\nhelm uninstall my-redis\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#20-update-repositories-with-helm-repo-update","title":"20. Update Repositories with <code>helm repo update</code>","text":"<p>Use <code>helm repo update</code> to refresh the local cache of chart information.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_19","title":"Scenario:","text":"<p>\u25e6 A new version of a chart was released but <code>helm search</code> doesn\u2019t show it.   \u25e6 You haven\u2019t updated your repository index in weeks and want the latest charts.   \u25e6 Similar to <code>apt update</code> or <code>yum update</code>, you need to sync the latest metadata.</p> <p>Hint: <code>helm repo update</code>, <code>helm repo list</code></p> Solution <pre><code># 1. List all configured repositories\nhelm repo list\n\n# 2. Update all repositories\nhelm repo update\n\n# Output shows each repository being refreshed:\n# Hang tight while we grab the latest from your chart repositories...\n# ...Successfully got an update from the \"bitnami\" chart repository\n# ...Successfully got an update from the \"stable\" chart repository\n# Update Complete.\n\n# 3. Update a specific repository\nhelm repo update bitnami\n\n# 4. Update multiple specific repositories\nhelm repo update bitnami stable\n\n# 5. Force update even if repository fails\nhelm repo update --fail-on-repo-update-fail=false\n\n# 6. Verify you can now see newer chart versions\nhelm search repo bitnami/nginx --versions | head -5\n\n# 7. Typical workflow: update before searching or installing\nhelm repo update\nhelm search repo redis\nhelm install my-redis bitnami/redis\n\n# Cleanup\nhelm uninstall my-redis\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#21-run-chart-tests-with-helm-test","title":"21. Run Chart Tests with <code>helm test</code>","text":"<p>Use <code>helm test</code> to run tests defined in the chart\u2019s <code>templates/tests/</code> directory.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_20","title":"Scenario:","text":"<p>\u25e6 You deployed a release and want to verify it\u2019s actually working correctly.   \u25e6 The chart includes test pods that validate connectivity, configuration, or functionality.   \u25e6 You want to include release validation in your CI/CD pipeline.</p> <p>Hint: <code>helm test</code>, <code>templates/tests/</code>, <code>helm.sh/hook: test</code></p> Solution <pre><code># 1. Create a test template if not already present\ncat &gt; my-nginx-app/templates/tests/test-connection.yaml &lt;&lt; 'TEMPLATE'\napiVersion: v1\nkind: Pod\nmetadata:\n  name: {{ include \"my-nginx-app.fullname\" . }}-test-connection\n  labels:\n    {{- include \"my-nginx-app.labels\" . | nindent 4 }}\n  annotations:\n    \"helm.sh/hook\": test\nspec:\n  containers:\n    - name: wget\n      image: busybox\n      command: ['wget']\n      args: ['{{ include \"my-nginx-app.fullname\" . }}:{{ .Values.service.port }}']\n  restartPolicy: Never\nTEMPLATE\n\n# 2. Install the release\nhelm install my-nginx my-nginx-app/\n\n# 3. Run the tests\nhelm test my-nginx\n\n# Output shows:\n# NAME: my-nginx\n# ...\n# Phase: Succeeded\n\n# 4. Run tests with logs displayed\nhelm test my-nginx --logs\n\n# 5. Run tests with timeout\nhelm test my-nginx --timeout 2m\n\n# 6. View test pod logs manually\nkubectl logs my-nginx-my-nginx-app-test-connection\n\n# 7. Filter which tests to run (if multiple tests exist)\nhelm test my-nginx --filter name=test-connection\n\n# 8. Clean up test pods after running (by default they remain)\nkubectl delete pod -l 'helm.sh/hook=test'\n\n# Or configure it in the test template with:\n# \"helm.sh/hook-delete-policy\": \"hook-succeeded,hook-failed\"\n\n# Cleanup\nhelm uninstall my-nginx\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#22-use-helm-get-all-to-retrieve-complete-release-info","title":"22. Use <code>helm get all</code> to Retrieve Complete Release Info","text":"<p>Use <code>helm get all</code> to retrieve all information about a deployed release in one command.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_21","title":"Scenario:","text":"<p>\u25e6 You need to debug why a release isn\u2019t working correctly.   \u25e6 You want to export the complete release configuration for documentation or backup.   \u25e6 You need to see the rendered manifests, computed values, and hooks all together.</p> <p>Hint: <code>helm get all</code>, <code>helm get manifest</code>, <code>helm get hooks</code></p> Solution <pre><code># 1. Install a release\nhelm install my-nginx my-nginx-app/ \\\n  --set replicaCount=2 \\\n  --set welcomePage.title=\"Debug Test\"\n\n# 2. Get all information about the release\nhelm get all my-nginx\n\n# Output includes:\n# - Release metadata\n# - User-supplied values\n# - Computed values\n# - Rendered Kubernetes manifests\n# - Hooks\n# - Notes\n\n# 3. Get all info from specific revision\nhelm upgrade my-nginx my-nginx-app/ --set replicaCount=3\nhelm get all my-nginx --revision 1\nhelm get all my-nginx --revision 2\n\n# 4. Get individual components\nhelm get manifest my-nginx       # Just the rendered manifests\nhelm get values my-nginx         # Just the user values\nhelm get hooks my-nginx          # Just the hooks\nhelm get notes my-nginx          # Just the NOTES.txt\n\n# 5. Export to file for backup/documentation\nhelm get all my-nginx &gt; my-nginx-release-backup.yaml\n\n# 6. Use template to extract specific information\nhelm get all my-nginx --template '{{.Release.Manifest}}'\n\n# 7. Compare two revisions\nhelm get all my-nginx --revision 1 &gt; rev1.yaml\nhelm get all my-nginx --revision 2 &gt; rev2.yaml\ndiff rev1.yaml rev2.yaml\n\n# Cleanup\nhelm uninstall my-nginx\nrm -f rev1.yaml rev2.yaml my-nginx-release-backup.yaml\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#23-use-helm-list-with-filters-and-formatting","title":"23. Use <code>helm list</code> with Filters and Formatting","text":"<p>Master <code>helm list</code> with various filters and output formats to manage multiple releases.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_22","title":"Scenario:","text":"<p>\u25e6 You have dozens of releases across multiple namespaces and need to find specific ones.   \u25e6 You want to script release management and need machine-readable output.   \u25e6 You need to filter releases by status (deployed, failed, pending).</p> <p>Hint: <code>helm list</code>, <code>--all-namespaces</code>, <code>--filter</code>, <code>-o json</code></p> Solution <pre><code># 1. Install multiple releases for testing\nhelm install nginx-dev my-nginx-app/ --set replicaCount=1\nhelm install nginx-staging my-nginx-app/ --set replicaCount=2\nhelm install nginx-prod my-nginx-app/ --set replicaCount=3 --namespace prod --create-namespace\n\n# 2. List all releases in current namespace\nhelm list\n\n# 3. List releases across all namespaces\nhelm list --all-namespaces\n\n# 4. List releases in specific namespace\nhelm list --namespace prod\n\n# 5. Filter releases by name pattern\nhelm list --filter 'nginx-.*'\nhelm list --filter 'nginx-dev'\n\n# 6. Show only deployed releases\nhelm list --deployed\n\n# 7. Show all releases including uninstalled (with --keep-history)\nhelm list --all\n\n# 8. Show failed releases\nhelm list --failed\n\n# 9. Show pending releases\nhelm list --pending\n\n# 10. Output as JSON (for scripting)\nhelm list -o json\n\n# 11. Output as YAML\nhelm list -o yaml\n\n# 12. Show extended information\nhelm list --all-namespaces -o wide\n\n# 13. Limit number of results\nhelm list --max 5\n\n# 14. Sort by date\nhelm list --date\n\n# 15. Reverse sort order\nhelm list --reverse\n\n# 16. Show specific columns only (use with jq for JSON output)\nhelm list -o json | jq '.[] | {name: .name, status: .status, namespace: .namespace}'\n\n# 17. Count releases\nhelm list --all-namespaces | wc -l\n\n# 18. Find releases using specific chart\nhelm list --all-namespaces -o json | jq '.[] | select(.chart | contains(\"my-nginx-app\"))'\n\n# Cleanup\nhelm uninstall nginx-dev nginx-staging\nhelm uninstall nginx-prod -n prod\nkubectl delete namespace prod\n</code></pre>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#24-chain-multiple-commands-for-release-management","title":"24. Chain Multiple Commands for Release Management","text":"<p>Practice chaining Helm commands for common workflows and debugging scenarios.</p>"},{"location":"Tasks/Kubernetes-Helm-Tasks/#scenario_23","title":"Scenario:","text":"<p>\u25e6 You need to quickly deploy, verify, and troubleshoot releases in rapid iteration cycles.   \u25e6 You want to create reusable scripts for release management.   \u25e6 You need to validate deployments in CI/CD pipelines.</p> <p>Hint: Combine <code>install</code>, <code>status</code>, <code>get values</code>, <code>test</code>, <code>upgrade</code>, <code>rollback</code></p> Solution <pre><code># \u2500\u2500 Workflow 1: Install, verify, test \u2500\u2500\nhelm install my-nginx my-nginx-app/ &amp;&amp; \\\n  helm status my-nginx &amp;&amp; \\\n  helm test my-nginx\n\n# \u2500\u2500 Workflow 2: Dry-run, lint, then install \u2500\u2500\nhelm lint my-nginx-app/ &amp;&amp; \\\n  helm install my-nginx my-nginx-app/ --dry-run --debug &amp;&amp; \\\n  helm install my-nginx my-nginx-app/\n\n# \u2500\u2500 Workflow 3: Template, validate, install \u2500\u2500\nhelm template my-nginx my-nginx-app/ | kubectl apply --dry-run=client -f - &amp;&amp; \\\n  helm install my-nginx my-nginx-app/\n\n# \u2500\u2500 Workflow 4: Upgrade or install (idempotent) \u2500\u2500\nhelm upgrade --install my-nginx my-nginx-app/ --wait --timeout 5m\n\n# \u2500\u2500 Workflow 5: Upgrade with backup and rollback on failure \u2500\u2500\nhelm get values my-nginx &gt; backup-values.yaml &amp;&amp; \\\n  helm upgrade my-nginx my-nginx-app/ --set replicaCount=5 --atomic\n\n# \u2500\u2500 Workflow 6: Install, check status, get all info \u2500\u2500\nhelm install my-nginx my-nginx-app/ &amp;&amp; \\\n  sleep 10 &amp;&amp; \\\n  helm status my-nginx &amp;&amp; \\\n  helm get all my-nginx\n\n# \u2500\u2500 Workflow 7: Compare before and after upgrade \u2500\u2500\nhelm get values my-nginx &gt; before.yaml &amp;&amp; \\\n  helm upgrade my-nginx my-nginx-app/ --set newKey=newValue &amp;&amp; \\\n  helm get values my-nginx &gt; after.yaml &amp;&amp; \\\n  diff before.yaml after.yaml\n\n# \u2500\u2500 Workflow 8: Install with custom values and verify \u2500\u2500\ncat &gt; custom.yaml &lt;&lt; EOF\nreplicaCount: 3\nwelcomePage:\n  title: \"Production\"\nEOF\nhelm install my-nginx my-nginx-app/ -f custom.yaml &amp;&amp; \\\n  kubectl get pods -l app.kubernetes.io/instance=my-nginx\n\n# \u2500\u2500 Workflow 9: Rollback if tests fail \u2500\u2500\nhelm upgrade my-nginx my-nginx-app/ --set replicaCount=10 &amp;&amp; \\\n  helm test my-nginx || helm rollback my-nginx\n\n# \u2500\u2500 Workflow 10: Clean reinstall \u2500\u2500\nhelm uninstall my-nginx 2&gt;/dev/null || true &amp;&amp; \\\n  helm install my-nginx my-nginx-app/ --wait\n\n# \u2500\u2500 Workflow 11: Multi-environment deployment \u2500\u2500\nfor env in dev staging prod; do\n  helm upgrade --install my-nginx-$env my-nginx-app/ \\\n    -f values-$env.yaml \\\n    --namespace $env --create-namespace\ndone\n\n# List all deployments\nhelm list --all-namespaces\n\n# \u2500\u2500 Workflow 12: Debugging failed release \u2500\u2500\nhelm status my-nginx &amp;&amp; \\\n  helm get values my-nginx --all &amp;&amp; \\\n  helm get manifest my-nginx | kubectl apply --dry-run=client -f - &amp;&amp; \\\n  kubectl describe pods -l app.kubernetes.io/instance=my-nginx\n\n# Cleanup all\nfor env in dev staging prod; do\n  helm uninstall my-nginx-$env -n $env 2&gt;/dev/null || true\n  kubectl delete namespace $env 2&gt;/dev/null || true\ndone\nhelm uninstall my-nginx 2&gt;/dev/null || true\nrm -f backup-values.yaml before.yaml after.yaml custom.yaml\n</code></pre>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/","title":"Kubernetes KEDA Tasks","text":"<ul> <li>Hands-on Kubernetes exercises covering KEDA (Kubernetes Event-Driven Autoscaling) installation, ScaledObjects, ScaledJobs, TriggerAuthentication, and real-world autoscaling patterns.</li> <li>Each task includes a description, scenario, and a detailed solution with step-by-step instructions.</li> <li>Practice these tasks to master event-driven autoscaling with KEDA.</li> </ul>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#table-of-contents","title":"Table of Contents","text":"<ul> <li>01. Install KEDA via Helm</li> <li>02. Create a CPU-Based ScaledObject</li> <li>03. Scale to Zero with Redis Queue</li> <li>04. Schedule Scaling with the Cron Trigger</li> <li>05. Use TriggerAuthentication with Secrets</li> <li>06. Combine Multiple Triggers</li> <li>07. Create a ScaledJob for Batch Processing</li> <li>08. Tune Scale-Up and Scale-Down Behavior</li> <li>09. Pause and Resume a ScaledObject</li> <li>10. Troubleshoot a Non-Scaling ScaledObject</li> </ul>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#01-install-keda-via-helm","title":"01. Install KEDA via Helm","text":"<p>Install KEDA on a Kubernetes cluster using the official Helm chart and verify all components.</p>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#scenario","title":"Scenario:","text":"<p>\u25e6 Your team wants to adopt event-driven autoscaling for queue-based workers.   \u25e6 KEDA extends the native HPA with 60+ event source scalers.</p> <p>Hint: <code>helm repo add kedacore</code>, <code>helm upgrade --install keda</code></p> Solution <pre><code># 1. Add the KEDA Helm repository\nhelm repo add kedacore https://kedacore.github.io/charts\nhelm repo update kedacore\n\n# 2. Install KEDA\nhelm upgrade --install keda kedacore/keda \\\n    --namespace keda \\\n    --create-namespace \\\n    --wait\n\n# 3. Verify pods are running\nkubectl get pods -n keda\n# keda-admission-webhooks-xxxx     1/1     Running\n# keda-operator-xxxx               1/1     Running\n# keda-operator-metrics-apiserver   1/1     Running\n\n# 4. Verify CRDs are registered\nkubectl get crd | grep keda\n# scaledobjects.keda.sh\n# scaledjobs.keda.sh\n# triggerauthentications.keda.sh\n# clustertriggerauthentications.keda.sh\n\n# 5. Verify the metrics API\nkubectl get apiservice | grep keda\n</code></pre>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#02-create-a-cpu-based-scaledobject","title":"02. Create a CPU-Based ScaledObject","text":"<p>Create a ScaledObject that scales a Deployment based on CPU utilization (threshold: 60%).</p>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#scenario_1","title":"Scenario:","text":"<p>\u25e6 You want to replace your existing HPA with KEDA to later add queue-based triggers.   \u25e6 The CPU scaler works identically to HPA but can be combined with other KEDA scalers.</p> <p>Hint: Use <code>type: cpu</code> with <code>metadata.type: Utilization</code> and <code>metadata.value: \"60\"</code>.</p> Solution <pre><code># 1. Create a namespace and deployment\nkubectl create namespace keda-tasks\nkubectl create deployment nginx-demo \\\n    --image=nginx:1.25 \\\n    --replicas=1 \\\n    --namespace=keda-tasks\nkubectl set resources deployment nginx-demo \\\n    --requests=cpu=50m,memory=64Mi \\\n    --namespace=keda-tasks\n\n# 2. Apply the ScaledObject\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cpu-scaler\n  namespace: keda-tasks\nspec:\n  scaleTargetRef:\n    name: nginx-demo\n  minReplicaCount: 1\n  maxReplicaCount: 10\n  triggers:\n    - type: cpu\n      metadata:\n        type: Utilization\n        value: \"60\"\nEOF\n\n# 3. Verify KEDA created an HPA\nkubectl get hpa -n keda-tasks\nkubectl get scaledobject -n keda-tasks\n\n# Cleanup\nkubectl delete namespace keda-tasks\n</code></pre>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#03-scale-to-zero-with-redis-queue","title":"03. Scale to Zero with Redis Queue","text":"<p>Deploy a Redis-backed worker that scales from 0 to N based on queue depth, and back to 0 when empty.</p>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#scenario_2","title":"Scenario:","text":"<p>\u25e6 Idle workers waste resources. You want pods only when there\u2019s work.   \u25e6 KEDA monitors the Redis list length and scales workers accordingly.</p> <p>Hint: Set <code>minReplicaCount: 0</code> and use the <code>redis</code> scaler with <code>listName</code> and <code>listLength</code>.</p> Solution <pre><code># 1. Create namespace and deploy Redis\nkubectl create namespace keda-tasks\nkubectl create deployment redis --image=redis:7-alpine -n keda-tasks\nkubectl expose deployment redis --port=6379 -n keda-tasks\n\n# 2. Create a worker deployment (starting at 0)\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: queue-worker\n  namespace: keda-tasks\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      app: queue-worker\n  template:\n    metadata:\n      labels:\n        app: queue-worker\n    spec:\n      containers:\n      - name: worker\n        image: redis:7-alpine\n        command: [\"/bin/sh\", \"-c\"]\n        args:\n          - |\n            while true; do\n              JOB=$(redis-cli -h redis LPOP work:queue)\n              if [ -n \"$JOB\" ]; then echo \"Processing: $JOB\"; sleep 2\n              else sleep 1; fi\n            done\nEOF\n\n# 3. Create the ScaledObject\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: queue-scaler\n  namespace: keda-tasks\nspec:\n  scaleTargetRef:\n    name: queue-worker\n  minReplicaCount: 0\n  maxReplicaCount: 10\n  cooldownPeriod: 30\n  pollingInterval: 5\n  triggers:\n    - type: redis\n      metadata:\n        address: redis.keda-tasks.svc:6379\n        listName: work:queue\n        listLength: \"5\"\nEOF\n\n# 4. Verify 0 pods\nkubectl get pods -n keda-tasks -l app=queue-worker\n\n# 5. Push jobs and watch scale-up\nkubectl exec deployment/redis -n keda-tasks -- \\\n    redis-cli RPUSH work:queue j1 j2 j3 j4 j5 j6 j7 j8 j9 j10 j11 j12 j13 j14 j15\nkubectl get pods -n keda-tasks -l app=queue-worker -w\n\n# Cleanup\nkubectl delete namespace keda-tasks\n</code></pre>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#04-schedule-scaling-with-the-cron-trigger","title":"04. Schedule Scaling with the Cron Trigger","text":"<p>Create a ScaledObject that scales to 5 replicas during business hours (Mon\u2013Fri, 08:00\u201318:00).</p>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#scenario_3","title":"Scenario:","text":"<p>\u25e6 Your API needs pre-warmed capacity every weekday morning.   \u25e6 The Cron scaler provides time-based replica scheduling.</p> <p>Hint: Use <code>type: cron</code> with <code>start</code>, <code>end</code>, <code>timezone</code>, and <code>desiredReplicas</code>.</p> Solution <pre><code>cat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: cron-scaler\n  namespace: keda-tasks\nspec:\n  scaleTargetRef:\n    name: nginx-demo\n  minReplicaCount: 1\n  maxReplicaCount: 10\n  triggers:\n    - type: cron\n      metadata:\n        timezone: \"UTC\"\n        start: \"0 8 * * 1-5\"\n        end: \"0 18 * * 1-5\"\n        desiredReplicas: \"5\"\nEOF\n\n# Check the ScaledObject\nkubectl describe scaledobject cron-scaler -n keda-tasks\n</code></pre>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#05-use-triggerauthentication-with-secrets","title":"05. Use TriggerAuthentication with Secrets","text":"<p>Create a TriggerAuthentication backed by a Kubernetes Secret and reference it in a ScaledObject.</p>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#scenario_4","title":"Scenario:","text":"<p>\u25e6 Your Redis requires authentication and you don\u2019t want the password in the ScaledObject.   \u25e6 TriggerAuthentication separates credentials from scaling configuration.</p> <p>Hint: Create a Secret, create a TriggerAuthentication with <code>secretTargetRef</code>, then use <code>authenticationRef</code> in the ScaledObject.</p> Solution <pre><code># 1. Create the Secret\nkubectl create secret generic redis-creds \\\n    --namespace keda-tasks \\\n    --from-literal=password='s3cret'\n\n# 2. Create TriggerAuthentication\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: keda.sh/v1alpha1\nkind: TriggerAuthentication\nmetadata:\n  name: redis-auth\n  namespace: keda-tasks\nspec:\n  secretTargetRef:\n    - parameter: password\n      name: redis-creds\n      key: password\nEOF\n\n# 3. Reference it in a ScaledObject\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: auth-scaler\n  namespace: keda-tasks\nspec:\n  scaleTargetRef:\n    name: queue-worker\n  minReplicaCount: 0\n  maxReplicaCount: 10\n  triggers:\n    - type: redis\n      authenticationRef:\n        name: redis-auth\n      metadata:\n        address: redis:6379\n        listName: secure:queue\n        listLength: \"5\"\nEOF\n\n# 4. Verify\nkubectl get triggerauthentication -n keda-tasks\nkubectl describe scaledobject auth-scaler -n keda-tasks\n</code></pre>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#06-combine-multiple-triggers","title":"06. Combine Multiple Triggers","text":"<p>Create a ScaledObject with both a Cron trigger and a CPU trigger in a single resource.</p>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#scenario_5","title":"Scenario:","text":"<p>\u25e6 You need a baseline of 3 pods during work hours, but CPU-driven bursting beyond that.   \u25e6 KEDA evaluates all triggers and uses the maximum demanded replicas.</p> <p>Hint: Add multiple entries in the <code>triggers</code> list.</p> Solution <pre><code>cat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: multi-trigger\n  namespace: keda-tasks\nspec:\n  scaleTargetRef:\n    name: nginx-demo\n  minReplicaCount: 1\n  maxReplicaCount: 15\n  triggers:\n    - type: cron\n      metadata:\n        timezone: \"UTC\"\n        start: \"0 8 * * 1-5\"\n        end: \"0 18 * * 1-5\"\n        desiredReplicas: \"3\"\n    - type: cpu\n      metadata:\n        type: Utilization\n        value: \"60\"\nEOF\n\n# KEDA uses whichever trigger demands MORE replicas\nkubectl get hpa -n keda-tasks\nkubectl describe scaledobject multi-trigger -n keda-tasks\n</code></pre>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#07-create-a-scaledjob-for-batch-processing","title":"07. Create a ScaledJob for Batch Processing","text":"<p>Create a ScaledJob that spawns one Job per batch of 5 items in a Redis list.</p>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#scenario_6","title":"Scenario:","text":"<p>\u25e6 Each batch task (e.g., video transcoding, report generation) runs as a short-lived Job.   \u25e6 ScaledJob creates new Jobs (not replica scaling) - one per event batch.</p> <p>Hint: Use <code>kind: ScaledJob</code> with <code>jobTargetRef</code> instead of <code>scaleTargetRef</code>.</p> Solution <pre><code>cat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: keda.sh/v1alpha1\nkind: ScaledJob\nmetadata:\n  name: batch-job\n  namespace: keda-tasks\nspec:\n  jobTargetRef:\n    parallelism: 1\n    completions: 1\n    backoffLimit: 2\n    template:\n      spec:\n        restartPolicy: Never\n        containers:\n        - name: processor\n          image: redis:7-alpine\n          command: [\"/bin/sh\", \"-c\"]\n          args:\n            - |\n              for i in $(seq 1 5); do\n                JOB=$(redis-cli -h redis LPOP batch:queue)\n                [ -n \"$JOB\" ] &amp;&amp; echo \"Processing: $JOB\" &amp;&amp; sleep 1\n              done\n  minReplicaCount: 0\n  maxReplicaCount: 20\n  pollingInterval: 10\n  successfulJobsHistoryLimit: 5\n  failedJobsHistoryLimit: 3\n  triggers:\n    - type: redis\n      metadata:\n        address: redis:6379\n        listName: batch:queue\n        listLength: \"5\"\nEOF\n\n# Push items\nkubectl exec deployment/redis -n keda-tasks -- \\\n    redis-cli RPUSH batch:queue b1 b2 b3 b4 b5 b6 b7 b8 b9 b10\n\n# Watch Jobs\nkubectl get jobs -n keda-tasks -w\nkubectl get scaledjob -n keda-tasks\n</code></pre>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#08-tune-scale-up-and-scale-down-behavior","title":"08. Tune Scale-Up and Scale-Down Behavior","text":"<p>Configure a ScaledObject with custom HPA behavior: fast scale-up, slow scale-down with a 2-minute stabilization window.</p>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#scenario_7","title":"Scenario:","text":"<p>\u25e6 Your service is latency-sensitive - scale up fast, but avoid flapping by scaling down slowly.   \u25e6 KEDA supports the same <code>behavior</code> config as native HPA.</p> <p>Hint: Use <code>spec.advanced.horizontalPodAutoscalerConfig.behavior</code>.</p> Solution <pre><code>cat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: tuned-scaler\n  namespace: keda-tasks\nspec:\n  scaleTargetRef:\n    name: nginx-demo\n  minReplicaCount: 1\n  maxReplicaCount: 20\n  advanced:\n    horizontalPodAutoscalerConfig:\n      behavior:\n        scaleUp:\n          stabilizationWindowSeconds: 0\n          policies:\n            - type: Pods\n              value: 4\n              periodSeconds: 15\n        scaleDown:\n          stabilizationWindowSeconds: 120\n          policies:\n            - type: Pods\n              value: 1\n              periodSeconds: 60\n  triggers:\n    - type: cpu\n      metadata:\n        type: Utilization\n        value: \"60\"\nEOF\n\nkubectl describe hpa -n keda-tasks\n</code></pre>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#09-pause-and-resume-a-scaledobject","title":"09. Pause and Resume a ScaledObject","text":"<p>Temporarily pause KEDA scaling at a fixed replica count, then resume.</p>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#scenario_8","title":"Scenario:","text":"<p>\u25e6 You\u2019re performing maintenance on the metric source (e.g., Redis migration).   \u25e6 You need to freeze replicas at the current count without deleting the ScaledObject.</p> <p>Hint: Use the <code>autoscaling.keda.sh/paused-replicas</code> annotation.</p> Solution <pre><code># 1. Pause at 3 replicas\nkubectl annotate scaledobject cpu-scaler \\\n    -n keda-tasks \\\n    autoscaling.keda.sh/paused-replicas=\"3\"\n\n# 2. Verify paused\nkubectl get scaledobject cpu-scaler -n keda-tasks -o yaml | grep -A2 annotations\nkubectl get deployment nginx-demo -n keda-tasks\n\n# 3. Resume\nkubectl annotate scaledobject cpu-scaler \\\n    -n keda-tasks \\\n    autoscaling.keda.sh/paused-replicas-\n\n# 4. Verify resumed\nkubectl describe scaledobject cpu-scaler -n keda-tasks\n</code></pre>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#10-troubleshoot-a-non-scaling-scaledobject","title":"10. Troubleshoot a Non-Scaling ScaledObject","text":"<p>Diagnose why a ScaledObject isn\u2019t scaling and fix the issue.</p>"},{"location":"Tasks/Kubernetes-KEDA-Tasks/#scenario_9","title":"Scenario:","text":"<p>\u25e6 A ScaledObject was applied but the Deployment stays at its initial replica count.   \u25e6 You need to check status conditions, KEDA operator logs, and the managed HPA.</p> <p>Hint: <code>kubectl describe scaledobject</code>, <code>kubectl logs -n keda</code>, <code>kubectl get hpa</code>.</p> Solution <pre><code># 1. Check ScaledObject status\nkubectl describe scaledobject &lt;name&gt; -n &lt;namespace&gt;\n# Look for:\n#   Ready: True/False\n#   Active: True/False\n#   External Metric Names\n\n# 2. Check the KEDA-managed HPA\nkubectl get hpa -n &lt;namespace&gt;\nkubectl describe hpa keda-hpa-&lt;name&gt; -n &lt;namespace&gt;\n\n# 3. Check KEDA operator logs for errors\nkubectl logs -n keda -l app=keda-operator --tail=100\n\n# 4. Common issues:\n# - Wrong address/host for the scaler \u2192 fix metadata.address\n# - Missing TriggerAuthentication \u2192 create one or fix the reference\n# - ScaledObject targeting wrong Deployment name \u2192 fix scaleTargetRef.name\n# - CRD validation error \u2192 check Events section\n\n# 5. Verify metric source connectivity\nkubectl run debug --rm -it --image=busybox -n &lt;namespace&gt; --restart=Never \\\n    -- sh -c \"nc -zv redis.keda-tasks.svc 6379\"\n\n# 6. Check if metrics are being exposed\nkubectl get --raw \"/apis/external.metrics.k8s.io/v1beta1\" | jq '.resources[].name'\n</code></pre>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/","title":"Kubernetes Kubebuilder Tasks","text":"<ul> <li>Hands-on Kubernetes exercises covering Kubebuilder operator development, CRD creation, reconciliation loops, webhooks, and testing.</li> <li>Each task includes a description, scenario, and a detailed solution with step-by-step instructions.</li> <li>Practice these tasks to master building production-grade Kubernetes operators.</li> </ul>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#table-of-contents","title":"Table of Contents","text":"<ul> <li>01. Initialize a Kubebuilder Project</li> <li>02. Create a CRD API and Controller</li> <li>03. Define CRD Types with Validation Markers</li> <li>04. Generate and Install CRDs</li> <li>05. Implement a Basic Reconciler</li> <li>06. Run the Controller Locally</li> <li>07. Add Owner References for Garbage Collection</li> <li>08. Update Status Subresource</li> <li>09. Add a Finalizer</li> <li>10. Write a Controller Test with envtest</li> </ul>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#01-initialize-a-kubebuilder-project","title":"01. Initialize a Kubebuilder Project","text":"<p>Scaffold a new operator project using <code>kubebuilder init</code> and explore the generated files.</p>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#scenario","title":"Scenario:","text":"<p>\u25e6 You\u2019re starting a new operator project and need the project skeleton.   \u25e6 <code>kubebuilder init</code> creates the Makefile, Go module, and base Kustomize configs.</p> <p>Hint: <code>kubebuilder init --domain &lt;domain&gt; --repo &lt;module&gt;</code></p> Solution <pre><code># 1. Create and enter project directory\nmkdir my-operator &amp;&amp; cd my-operator\n\n# 2. Initialize the project\nkubebuilder init \\\n    --domain example.com \\\n    --repo example.com/my-operator\n\n# 3. Explore generated files\nls -la\ncat go.mod\ncat Makefile | head -30\ncat cmd/main.go | head -20\n\n# 4. View available Make targets\nmake help\n\n# Cleanup (if needed)\ncd .. &amp;&amp; rm -rf my-operator\n</code></pre>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#02-create-a-crd-api-and-controller","title":"02. Create a CRD API and Controller","text":"<p>Use <code>kubebuilder create api</code> to scaffold a new CRD type and its controller.</p>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#scenario_1","title":"Scenario:","text":"<p>\u25e6 You need a custom resource called <code>MyApp</code> in the <code>apps</code> group.   \u25e6 Kubebuilder scaffolds both the Go type and the controller stub.</p> <p>Hint: <code>kubebuilder create api --group apps --version v1 --kind MyApp</code></p> Solution <pre><code># 1. Create the API (answer y to both prompts)\nkubebuilder create api \\\n    --group apps \\\n    --version v1 \\\n    --kind MyApp\n\n# 2. Inspect the generated type\ncat api/v1/myapp_types.go\n\n# 3. Inspect the generated controller\ncat internal/controller/myapp_controller.go\n\n# 4. Check that main.go was updated\ngrep MyApp cmd/main.go\n</code></pre>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#03-define-crd-types-with-validation-markers","title":"03. Define CRD Types with Validation Markers","text":"<p>Add fields to the CRD spec with Kubebuilder validation markers for min/max, enums, and defaults.</p>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#scenario_2","title":"Scenario:","text":"<p>\u25e6 Your CRD needs a <code>replicas</code> field (1\u201310, default 1) and a <code>tier</code> field (enum: basic/premium).   \u25e6 Markers auto-generate OpenAPI v3 validation in the CRD YAML.</p> <p>Hint: Use <code>//+kubebuilder:validation:Minimum=1</code>, <code>//+kubebuilder:default=1</code>, <code>//+kubebuilder:validation:Enum=basic;premium</code>.</p> Solution <pre><code>// Edit api/v1/myapp_types.go - replace MyAppSpec:\n\ntype MyAppSpec struct {\n    // Replicas is the desired number of pods.\n    // +kubebuilder:validation:Minimum=1\n    // +kubebuilder:validation:Maximum=10\n    // +kubebuilder:default=1\n    Replicas int32 `json:\"replicas,omitempty\"`\n\n    // Tier is the service tier.\n    // +kubebuilder:validation:Enum=basic;premium\n    // +kubebuilder:default=basic\n    Tier string `json:\"tier,omitempty\"`\n\n    // Message is displayed by the application.\n    // +kubebuilder:validation:MinLength=1\n    // +kubebuilder:validation:MaxLength=200\n    Message string `json:\"message\"`\n}\n</code></pre> <pre><code># Regenerate deepcopy functions\nmake generate\n\n# Regenerate CRD YAML with validation\nmake manifests\n\n# Inspect the generated CRD\ncat config/crd/bases/*.yaml | grep -A20 \"properties:\"\n</code></pre>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#04-generate-and-install-crds","title":"04. Generate and Install CRDs","text":"<p>Run <code>make manifests</code>, <code>make install</code>, and verify the CRD is registered in the cluster.</p>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#scenario_3","title":"Scenario:","text":"<p>\u25e6 After defining your types, you need to generate the CRD YAML and apply it to the cluster.   \u25e6 This makes <code>kubectl get myapps</code> work.</p> <p>Hint: <code>make generate &amp;&amp; make manifests &amp;&amp; make install</code></p> Solution <pre><code># 1. Generate deepcopy + CRD + RBAC\nmake generate\nmake manifests\n\n# 2. Install CRDs into the cluster\nmake install\n\n# 3. Verify the CRD exists\nkubectl get crds | grep example.com\nkubectl describe crd myapps.apps.example.com\n\n# 4. Test that the API resource is available\nkubectl get myapps\n# \"No resources found in default namespace.\"\n\n# 5. Check the short name (if configured)\nkubectl api-resources --api-group=apps.example.com\n</code></pre>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#05-implement-a-basic-reconciler","title":"05. Implement a Basic Reconciler","text":"<p>Write a reconciler that creates a Deployment when a CR is created.</p>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#scenario_4","title":"Scenario:","text":"<p>\u25e6 When a user creates a <code>MyApp</code> CR, your controller should create a corresponding Deployment.   \u25e6 The reconciler fetches the CR, checks if a Deployment exists, and creates it if missing.</p> <p>Hint: Use <code>r.Get()</code> to fetch, <code>errors.IsNotFound()</code> to check, <code>r.Create()</code> to create.</p> Solution <pre><code>// In internal/controller/myapp_controller.go - Reconcile method:\n\nfunc (r *MyAppReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n    logger := log.FromContext(ctx)\n\n    // Fetch the CR\n    myapp := &amp;appsv1.MyApp{}\n    if err := r.Get(ctx, req.NamespacedName, myapp); err != nil {\n        if errors.IsNotFound(err) {\n            return ctrl.Result{}, nil\n        }\n        return ctrl.Result{}, err\n    }\n\n    // Check if Deployment exists\n    dep := &amp;appsv1.Deployment{}\n    err := r.Get(ctx, types.NamespacedName{\n        Name: myapp.Name, Namespace: myapp.Namespace,\n    }, dep)\n\n    if errors.IsNotFound(err) {\n        logger.Info(\"Creating Deployment\", \"name\", myapp.Name)\n        // Build the Deployment\n        dep = buildDeployment(myapp)\n        ctrl.SetControllerReference(myapp, dep, r.Scheme)\n        return ctrl.Result{}, r.Create(ctx, dep)\n    }\n\n    return ctrl.Result{}, err\n}\n</code></pre> <pre><code># Run locally\nmake run\n\n# In another terminal, create a CR\nkubectl apply -f config/samples/apps_v1_myapp.yaml\n\n# Verify the Deployment was created\nkubectl get deployments\n</code></pre>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#06-run-the-controller-locally","title":"06. Run the Controller Locally","text":"<p>Use <code>make run</code> to run the operator on your machine against the cluster.</p>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#scenario_5","title":"Scenario:","text":"<p>\u25e6 During development, you run the controller locally using your kubeconfig.   \u25e6 This is faster than building a Docker image for every change.</p> <p>Hint: <code>make install &amp;&amp; make run</code></p> Solution <pre><code># 1. Ensure CRDs are installed\nmake install\n\n# 2. Run the controller\nmake run\n# INFO    Starting manager\n# INFO    Starting Controller    {\"controller\": \"myapp\"}\n\n# 3. In another terminal, create and delete CRs to test\nkubectl apply -f config/samples/apps_v1_myapp.yaml\nkubectl get myapps\nkubectl delete myapp my-myapp\n\n# 4. Stop the controller with Ctrl+C\n</code></pre>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#07-add-owner-references-for-garbage-collection","title":"07. Add Owner References for Garbage Collection","text":"<p>Set owner references on child resources so they are automatically deleted when the parent CR is deleted.</p>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#scenario_6","title":"Scenario:","text":"<p>\u25e6 When a user deletes a <code>MyApp</code> CR, the Deployment, Service, and ConfigMap should be cleaned up.   \u25e6 Owner references enable Kubernetes garbage collection.</p> <p>Hint: Use <code>ctrl.SetControllerReference(parent, child, r.Scheme)</code> before creating the child.</p> Solution <pre><code>// Before r.Create(ctx, deployment):\nif err := ctrl.SetControllerReference(myapp, deployment, r.Scheme); err != nil {\n    return ctrl.Result{}, err\n}\n</code></pre> <pre><code># Test: create a CR, verify child resources exist\nkubectl apply -f config/samples/apps_v1_myapp.yaml\nkubectl get deployment -l app.kubernetes.io/managed-by=my-operator\n\n# Verify owner reference is set\nkubectl get deployment &lt;name&gt; -o jsonpath='{.metadata.ownerReferences}' | jq\n\n# Delete the CR - children should be garbage-collected\nkubectl delete myapp my-myapp\nkubectl get deployments  # Should be gone\n</code></pre>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#08-update-status-subresource","title":"08. Update Status Subresource","text":"<p>Update the CR\u2019s <code>.status</code> fields to reflect the current state of managed resources.</p>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#scenario_7","title":"Scenario:","text":"<p>\u25e6 Users need to see the current state (e.g., available replicas, phase) via <code>kubectl get myapps</code>.   \u25e6 Status updates use the <code>/status</code> subresource to avoid triggering spec watches.</p> <p>Hint: Use <code>r.Status().Update(ctx, updated)</code> after computing the status.</p> Solution <pre><code>// At the end of Reconcile(), after reconciling child resources:\nupdated := myapp.DeepCopy()\nupdated.Status.AvailableReplicas = deployment.Status.AvailableReplicas\nupdated.Status.Phase = \"Running\"\n\nif err := r.Status().Update(ctx, updated); err != nil {\n    return ctrl.Result{}, err\n}\n</code></pre> <pre><code># Apply a CR\nkubectl apply -f config/samples/apps_v1_myapp.yaml\n\n# Check status\nkubectl get myapp my-myapp -o jsonpath='{.status}' | jq\n\n# With printer columns configured:\nkubectl get myapps\n# NAME       REPLICAS   AVAILABLE   PHASE\n# my-myapp   2          2           Running\n</code></pre>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#09-add-a-finalizer","title":"09. Add a Finalizer","text":"<p>Implement a finalizer that runs custom cleanup logic before the CR is deleted.</p>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#scenario_8","title":"Scenario:","text":"<p>\u25e6 Your operator manages external resources (e.g., DNS records, cloud storage) that need cleanup.   \u25e6 Finalizers prevent deletion until cleanup is done.</p> <p>Hint: Use <code>controllerutil.AddFinalizer/RemoveFinalizer</code>, check <code>DeletionTimestamp.IsZero()</code>.</p> Solution <pre><code>const myFinalizer = \"apps.example.com/finalizer\"\n\n// In Reconcile(), after fetching the CR:\nif myapp.DeletionTimestamp.IsZero() {\n    if !controllerutil.ContainsFinalizer(myapp, myFinalizer) {\n        controllerutil.AddFinalizer(myapp, myFinalizer)\n        return ctrl.Result{}, r.Update(ctx, myapp)\n    }\n} else {\n    if controllerutil.ContainsFinalizer(myapp, myFinalizer) {\n        logger.Info(\"Running cleanup for\", \"name\", myapp.Name)\n        // Do external cleanup here...\n\n        controllerutil.RemoveFinalizer(myapp, myFinalizer)\n        return ctrl.Result{}, r.Update(ctx, myapp)\n    }\n    return ctrl.Result{}, nil\n}\n</code></pre> <pre><code># Create and then delete - observe cleanup in operator logs\nkubectl apply -f config/samples/apps_v1_myapp.yaml\nkubectl delete myapp my-myapp\n\n# The operator log should show \"Running cleanup for\"\n</code></pre>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#10-write-a-controller-test-with-envtest","title":"10. Write a Controller Test with envtest","text":"<p>Write a Ginkgo/Gomega integration test that verifies your controller creates a Deployment.</p>"},{"location":"Tasks/Kubernetes-Kubebuilder-Tasks/#scenario_9","title":"Scenario:","text":"<p>\u25e6 You need automated tests for your operator that run without a real cluster.   \u25e6 <code>envtest</code> starts a local API server and etcd for testing.</p> <p>Hint: Use <code>k8sClient.Create()</code> to create a CR, then <code>Eventually()</code> to wait for the Deployment.</p> Solution <pre><code>// internal/controller/myapp_controller_test.go\nvar _ = Describe(\"MyApp Controller\", func() {\n    ctx := context.Background()\n\n    It(\"should create a Deployment when a MyApp is created\", func() {\n        myapp := &amp;v1.MyApp{\n            ObjectMeta: metav1.ObjectMeta{\n                Name:      \"test-app\",\n                Namespace: \"default\",\n            },\n            Spec: v1.MyAppSpec{\n                Replicas: 2,\n                Message:  \"test\",\n            },\n        }\n        Expect(k8sClient.Create(ctx, myapp)).To(Succeed())\n\n        deployment := &amp;appsv1.Deployment{}\n        Eventually(func() error {\n            return k8sClient.Get(ctx, types.NamespacedName{\n                Name:      \"test-app\",\n                Namespace: \"default\",\n            }, deployment)\n        }, time.Second*30, time.Millisecond*250).Should(Succeed())\n\n        Expect(*deployment.Spec.Replicas).To(Equal(int32(2)))\n    })\n})\n</code></pre> <pre><code># Run tests\nmake test\n\n# Verbose output\nmake test ARGS=\"-v\"\n</code></pre>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/","title":"Kubernetes Scheduling Tasks","text":"<ul> <li>Hands-on Kubernetes exercises covering Node Affinity, Pod Affinity, Pod Anti-Affinity, Taints, Tolerations, and Topology Spread Constraints.</li> <li>Each task includes a description, scenario, and a detailed solution with step-by-step instructions.</li> <li>Practice these tasks to master fine-grained Pod placement and scheduling strategies.</li> </ul>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#table-of-contents","title":"Table of Contents","text":"<ul> <li>01. Label Nodes and Use nodeSelector</li> <li>02. Required Node Affinity with Multiple Labels</li> <li>03. Preferred Node Affinity with Weights</li> <li>04. Pod Anti-Affinity for High Availability</li> <li>05. Pod Affinity to Co-Locate Services</li> <li>06. Taint a Node and Add a Toleration</li> <li>07. NoExecute Taint with tolerationSeconds</li> <li>08. Topology Spread Constraints</li> <li>09. Combine Node Affinity with Taints</li> <li>10. Debug a Pending Pod</li> </ul>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#01-label-nodes-and-use-nodeselector","title":"01. Label Nodes and Use nodeSelector","text":"<p>Add a custom label to a node and schedule a Pod on it using <code>nodeSelector</code>.</p>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#scenario","title":"Scenario:","text":"<p>\u25e6 You have a node with SSD storage and want to ensure a database Pod only runs on it.   \u25e6 <code>nodeSelector</code> is the simplest scheduling constraint.</p> <p>Hint: <code>kubectl label nodes</code>, then use <code>spec.nodeSelector</code> in the Pod spec.</p> Solution <pre><code># 1. List nodes\nkubectl get nodes\n\n# 2. Label a node\nkubectl label nodes &lt;node-name&gt; disk-type=ssd\n\n# 3. Create a Pod with nodeSelector\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ssd-pod\nspec:\n  nodeSelector:\n    disk-type: ssd\n  containers:\n  - name: app\n    image: nginx:1.25\nEOF\n\n# 4. Verify the Pod landed on the correct node\nkubectl get pod ssd-pod -o wide\n\n# 5. Cleanup\nkubectl delete pod ssd-pod\nkubectl label nodes &lt;node-name&gt; disk-type-\n</code></pre>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#02-required-node-affinity-with-multiple-labels","title":"02. Required Node Affinity with Multiple Labels","text":"<p>Schedule a Pod that requires nodes with BOTH <code>environment=production</code> AND <code>zone=us-east</code> labels.</p>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#scenario_1","title":"Scenario:","text":"<p>\u25e6 Your production workload must run in a specific zone on production-labeled nodes.   \u25e6 Node Affinity with the <code>In</code> operator lets you express this constraint.</p> <p>Hint: Use <code>requiredDuringSchedulingIgnoredDuringExecution</code> with multiple <code>matchExpressions</code> in a single <code>nodeSelectorTerms</code> entry.</p> Solution <pre><code># 1. Label nodes\nkubectl label nodes &lt;node-name&gt; environment=production zone=us-east\n\n# 2. Create the Pod\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-east-pod\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: environment\n            operator: In\n            values: [production]\n          - key: zone\n            operator: In\n            values: [us-east]\n  containers:\n  - name: app\n    image: nginx:1.25\nEOF\n\n# 3. Verify\nkubectl get pod prod-east-pod -o wide\nkubectl describe pod prod-east-pod | grep \"Node:\"\n\n# 4. Cleanup\nkubectl delete pod prod-east-pod\nkubectl label nodes &lt;node-name&gt; environment- zone-\n</code></pre>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#03-preferred-node-affinity-with-weights","title":"03. Preferred Node Affinity with Weights","text":"<p>Deploy a Pod that strongly prefers production nodes (weight 80) and weakly prefers SSD nodes (weight 20).</p>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#scenario_2","title":"Scenario:","text":"<p>\u25e6 You want soft scheduling preferences - the Pod should schedule even if neither preference is met.   \u25e6 Weights (1\u2013100) let you prioritize multiple preferences.</p> <p>Hint: Use <code>preferredDuringSchedulingIgnoredDuringExecution</code> with two entries at different weights.</p> Solution <pre><code>cat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: weighted-pod\nspec:\n  affinity:\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 80\n        preference:\n          matchExpressions:\n          - key: environment\n            operator: In\n            values: [production]\n      - weight: 20\n        preference:\n          matchExpressions:\n          - key: disk-type\n            operator: In\n            values: [ssd]\n  containers:\n  - name: app\n    image: nginx:1.25\nEOF\n\nkubectl get pod weighted-pod -o wide\nkubectl describe pod weighted-pod | grep \"Node:\"\n\n# Cleanup\nkubectl delete pod weighted-pod\n</code></pre>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#04-pod-anti-affinity-for-high-availability","title":"04. Pod Anti-Affinity for High Availability","text":"<p>Deploy a 3-replica Deployment where no two replicas land on the same node.</p>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#scenario_3","title":"Scenario:","text":"<p>\u25e6 For high availability, replicas should be spread across different nodes.   \u25e6 If you have fewer nodes than replicas, some Pods will stay Pending with required anti-affinity.</p> <p>Hint: Use <code>podAntiAffinity</code> with <code>requiredDuringSchedulingIgnoredDuringExecution</code> and <code>topologyKey: kubernetes.io/hostname</code>.</p> Solution <pre><code>cat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ha-web\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ha-web\n  template:\n    metadata:\n      labels:\n        app: ha-web\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: ha-web\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: web\n        image: nginx:1.25\nEOF\n\n# Verify each pod is on a different node\nkubectl get pods -l app=ha-web -o wide\n\n# Cleanup\nkubectl delete deployment ha-web\n</code></pre>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#05-pod-affinity-to-co-locate-services","title":"05. Pod Affinity to Co-Locate Services","text":"<p>Deploy a cache Pod and an app Pod that must be on the same node as the cache.</p>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#scenario_4","title":"Scenario:","text":"<p>\u25e6 Your application benefits from sub-millisecond latency to the local cache.   \u25e6 Pod Affinity ensures co-location on the same node.</p> <p>Hint: Use <code>podAffinity</code> with <code>topologyKey: kubernetes.io/hostname</code> matching the cache Pod\u2019s labels.</p> Solution <pre><code># 1. Deploy the cache\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: cache\n  labels:\n    app: cache\nspec:\n  containers:\n  - name: redis\n    image: redis:7-alpine\nEOF\n\n# 2. Deploy the app with affinity to cache\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: app-near-cache\nspec:\n  affinity:\n    podAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchLabels:\n            app: cache\n        topologyKey: kubernetes.io/hostname\n  containers:\n  - name: app\n    image: nginx:1.25\nEOF\n\n# 3. Verify both are on the same node\nkubectl get pods cache app-near-cache -o wide\n\n# Cleanup\nkubectl delete pod cache app-near-cache\n</code></pre>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#06-taint-a-node-and-add-a-toleration","title":"06. Taint a Node and Add a Toleration","text":"<p>Taint a node with <code>NoSchedule</code>, verify a regular Pod is rejected, then deploy a Pod with a matching toleration.</p>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#scenario_5","title":"Scenario:","text":"<p>\u25e6 You have dedicated GPU nodes that should only accept GPU workloads.   \u25e6 Taints repel Pods; tolerations opt-in specific Pods.</p> <p>Hint: <code>kubectl taint nodes</code>, then add a <code>tolerations</code> block to the Pod spec.</p> Solution <pre><code># 1. Taint a node\nkubectl taint nodes &lt;node-name&gt; dedicated=gpu:NoSchedule\n\n# 2. Try a regular Pod (will stay Pending if this is the only node)\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: regular-pod\nspec:\n  containers:\n  - name: app\n    image: nginx:1.25\nEOF\n\nkubectl describe pod regular-pod | grep -A5 \"Events:\"\n\n# 3. Deploy a Pod with toleration\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod\nspec:\n  tolerations:\n  - key: \"dedicated\"\n    operator: \"Equal\"\n    value: \"gpu\"\n    effect: \"NoSchedule\"\n  containers:\n  - name: app\n    image: nginx:1.25\nEOF\n\nkubectl get pod gpu-pod -o wide\n\n# Cleanup\nkubectl delete pod regular-pod gpu-pod\nkubectl taint nodes &lt;node-name&gt; dedicated=gpu:NoSchedule-\n</code></pre>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#07-noexecute-taint-with-tolerationseconds","title":"07. NoExecute Taint with tolerationSeconds","text":"<p>Deploy a Pod with a <code>NoExecute</code> toleration and <code>tolerationSeconds: 60</code>. Apply the taint and observe the Pod being evicted after 60 seconds.</p>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#scenario_6","title":"Scenario:","text":"<p>\u25e6 During planned maintenance, you want to give running Pods a grace period before eviction.   \u25e6 <code>tolerationSeconds</code> controls how long a Pod survives after the taint is applied.</p> <p>Hint: Use <code>effect: NoExecute</code> with <code>tolerationSeconds</code> in the Pod toleration.</p> Solution <pre><code># 1. Deploy a Pod with tolerationSeconds\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: graceful-pod\nspec:\n  tolerations:\n  - key: \"maintenance\"\n    operator: \"Equal\"\n    value: \"true\"\n    effect: \"NoExecute\"\n    tolerationSeconds: 60\n  containers:\n  - name: app\n    image: nginx:1.25\nEOF\n\n# 2. Verify it's running\nkubectl get pod graceful-pod -o wide\nNODE=$(kubectl get pod graceful-pod -o jsonpath='{.spec.nodeName}')\n\n# 3. Taint the node with NoExecute\nkubectl taint nodes $NODE maintenance=true:NoExecute\n\n# 4. Watch the Pod - it survives ~60s then is evicted\nkubectl get pod graceful-pod -w\n\n# Cleanup\nkubectl taint nodes $NODE maintenance=true:NoExecute-\n</code></pre>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#08-topology-spread-constraints","title":"08. Topology Spread Constraints","text":"<p>Deploy 6 replicas of a Deployment with <code>maxSkew: 1</code> across availability zones.</p>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#scenario_7","title":"Scenario:","text":"<p>\u25e6 You need even distribution of pods across zones for resilience.   \u25e6 Topology Spread Constraints provide finer control than Anti-Affinity.</p> <p>Hint: Use <code>topologySpreadConstraints</code> with <code>topologyKey: topology.kubernetes.io/zone</code>.</p> Solution <pre><code>cat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zone-spread\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: zone-spread\n  template:\n    metadata:\n      labels:\n        app: zone-spread\n    spec:\n      topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: topology.kubernetes.io/zone\n        whenUnsatisfiable: DoNotSchedule\n        labelSelector:\n          matchLabels:\n            app: zone-spread\n      containers:\n      - name: app\n        image: nginx:1.25\nEOF\n\n# Verify distribution\nkubectl get pods -l app=zone-spread -o wide\n\n# Cleanup\nkubectl delete deployment zone-spread\n</code></pre>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#09-combine-node-affinity-with-taints","title":"09. Combine Node Affinity with Taints","text":"<p>Create a dedicated node pool pattern: taint the node (repel others) and use Node Affinity (attract your Pods).</p>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#scenario_8","title":"Scenario:","text":"<p>\u25e6 You want to isolate monitoring workloads on dedicated nodes.   \u25e6 The pattern is: Taint (repel) + Affinity (attract) + Toleration (allow).</p> <p>Hint: Label and taint the node, then create a Pod with both <code>nodeAffinity</code> and <code>tolerations</code>.</p> Solution <pre><code># 1. Setup the dedicated node\nkubectl label nodes &lt;node-name&gt; role=monitoring\nkubectl taint nodes &lt;node-name&gt; role=monitoring:NoSchedule\n\n# 2. Deploy a monitoring Pod\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: monitor-pod\nspec:\n  tolerations:\n  - key: \"role\"\n    operator: \"Equal\"\n    value: \"monitoring\"\n    effect: \"NoSchedule\"\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: role\n            operator: In\n            values: [monitoring]\n  containers:\n  - name: prometheus\n    image: prom/prometheus:latest\nEOF\n\n# 3. Verify it landed on the correct node\nkubectl get pod monitor-pod -o wide\n\n# Cleanup\nkubectl delete pod monitor-pod\nkubectl taint nodes &lt;node-name&gt; role=monitoring:NoSchedule-\nkubectl label nodes &lt;node-name&gt; role-\n</code></pre>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#10-debug-a-pending-pod","title":"10. Debug a Pending Pod","text":"<p>Given a Pod stuck in Pending, use kubectl commands to identify and resolve the scheduling failure.</p>"},{"location":"Tasks/Kubernetes-Scheduling-Tasks/#scenario_9","title":"Scenario:","text":"<p>\u25e6 A colleague deployed a Pod that\u2019s stuck in Pending. You need to diagnose the issue.   \u25e6 Common causes: missing labels, unmatched taints, insufficient resources.</p> <p>Hint: <code>kubectl describe pod</code>, <code>kubectl get events</code>, check node labels and taints.</p> Solution <pre><code># 1. Create a Pod with an impossible affinity (to simulate the issue)\ncat &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: stuck-pod\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: nonexistent-label\n            operator: In\n            values: [does-not-exist]\n  containers:\n  - name: app\n    image: nginx:1.25\nEOF\n\n# 2. Check pod status\nkubectl get pod stuck-pod\n# STATUS: Pending\n\n# 3. Describe for scheduling events\nkubectl describe pod stuck-pod | grep -A10 \"Events:\"\n# \"0/N nodes are available: N node(s) didn't match Pod's node affinity/selector\"\n\n# 4. Check node labels to understand what's available\nkubectl get nodes --show-labels\n\n# 5. Check node taints\nkubectl describe nodes | grep -A3 \"Taints:\"\n\n# 6. Fix: either label a node or remove the affinity constraint\n# Option A: Label a node to satisfy the affinity\nkubectl label nodes &lt;node-name&gt; nonexistent-label=does-not-exist\n\n# 7. Verify the Pod is now scheduled\nkubectl get pod stuck-pod -o wide\n\n# Cleanup\nkubectl delete pod stuck-pod\nkubectl label nodes &lt;node-name&gt; nonexistent-label-\n</code></pre>"},{"location":"Tasks/Kubernetes-Service-Tasks/","title":"Kubernetes Service Tasks","text":"<ul> <li>Hands-on Kubernetes exercises covering Services, Networking, and Service Discovery.</li> <li>Each task includes a description and a detailed solution with step-by-step instructions.</li> <li>Practice these tasks to master how Kubernetes exposes applications and manages traffic.</li> </ul>"},{"location":"Tasks/Kubernetes-Service-Tasks/#table-of-contents","title":"Table of Contents","text":"<ul> <li>01. Basic Service Exposure (ClusterIP)</li> <li>02. NodePort &amp; LoadBalancer</li> <li>03. Service Discovery with DNS (FQDN)</li> <li>04. Headless Services</li> <li>05. ExternalName Service</li> <li>06. Manual Endpoints</li> <li>07. Session Affinity</li> <li>08. Multi-Port Service</li> </ul>"},{"location":"Tasks/Kubernetes-Service-Tasks/#01-basic-service-exposure-clusterip","title":"01. Basic Service Exposure (ClusterIP)","text":"<p>Run an <code>nginx</code> pod and expose it via a Service (ClusterIP) to access it from within the cluster.</p>"},{"location":"Tasks/Kubernetes-Service-Tasks/#scenario","title":"Scenario:","text":"<p>\u25e6 You have an application running in a pod, but it needs to be accessible by other pods.   \u25e6 ClusterIP is the default service type, providing internal stable IP.</p> <p>Hint: <code>kubectl expose</code>, <code>kubectl get services</code>, <code>kubectl port-forward</code></p> Solution <pre><code># 1. Run an nginx pod\nkubectl run nginx-web --image=nginx:alpine --port=80\n\n# 2. Expose the pod as a Service (ClusterIP by default)\nkubectl expose pod nginx-web --name=nginx-svc --port=80 --target-port=80\n\n# 3. Verify the service\nkubectl get svc\n\n# 4. Access it (using port-forward for local access)\nkubectl port-forward svc/nginx-svc 8080:80\n# (Open localhost:8080 in browser)\n\n# Cleanup\nkubectl delete pod nginx-web\nkubectl delete svc nginx-svc\n</code></pre>"},{"location":"Tasks/Kubernetes-Service-Tasks/#02-nodeport-loadbalancer","title":"02. NodePort &amp; LoadBalancer","text":"<p>Expose a deployment using <code>NodePort</code> to access it via the node\u2019s IP, and then switch it to <code>LoadBalancer</code> (simulated or real).</p>"},{"location":"Tasks/Kubernetes-Service-Tasks/#scenario_1","title":"Scenario:","text":"<p>\u25e6 You need to make your application accessible from outside the Kubernetes cluster.   \u25e6 <code>NodePort</code> opens a specific port on all nodes, while <code>LoadBalancer</code> provisions an external IP (cloud provider dependent).</p> <p>Hint: <code>type: NodePort</code>, <code>type: LoadBalancer</code></p> Solution <pre><code># 1. Create a deployment\nkubectl create deployment web-server --image=nginx:alpine --replicas=2\n\n# 2. Expose as NodePort\nkubectl expose deployment web-server --type=NodePort --name=web-nodeport --port=80\n\n# 3. Get the allocated NodePort (e.g., 30xxx)\nkubectl get svc web-nodeport\n\n# 4. (Optional) Patch it to be a LoadBalancer\nkubectl patch svc web-nodeport -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'\n\n# 5. Verify external IP (it might stay &lt;pending&gt; on Minikube/Kind without addons)\nkubectl get svc web-nodeport\n\n# Cleanup\nkubectl delete deployment web-server\nkubectl delete svc web-nodeport\n</code></pre>"},{"location":"Tasks/Kubernetes-Service-Tasks/#03-service-discovery-with-dns-fqdn","title":"03. Service Discovery with DNS (FQDN)","text":"<p>Create two pods in different namespaces and verify they can communicate using the Fully Qualified Domain Name (FQDN).</p>"},{"location":"Tasks/Kubernetes-Service-Tasks/#scenario_2","title":"Scenario:","text":"<p>\u25e6 Microservices often live in different namespaces (e.g., <code>frontend</code> vs <code>backend</code>).   \u25e6 You need to ensure they can talk to each other using Kubernetes internal DNS.</p> <p>Hint: <code>nslookup</code>, <code>&lt;service&gt;.&lt;namespace&gt;.svc.cluster.local</code></p> Solution <pre><code># 1. Create two namespaces\nkubectl create ns app-a\nkubectl create ns app-b\n\n# 2. Run a target pod and service in app-b\nkubectl run backend --image=nginx:alpine -n app-b\nkubectl expose pod backend --name=backend-svc --port=80 -n app-b\n\n# 3. Run a client pod in app-a\nkubectl run client --image=busybox -n app-a -- sleep 3600\n\n# 4. Test DNS resolution from client to backend\n# FQDN format: service-name.namespace.svc.cluster.local\nkubectl exec -it client -n app-a -- nslookup backend-svc.app-b.svc.cluster.local\n\n# 5. Test connectivity\nkubectl exec -it client -n app-a -- wget -O- backend-svc.app-b.svc.cluster.local\n\n# Cleanup\nkubectl delete ns app-a app-b\n</code></pre>"},{"location":"Tasks/Kubernetes-Service-Tasks/#04-headless-services","title":"04. Headless Services","text":"<p>Create a Headless Service (ClusterIP: None) and verify that DNS returns the IPs of the individual pods instead of a single Service IP.</p>"},{"location":"Tasks/Kubernetes-Service-Tasks/#scenario_3","title":"Scenario:","text":"<p>\u25e6 You are deploying a distributed stateful application (like Cassandra, MongoDB, or Kafka) that needs to discover all peer nodes directly.   \u25e6 Headless services allow direct pod-to-pod communication without load balancing.</p> <p>Hint: <code>clusterIP: None</code></p> Solution <pre><code># 1. Create a Headless Service\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: headless-svc\nspec:\n  clusterIP: None\n  selector:\n    app: headless-app\n  ports:\n    - port: 80\nEOF\n\n# 2. Create pods matching the selector\nkubectl run pod-1 --image=nginx:alpine --labels=app=headless-app\nkubectl run pod-2 --image=nginx:alpine --labels=app=headless-app\n\n# 3. Verify DNS resolution (should return multiple IPs)\nkubectl run dns-test --image=busybox --restart=Never -- nslookup headless-svc\n\n# 4. Check logs to see the IPs\nkubectl logs dns-test\n\n# Cleanup\nkubectl delete pod pod-1 pod-2 dns-test\nkubectl delete svc headless-svc\n</code></pre>"},{"location":"Tasks/Kubernetes-Service-Tasks/#05-externalname-service","title":"05. ExternalName Service","text":"<p>Create a Service that maps to an external DNS name (e.g., <code>google.com</code>) instead of a pod selector.</p>"},{"location":"Tasks/Kubernetes-Service-Tasks/#scenario_4","title":"Scenario:","text":"<p>\u25e6 You want to refer to an external database or API (e.g., AWS RDS, external API) using a local Kubernetes service name.   \u25e6 This allows you to change the external endpoint later without changing your application code.</p> <p>Hint: <code>type: ExternalName</code>, <code>externalName: example.com</code></p> Solution <pre><code># 1. Create ExternalName service\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-external-service\nspec:\n  type: ExternalName\n  externalName: google.com\nEOF\n\n# 2. Test resolution (it should be a CNAME to google.com)\nkubectl run ext-test --image=busybox --restart=Never -- nslookup my-external-service\n\n# 3. Check logs\nkubectl logs ext-test\n\n# Cleanup\nkubectl delete svc my-external-service\nkubectl delete pod ext-test\n</code></pre>"},{"location":"Tasks/Kubernetes-Service-Tasks/#06-manual-endpoints","title":"06. Manual Endpoints","text":"<p>Create a Service without a selector, and manually create an Endpoints object to point to an external IP (or a specific pod IP).</p>"},{"location":"Tasks/Kubernetes-Service-Tasks/#scenario_5","title":"Scenario:","text":"<p>\u25e6 You want to use a Kubernetes Service to point to a specific IP address that isn\u2019t managed by a Kubernetes Pod selector (e.g., a legacy server or a database outside the cluster).</p> <p>Hint: <code>kind: Endpoints</code>, same name as Service</p> Solution <pre><code># 1. Create a Service without a selector\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: manual-svc\nspec:\n  ports:\n    - port: 80\n      targetPort: 80\nEOF\n\n# 2. Create Endpoints manually (Use an IP you know, e.g., 1.1.1.1 or a pod IP)\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: manual-svc\nsubsets:\n  - addresses:\n      - ip: 1.1.1.1\n    ports:\n      - port: 80\nEOF\n\n# 3. Describe service to see endpoints\nkubectl describe svc manual-svc\n\n# Cleanup\nkubectl delete svc manual-svc\nkubectl delete endpoints manual-svc\n</code></pre>"},{"location":"Tasks/Kubernetes-Service-Tasks/#07-session-affinity","title":"07. Session Affinity","text":"<p>Create a Service with <code>sessionAffinity: ClientIP</code> and verify that requests from the same client pod go to the same backend pod (if possible to observe).</p>"},{"location":"Tasks/Kubernetes-Service-Tasks/#scenario_6","title":"Scenario:","text":"<p>\u25e6 Your application stores session state locally in the container (not recommended, but happens).   \u25e6 You need to ensure a user always hits the same pod during their session.</p> <p>Hint: <code>sessionAffinity: ClientIP</code></p> Solution <pre><code># 1. Create a deployment with 3 replicas\nkubectl create deployment session-app --image=nginx:alpine --replicas=3\n\n# 2. Expose with ClientIP affinity\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: session-svc\nspec:\n  selector:\n    app: session-app\n  ports:\n    - port: 80\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 10800\nEOF\n\n# 3. Verify affinity setting\nkubectl describe svc session-svc\n\n# Cleanup\nkubectl delete deployment session-app\nkubectl delete svc session-svc\n</code></pre>"},{"location":"Tasks/Kubernetes-Service-Tasks/#08-multi-port-service","title":"08. Multi-Port Service","text":"<p>Create a Service that exposes both port 80 (HTTP) and 443 (HTTPS) for the same set of pods.</p>"},{"location":"Tasks/Kubernetes-Service-Tasks/#scenario_7","title":"Scenario:","text":"<p>\u25e6 Your application serves both HTTP and HTTPS traffic.   \u25e6 You need a single Service to handle both ports.</p> <p>Hint: <code>ports</code> array in Service spec</p> Solution <pre><code># 1. Create a pod that exposes port 80\nkubectl run web-multi --image=nginx:alpine --port=80\n\n# 2. Create a multi-port service\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: multi-port-svc\nspec:\n  selector:\n    run: web-multi\n  ports:\n    - name: http\n      port: 80\n      targetPort: 80\n    - name: https\n      port: 443\n      targetPort: 80 # Mapping 443 to 80 just for demo since nginx listens on 80\nEOF\n\n# 3. Verify ports\nkubectl get svc multi-port-svc\n\n# Cleanup\nkubectl delete pod web-multi\nkubectl delete svc multi-port-svc\n</code></pre>"}]}