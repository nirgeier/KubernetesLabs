apiVersion: v1
kind: ConfigMap
metadata:
  name: log-processor-script
  namespace: {{ .Values.namespace }}
  labels:
    app: log-processor
data:
  process-logs.sh: |
    #!/bin/bash

    set -e

    ELASTICSEARCH_URL="http://{{ .Values.elasticsearch.host }}:{{ .Values.elasticsearch.port }}"
    LOG_DIR="/filebeat-logs"
    PROCESSED_DIR="/filebeat-logs/processed"
    KEEP_ORIGINALS="{{ .Values.processing.keepOriginalFiles }}"
    CREATE_BACKUPS="{{ .Values.processing.createBackups }}"

    echo "Starting log processor..."
    echo "Elasticsearch URL: $ELASTICSEARCH_URL"
    echo "Log directory: $LOG_DIR"
    echo "Keep original files: $KEEP_ORIGINALS"
    echo "Create backups: $CREATE_BACKUPS"

    # Create processed directory if it doesn't exist
    mkdir -p "$PROCESSED_DIR"

    # Function to check if Elasticsearch is ready
    check_elasticsearch() {
      echo "Checking Elasticsearch connectivity..."
      max_retries=30
      retry=0

      while [ $retry -lt $max_retries ]; do
        if curl -s -f "$ELASTICSEARCH_URL/_cluster/health" > /dev/null 2>&1; then
          echo "Elasticsearch is ready!"
          return 0
        fi
        retry=$((retry + 1))
        echo "Waiting for Elasticsearch... (attempt $retry/$max_retries)"
        sleep 2
      done

      echo "ERROR: Elasticsearch is not available after $max_retries attempts"
      return 1
    }

    # Check Elasticsearch availability
    if ! check_elasticsearch; then
      exit 1
    fi

    # Process log files
    processed_count=0
    error_count=0

    echo "Looking for log files in $LOG_DIR..."

    # Find all filebeat log files (excluding already processed ones)
    for log_file in "$LOG_DIR"/filebeat*; do
      # Skip if no files found
      if [ ! -f "$log_file" ]; then
        echo "No log files found to process"
        continue
      fi

      # Skip if already processed
      filename=$(basename "$log_file")
      if [ -f "$PROCESSED_DIR/$filename.processed" ]; then
        echo "Skipping already processed file: $filename"
        continue
      fi

      echo "Processing file: $filename"

      # Read each line from the log file and send to Elasticsearch
      line_count=0
      while IFS= read -r line; do
        # Skip empty lines
        if [ -z "$line" ]; then
          continue
        fi

        # Generate index name with current date
        INDEX_NAME="filebeat-8.11.0-$(date +%Y.%m.%d)"

        # Send to Elasticsearch bulk API
        response=$(curl -s -w "\n%{http_code}" -X POST "$ELASTICSEARCH_URL/$INDEX_NAME/_doc" \
          -H 'Content-Type: application/json' \
          -d "$line")

        http_code=$(echo "$response" | tail -n 1)

        if [ "$http_code" -eq 201 ] || [ "$http_code" -eq 200 ]; then
          line_count=$((line_count + 1))
        else
          echo "ERROR: Failed to send log entry (HTTP $http_code)"
          error_count=$((error_count + 1))
        fi

      done < "$log_file"

      echo "Processed $line_count log entries from $filename"

      # Mark file as processed
      touch "$PROCESSED_DIR/$filename.processed"

      # Handle file based on configuration
      if [ "$KEEP_ORIGINALS" = "true" ]; then
        echo "Keeping original file: $log_file"
        if [ "$CREATE_BACKUPS" = "true" ]; then
          echo "Creating backup copy in processed folder"
          cp "$log_file" "$PROCESSED_DIR/$filename.backup"
        fi
      else
        echo "Moving processed file to: $PROCESSED_DIR/$filename"
        mv "$log_file" "$PROCESSED_DIR/$filename"
      fi

      processed_count=$((processed_count + 1))
    done

    echo "================================"
    echo "Log processing complete!"
    echo "Files processed: $processed_count"
    echo "Errors: $error_count"
    echo "================================"

    # Refresh the index to make docs searchable immediately
    echo "Refreshing Elasticsearch indices..."
    curl -s -X POST "$ELASTICSEARCH_URL/filebeat-*/_refresh" > /dev/null

    exit 0
