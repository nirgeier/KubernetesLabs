{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kubernetes Labs","text":""},{"location":"#lab-overview","title":"\ud83d\udccb Lab Overview","text":"<p>Welcome to the hands-on Kubernetes labs! This comprehensive series of labs will guide you through essential Kubernetes concepts and advanced topics.</p>"},{"location":"#available-labs","title":"\ud83d\uddc2\ufe0f Available Labs","text":""},{"location":"#foundation-labs","title":"Foundation Labs","text":"Lab Topic Description 00 Verify Cluster Ensure your Kubernetes cluster is properly configured 01 Namespace Learn to organize resources with namespaces 02 Deployments (Imperative) Create deployments using kubectl commands 03 Deployments (Declarative) Create deployments using YAML manifests 04 Rollout Manage deployment updates and rollbacks 05 Services Expose applications with Kubernetes services"},{"location":"#storage-statefulsets","title":"Storage &amp; StatefulSets","text":"Lab Topic Description 06 DataStore Work with persistent storage in Kubernetes 09 StatefulSet Deploy stateful applications 12 WordPress MySQL PVC Complete stateful application with persistent storage"},{"location":"#networking-ingress","title":"Networking &amp; Ingress","text":"Lab Topic Description 07 Nginx Ingress Configure ingress controllers for external access 10 Istio Implement service mesh for microservices"},{"location":"#configuration-management","title":"Configuration Management","text":"Lab Topic Description 08 Kustomization Manage configurations with Kustomize 13 Helm Chart Package and deploy applications with Helm"},{"location":"#gitops-cicd","title":"GitOps &amp; CI/CD","text":"Lab Topic Description 18 ArgoCD Implement GitOps with ArgoCD"},{"location":"#observability","title":"Observability","text":"Lab Topic Description 14 Logging Centralized logging with Fluentd 15 Prometheus &amp; Grafana Monitoring and visualization 23 Metric Server Resource metrics collection"},{"location":"#advanced-topics","title":"Advanced Topics","text":"Lab Topic Description 11 Custom Resource Definition Extend Kubernetes API with CRDs 16 Affinity, Taint &amp; Toleration Control pod scheduling 17 Pod Disruption Budgets Ensure availability during disruptions 19 Custom Scheduler Build custom scheduling logic 20 CronJob Schedule recurring tasks 21 Auditing Track cluster activities 21 Kube API Work with Kubernetes API 24 Helm Operator Manage Helm releases with operators 25 Kubebuilder Build Kubernetes operators"},{"location":"#tools-utilities","title":"Tools &amp; Utilities","text":"Lab Topic Description 22 Rancher Multi-cluster management platform 26 k9s Terminal-based Kubernetes UI 27 Krew kubectl plugin manager 28 Kubeapps Application dashboard for Kubernetes 29 Kubeadm Bootstrap Kubernetes clusters 30 k9s (Advanced) Advanced k9s usage"},{"location":"#learning-path","title":"\ud83c\udfaf Learning Path","text":""},{"location":"#beginner-track","title":"Beginner Track","text":"<p>Start here if you\u2019re new to Kubernetes: 1. Lab 00: Verify Cluster  2. Lab 01: Namespace  3. Lab 02: Deployments (Imperative)  4. Lab 03: Deployments (Declarative)  5. Lab 05: Services </p>"},{"location":"#intermediate-track","title":"Intermediate Track","text":"<p>For those with basic Kubernetes knowledge: 1. Lab 04: Rollout  2. Lab 06: DataStore  3. Lab 07: Nginx Ingress  4. Lab 08: Kustomization  5. Lab 13: Helm Chart </p>"},{"location":"#advanced-track","title":"Advanced Track","text":"<p>For experienced Kubernetes users: 1. Lab 10: Istio 2. Lab 11: Custom Resource Definition 3. Lab 18: ArgoCD 4. Lab 19: Custom Scheduler 5. Lab 25: Kubebuilder</p>"},{"location":"#tips-for-success","title":"\ud83d\udca1 Tips for Success","text":"<ul> <li>Take your time: Don\u2019t rush through the labs</li> <li>Practice regularly: Repetition builds muscle memory</li> <li>Experiment: Try variations of the examples</li> <li>Read the docs: Kubernetes documentation is excellent</li> <li>Join the community: Engage with other learners</li> </ul>"},{"location":"#get-started","title":"\ud83d\ude80 Get Started","text":"<p>Ready to begin? Click on any lab on the left menu, or start with Lab 00: Verify Cluster!</p>"},{"location":"welcome/","title":"Welcome to Kubernetes Labs","text":"<p>Welcome to the Kubernetes Labs repository! This is a comprehensive collection of hands-on labs designed to help you learn and master Kubernetes concepts, from basic deployments to advanced topics like Istio, ArgoCD and custom schedulers.</p>"},{"location":"welcome/#what-youll-learn","title":"\ud83d\udcda What You\u2019ll Learn","text":"<p>This lab series covers a wide range of <code>Kubernetes</code> topics:</p> <ul> <li>Basics: Namespaces, Deployments, Services and Rollouts</li> <li>Storage: DataStores, Persistent Volume Claims and StatefulSets</li> <li>Networking: Ingress Controllers and Service Mesh (Istio)</li> <li>Configuration Management: Kustomization and Helm Charts</li> <li>GitOps: ArgoCD for continuous deployment</li> <li>Observability: Logging, Prometheus and Grafana</li> <li>Advanced Topics: Custom Resource Definitions (CRDs), Custom Schedulers and Pod Disruption Budgets</li> <li>Tools: k9s, Krew, Kubeapps, Kubeadm and Rancher</li> </ul>"},{"location":"welcome/#who-is-this-for","title":"\ud83c\udfaf Who Is This For?","text":"<p>These labs are designed for:</p> <ul> <li>Beginners wanting to get started with Kubernetes</li> <li>Intermediate users looking to deepen their knowledge</li> <li>Advanced practitioners seeking to master complex Kubernetes patterns</li> <li>DevOps Engineers preparing for Kubernetes certifications (CKA, CKAD, CKS)</li> </ul>"},{"location":"welcome/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<p>Before starting these labs, you should have:</p> <ul> <li>Basic understanding of containerization (Docker)</li> <li>Command-line (CLI) familiarity</li> <li>A Kubernetes cluster (Minikube, Kind, or cloud-based cluster)</li> <li>kubectl installed and configured</li> </ul>"},{"location":"welcome/#how-to-use-this-repository","title":"\ud83d\udcd6 How to Use This Repository","text":"<ol> <li>Start with the Basics: Begin with Lab 00 (Verify Cluster) to ensure your environment is set up correctly</li> <li>Progress Sequentially: The labs are numbered in a logical progression</li> <li>Hands-On Practice: Each lab includes practical exercises and examples</li> <li>Experiment: Don\u2019t be afraid to modify the examples and see what happens</li> </ol>"},{"location":"welcome/#lab-structure","title":"\ud83c\udf93 Lab Structure","text":"<p>Each lab includes:</p> <ul> <li>Objectives: What you\u2019ll learn in the lab</li> <li>Step-by-step instructions: Detailed walkthrough of concepts</li> <li>Example YAML files: Ready-to-use Kubernetes manifests</li> <li>Verification steps: How to confirm everything is working</li> <li>Challenges: Optional exercises to test your understanding</li> </ul>"},{"location":"welcome/#getting-started","title":"\ud83d\udea6 Getting Started","text":"<p>Ready to begin? Head over to the Labs section and start with:</p> <ol> <li>00 Verify Cluster - Verify your Kubernetes cluster is ready</li> <li>01 Namespace - Learn about Kubernetes namespaces</li> <li>02 Deployments Imperative - Create deployments using kubectl</li> </ol>"},{"location":"welcome/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! If you find issues or have suggestions for improvements, please feel free to open an issue or submit a pull request.</p>"},{"location":"welcome/#contact","title":"\ud83d\udcec Contact","text":"<p>For questions or feedback, please reach out through the social links located above the header of this page.</p> <p>Happy Learning! \ud83c\udf89</p> <p>Let\u2019s dive into the world of Kubernetes together!</p>"},{"location":"00-VerifyCluster/","title":"00 Verify Cluster","text":""},{"location":"00-VerifyCluster/#_1","title":"00 Verify Cluster","text":""},{"location":"00-VerifyCluster/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"00-VerifyCluster/#verify-pre-requirements","title":"Verify pre-requirements","text":"<ul> <li><code>kubectl</code> - short for Kubernetes Controller - is the CLI for Kubernetes cluster and is required in order to be able to run the labs.</li> <li>In order to install <code>kubectl</code> and if required creating a local cluster, please refer to Kubernetes - Install Tools</li> </ul>"},{"location":"00-VerifyCluster/#01-installing-minikube","title":"01. Installing minikube","text":"<ul> <li>If you don\u2019t have an existing cluster you can use google cloud for the labs hands-on</li> <li> <p>Click on the button below to be able to run the labs on Google Shell  [Use: CTRL + click to open in new window] <li> <p>Run the following script in the opened terminal</p> </li> <pre><code># Download minikube\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\n\n# Install minikube\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n</code></pre>"},{"location":"00-VerifyCluster/#02-start-minikube","title":"02. Start minikube","text":"<pre><code>minikube start\n</code></pre> <ul> <li>You should see an output like this:</li> </ul> <pre><code>* minikube v1.16.0 on Debian 10.7\n  - MINIKUBE_FORCE_SYSTEMD=true\n  - MINIKUBE_HOME=/google/minikube\n  - MINIKUBE_WANTUPDATENOTIFICATION=false\n* Automatically selected the docker driver\n* Starting control plane node minikube in cluster minikube\n* Pulling base image ...\n* Downloading Kubernetes v1.20.0 preload ...\n    &gt; preloaded-images-k8s-v8-v1....: 491.00 MiB / 491.00 MiB  100.00% 86.82 Mi\n* Creating docker container (CPUs=2, Memory=4000MB) ...\n* Preparing Kubernetes v1.20.0 on Docker 20.10.0 ...\n  - Generating certificates and keys ...\n  - Booting up control plane ...\n  - Configuring RBAC rules ...\n* Verifying Kubernetes components...\n* Enabled addons: default-storageclass, storage-provisioner\n* Done! kubectl is now configured to use \"minikube\" cluster and \"default\" namespace by default\n</code></pre>"},{"location":"00-VerifyCluster/#03-check-the-minikube-status","title":"03. Check the minikube status","text":"<pre><code>minikube status\n</code></pre> <ul> <li>You should see output similar to this one:</li> </ul> <pre><code>minikube\ntype: Control Plane\nhost: Running\nkubelet: Running\napiserver: Running\nkubeconfig: Configured\ntimeToStop: Nonexistent\n</code></pre>"},{"location":"00-VerifyCluster/#04-verify-that-the-cluster-is-up-and-running","title":"04. Verify that the cluster is up and running","text":"<pre><code>$ kubectl cluster-info\n\nKubernetes control plane is running at https://192.168.49.2:8443\nKubeDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n</code></pre> <ul> <li>Verify that <code>kubectl</code> is installed and configured</li> </ul> <pre><code>kubectl config view\n</code></pre> <ul> <li>You should get something like the following <pre><code>apiVersion: v1\nclusters:\n  - cluster:\n      certificate-authority: /google/minikube/.minikube/ca.crt\n      server: https://192.168.49.2:8443\n    name: minikube\ncontexts:\n  - context:\n      cluster: minikube\n      namespace: default\n      user: minikube\n    name: minikube\ncurrent-context: minikube\nkind: Config\npreferences: {}\nusers:\n  - name: minikube\n    user:\n      client-certificate: /google/minikube/.minikube/profiles/minikube/client.crt\n      client-key: /google/minikube/.minikube/profiles/minikube/client.key\n</code></pre></li> </ul>"},{"location":"00-VerifyCluster/#05-verify-that-you-can-talk-to-your-cluster","title":"05. Verify that you can \u201ctalk\u201d to your cluster","text":"<pre><code># In this sample we have minikube pod running\n$ kubectl get nodes\nNAME       STATUS   ROLES                  AGE    VERSION\nminikube   Ready    control-plane,master   3m9s   v1.20.0\n</code></pre>"},{"location":"01-Namespace/","title":"01 Namespace","text":""},{"location":"01-Namespace/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"01-Namespace/#namespaces","title":"Namespaces","text":"<ul> <li>Kubernetes supports multiple virtual clusters backed by the same physical cluster.</li> <li>These virtual clusters are called <code>namespaces</code>.</li> <li><code>Namespaces</code> are the default way for Kubernetes to separate resources.</li> <li>Using <code>namespaces</code> we can isolate the development, improve security and much more.</li> <li>Kubernetes clusters has a builtin <code>namespace</code> called default and might contain more <code>namespaces</code>, like <code>kube-system</code>, for example.</li> </ul>"},{"location":"01-Namespace/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> </ul> <p> CTRL + click to open in new window</p>"},{"location":"01-Namespace/#01-create-namespace","title":"01. Create Namespace","text":"<pre><code># In this sample `codewizard` is the desired namespace\n$ kubectl create namespace codewizard\nnamespace/codewizard created\n\n### !!! Try to create the following namespace (with _ &amp; -), and see what happens:\n$ kubectl create namespace my_namespace-\n</code></pre>"},{"location":"01-Namespace/#02-setting-the-default-namespace-for-kubectl","title":"02. Setting the default Namespace for <code>kubectl</code>","text":"<ul> <li>To set the default namespace run:</li> </ul> <pre><code>$ kubectl config set-context $(kubectl config current-context) --namespace=codewizard\n\nContext minikube modified.\n</code></pre>"},{"location":"01-Namespace/#03-verify-that-youve-updated-the-namespace","title":"03. Verify that you\u2019ve updated the namespace","text":"<pre><code>$ kubectl config get-contexts\nCURRENT     NAME                 CLUSTER          AUTHINFO         NAMESPACE\n            docker-desktop       docker-desktop   docker-desktop\n            docker-for-desktop   docker-desktop   docker-desktop\n*           minikube             minikube         minikube         codewizard\n</code></pre>"},{"location":"01-Namespace/#04-using-the-n-flag","title":"0.4 Using the <code>-n</code> Flag:","text":"<ul> <li>When using <code>kubectl</code> you can pass the <code>-n</code> flag in order to execute the <code>kubectl</code> command on a desired <code>namespace</code>.</li> <li>For example:</li> </ul> <pre><code># get resources of a specific workspace\n$ kubectl get pods -n &lt;namespace&gt;\n</code></pre>"},{"location":"02-Deployments-Imperative/","title":"02 Deployments Imperative","text":""},{"location":"02-Deployments-Imperative/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"02-Deployments-Imperative/#deployment-imperative","title":"Deployment - Imperative","text":""},{"location":"02-Deployments-Imperative/#creating-deployments-using-kubectl-create","title":"Creating deployments using <code>kubectl create</code>","text":"<ul> <li>We start with creating the following deployment   praqma/network-multitool</li> <li>This is a multitool for container/network testing and troubleshooting.</li> </ul>"},{"location":"02-Deployments-Imperative/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> </ul> <p> CTRL + click to open in new window</p>"},{"location":"02-Deployments-Imperative/#01-create-namespace","title":"01. Create Namespace","text":"<ul> <li>As completed in the previous lab, create the desired namespace [codewizard]:</li> </ul> <pre><code>$ kubectl create namespace codewizard\nnamespace/codewizard created\n</code></pre> <ul> <li>In order to set this is as the default namespace, please refer to set default namespace.</li> </ul>"},{"location":"02-Deployments-Imperative/#02-deploy-multitool-image","title":"02. Deploy Multitool Image","text":"<pre><code># Deploy the first container\n$ kubectl create deployment multitool -n codewizard --image=praqma/network-multitool\ndeployment.apps/multitool created\n</code></pre> <ul> <li><code>kubectl create deployment</code> actually creating a replica set for us.</li> <li>We can verify it by running:</li> </ul> <pre><code>$ kubectl get all -n codewizard\n\n## Expected output:\nNAME                                    READY    UP-TO-DATE  AVAILABLE\ndeployment.apps/multitool               1/1      1           1\n\nNAME                                    DESIRED  CURRENT     READY\nreplicaset.apps/multitool-7885b5f94f    1        1           1\n\nNAME                                    READY    STATUS      RESTARTS\npod/multitool-7885b5f94f-9s7xh          1/1      Running     0\n</code></pre>"},{"location":"02-Deployments-Imperative/#03-test-the-deployment","title":"03. Test the Deployment","text":"<ul> <li>The above deployment contains a container named, <code>multitool</code>.</li> <li>In order for us to be able to access this <code>multitool</code> container, we need to create a resource of type <code>Service</code> which will \u201copen\u201d the server for incoming traffic.</li> </ul>"},{"location":"02-Deployments-Imperative/#create-a-service-using-kubectl-expose","title":"Create a service using <code>kubectl expose</code>","text":"<pre><code># \"Expose\" the desired port for incoming traffic\n# This command is equivalent to declare a `kind: Service` im YAML file\n\n$ kubectl expose deployment -n codewizard multitool --port 80 --type NodePort\nservice/multitool exposed\n</code></pre> <ul> <li>Verify that the service have been created by running:</li> </ul> <p><pre><code>$ kubectl get service -n codewizard\n\n# The output should be something like\nNAME                TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nservice/multitool   NodePort   10.102.73.248   &lt;none&gt;        80:31418/TCP   3s\n</code></pre> </p>"},{"location":"02-Deployments-Imperative/#find-the-port-the-ip-which-was-assigned-to-our-pod-by-the-cluster","title":"Find the port &amp; the IP which was assigned to our pod by the cluster.","text":"<ul> <li>Grab the port from the previous output.</li> <li>Port: In the above sample its <code>31418</code> [<code>80:31418/TCP</code>]</li> <li>IP: we will need to grab the cluster ip using <code>kubectl cluster-info</code></li> </ul> <pre><code># get the IP\n$ kubectl cluster-info\n\n# You should get output similar to this one\nKubernetes control plane is running at https://192.168.49.2:8443\nKubeDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\n# Programmatically get the port and the IP\nCLUSTER_IP=$(kubectl get nodes \\\n            --selector=node-role.kubernetes.io/control-plane \\\n            -o jsonpath='{$.items[*].status.addresses[?(@.type==\"InternalIP\")].address}')\n\nNODE_PORT=$(kubectl get -o \\\n            jsonpath=\"{.spec.ports[0].nodePort}\" \\\n            services multitool -n codewizard)\n</code></pre> <ul> <li>In this sample the cluster-ip is <code>192.168.49.2</code></li> </ul>"},{"location":"02-Deployments-Imperative/#test-the-deployment","title":"Test the deployment","text":"<ul> <li>Test to see if the deployment worked using the <code>ip address and port number</code> we have retrieved above.</li> <li>Execute <code>curl</code> with the following parameters: <code>http://${CLUSTER_IP}:${NODE_PORT}</code></li> </ul> <p><pre><code>curl http://${CLUSTER_IP}:${NODE_PORT}\n\n# Or in the above sample\ncurl 192.168.49.2:30436\n\n# The output should be similar to this:\nPraqma Network MultiTool (with NGINX) ...\n</code></pre> - If you get the above output, congratulations! You have successfully created a deployment using imperative commands.</p>"},{"location":"03-Deployments-Declarative/","title":"03 Deployments Declarative","text":""},{"location":"03-Deployments-Declarative/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"03-Deployments-Declarative/#deployment-declarative","title":"Deployment - Declarative","text":""},{"location":"03-Deployments-Declarative/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> </ul> <p> CTRL + click to open in new window</p>"},{"location":"03-Deployments-Declarative/#01-create-namespace","title":"01. Create Namespace","text":"<ul> <li>As completed in the previous lab, create the desired namespace [codewizard]:</li> </ul> <pre><code>$ kubectl create namespace codewizard\nnamespace/codewizard created\n</code></pre> <ul> <li>In order to set this is as the default namespace, please refer to set default namespace.</li> </ul>"},{"location":"03-Deployments-Declarative/#02-deploy-nginx-using-yaml-file-declarative","title":"02. Deploy nginx using yaml file (declarative)","text":"<ul> <li>Let\u2019s create the <code>YAML</code> file for the deployment.</li> <li>If this is your first <code>k8s</code> <code>YAML</code> file, its advisable that you type it in order to get the feeling of the structure.</li> <li>Save the file with the following name: <code>nginx.yaml</code></li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment # We use a deployment and not pod !!!!\nmetadata:\n  name: nginx # Deployment name\n  namespace: codewizard\n  labels:\n    app: nginx # Deployment label\nspec:\n  replicas: 2\n  selector:\n    matchLabels: # Labels for the replica selector\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx # Labels for the replica selector\n        version: \"1.17\" # Specify specific verion if required\n    spec:\n      containers:\n        - name: nginx # The name of the pod\n          image: nginx:1.17 # The image which we will deploy\n          ports:\n            - containerPort: 80\n</code></pre> <ul> <li>Create the deployment using the <code>-f</code> flag &amp; <code>--record=true</code></li> </ul> <pre><code>$ kubectl apply -n codewizard -f nginx.yaml --record=true\ndeployment.extensions/nginx created\n</code></pre>"},{"location":"03-Deployments-Declarative/#03-verify-that-the-deployment-has-been-created","title":"03. Verify that the deployment has been created:","text":"<pre><code>$ kubectl get deployments -n codewizard\nNAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE\nmultitool   1         1         1            1\nnginx       1         1         1            1\n</code></pre>"},{"location":"03-Deployments-Declarative/#04-check-if-the-pods-are-running","title":"04. Check if the pods are running:","text":"<pre><code>$ kubectl get pods -n codewizard\nNAME                         READY   STATUS    RESTARTS\nmultitool-7885b5f94f-9s7xh   1/1     Running   0\nnginx-647fb5956d-v8d2w       1/1     Running   0\n</code></pre>"},{"location":"03-Deployments-Declarative/#05-playing-with-k8s-replicas","title":"05. Playing with K8S replicas","text":"<ul> <li>Let\u2019s play with the replica and see K8S in action.</li> <li>Open a second terminal and execute:</li> </ul> <pre><code>$ kubectl get pods -n codewizard --watch\n</code></pre>"},{"location":"03-Deployments-Declarative/#05-update-the-nginxyaml-file-with-replicas-value-of-5","title":"05. Update the <code>nginx.yaml</code> file with replica\u2019s value of 5:","text":"<pre><code>spec:\n  replicas: 5\n</code></pre>"},{"location":"03-Deployments-Declarative/#06-update-the-deployment-using-kubectl-apply","title":"06. Update the deployment using <code>kubectl apply</code>","text":"<p><pre><code>$ kubectl apply -n codewizard -f nginx.yaml --record=true\ndeployment.apps/nginx configured\n</code></pre> </p> <ul> <li>Switch to the second terminal and you should see something like the following:</li> </ul> <pre><code>$ kubectl get pods --watch -n codewizard\nNAME                         READY   STATUS    RESTARTS   AGE\nmultitool-74477484b8-dj7th   1/1     Running   0          20m\nnginx-dc8bb9b45-hqdv9        1/1     Running   0          111s\nnginx-dc8bb9b45-vdmp5        0/1     Pending   0          0s\nnginx-dc8bb9b45-28wwq        0/1     Pending   0          0s\nnginx-dc8bb9b45-wkc68        0/1     Pending   0          0s\nnginx-dc8bb9b45-vdmp5        0/1     Pending   0          0s\nnginx-dc8bb9b45-28wwq        0/1     Pending   0          0s\nnginx-dc8bb9b45-x7j4g        0/1     Pending   0          0s\nnginx-dc8bb9b45-wkc68        0/1     Pending   0          0s\nnginx-dc8bb9b45-x7j4g        0/1     Pending   0          0s\nnginx-dc8bb9b45-vdmp5        0/1     ContainerCreating   0          0s\nnginx-dc8bb9b45-28wwq        0/1     ContainerCreating   0          0s\nnginx-dc8bb9b45-wkc68        0/1     ContainerCreating   0          0s\nnginx-dc8bb9b45-x7j4g        0/1     ContainerCreating   0          0s\nnginx-dc8bb9b45-vdmp5        1/1     Running             0          2s\nnginx-dc8bb9b45-28wwq        1/1     Running             0          3s\nnginx-dc8bb9b45-x7j4g        1/1     Running             0          3s\nnginx-dc8bb9b45-wkc68        1/1     Running             0          3s\n</code></pre> <p> - Can you explain what do you see?</p> <p><code>Why are there more containers than requested?</code></p>"},{"location":"03-Deployments-Declarative/#07-scaling-down-with-kubectl-scale","title":"07. Scaling down with <code>kubectl scale</code>","text":"<ul> <li>Scaling down using <code>kubectl</code>, and not by editing the <code>YAML</code> file:</li> </ul> <pre><code>kubectl scale -n codewizard --replicas=1 deployment/nginx\n</code></pre> <ul> <li>Switch to the second terminal. The current output should show something like this:</li> </ul> <pre><code>NAME                         READY   STATUS    RESTARTS   AGE\nmultitool-74477484b8-dj7th   1/1     Running   0          29m\nnginx-dc8bb9b45-28wwq        1/1     Running   0          4m41s\nnginx-dc8bb9b45-hqdv9        1/1     Running   0          10m\nnginx-dc8bb9b45-vdmp5        1/1     Running   0          4m41s\nnginx-dc8bb9b45-wkc68        1/1     Running   0          4m41s\nnginx-dc8bb9b45-x7j4g        1/1     Running   0          4m41s\nnginx-dc8bb9b45-x7j4g        1/1     Terminating   0          6m21s\nnginx-dc8bb9b45-vdmp5        1/1     Terminating   0          6m21s\nnginx-dc8bb9b45-28wwq        1/1     Terminating   0          6m21s\nnginx-dc8bb9b45-wkc68        1/1     Terminating   0          6m21s\nnginx-dc8bb9b45-x7j4g        0/1     Terminating   0          6m22s\nnginx-dc8bb9b45-vdmp5        0/1     Terminating   0          6m22s\nnginx-dc8bb9b45-wkc68        0/1     Terminating   0          6m22s\nnginx-dc8bb9b45-28wwq        0/1     Terminating   0          6m22s\nnginx-dc8bb9b45-28wwq        0/1     Terminating   0          6m26s\nnginx-dc8bb9b45-28wwq        0/1     Terminating   0          6m26s\nnginx-dc8bb9b45-vdmp5        0/1     Terminating   0          6m26s\nnginx-dc8bb9b45-vdmp5        0/1     Terminating   0          6m26s\nnginx-dc8bb9b45-wkc68        0/1     Terminating   0          6m27s\nnginx-dc8bb9b45-wkc68        0/1     Terminating   0          6m27s\nnginx-dc8bb9b45-x7j4g        0/1     Terminating   0          6m27s\nnginx-dc8bb9b45-x7j4g        0/1     Terminating   0          6m27s\n</code></pre>"},{"location":"04-Rollout/","title":"04 Rollout","text":""},{"location":"04-Rollout/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"04-Rollout/#rollout-rolling-update","title":"Rollout (Rolling Update)","text":"<ul> <li>In this step we will deploy the same application with several different versions and we will \u201cswitch\u201d between them.</li> <li>For learning purposes we will play a little with the <code>CLI</code>.</li> </ul>"},{"location":"04-Rollout/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> </ul> <p> CTRL + click to open in new window</p>"},{"location":"04-Rollout/#01-create-namespace","title":"01. Create namespace","text":"<ul> <li>As completed in the previous lab, create the desired namespace [codewizard]:</li> </ul> <pre><code>$ kubectl create namespace codewizard\nnamespace/codewizard created\n</code></pre> <ul> <li>In order to set this is as the default namespace, please refer to set default namespace.</li> </ul>"},{"location":"04-Rollout/#02-create-the-desired-deployment","title":"02. Create the desired deployment","text":"<ul> <li> <p>We will use the <code>save-config</code> flag</p> <p><code>save-config</code> If true, the configuration of current object will be saved in its annotation. Otherwise, the annotation will be unchanged. This flag is useful when you want to perform <code>kubectl apply</code> on this object in the future.</p> </li> <li> <p>Let\u2019s run the following: <pre><code>$ kubectl create deployment -n codewizard nginx --image=nginx:1.17 --save-config\n</code></pre> Note that in case we already have this deployed, we will get an error message.</p> </li> </ul>"},{"location":"04-Rollout/#03-expose-nginx-as-a-service","title":"03. Expose nginx as a service","text":"<p><pre><code>$ kubectl expose deployment -n codewizard nginx --port 80 --type NodePort\nservice/nginx exposed\n</code></pre> Again, note that in case we already have this service we will get an error message as well.</p>"},{"location":"04-Rollout/#04-verify-that-the-pods-and-the-service-are-running","title":"04. Verify that the pods and the service are running","text":"<pre><code>$ kubectl get all -n codewizard\n\n# The output should be similar to this\nNAME                        READY      STATUS    RESTARTS   AGE\npod/nginx-db749865c-lmgtv   1/1        Running   0          66s\n\nNAME                        TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE\nservice/nginx               NodePort   10.102.79.9   &lt;none&gt;        80:31204/TCP   30s\n\nNAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nginx   1/1     1            1           66s\n\nNAME                              DESIRED   CURRENT   READY   AGE\nreplicaset.apps/nginx-db749865c   1         1         1       66s\n</code></pre>"},{"location":"04-Rollout/#05-change-the-number-of-replicas-to-3","title":"05. Change the number of replicas to 3","text":"<pre><code>$ kubectl scale deployment -n codewizard nginx --replicas=3\ndeployment.apps/nginx scaled\n</code></pre>"},{"location":"04-Rollout/#06-verify-that-now-we-have-3-replicas","title":"06. Verify that now we have 3 replicas","text":"<pre><code>$ kubectl get pods -n codewizard\nNAME                    READY   STATUS    RESTARTS   AGE\nnginx-db749865c-f5mkt   1/1     Running   0          86s\nnginx-db749865c-jgcvb   1/1     Running   0          86s\nnginx-db749865c-lmgtv   1/1     Running   0          4m44s\n</code></pre>"},{"location":"04-Rollout/#07-test-the-deployment","title":"07. Test the deployment","text":"<pre><code># !!! Get the Ip &amp; port for this service\n$ kubectl get services -n codewizard -o wide \n\n# Write down the port number\nNAME    TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE    SELECTOR\nnginx   NodePort   10.102.79.9   &lt;none&gt;        80:31204/TCP   7m7s   app=nginx\n\n# Get the cluster IP and port\n$ kubectl cluster-info  \nKubernetes control plane is running at https://192.168.49.2:8443\n\n# Using the above &lt;host&gt;:&lt;port&gt; test the nginx\n# -I is for getting the headers\n$ curl -sI &lt;host&gt;:&lt;port&gt;\n\n# The response should display the nginx version\nexample: curl -sI 192.168.49.2:31204\n\nHTTP/1.1 200 OK\nServer: nginx/1.17.10 &lt;------------ This is the pod version\nDate: Fri, 15 Jan 2021 20:13:48 GMT\nContent-Type: text/html\nContent-Length: 612\nLast-Modified: Tue, 14 Apr 2020 14:19:26 GMT\nConnection: keep-alive\nETag: \"5e95c66e-264\"\nAccept-Ranges: bytes\n...\n</code></pre>"},{"location":"04-Rollout/#08-deploy-another-version-of-nginx","title":"08. Deploy another version of nginx","text":"<pre><code># Deploy another version of nginx (1.16)\n$ kubectl set image deployment -n codewizard nginx nginx=nginx:1.16 --record\ndeployment.apps/nginx image updated\n\n# Check to verify that the new version deployed - same as in previous step\n$ curl -sI &lt;host&gt;:&lt;port&gt;\n\n# The response should display the new version\nHTTP/1.1 200 OK\nServer: nginx/1.16.1 &lt;------------ This is the pod version (new version)\nDate: Fri, 15 Jan 2021 20:16:11 GMT\nContent-Type: text/html\nContent-Length: 612\nLast-Modified: Tue, 13 Aug 2019 10:05:00 GMT\nConnection: keep-alive\nETag: \"5d528b4c-264\"\nAccept-Ranges: bytes\n</code></pre>"},{"location":"04-Rollout/#09-investigate-rollout-history","title":"09. Investigate rollout history:","text":"<ul> <li>The rollout history command print out all the saved records:</li> </ul> <pre><code>$ kubectl rollout history deployment nginx -n codewizard\ndeployment.apps/nginx\nREVISION  CHANGE-CAUSE\n1         &lt;none&gt;\n2         kubectl set image deployment nginx nginx=nginx:1.16 --record=true\n3         kubectl set image deployment nginx nginx=nginx:1.15 --record=true\n</code></pre>"},{"location":"04-Rollout/#10-lets-see-what-was-changed-during-the-previous-updates","title":"10. Let\u2019s see what was changed during the previous updates:","text":"<ul> <li>Print out the rollout changes:</li> </ul> <pre><code># replace the X with 1 or 2 or any number revision id\n$ kubectl rollout history deployment nginx -n codewizard --revision=&lt;X&gt;  # replace here\ndeployment.apps/nginx with revision #1\nPod Template:\n  Labels:       app=nginx\n        pod-template-hash=db749865c\n  Containers:\n   nginx:\n    Image:      nginx:1.17\n    Port:       &lt;none&gt;\n    Host Port:  &lt;none&gt;\n    Environment:        &lt;none&gt;\n    Mounts:     &lt;none&gt;\n  Volumes:      &lt;none&gt;\n</code></pre>"},{"location":"04-Rollout/#11-undo-the-version-upgrade-by-rolling-back-and-restoring-previous-version","title":"11. Undo the version upgrade by rolling back and restoring previous version","text":"<pre><code># Check the current nginx version\n$ curl -sI &lt;host&gt;:&lt;port&gt;\n\n# Undo the last deployment\n$ kubectl rollout undo deployment nginx\ndeployment.apps/nginx rolled back\n\n# Verify that we have the previous version\n$ curl -sI &lt;host&gt;:&lt;port&gt;\n</code></pre>"},{"location":"04-Rollout/#12-rolling-restart","title":"12. Rolling Restart","text":"<ul> <li>If we deploy using <code>imagePullPolicy: always</code> set in the <code>YAML</code> file, we can use <code>rollout restart</code> to force <code>K8S</code> to grab the latest image.</li> <li>This is the fastest restart method these days</li> </ul> <pre><code># Force pods restart\nkubectl rollout restart deployment [deployment_name]\n</code></pre>"},{"location":"05-Services/","title":"05 Services","text":""},{"location":"05-Services/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"05-Services/#service-discovery","title":"Service Discovery","text":"<ul> <li>In the following lab we will learn what is a <code>Service</code> and go over the different <code>Service</code> types.</li> </ul>"},{"location":"05-Services/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> </ul> <p> CTRL + click to open in new window</p>"},{"location":"05-Services/#01-some-general-notes-on-what-is-a-service","title":"01. Some general notes on what is a <code>Service</code>","text":"<ul> <li><code>Service</code> is a unit of application behavior bound to a unique name in a <code>service registry</code>. </li> <li><code>Service</code> consist of multiple <code>network endpoints</code> implemented by workload instances running on pods, containers, VMs etc.</li> <li><code>Service</code> allow us to gain access to any given pod or container (e.g., a web service).</li> <li>A <code>service</code> is (normally) created on top of an existing deployment and exposing it to the \u201cworld\u201d, using IP(s) &amp; port(s).</li> <li><code>K8S</code> define 3 main ways (+FQDN internally) to define a service, which means that we have 4 different ways to access Pods.</li> <li>There are several proxy mode which inplements diffrent behaviour, for example in <code>user proxy mode</code> for each <code>Service</code> <code>kube-proxy</code> opens a port (randomly chosen) on the local node. Any connections to this \u201cproxy port\u201d are proxied to one of the Service\u2019s backend Pods (as reported via Endpoints).</li> <li>All the service types are assigned with a <code>Cluster-IP</code>.</li> <li>Every service also creates <code>Endoint(s)</code>, which point to the actual pods. <code>Endpoints</code> are usually referred to as <code>back-ends</code> of a particular service.</li> </ul>"},{"location":"05-Services/#01-create-namespace-and-clear-previous-data-if-there-is-any","title":"01. Create namespace and clear previous data if there is any","text":"<pre><code># If the namespace already exists and contains data form previous steps, let's clean it\nkubectl delete namespace codewizard\n\n# Create the desired namespace [codewizard]\n$ kubectl create namespace codewizard\nnamespace/codewizard created\n</code></pre>"},{"location":"05-Services/#02-create-the-required-resources-for-this-hand-on","title":"02. Create the required resources for this hand-on","text":"<p><pre><code># Network tools pod\n$ kubectl create deployment -n codewizard multitool --image=praqma/network-multitool\ndeployment.apps/multitool created\n\n# nginx pod\n$ kubectl create deployment -n codewizard nginx --image=nginx\ndeployment.apps/nginx created\n\n# Verify that the pods running\n$ kubectl get all -n codewizard\n\nNAME                             READY   STATUS    RESTARTS   AGE\npod/multitool-74477484b8-bdrwr   1/1     Running   0          29s\npod/nginx-6799fc88d8-p2fjn       1/1     Running   0          7s\nNAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/multitool   1/1     1            1           30s\ndeployment.apps/nginx       1/1     1            1           8s\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/multitool-74477484b8   1         1         1       30s\nreplicaset.apps/nginx-6799fc88d8       1         1         1       8s\n</code></pre> </p>"},{"location":"05-Services/#service-types","title":"Service types","text":"<ul> <li>As previously mentioned, there are several services type. Let\u2019s practice them:</li> </ul>"},{"location":"05-Services/#service-type-clusterip","title":"Service type: ClusterIP","text":"<ul> <li>If not specified, the default service type is <code>ClusterIP</code>.</li> <li>In order to expose the deployment as a service, use: <code>--type=ClusterIP</code></li> <li><code>ClusterIP</code> will expose the pods within the cluster. Since we don\u2019t have an <code>external IP</code>, it will not be reachable from outside the cluster.</li> <li>When the service is created <code>K8S</code> attaches a DNS record to the service in the following format: <code>&lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local</code></li> </ul>"},{"location":"05-Services/#03-expose-the-nginx-with-clusterip","title":"03. Expose the nginx with ClusterIP","text":"<pre><code># Expose the service on port 80\n$ kubectl expose deployment nginx -n codewizard --port 80 --type ClusterIP\nservice/nginx exposed\n\n# Check the services and see it's type\n# Grab the ClusterIP - we will use it in the next steps\n$ kubectl get services -n codewizard\n\nNAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)\nnginx        ClusterIP   10.109.78.182   &lt;none&gt;        80/TCP\n</code></pre>"},{"location":"05-Services/#04-test-the-nginx-with-clusterip","title":"04. Test the nginx with ClusterIP","text":"<ul> <li>Since the service is a <code>ClusterIP</code>, we will test if we can access the service using the multitool pod.</li> </ul> <pre><code># Get the name of the multitool pod to be used\n$ kubectl get pods -n codewizard\nNAME\nmultitool-XXXXXX-XXXXX\n\n# Run an interactive shell inside the network-multitool-container (same concept as with Docker)\n$ kubectl exec -it &lt;pod name&gt; -n codewizard -- sh\n</code></pre> <ul> <li>Connect to the service in any of the following ways:</li> </ul>"},{"location":"05-Services/#test-the-nginx-with-clusterip","title":"Test the nginx with ClusterIP","text":""},{"location":"05-Services/#1-using-the-ip-from-the-services-output-grab-the-server-response","title":"1. using the IP from the services output. grab the server response:","text":"<pre><code>bash-5.0# curl -s &lt;ClusterIP&gt;\n</code></pre> <pre><code># Expected output:\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nhtml { color-scheme: light dark; }\nbody { width: 35em; margin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif; }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"05-Services/#2-test-the-nginx-using-the-deployment-name-using-the-service-name-since-its-the-dns-name-behind-the-scenes","title":"2. Test the nginx using the deployment name - using the service name since its the DNS name behind the scenes","text":"<pre><code>bash-5.0# curl -s nginx\n</code></pre> <pre><code># Expected output:\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Welcome to nginx!&lt;/title&gt;\n    &lt;style&gt;\n      body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n      }\n    &lt;/style&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n    &lt;p&gt;\n      If you see this page, the nginx web server is successfully installed and\n      working. Further configuration is required.\n    &lt;/p&gt;\n    &lt;p&gt;\n      For online documentation and support please refer to\n      &lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br /&gt;\n      Commercial support is available at\n      &lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.\n    &lt;/p&gt;\n    &lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"05-Services/#3-using-the-full-dns-name-for-every-service-we-have-a-full-fqdn-fully-qualified-domain-name-so-we-can-use-it-as-well","title":"3. using the full DNS name - for every service we have a full <code>FQDN</code> (Fully qualified domain name) so we can use it as well","text":"<p><pre><code># bash-5.0# curl -s &lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local\nbash-5.0# curl -s nginx.codewizard.svc.cluster.local\n</code></pre> </p>"},{"location":"05-Services/#service-type-nodeport","title":"Service type: NodePort","text":"<ul> <li><code>NodePort</code>: Exposes the Service on each Node\u2019s IP at a static port (the <code>NodePort</code>).</li> <li>A <code>ClusterIP</code> Service, to which the <code>NodePort</code> Service routes, is automatically created.</li> <li><code>NodePort</code> service is reachable from outside the cluster, by requesting <code>&lt;Node IP&gt;:&lt;Node Port&gt;</code>.</li> </ul>"},{"location":"05-Services/#05-create-nodeport","title":"05. Create NodePort","text":""},{"location":"05-Services/#1-delete-previous-service","title":"1. Delete previous service","text":"<pre><code># Delete the existing service from previous steps\n$ kubectl delete svc nginx -n codewizard\nservice \"nginx\" deleted from codewizard namespace\n</code></pre>"},{"location":"05-Services/#2-create-nodeport-service","title":"2. Create <code>NodePort</code> service","text":"<p><pre><code># As before but this time the type is a NodePort\n$ kubectl expose deployment -n codewizard nginx --port 80 --type NodePort\nservice/nginx exposed\n\n# Verify that the type is set to NodePort.\n# This time you should see ClusterIP and port as well\n$ kubectl get svc -n codewizard\nNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)\nnginx        NodePort    100.65.29.172  &lt;none&gt;        80:32593/TCP\n</code></pre> </p>"},{"location":"05-Services/#3-test-the-nodeport-service","title":"3. Test the <code>NodePort</code> service","text":"<ul> <li> <p>If we have the host IP and the node port number, we can connect directly to the pod.</p> </li> <li> <p>If you followed the previous labs, you should be able to do it yourself by now......</p> </li> </ul> <pre><code># Tiny clue....\n$ kubectl cluster-info\n$ kubectl get services\n\n# Executing curl &lt;cluster host ip&gt;:&lt;port&gt; you should see the flowing Output\nWelcome to nginx!\n...\nThank you for using nginx.\n</code></pre>"},{"location":"05-Services/#service-type-loadbalancer","title":"Service type: LoadBalancer","text":"<p>Note</p> <p>We cannot test a <code>LoadBalancer</code> service locally on a localhost, but only on a cluster which can provide an <code>external-IP</code></p>"},{"location":"05-Services/#06-create-loadbalancer-only-if-you-are-on-real-cloud","title":"06. Create LoadBalancer (only if you are on real cloud)","text":""},{"location":"05-Services/#1-delete-previous-service_1","title":"1. Delete previous service","text":"<p><pre><code># Delete the existing service from previous steps\n$ kubectl delete svc nginx -n codewizard\nservice \"nginx\" deleted\n</code></pre> </p>"},{"location":"05-Services/#2-create-loadbalancer-service","title":"2. Create <code>LoadBalancer</code> Service","text":"<p><pre><code># As before this time the type is a LoadBalancer\n$ kubectl expose deployment nginx -n codewizard --port 80 --type LoadBalancer\nservice/nginx exposed\n\n# In real cloud we should se an EXTERNAL-IP and we can access the service\n# via the internet\n$ kubectl get svc\nNAME         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)\nnginx        LoadBalancer   100.69.15.89   35.205.60.29  80:31354/TCP\n</code></pre> </p>"},{"location":"05-Services/#3-test-the-loadbalancer-service","title":"3. Test the <code>LoadBalancer</code> Service","text":"<pre><code># Testing load balancer only require us to use the EXTERNAL-IP\n$ curl -s &lt;EXTERNAL-IP&gt;\n</code></pre>"},{"location":"06-DataStore/","title":"06 DataStore","text":""},{"location":"06-DataStore/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"06-DataStore/#data-store","title":"Data Store","text":""},{"location":"06-DataStore/#secrets-and-configmaps","title":"Secrets and ConfigMaps","text":"<ul> <li>Secrets &amp; ConfigMap are ways to store and inject configurations into your deployments.</li> <li>Secrets usually store passwords, certificates, API keys and more.</li> <li>ConfigMap usually store configuration (data).</li> </ul>"},{"location":"06-DataStore/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> </ul> <p> CTRL + click to open in new window</p>"},{"location":"06-DataStore/#first-lets-play-a-bit-with-secrets","title":"First, Let\u2019s play a bit with Secrets","text":""},{"location":"06-DataStore/#01-create-namespace-and-clear-previous-data-if-there-is-any","title":"01. Create namespace and clear previous data (if there is any)","text":"<pre><code># If the namespace already exist and contains data from previous steps, lets clean it\nkubectl delete namespace codewizard\n\n# Create the desired namespace [codewizard]\n$ kubectl create namespace codewizard\nnamespace/codewizard created\n</code></pre> <p>Note</p> <p>You can skip section number 02. if you don\u2019t wish to build and push your docker container</p>"},{"location":"06-DataStore/#02-build-the-docker-container","title":"02. Build the docker container","text":""},{"location":"06-DataStore/#1-write-the-server-code","title":"1. write the server code","text":"<ul> <li>For this demo we will use a tiny NodeJS server which will consume the desired configuration values from the secret</li> <li>This is the code of our server server.js:</li> </ul> <pre><code>//\n// server.js\n//\nconst \n  // Get those values in runtime.\n  // The variables will be passed from the Docker file and later on from K8S ConfingMap/ecret\n  language = process.env.LANGUAGE,\n  token = process.env.TOKEN;\n\nrequire(\"http\")\n  .createServer((request, response) =&gt; {\n    response.write(`Language: ${language}`);\n    response.write(`Token   : ${token}\\n`);\n    response.end(`\\n`);\n  })\n  // Set the default port to 5000\n  .listen(process.env.PORT || 5000 );\n</code></pre>"},{"location":"06-DataStore/#2-write-the-dockerfile","title":"2. Write the DockerFile","text":"<ul> <li>First, let\u2019s wrap it up as <code>docker container</code></li> <li>If you wish, you can skip this and use the existing docker image: <code>nirgeier/k8s-secrets-sample</code></li> <li>In the <code>Dockerfile</code> we will set the <code>ENV</code> for or variables</li> </ul> <pre><code># Base Image\nFROM        node\n\n# exposed port - same port is defined in the server.js\nEXPOSE      5000\n\n# The \"configuration\" which we pass in runtime\n# The server will \"read\" those variables at run time and will print them out\nENV         LANGUAGE    Hebrew\nENV         TOKEN       Hard-To-Guess\n\n# Copy the server to the container\nCOPY        server.js .\n\n# start the server\nENTRYPOINT  node server.js\n</code></pre>"},{"location":"06-DataStore/#3-build-the-docker-container","title":"3. Build the docker container","text":"<pre><code># The container name is prefixed with the Dockerhub account\n# !!! You should replace the prefix to your dockerhub account\n# In the sample the username is `nirgeier`\n$ docker build . -t nirgeier/k8s-secrets-sample\n\n# The output should be similar to this\nSending build context to Docker daemon   12.8kB\nStep 1/6 : FROM        node\nlatest: Pulling from library/node\n2587235a7635: Pull complete\n953fe5c215cb: Pull complete\nd4d3f270c7de: Pull complete\ned36dafe30e3: Pull complete\n00e912dd434d: Pull complete\ndd25ee3ea38e: Pull complete\n7e835b17ced9: Pull complete\n79ae84aa9e91: Pull complete\n629164f2c016: Pull complete\nDigest: sha256:3a9d0636755ebcc8e24148a148b395c1608a94bb1b4a219829c9a3f54378accb\nStatus: Downloaded newer image for node:latest\n ---&gt; d6740064592f\nStep 2/6 : EXPOSE      5000\n ---&gt; Running in 060220eaa65b\nRemoving intermediate container 060220eaa65b\n ---&gt; 68262a3e6741\nStep 3/6 : ENV         LANGUAGE    Hebrew\n ---&gt; Running in c404e7e6fa16\nRemoving intermediate container c404e7e6fa16\n ---&gt; 45fcf1fe03aa\nStep 4/6 : ENV         TOKEN       Hard-To-Guess\n ---&gt; Running in d3c1491f9de5\nRemoving intermediate container d3c1491f9de5\n ---&gt; 71e8acdbdab2\nStep 5/6 : COPY        server.js .\n ---&gt; 42233d2b66a8\nStep 6/6 : ENTRYPOINT  node server.js\n ---&gt; Running in 223629e16589\nRemoving intermediate container 223629e16589\n ---&gt; f5cbb1895d66\nSuccessfully built f5cbb1895d66\nSuccessfully tagged nirgeier/k8s-secrets-sample:latest\n</code></pre>"},{"location":"06-DataStore/#4-test-the-container","title":"4. Test the container","text":"<pre><code># Run the docker container which you build earlier,\n# replace the name if you used your own name\n# and check the response from the server.\n# It should print out the variables which were defined inside the DockerFile \n$ docker run -d -p5000:5000 nirgeier/k8s-secrets-sample --name server\n\n# Get the response from the container \n# The port is the one which we exposed inside the DockerFile\ncurl 127.0.0.1:5000\n\n# Response:\nLanguage: Hebrew\nToken   : Hard-To-Guess\n</code></pre> <ul> <li>Stop the container</li> </ul> <pre><code># Stop the running container\n# We are using the name which we passed in the `docker run` command --name &lt;container name&gt;\ndocker stop server\n</code></pre> <ul> <li>Push the container to your docker hub account if you wish</li> </ul>"},{"location":"06-DataStore/#03-using-k8s-deployment-secretsconfigmap","title":"03. Using K8S deployment &amp; Secrets/ConfigMap","text":""},{"location":"06-DataStore/#1-writing-the-deployment-service-file","title":"1. Writing the deployment &amp; service file","text":"<ul> <li>Deploy the docker container that you have prepared in the previous step with the following <code>Deployment</code> file.</li> <li>In this sample we will define the values in the <code>YAML</code> file, later on we will use Secrets/ConfigMap variables-from-yaml.yaml</li> </ul> <p><pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: codewizard\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: secrets-app\n  namespace: codewizard\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: secrets-app\n  template:\n    metadata:\n      labels:\n        name: secrets-app\n    spec:\n      containers:\n        - name: secrets-app\n          image: nirgeier/k8s-secrets-sample\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 5000\n          env:\n            - name: LANGUAGE\n              value: Hebrew\n            - name: TOKEN\n              value: Hard-To-Guess2\n          resources:\n            limits:\n              cpu: \"500m\"\n              memory: \"256Mi\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: codewizard-secrets\n  namespace: codewizard\nspec:\n  selector:\n    app: codewizard-secrets\n  ports:\n    - protocol: TCP\n      port: 5000\n      targetPort: 5000\n</code></pre> </p>"},{"location":"06-DataStore/#2-deploy-to-cluster","title":"2. Deploy to cluster","text":"<p><pre><code>$ kubectl apply -n codewizard -f variables-from-yaml.yaml\ndeployment.apps/codewizard-secrets configured\nservice/codewizard-secrets created\n</code></pre> </p>"},{"location":"06-DataStore/#3-test-the-app","title":"3. Test the app","text":"<ul> <li>We will need a second container for executing the curl request.</li> <li>We will use a <code>busyBox image</code> for this purpose.</li> </ul> <pre><code># grab the name of the pod\n$ kubectl get pods -n codewizard\n\n# Output\nNAME                                  READY   STATUS    RESTARTS   AGE\ncodewizard-secrets-56f556c758-2mknc   1/1     Running   0          6m27s\n\n# Login to the container and test the reponse\n# kubectl exec -it -n codewizard &lt;pod name&gt; -- sh\n# For the above output we will use\nkubectl exec -it -n codewizard codewizard-secrets-56f556c758-2mknc -- sh\n\n# Now get the server response (from inside the container)\n$ curl localhost:5000                    \n\n# Response\nLanguage: Hebrew\nToken   : Hard-To-Guess2\n</code></pre>"},{"location":"06-DataStore/#04-using-secrets-config-maps","title":"04. Using Secrets &amp; config maps","text":""},{"location":"06-DataStore/#1-create-the-desired-secret-and-config-map-for-this-lab","title":"1. Create the desired secret and config map for this lab","text":"<pre><code># Create the secret \n#   Key   = Token\n#   Value = Hard-To-Guess3\n$ kubectl create -n codewizard secret generic token --from-literal=TOKEN=Hard-To-Guess3\nsecret/token created\n\n# Create the config map \n#   Key   = LANGUAGE\n#   Value = English\n$ kubectl create -n codewizard configmap language --from-literal=LANGUAGE=English\nconfigmap/language created\n\n# Verify that the resources have been created:\n$ kubectl get secrets,cm -n codewizard\nNAME                         TYPE                                  DATA   AGE\nsecret/default-token-8hzhn   kubernetes.io/service-account-token   3      14m\nsecret/token                 Opaque                                1      80s\nNAME                         DATA   AGE\nconfigmap/kube-root-ca.crt   1      14m\nconfigmap/language           1      44s\n\n# Like other resources we can use describe to view the resource\n$ kubectl describe secret token -n codewizard\nName:         token\nNamespace:    codewizard\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\n\nType:  Opaque &lt;----- The content is stored as BASE64\n\nData\n====\nTOKEN:  14 bytes\n\n# Same way for the ConfigMap\n$ kubectl describe cm language -n codewizard\nName:         language\nNamespace:    codewizard\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nData\n====\nLANGUAGE:\n----\nEnglish\nEvents:  &lt;none&gt;\n</code></pre>"},{"location":"06-DataStore/#2-update-the-deployment-to-read-the-values-from-secrets-configmap","title":"2. Update the deployment to read the values from Secrets &amp; ConfigMap","text":"<ul> <li>Change the <code>env</code> section to the following:</li> </ul> <pre><code>          env:\n            - name: LANGUAGE\n              valueFrom:\n                configMapKeyRef:    # This value will be read from the config map\n                  name:   language  # The name of the ConfigMap\n                  key:    LANGUAGE  # The key in the config map\n            - name: TOKEN\n              valueFrom:\n                  secretKeyRef:         # This value will be read from the secret\n                      name:   token     # The name of the secret\n                      key:    TOKEN     # The key in the secret\n</code></pre>"},{"location":"06-DataStore/#3-update-the-deployment-to-read-values-from-k8s-resources","title":"3. Update the deployment to read values from K8S resources","text":"<p><pre><code>$ kubectl apply -n codewizard -f variables-from-secrets.yaml\ndeployment.apps/codewizard-secrets configured\nservice/codewizard-secrets unchanged\n</code></pre> </p>"},{"location":"06-DataStore/#4-test-the-changes","title":"4. Test the changes","text":"<ul> <li>Refer to step 3.3 for testing your server</li> </ul> <pre><code># Login to the server\n# In this sample, the pod name is: codewizard-secrets-76d99bdc54-s66vl\nkubectl exec -it codewizard-secrets-76d99bdc54-s66vl -n codewizard -- sh\n\n# Test the changes to verify that they are set from the Secret/ConfigMap\ncurl localhost:5000\n\n# Out put should be\nLanguage: English\nToken   : Hard-To-Guess3\n</code></pre> <p>Note</p> <p>Pods are not recreated or updated automatically when <code>Secrets</code> or <code>ConfigMaps</code> change, so you will have to restart your pods manually</p> <ul> <li>To update existing secrets or ConfigMap:</li> </ul> <pre><code>$ kubectl create secret generic token -n codewizard --from-literal=Token=Token3 -o yaml --dry-run=client | kubectl replace -f -\nsecret/token replaced\n</code></pre> <ul> <li>Test your server and verify that you see the old values.</li> <li>Delete the old pods so they can come back to life with the new values.</li> <li>Test your server again, now you should be able to see the changes.</li> </ul>"},{"location":"07-nginx-Ingress/","title":"07 Nginx Ingress","text":""},{"location":"07-nginx-Ingress/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"07-nginx-Ingress/#nginx-ingress","title":"Nginx-Ingress","text":"<p>Important</p> <p>We cannot see it in action on a <code>localhost</code> (meaning that it will not get an external IP) unless we use the explicit <code>http://host:port</code> format.</p> <ul> <li>Kubernetes <code>ingress</code> object is a <code>DNS</code></li> <li>To enable an <code>ingress object</code>, we need an <code>ingress controller</code></li> <li>In this demo we will use <code>Nginx-Ingress</code></li> </ul>"},{"location":"07-nginx-Ingress/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> </ul> <p> CTRL + click to open in new window</p>"},{"location":"07-nginx-Ingress/#01-deploy-sample-app","title":"01. Deploy sample app","text":"<ul> <li>To get started with <code>Nginx-Ingress</code>, we will deploy out previous app:</li> </ul> <pre><code># Create 3 containers\n$ kubectl create deployment ingress-pods --image=nirgeier/k8s-secrets-sample --replicas=3\n\n# Expose the service\n$ kubectl expose deployment ingress-pods --port=5000\n</code></pre>"},{"location":"07-nginx-Ingress/#02-deploy-default-backend","title":"02. Deploy default backend","text":"<ul> <li>Now lets deploy the <code>Nginx-Ingress</code> (grabbed from the official site):</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n    name: default-http-backend\nspec:\n    replicas: 1\n    selector:\n        matchLabels:\n        app: default-http-backend\n    template:\n        metadata:\n        labels:\n            app: default-http-backend\n        spec:\n            terminationGracePeriodSeconds: 60\n            containers:\n            - name: default-http-backend\n                # Any image is permissable as long as:\n                # 1. It serves a 404 page at /\n                # 2. It serves 200 on a /healthz endpoint\n                image: gcr.io/google_containers/defaultbackend:1.0\n                livenessProbe:\n                httpGet:\n                    path: /healthz\n                    port: 8080\n                    scheme: HTTP\n                initialDelaySeconds: 30\n                timeoutSeconds: 5\n                ports:\n                - containerPort: 8080\n                resources:\n                limits:\n                    cpu: 10m\n                    memory: 20Mi\n                requests:\n                    cpu: 10m\n                    memory: 20Mi\n</code></pre>"},{"location":"07-nginx-Ingress/#03-create-service","title":"03. Create service","text":"<ul> <li>Next, let\u2019s create the service:</li> </ul> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n    name: default-http-backend\nspec:\n    selector:\n    app: default-http-backend\n    ports:\n    - protocol: TCP\n    port: 80\n    targetPort: 8080\n    type: NodePort\n</code></pre>"},{"location":"07-nginx-Ingress/#04-import-ssl-certificate","title":"04. Import <code>ssl</code> certificate","text":"<ul> <li>In this demo we will use certificate.    </li> <li>The certificate is in the same folder as this file</li> <li>The certificate is for the hostname: <code>ingress.local</code></li> </ul> <pre><code># If you wish to create the certificate use this script\n### ---&gt; The common Name fiels is your host for later on\n###      Common Name (e.g. server FQDN or YOUR name) []:\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout certificate.key -out certificate.crt\n\n# Create a pem file\n# The purpose of the DH parameters is to exchange secrets\n$ openssl dhparam -out certificate.pem 2048\n</code></pre> <ul> <li>Store the certificate in secret:</li> </ul> <pre><code># Store the certificate\n$ kubectl create secret tls tls-certificate --key certificate.key --cert certificate.crt\nsecret/tls-certificate created\n\n# Store the DH parameters\n$ kubectl create secret generic tls-dhparam --from-file=certificate.pem\nsecret/tls-dhparam created\n</code></pre>"},{"location":"07-nginx-Ingress/#05-deploy-the-ingress","title":"05. Deploy the ingress","text":"<ul> <li>Now that we have the certificate, we can deploy the <code>Ingress</code>:</li> </ul> <pre><code># Ingress.yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n    name: my-first-ingress\nannotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    nginx.org/ssl-services: \"my-service\"\nspec:\n    tls:\n        - hosts:\n        - myapp.local\n        secretName: tls-certificate\n    rules:\n    - host: myapp.local\n        http:\n        paths:\n        - path: /\n            backend:\n            serviceName: ingress-pods\n            servicePort: 5000\n</code></pre>"},{"location":"07-nginx-Ingress/#06-enable-the-ingress-addon","title":"06. Enable the ingress addon","text":"<ul> <li>The <code>Ingress</code> is not enabled by default, so we have to \u201cturn it on\u201d:</li> </ul> <pre><code>$ minikube addons enable ingress\n\u2705  ingress was successfully enabled\n</code></pre>"},{"location":"08-Kustomization/","title":"08 Kustomization","text":""},{"location":"08-Kustomization/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"08-Kustomization/#kustomization-kubectl-kustomize","title":"Kustomization - <code>kubectl kustomize</code>","text":""},{"location":"08-Kustomization/#declarative-configuration-in-kubernetes","title":"Declarative Configuration in Kubernetes","text":"<ul> <li><code>Kustomize</code> is a very powerful too for customizing and building Kubernetes resources.</li> <li><code>Kustomize</code> started at 2017, and added to <code>kubectl</code> since version 1.14.</li> <li><code>Kustomize</code> has many useful features for managing and deploying resource.</li> <li>When you execute a Kustomization beside using the builtin features, it will also re-order the resources in a logical way for the K8S to be deployed.</li> </ul>"},{"location":"08-Kustomization/#01-re-order-the-resources","title":"01. Re-order the resources","text":"<ul> <li> <p><code>Kustomization</code> re-orders the <code>Kind</code> for optimization. For this demo, we will need an existing <code>namespace</code> before using it.</p> </li> <li> <p>The order of the resources is defined in the source code</p> </li> </ul> <pre><code>// An attempt to order things to help k8s, e.g.\n// - Namespace should be first.\n// - Service should come before things that refer to it.\n// In some cases order just specified to provide determinism.\nvar orderFirst = []string{\n    \"Namespace\",\n    \"ResourceQuota\",\n    \"StorageClass\",\n    \"CustomResourceDefinition\",\n    \"ServiceAccount\",\n    \"PodSecurityPolicy\",\n    \"Role\",\n    \"ClusterRole\",\n    \"RoleBinding\",\n    \"ClusterRoleBinding\",\n    \"ConfigMap\",\n    \"Secret\",\n    \"Endpoints\",\n    \"Service\",\n    \"LimitRange\",\n    \"PriorityClass\",\n    \"PersistentVolume\",\n    \"PersistentVolumeClaim\",\n    \"Deployment\",\n    \"StatefulSet\",\n    \"CronJob\",\n    \"PodDisruptionBudget\",\n}\n\nvar orderLast = []string{\n    \"MutatingWebhookConfiguration\",\n    \"ValidatingWebhookConfiguration\",\n}\n</code></pre>"},{"location":"08-Kustomization/#02-base-resource-for-our-demo","title":"02. Base resource for our demo","text":"<ul> <li>In the following samples we will refer to the following <code>base.yaml</code> file:</li> </ul> <pre><code># base.yaml\n# This is the base file for all the demos in this folder\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n        - name: myapp\n          image: __image__\n</code></pre>"},{"location":"08-Kustomization/#03-common-features","title":"03. Common Features","text":"<ul> <li>common Annotation</li> <li>common Labels</li> <li>Generators</li> <li>Config Map Generator<ul> <li>From Env</li> <li>From File</li> <li>From Literal</li> </ul> </li> <li>Secret Generator</li> <li>images</li> <li>Namespaces</li> <li>Prefix / Suffix</li> <li>Replicas</li> <li>Patches<ul> <li>Patch Add/Update</li> <li>Patch Delete</li> <li>Patch Replace</li> </ul> </li> </ul>"},{"location":"08-Kustomization/#commonannotation","title":"<code>commonAnnotation</code>","text":"<pre><code>kubectl kustomize samples/01-commonAnnotation\n</code></pre> <pre><code>### FileName: kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n# This will add annotation under every metadata entry\n# ex: main metadata, spec.metadata etc\ncommonAnnotations:\n  author: nirgeier@gmail.com\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>### commonAnnotation output\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    ### Annotation added here\n    author: nirgeier@gmail.com\n    name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      ### Annotation added here\n      annotations:\n        author: nirgeier@gmail.com\n      labels:\n        app: myapp\n    spec:\n      containers:\n        - image: __image__\n          name: myapp\n</code></pre>"},{"location":"08-Kustomization/#commonlabels","title":"<code>commonLabels</code>","text":"<pre><code>kubectl kustomize samples/02-commonLabels\n</code></pre> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n# This will add annotation under every metadata entry\n# ex: main metadata, spec.metadata etc\ncommonLabels:\n  author: nirgeier@gmail.com\n  env: codeWizard-cluster\n\nbases:\n  - ../_base\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n    # Labels added ....\n    labels:\n    author: nirgeier@gmail.com\n    env: codeWizard-cluster\n  name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n      # Labels added ....\n      author: nirgeier@gmail.com\n      env: codeWizard-cluster\n  template:\n    metadata:\n      labels:\n        app: myapp\n        # Labels added ....\n        author: nirgeier@gmail.com\n        env: codeWizard-cluster\n    spec:\n      containers:\n      - image: __image__\n        name: myapp\n</code></pre>"},{"location":"08-Kustomization/#generators","title":"Generators","text":"<ul> <li>Kustomization also support generate <code>ConfigMap</code> / <code>Secret</code> in several ways.</li> <li>The default behavior is adding the output hash value as suffix to the name, e.g.: <code>secretMapFromFile-495dtcb64g</code></li> </ul> <pre><code>apiVersion: v1\ndata:\n  APP_ENV: ZGV2ZWxvcG1lbnQ=\n  LOG_DEBUG: dHJ1ZQ==\n  NODE_ENV: ZGV2\n  REGION: d2V1\nkind: Secret\nmetadata:\n  name: secretMapFromFile-495dtcb64g # &lt;--------------------------\ntype: Opaque\n</code></pre> <ul> <li>We can disable the suffix with the following addition to the <code>kustomization.yaml</code></li> </ul> <pre><code>generatorOptions:\n  disableNameSuffixHash: true\n</code></pre>"},{"location":"08-Kustomization/#configmapgenerator","title":"<code>configMapGenerator</code>","text":""},{"location":"08-Kustomization/#from-env","title":"From Env","text":"<ul> <li><code>.env</code> <pre><code>key1=value1\nenv=qa\n</code></pre></li> <li> <p><code>kustomization.yaml</code> <pre><code># Generate config file from env file\nconfigMapGenerator:\n  - name: configMapFromEnv\n    env: .env\n</code></pre></p> </li> <li> <p>The output of <code>configMapFromEnv</code>:   <pre><code>apiVersion: v1\ndata:\n  env: qa\n  key1: value1\nkind: ConfigMap\nmetadata:\n  name: configMapFromEnv-c9655hf97k\n</code></pre></p> </li> </ul>"},{"location":"08-Kustomization/#from-file","title":"From File","text":"<ul> <li><code>.env</code> <pre><code>key1=value1\nenv=qa\n</code></pre></li> <li> <p><code>kustomization.yaml</code> <pre><code># Generate config file from env file\nconfigMapGenerator:\n  - name: configMapFromEnv\n    files: \n    - .env\n</code></pre></p> </li> <li> <p>The output of <code>configMapFromEnv</code>:   <pre><code>apiVersion: v1\ndata:\n  .env: \"key1=value1\\r\\nenv=qa\" # &lt;--------------------------\nkind: ConfigMap\nmetadata:\n  name: configFromFile-dfhmctd84d\n</code></pre></p> </li> </ul>"},{"location":"08-Kustomization/#from-literal","title":"From Literal","text":"<ul> <li><code>.env</code> <pre><code>key1=value1\nenv=qa\n</code></pre></li> <li> <p><code>kustomization.yaml</code> <pre><code>configMapGenerator:\n  - name: configFromLiterals\n    literals:\n      - Key1=value1\n      - Key2=value2\n</code></pre></p> </li> <li> <p>The output of <code>configMapFromEnv</code>:   <pre><code>apiVersion: v1\ndata:\n  Key1: value1\n  Key2: value2\nkind: ConfigMap\nmetadata:\n  name: configFromLiterals-h777b4gdf5\n</code></pre></p> </li> </ul>"},{"location":"08-Kustomization/#secret-generator","title":"<code>Secret</code> Generator","text":"<pre><code># Similar to configMap but with an additional type field\nsecretGenerator:\n  # Generate secret from env file\n  - name: secretMapFromFile\n    env: .env\n    type: Opaque\ngeneratorOptions:\n  disableNameSuffixHash: true\n</code></pre>"},{"location":"08-Kustomization/#images","title":"<code>images</code>","text":"<ul> <li>Modify the name, tags and/or digest for images.</li> </ul> <pre><code>kubectl kustomize samples/04-images\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - ./base.yaml\n\nimages:\n  # The image as its defined in the Deployment file\n  - name: __image__\n    # The new name to set\n    newName: my-registry/my-image\n    # optional: image tag\n    newTag: v1\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n        # --- This image was updated\n        - image: my-registry/my-image:v1\n          name: myapp\n</code></pre>"},{"location":"08-Kustomization/#namespaces","title":"<code>Namespaces</code>","text":"<pre><code>kubectl kustomize samples/05-Namespace\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n# Add the desired namespace to all resources\nnamespace: kustomize-namespace\n\nbases:\n  - ../_base\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  # Namespace added here\n  namespace: kustomize-namespace\n</code></pre>"},{"location":"08-Kustomization/#prefix-suffix","title":"<code>Prefix-suffix</code>","text":"<pre><code>kubectl kustomize samples/06-Prefix-Suffix\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\n# Add the desired Prefix to all resources\nnamePrefix: prefix-codeWizard-\nnameSuffix: -suffix-codeWizard\n\nbases:\n  - ../_base\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prefix-codeWizard-myapp-suffix-codeWizard\n</code></pre>"},{"location":"08-Kustomization/#replicas","title":"<code>Replicas</code>","text":"<ul> <li>deployment</li> </ul> <pre><code># deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment\nspec:\n  replicas: 5\n  selector:\n    name: deployment\n  template:\n    containers:\n      - name: container\n        image: registry/conatiner:latest\n</code></pre> <ul> <li>kustomization</li> </ul> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nreplicas:\n  - name: deployment\n    count: 10\n\nresources:\n  - deployment.yaml\n</code></pre> <ul> <li>Output:</li> </ul> <p>Note</p> <p>There is a bug with the <code>replicas</code> entries which return error for some reason.</p> <pre><code>$ kubectl kustomize .\n\n# For some reason we get this error:\nError: json: unknown field \"replicas\"\n\n# Workaround for this error for now is:\n$ kustomize build .\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployment\nspec:\n  replicas: 10\n  selector:\n    name: deployment\n  template:\n    containers:\n      - image: registry/conatiner:latest\n        name: container\n</code></pre>"},{"location":"08-Kustomization/#patches","title":"Patches","text":"<ul> <li>There are several types of patches like [<code>replace</code>, <code>delete</code>, <code>patchesStrategicMerge</code>]</li> <li>For this demo we will demonstrate <code>patchesStrategicMerge</code></li> </ul>"},{"location":"08-Kustomization/#patch-addupdate","title":"Patch Add/Update","text":"<pre><code>kubectl kustomize samples/08-Patches/patch-add-update\n</code></pre> <pre><code># File: patch-memory.yaml\n# -----------------------\n# Patch limits.memory\napiVersion: apps/v1\nkind: Deployment\n# Set the desired deployment to patch\nmetadata:\n  name: myapp\nspec:\n  # patch the memory limit\n  template:\n    spec:\n      containers:\n        - name: patch-name\n          resources:\n            limits:\n              memory: 512Mi\n</code></pre> <pre><code># File: patch-replicas.yaml\n# -------------------------\napiVersion: apps/v1\nkind: Deployment\n# Set the desired deployment to patch\nmetadata:\n  name: myapp\nspec:\n  # This is the patch for this demo\n  replicas: 3\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nbases:\n  - ../_base\n\npatchesStrategicMerge:\n- patch-memory.yaml\n- patch-replicas.yaml\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  # This is the first patch\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      # This is the second patch\n      containers:\n      - name: patch-name\n        resources:\n          limits:\n            memory: 512Mi\n      - image: __image__\n        name: myapp\n</code></pre>"},{"location":"08-Kustomization/#patch-delete","title":"Patch-Delete","text":"<pre><code>kubectl kustomize samples/08-Patches/patch-delete\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nbases:\n  - ../../_base\n\npatchesStrategicMerge:\n- patch-delete.yaml\n</code></pre> <pre><code># patch-delete.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  template:\n    spec:\n      containers:\n        # Remove this section, in this demo it will remove the \n        # image with the `name: myapp` \n        - $patch: delete\n          name: myapp\n          image: __image__\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n</code></pre>"},{"location":"08-Kustomization/#patch-replace","title":"Patch Replace","text":"<pre><code>kubectl kustomize samples/08-Patches/patch-replace/\n</code></pre> <pre><code># kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nbases:\n  - ../../_base\n\npatchesStrategicMerge:\n- patch-replace.yaml\n</code></pre> <pre><code># patch-replace.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  template:\n    spec:\n      containers:\n        # Remove this section, in this demo it will remove the \n        # image with the `name: myapp` \n        - $patch: replace\n        - name: myapp\n          image: nginx:latest\n          args:\n          - one\n          - two\n</code></pre> <ul> <li>Output:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - args:\n        - one\n        - two\n        image: nginx:latest\n        name: myapp\n</code></pre>"},{"location":"09-StatefulSet/","title":"09 StatefulSet","text":""},{"location":"09-StatefulSet/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"09-StatefulSet/#statefulsets","title":"StatefulSets","text":""},{"location":"09-StatefulSet/#the-difference-between-a-statefulset-and-a-deployment","title":"The Difference Between a <code>Statefulset</code> And a <code>Deployment</code>","text":""},{"location":"09-StatefulSet/#stateless-application","title":"<code>Stateless</code> application","text":"<ul> <li>A stateless application is one that does not care which network it is using, and it does not need permanent storage and can be scaled up and down without the need to re-use the same network or persistence.</li> <li>Deployment is the suitable kind for Stateless applications.</li> <li>The most trivial example of stateless app is a <code>Web Server</code>.</li> </ul>"},{"location":"09-StatefulSet/#stateful-application","title":"<code>Stateful</code> application","text":"<ul> <li>Stateful applications are apps which in order to work properly need to use the same resources, such as network, storage etc.</li> <li>Usually with <code>Stateful</code> applications you will need to ensure that pods can reach each other through a unique identity that does not change (e.g., hostnames, IP).</li> <li>The most trivial example of Stateful app is a database of any kind.</li> </ul> <p>Stateful Notes</p> <ul> <li>Like a Deployment, a <code>StatefulSet</code> manages Pods that are based on an identical container spec.</li> <li>Unlike a Deployment, a <code>StatefulSet</code> maintains a sticky identity for each of their Pods.</li> <li>These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</li> <li>Deleting and/or scaling down a <code>StatefulSet</code> will not delete the volumes associated with the <code>StatefulSet</code>. This is done to ensure data safety.</li> <li><code>StatefulSet</code> keeps a unique identity for each Pod and assign the same identity to those pods when they are rescheduled (update, restart etc).</li> <li>The storage for a given Pod must either be provisioned by a <code>PersistentVolume</code> provisioner, based on the requested storage class, or pre-provisioned by an admin.</li> <li><code>StatefulSet</code> manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.</li> <li>A <code>stateful</code> app needs to use a dedicated storage.</li> </ul>"},{"location":"09-StatefulSet/#stable-network-identity","title":"Stable Network Identity","text":"<ul> <li>A <code>Stateful</code> application node must have a unique hostname and IP address so that other nodes in the same application know how to reach it.</li> <li>A <code>ReplicaSet</code> assign a random hostname and IP address to each Pod. In such a case, we must use a service which exposes those Pods for us.</li> </ul>"},{"location":"09-StatefulSet/#start-and-termination-order","title":"Start and Termination Order","text":"<ul> <li>Each <code>StatefulSet</code> follows this naming pattern: <code>$(statefulSet name)-$(ordinal)</code></li> <li><code>Stateful</code> applications restarted or re-created, following the creation order.</li> <li>A <code>ReplicaSet</code> does not follow a specific order when starting or killing its pods.</li> </ul>"},{"location":"09-StatefulSet/#statefulset-volumes","title":"StatefulSet Volumes","text":"<ul> <li><code>StatefulSet</code> does not create a volume for you.</li> <li>When a <code>StatefulSet</code> is deleted, the respective volumes are not deleted with it.</li> </ul>"},{"location":"09-StatefulSet/#to-address-all-these-requirements-kubernetes-offers-the-statefulset-primitive","title":"To address all these requirements, Kubernetes offers the <code>StatefulSet primitive</code>.","text":""},{"location":"09-StatefulSet/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> </ul> <p> CTRL + click to open in new window</p>"},{"location":"09-StatefulSet/#01-create-namespace-and-clear-previous-data-if-there-is-any","title":"01. Create namespace and clear previous data if there is any","text":"<pre><code># If the namespace already exist and contains data form previous steps, lets clean it\nkubectl delete namespace codewizard\n\n# Create the desired namespace [codewizard]\n$ kubectl create namespace codewizard\nnamespace/codewizard created\n</code></pre>"},{"location":"09-StatefulSet/#02-create-and-test-the-stateful-application","title":"02. Create and test the Stateful application","text":"<ul> <li> <p>In order to deploy the Stateful set we will need the following resources:</p> </li> <li> <p><code>ConfigMap</code></p> </li> <li><code>Service</code></li> <li><code>StatefulSet</code></li> <li> <p><code>PersistentVolumeClaim or</code>PersistentVolume`</p> </li> <li> <p>All the resources including <code>kustomization</code> script are defined inside the base folder</p> </li> </ul> <ul> <li>ConfigMap.yaml</li> </ul> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: postgres-config\n  labels:\n    app: postgres\ndata:\n  # The following names are the one defined in the officail postgres docs\n\n  # The name of the database we will use in this demo\n  POSTGRES_DB: codewizard\n  # the user name for this demo\n  POSTGRES_USER: codewizard\n  # The password for this demo\n  POSTGRES_PASSWORD: admin123\n</code></pre> <ul> <li> <p>Service.yaml <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\n  labels:\n    app: postgres\nspec:\n  selector:\n    app: postgres\n  # Service of type nodeport\n  type: NodePort\n  # The deafult port for postgres\n  ports:\n    - port: 5432\n</code></pre></p> </li> <li> <p>PersistentVolumeClaim.yaml</p> </li> </ul> <pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: postgres-pv-claim\n  labels:\n    app: postgres\nspec:\n  # in this demo we use GCP so we are using the 'standard' StorageClass\n  # We can of course define our own StorageClass resource\n  storageClassName: standard\n\n  # The access modes are:\n  #   ReadWriteOnce - The volume can be mounted as read-write by a single node\n  #   ReadWriteMany - The volume can be mounted as read-write by a many node\n  #   ReadOnlyMany  - The volume can be mounted as read-only  by many nodes\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre> <ul> <li>StatefulSet.yaml</li> </ul> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres\nspec:\n  replicas: 1\n  # StatefulSet must contain a serviceName\n  serviceName: postgres\n  selector:\n    matchLabels:\n      app: postgres # has to match .spec.template.metadata.labels\n  template:\n    metadata:\n      labels:\n        app: postgres # has to match .spec.selector.matchLabels\n    spec:\n      containers:\n        - name: postgres\n          image: postgres:10.4\n          imagePullPolicy: \"IfNotPresent\"\n          # The default DB port\n          ports:\n            - containerPort: 5432\n          # Load the required configuration env values form the configMap\n          envFrom:\n            - configMapRef:\n                name: postgres-config\n          # Use volume for storage\n          volumeMounts:\n            - mountPath: /var/lib/postgresql/data\n              name: postgredb\n      # We can use PersistentVolume or PersistentVolumeClaim.\n      # In this sample we are useing PersistentVolumeClaim\n      volumes:\n        - name: postgredb\n          persistentVolumeClaim:\n            # reference to Pre-Define PVC\n            claimName: postgres-pv-claim\n</code></pre> <p>Note: You can use the kustomization file to create or apply all the above resources</p> <pre><code># Generate and apply the required resources using kustomization\nkubectl kustomize PostgreSQL/ | kubectl apply -f -\n</code></pre>"},{"location":"09-StatefulSet/#03-test-the-stateful-application","title":"03. Test the Stateful application","text":"<ul> <li>Use the - testDB.sh to test the StatefulSet</li> <li>Don\u2019t forget to set the execution flag <code>chmod +x testDb.sh</code> if required</li> </ul> <pre><code>### Test to see if the StatefulSet \"saves\" the state of the pods\n\n# Programmatically get the port and the IP\nexport CLUSTER_IP=$(kubectl get nodes \\\n            --selector=node-role.kubernetes.io/control-plane \\\n            -o jsonpath='{$.items[*].status.addresses[?(@.type==\"InternalIP\")].address}')\n\nexport NODE_PORT=$(kubectl get \\\n            services postgres \\\n            -o jsonpath=\"{.spec.ports[0].nodePort}\" \\\n            -n codewizard)\n\nexport POSTGRES_DB=$(kubectl get \\\n            configmap postgres-config \\\n            -o jsonpath='{.data.POSTGRES_DB}' \\\n            -n codewizard)\n\nexport POSTGRES_USER=$(kubectl get \\\n            configmap postgres-config \\\n            -o jsonpath='{.data.POSTGRES_USER}' \\\n            -n codewizard)\n\nexport PGPASSWORD=$(kubectl get \\\n            configmap postgres-config \\\n            -o jsonpath='{.data.POSTGRES_PASSWORD}' \\\n            -n codewizard)\n\n# Check to see if we have all the required variables\nprintenv | grep POST*\n\n# Connect to postgres and create table if required.\n# Once the table exists - add row into the table\n# you can run this command as amny times as you like\npsql \\\n    -U ${POSTGRES_USER} \\\n    -h ${CLUSTER_IP} \\\n    -d ${POSTGRES_DB} \\\n    -p ${NODE_PORT} \\\n    -c \"CREATE TABLE IF NOT EXISTS stateful (str VARCHAR); INSERT INTO stateful values (1); SELECT count(*) FROM stateful\"\n</code></pre>"},{"location":"09-StatefulSet/#04-scale-down-the-statefulset-and-check-that-its-down","title":"04. Scale down the StatefulSet and check that its down","text":""},{"location":"09-StatefulSet/#0401-scale-down-the-statefulset-to-0","title":"04.01. Scale down the <code>Statefulset</code> to 0","text":"<pre><code># scale down the `Statefulset` to 0\nkubectl scale statefulset postgres -n codewizard --replicas=0\n</code></pre>"},{"location":"09-StatefulSet/#0402-verify-that-the-pods-terminated","title":"04.02. Verify that the pods Terminated","text":"<pre><code># Wait until the pods will be terminated\nkubectl get pods -n codewizard --watch\nNAME         READY   STATUS    RESTARTS   AGE\npostgres-0   1/1     Running   0          32m\npostgres-0   1/1     Terminating   0      32m\npostgres-0   0/1     Terminating   0      32m\npostgres-0   0/1     Terminating   0      33m\npostgres-0   0/1     Terminating   0      33m\n</code></pre>"},{"location":"09-StatefulSet/#0403-verify-that-the-db-is-not-reachable","title":"04.03. Verify that the DB is not reachable","text":"<ul> <li>If the DB is not reachable it mean that all the pods are down</li> </ul> <pre><code>psql \\\n    -U ${POSTGRES_USER} \\\n    -h ${CLUSTER_IP} \\\n    -d ${POSTGRES_DB} \\\n    -p ${NODE_PORT} \\\n    -c \"SELECT count(*) FROM stateful\"\n\n# You should get output similar to this one:\npsql: error: could not connect to server: Connection refused\n        Is the server running on host \"192.168.49.2\" and accepting\n        TCP/IP connections on port 32570?\n</code></pre>"},{"location":"09-StatefulSet/#05-scale-up-again-and-verify-that-we-still-have-the-prevoius-data","title":"05. Scale up again and verify that we still have the prevoius data","text":""},{"location":"09-StatefulSet/#0501-scale-up-the-statefulset-to-1-or-more","title":"05.01. scale up the <code>Statefulset</code> to 1 or more","text":"<pre><code># scale up the `Statefulset`\nkubectl scale statefulset postgres -n codewizard --replicas=1\n</code></pre>"},{"location":"09-StatefulSet/#0502-verify-that-the-pods-is-in-running-status","title":"05.02. Verify that the pods is in Running status","text":"<pre><code>kubectl get pods -n codewizard --watch\nNAME         READY   STATUS    RESTARTS   AGE\npostgres-0   1/1     Running   0          5s\n</code></pre>"},{"location":"09-StatefulSet/#0503-verify-that-the-pods-is-using-the-previous-data","title":"05.03. Verify that the pods is using the previous data","text":"<pre><code>psql \\\n    -U ${POSTGRES_USER} \\\n    -h ${CLUSTER_IP} \\\n    -d ${POSTGRES_DB} \\\n    -p ${NODE_PORT} \\\n    -c \"SELECT count(*) FROM stateful\"\n# The output should be similar to this one\n\n count\n-------\n     2\n(1 row)\n</code></pre>"},{"location":"10-Istio/","title":"K8S Hands-on","text":""},{"location":"10-Istio/#istio","title":"Istio","text":"<p>Istio is an open-source service mesh that provides a way to manage microservices traffic, security, and observability in a Kubernetes cluster.</p>"},{"location":"10-Istio/#pre-requirements","title":"Pre Requirements","text":"<ul> <li>Helm installed</li> <li>K8S cluster - Setting up minikube cluster instruction</li> <li>kubectl) configured to interact with your cluster</li> </ul>"},{"location":"10-Istio/#ctrl-click-to-open-in-new-window","title":"CTRL + click to open in new window","text":""},{"location":"10-Istio/#istio-and-kiali","title":"Istio and Kiali","text":"<ul> <li>This guide provides a detailed walkthrough on installing and configuring Istio  and Kiali  on a Kubernetes cluster. </li> <li>We will also learn how to visualize your service mesh with Istio and Kiali and create a demo Istio VirtualService.</li> </ul>"},{"location":"10-Istio/#01-introduction","title":"01. Introduction","text":""},{"location":"10-Istio/#istio_1","title":"<code>Istio</code>","text":"<ul> <li>Istio  is an open-source service mesh that provides a way to manage microservices traffic, security, and observability in a Kubernetes cluster. </li> <li>It acts as a layer of infrastructure that sits between your services, intercepting and controlling the traffic between them. </li> <li>Istio key features: </li> </ul> Feature Description Traffic Management Istio enables sophisticated traffic control capabilities, such as routing, load balancing, retries, timeouts, and circuit breakers for microservices. Service Discovery Automatically discovers services in the mesh, enabling dynamic routing and management of microservices without the need for manual configuration. Load Balancing Istio provides various load balancing algorithms (round-robin, weighted, etc.) to distribute traffic between microservices, ensuring optimal performance. Traffic Shaping Allows fine-grained control of traffic between services, such as A/B testing, canary releases, or blue/green deployments by defining routing rules. Fault Injection Supports fault injection to simulate network failures, latency, or errors in microservices to test resilience and robustness of the application. Mutual TLS (mTLS) Istio can automatically encrypt traffic between services using mutual TLS (mTLS) to ensure secure communication and provide strong identity-based access control. Authentication &amp; Authorization Provides identity and access management through role-based access control (RBAC) and integration with external identity providers (e.g., OAuth, JWT). Telemetry &amp; Observability Istio collects metrics, logs, and traces for monitoring service\u2019s performance and behavior. It integrates with tools like Prometheus, Grafana, and Jaeger. Distributed Tracing Istio integrates with tracing systems like Jaeger and Zipkin to provide end-to-end tracing for debugging and monitoring service interactions. Policy Enforcement Istio provides fine-grained control over traffic policies, such as rate limiting, quotas, and security policies, using its Policy and Telemetry components. Resilience &amp; Retries Istio can retry failed requests, set timeouts, and apply circuit breakers to prevent cascading failures and enhance the reliability of services. Sidecar Proxy (Envoy) Istio uses Envoy as a sidecar proxy to intercept and manage network traffic, providing a transparent proxy between microservices. Automatic Sidecar Injection Istio automatically injects the Envoy proxy into application pods via Kubernetes annotations, simplifying the management of service communication. Service Mesh Topology Visualizes and manages the network of microservices, allowing users to monitor how services interact with each other and troubleshoot issues. Canary Deployments Supports canary releases and traffic splitting, which allows gradual rollout of new versions of services for safe deployments and testing. Multi-cluster Support Istio supports a multi-cluster environment, allowing you to deploy services across different Kubernetes clusters while maintaining a unified service mesh. Integration with Existing Tools Istio integrates seamlessly with other tools such as Prometheus, Grafana, Jaeger, and Kiali for observability, monitoring, and tracing. Service-Level Agreements (SLAs) Provides mechanisms to define service-level objectives (SLOs) and monitor them, ensuring services meet expected performance and reliability standards. <ul> <li>Istio core components: </li> </ul> Components Description Envoy Proxy A sidecar proxy that intercepts traffic to and from microservices. Pilot Manages configuration and distributes traffic management rules. Mixer Provides policy enforcement and telemetry data collection. Citadel Handles security-related tasks like identity and certificate management."},{"location":"10-Istio/#kiali","title":"<code>Kiali</code>","text":"<ul> <li>Kiali  is a graphical user interface (GUI) for <code>Istio</code>. </li> <li>It helps you visualize the service mesh and provides insights on how microservices are interacting with each other. </li> <li> <p><code>Kiali</code> integrates deeply with Istio, allowing you to view:</p> <ul> <li>The service mesh topology, showing services, traffic flow, and dependencies.</li> <li>Metrics like request rates, latencies, and error rates.</li> <li>Distributed tracing  (if enabled) for better debugging and troubleshooting.</li> <li>Istio configuration  to visualize resources like VirtualServices, DestinationRules, and more.</li> </ul> </li> <li> <p><code>Kiali</code> simplifies the operation of <code>Istio</code> by offering an intuitive way to manage and visualize your service mesh.</p> </li> </ul>"},{"location":"10-Istio/#part-01-installing-istio-and-kiali","title":"Part 01 - Installing Istio and Kiali","text":""},{"location":"10-Istio/#step-01-install-istio-using-istioctl","title":"Step 01: Install Istio Using Istioctl","text":"<ul> <li>Istio supplies an installer</li> <li>Go to the Istio releases page and download the latest version of Istio. </li> <li>Alternatively, use <code>istioctl</code> to install Istio, as follows:</li> </ul> <pre><code># Install Istio using istioctl\necho  \"Installing Istio...\"\ncurl  -L https://istio.io/downloadIstio | sh -\ncd    istio-*\n\n# Add bin directory to your $PATH\nexport PATH=$PWD/bin:$PATH\n\n# Install istio with all features enabled (demo profile)\nistioctl install --set profile=demo -y\n</code></pre>"},{"location":"10-Istio/#step-02-verify-istio-installation","title":"Step 02: Verify Istio installation","text":"<ul> <li>Check the Istio system components in the <code>istio-system</code> namespace:</li> </ul> <pre><code># Verify Istio installation\nkubectl get pods -n istio-system\n</code></pre> <ul> <li>You should see several pods, including the Istio control plane components like: <ul> <li><code>istiod</code></li> <li><code>istio-ingressgateway</code></li> <li><code>istio-egressgateway</code></li> </ul> </li> </ul> <pre><code>kubectl get pods -n istio-system\n</code></pre> <pre><code>NAME                                    READY   STATUS    RESTARTS   AGE\nistio-egressgateway-684f5dc857-bzww6    1/1     Running   0          21m\nistio-ingressgateway-6b5bd79c5c-9n8tg   1/1     Running   0          21m\nistiod-68885d595-vv2ft                  1/1     Running   0          22m\n</code></pre>"},{"location":"10-Istio/#step-03-install-kiali","title":"Step 03: Install Kiali","text":"<ul> <li>We will install Kiali using Helm:</li> </ul> <pre><code># Add the Kiali Helm chart repository\nhelm repo add kiali https://kiali.org/helm-charts\nhelm repo update\n</code></pre> <ul> <li>Install Kiali: </li> </ul> <pre><code># Install Kiali into the `istio-system` namespace \n# this is the default namespace for Istio components\n#\n# Install Kiali with anonymous authentication\n#\nhelm install  kiali-server              \\\n              kiali/kiali-server        \\\n              --namespace istio-system  \\\n              --set auth.strategy=\"anonymous\"\n</code></pre>"},{"location":"10-Istio/#step-04-verify-kiali-installation","title":"Step 04: Verify Kiali installation","text":"<ul> <li>Once installed, check the status of <code>Kiali</code>:</li> </ul> <pre><code>kubectl get pods -n istio-system\n</code></pre> <ul> <li>You should see the <code>Kiali</code> pod running, along with the <code>Istio</code> components from previous step.</li> </ul> <pre><code>NAME                                    READY   STATUS    RESTARTS   AGE\nistio-egressgateway-684f5dc857-bzww6    1/1     Running   0          28m\nistio-ingressgateway-6b5bd79c5c-9n8tg   1/1     Running   0          28m\nistiod-68885d595-vv2ft                  1/1     Running   0          28m\nkiali-68ccc848b6-j4q28                  1/1     Running   0          27m\n</code></pre>"},{"location":"10-Istio/#part-02-viewing-the-network-with-istio","title":"Part 02 - Viewing the Network with Istio","text":"<ul> <li><code>Istio</code> uses a sidecar proxy model, where an envoy proxy is deployed alongside each microservice pod. </li> <li>This proxy intercepts and manages traffic between the services.</li> </ul>"},{"location":"10-Istio/#step-01-enable-istio-injection","title":"Step 01: Enable Istio Injection","text":"<ul> <li>You need to enable Istio sidecar injection for your Kubernetes namespace. </li> <li>This will ensure that new pods in the <code>default</code> namespace will automatically have the envoy proxy sidecar injected.</li> <li>For example, to enable injection in the <code>default</code> namespace:</li> </ul> <pre><code>kubectl label namespace default istio-injection=enabled\n</code></pre>"},{"location":"10-Istio/#step-02-deploy-sample-application","title":"Step 02: Deploy Sample Application","text":"<ul> <li>To see <code>Istio</code> in action, deploy a sample application, such as Bookinfo, which is available in <code>Istio</code>\u2019s demo repository.</li> </ul> <pre><code># Deploy the sample application supplied by istio\nkubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre>"},{"location":"10-Istio/#step-03-verify-the-sample-application","title":"Step 03: Verify the Sample Application","text":"<ul> <li>To check that the application pods are running, execute the following command:</li> </ul> <pre><code>kubectl get pods\n</code></pre>"},{"location":"10-Istio/#step-04-expose-the-application","title":"Step 04: Expose the Application","text":"<ul> <li>To expose the application via <code>Istio's</code> ingress gateway, create an <code>Istio</code> Gateway  and VirtualService .</li> </ul> <pre><code># This will expose the 'Bookinfo' application to the external world via Istio ingress gateway.\nkubectl   apply -f \\\n          samples/bookinfo/networking/bookinfo-gateway.yaml\n</code></pre>"},{"location":"10-Istio/#part-03-visualizing-the-network-with-kiali","title":"Part 03 - Visualizing the Network with Kiali","text":""},{"location":"10-Istio/#step-01-access-kiali-dashboard","title":"Step 01: Access Kiali Dashboard","text":"<ul> <li>Once <code>Kiali</code> is installed, you can access it\u2019s dashboard. </li> <li>First, port-forward to the <code>Kiali</code> service:</li> </ul> <pre><code>kubectl   port-forward        \\\n          -n istio-system     \\\n          svc/kiali 20001:20001\n</code></pre> <ul> <li>Now, open your browser and go to http://localhost:20001</li> <li>Username : <code>admin</code></li> <li>Password : (Leave blank if anonymous access is enabled)</li> </ul>"},{"location":"10-Istio/#step-02-explore-the-service-mesh-topology","title":"Step 02: Explore the Service Mesh Topology","text":"<ul> <li>Once inside the <code>Kiali</code> dashboard, open the <code>Mesh</code> View</li> <li>You will see a graph  of your services in the mesh.</li> <li>The graph shows the interactions between microservices, along with traffic flows, success/error rates, and latency.</li> <li>You can use the <code>Kiali</code> interface to:<ul> <li>Zoom in/out  of the topology.</li> <li>View detailed metrics for each service.</li> <li>Understand the traffic flow, including retries, timeouts, and error rates.</li> </ul> </li> </ul>"},{"location":"10-Istio/#part-04-creating-a-demo-istio-virtualservice","title":"Part 04: Creating a Demo Istio VirtualService","text":"<ul> <li>In <code>Istio</code>, VirtualServices  are used to define the routing rules for your services.</li> </ul>"},{"location":"10-Istio/#step-01-define-a-virtualservice","title":"Step 01: Define a VirtualService","text":"<ul> <li>Create a <code>VirtualService</code> resource to route traffic to the <code>ratings</code> service in the Bookinfo demo app.</li> </ul> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: ratings-vs\n  namespace: default\nspec:\n  hosts:\n    - ratings\n  http:\n    - route:\n        - destination:\n            host: ratings\n            subset: v2\n</code></pre>"},{"location":"10-Istio/#step-02-apply-the-virtualservice","title":"Step 02: Apply the VirtualService","text":"<ul> <li>Apply the <code>VirtualService</code>.</li> <li>This will route all traffic for the <code>ratings</code> service to version <code>v2</code>.</li> </ul> <pre><code>kubectl apply -f ratings-virtualservice.yaml\n</code></pre>"},{"location":"10-Istio/#step-03-verify-the-routing","title":"Step 03: Verify the Routing","text":"<ul> <li>You can use <code>Kiali</code> to visualize the traffic flow and verify that routing is happening as expected.</li> <li>The <code>Kiali</code> dashboard should reflect the new route configuration for <code>ratings</code>.</li> </ul>"},{"location":"10-Istio/#conclusion","title":"Conclusion","text":"<ul> <li>You have now successfully installed <code>Istio</code> and <code>Kiali</code>, set up a service mesh, and visualized your network\u2019s behavior. </li> <li>The combination of <code>Istio's</code> powerful traffic management features and <code>Kiali's</code> intuitive visualization interface makes it easier to manage and monitor microservices in a Kubernetes cluster.</li> </ul>"},{"location":"11-CRD-Custom-Resource-Definition/","title":"11 CRD Custom Resource Definition","text":""},{"location":"11-CRD-Custom-Resource-Definition/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"11-CRD-Custom-Resource-Definition/#custom-resources-definition-crd","title":"Custom resources definition (CRD)","text":""},{"location":"11-CRD-Custom-Resource-Definition/#intro","title":"Intro","text":"<ul> <li><code>Custom resources definition</code> (CRD) was added to Kubernetes 1.7.</li> <li><code>CRD</code> added the ability to define custom objects/resources.</li> </ul>"},{"location":"11-CRD-Custom-Resource-Definition/#what-is-a-custom-resource-definitioncrd","title":"What is a Custom Resource Definition(CRD)","text":"<ul> <li> <p>A resource is an endpoint in the Kubernetes API that stores a collection of API objects of a certain kind; for example, the builtin pods resource contains a collection of Pod objects.</p> </li> <li> <p>A custom resource is an extension of the Kubernetes API that is not necessarily available in a default Kubernetes installation. It represents a customization of a particular Kubernetes installation. However, many core Kubernetes functions are now built using custom resources, making Kubernetes more modular.</p> </li> <li> <p>Custom resources can appear and disappear in a running cluster through dynamic registration, and cluster admins can update custom resources independently of the cluster itself. </p> </li> <li> <p>Once a custom resource is installed, users can create and access its objects using <code>kubectl</code>, just as they do for built-in resources like Pods.</p> </li> <li> <p>The custom resource created is also stored in the <code>etcd</code> cluster with proper replication and lifecycle management.</p> </li> </ul>"},{"location":"12-Wordpress-MySQL-PVC/","title":"12 Wordpress MySQL PVC","text":""},{"location":"12-Wordpress-MySQL-PVC/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"12-Wordpress-MySQL-PVC/#wordpress-mysql-pvc","title":"WordPress, MySQL, PVC","text":"<ul> <li>In this tutorial you will deploy a WordPress site and a MySQL database.</li> <li>You will use <code>PersistentVolumes</code> and <code>PersistentVolumeClaims</code> as storage.</li> </ul>"},{"location":"12-Wordpress-MySQL-PVC/#walkthrough","title":"Walkthrough","text":"<ul> <li>Patch <code>minikube</code> so we can use <code>Service: LoadBalancer</code></li> </ul> <pre><code># Source:\n# https://github.com/knative/serving/blob/b31d96e03bfa1752031d0bc4ae2a3a00744d6cd5/docs/creating-a-kubernetes-cluster.md#loadbalancer-support-in-minikube\n\nsudo ip route add \\\n    $(cat ~/.minikube/profiles/minikube/config.json | \\\n    jq -r \".KubernetesConfig.ServiceCIDR\") \\\n    via $(minikube ip)\n\nkubectl run minikube-lb-patch \\\n    --replicas=1 \\\n    --image=elsonrodriguez/minikube-lb-patch:0.1 \\--namespace=kube-system\n</code></pre> <ul> <li>Create the desired <code>Namespace</code></li> <li>Create the <code>MySQL</code> resources:<ul> <li>Create <code>Service</code></li> <li>Create <code>PersistentVolumeClaims</code></li> <li>Create <code>Deployment</code></li> <li>Create <code>password file</code></li> </ul> </li> <li>Create the WordPress resources:<ul> <li>Create <code>Service</code></li> <li>Create <code>PersistentVolumeClaims</code></li> <li>Create <code>Deployment</code></li> </ul> </li> <li>Create a <code>kustomization.yaml</code> with:<ul> <li><code>Secret generator</code></li> <li><code>MySQL</code> resources</li> <li><code>WordPress</code> resources</li> </ul> </li> <li>Deploy the stack</li> <li>Port forward from the host to the application</li> <li>We use a port forward so we will be able to test and verify if the WordPress is actually running:</li> </ul> <pre><code>kubectl port-forward service/wordpress 8080:32267 -n wp-demo\n</code></pre>"},{"location":"13-HelmChart/","title":"13 HelmChart","text":""},{"location":"13-HelmChart/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"13-HelmChart/#helm-chart","title":"Helm Chart","text":"<ul> <li>Welcome to the <code>Helm</code> Chart hands-on lab! In this tutorial, you\u2019ll learn the essentials of <code>Helm</code> (version 3), the package manager for Kubernetes. </li> <li>You\u2019ll build, package, install, and manage applications using <code>Helm</code> charts, gaining practical experience with real Kubernetes resources.</li> </ul>"},{"location":"13-HelmChart/#pre-requirements","title":"Pre requirements","text":"<ul> <li><code>Helm</code> installed</li> <li>K8S cluster - Setting up minikube cluster instruction</li> <li>kubectl configured to interact with your cluster</li> </ul>"},{"location":"13-HelmChart/#ctrl-click-to-open-in-new-window","title":"CTRL + click to open in new window","text":""},{"location":"13-HelmChart/#what-will-you-learn","title":"What will you learn","text":"<ul> <li>What <code>Helm</code> is and why is it useful</li> <li><code>Helm</code> chart structure and key files</li> <li>Common <code>Helm</code> commands for managing releases</li> <li>How to create, pack, install, upgrade, and rollback a <code>Helm</code> chart</li> <li>Troubleshooting and best practices</li> </ul>"},{"location":"13-HelmChart/#introduction","title":"Introduction","text":"<ul> <li><code>Helm</code> is the package manager for Kubernetes. </li> <li>It simplifies the deployment, management, and upgrade of applications on your Kubernetes cluster. </li> <li><code>Helm</code> helps you manage Kubernetes applications by providing a way to define, install, and upgrade complex Kubernetes applications.</li> <li> <p>When packing applications as <code>Helm</code> charts, you gain a standardized and reusable approach for deploying and managing your services.</p> </li> <li> <p>A <code>Helm</code> chart consists of a few files that define the Kubernetes resources that will be created when the chart is installed. </p> </li> <li>These files include the:<ul> <li><code>Chart.yaml</code> file, which contains metadata about the chart, such as its name and version, and the chart\u2019s dependencies and maintainers.</li> <li><code>values.yaml</code> file, which contains the configuration values for the chart. </li> <li>The <code>templates</code> directory which contains the Kubernetes resource templates to be used to create the actual resources in the cluster.</li> </ul> </li> </ul>"},{"location":"13-HelmChart/#terminology","title":"Terminology","text":"<ul> <li> <p><code>Chart</code></p> <ul> <li>A <code>Helm</code> package is called a chart.</li> <li>Charts are versioned, shareable packages that contain all the Kubernetes resources needed to run an application.</li> </ul> </li> <li> <p><code>Release</code></p> <ul> <li>A specific instance of a chart is called a release.</li> <li>Each release is a deployed version of a chart, with its own configuration, resources, and revision history.</li> </ul> </li> <li> <p><code>Repository</code></p> <ul> <li>A collection of charts is stored in a <code>Helm</code> repository.</li> <li><code>Helm</code> charts can be hosted in public or private repositories for easy sharing and distribution.</li> </ul> </li> </ul>"},{"location":"13-HelmChart/#chart-files-and-folders","title":"Chart files and folders","text":"Filename/Folder Description <code>Chart.yaml</code> Contains metadata about the chart, including its name, version, dependencies, and maintainers. <code>values.yaml</code> Defines default configuration values for the chart. Users can override these values during installation. <code>templates/</code> Directory containing Kubernetes manifest templates written in the Go template language. <code>charts/</code> Directory containing dependencies of the chart. <code>README.md</code> Documentation for the chart, explaining how to use and configure it."},{"location":"13-HelmChart/#codewizard-helm-demo-helm-chart-tructure","title":"codewizard-helm-demo Helm Chart tructure","text":"<pre><code>- Chart.yaml        # Defines chart metadata and values schema\n- values.yaml       # Default configuration values\n- templates/        # Deployment templates using Go templating language\n  - deployment.yaml # Deployment manifest template\n  - service.yaml    # Service manifest template\n- README.md         # Documentation for your chart \n</code></pre>"},{"location":"13-HelmChart/#common-helm-commands","title":"Common <code>Helm</code> Commands","text":"<p>Here are some of the most common <code>Helm</code> commands you\u2019ll use when working with <code>Helm</code> charts:</p> Command Description <code>helm</code> create <code>chart-name</code> Create a new <code>Helm</code> chart with the specified name. <code>helm</code> install <code>release-name</code> <code>chart-path</code> Install a <code>Helm</code> chart to your Kubernetes cluster. <code>helm</code> upgrade <code>release-name</code> <code>chart-path</code> Upgrade an installed release with a new version of a chart. <code>helm</code> uninstall <code>release-name</code> Uninstall a release from the Kubernetes cluster. <code>helm</code> list List all installed <code>Helm</code> releases in the cluster. <code>helm</code> status <code>release-name</code> Show the status of a deployed <code>Helm</code> release. <code>helm</code> rollback <code>release-name</code> <code>revision</code> Rollback a release to a previous revision. <code>helm</code> get all <code>release-name</code> Retrieve all information about a deployed release (e.g., templates, values). <code>helm</code> show values <code>chart-name</code> Show the default values of a <code>Helm</code> chart. <code>helm</code> template <code>chart-name</code> Generate the output of the <code>Helm</code> chart. <code>helm</code> lint <code>chart-path</code> This command takes a path to a chart and runs a series of tests to verify that the chart is well-formed. <code>helm</code> history <code>chart-name</code> This command takes a path to a chart and runs a series of tests to verify that the chart is well-formed."},{"location":"13-HelmChart/#lab","title":"Lab","text":""},{"location":"13-HelmChart/#step-01-installing-helm","title":"Step 01 - Installing <code>Helm</code>","text":"<ul> <li> <p>Before you can use the <code>codewizard-helm-demo</code> chart, you\u2019ll need to install <code>Helm</code> on your local machine. </p> </li> <li> <p><code>Helm</code> install methods by OS:</p> </li> </ul> OS Command Linux <code>curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 \\| bash</code> MacOS <code>brew install helm</code> Windows (via Chocolatey) <code>choco install kubernetes-helm</code>"},{"location":"13-HelmChart/#verify-installation","title":"Verify Installation","text":"<ul> <li>To confirm that <code>Helm</code> is installed correctly, run:</li> </ul> <pre><code>$ helm version\n\n## Expected output\nversion.BuildInfo{Version:\"xx\", GitCommit:\"xx\", GitTreeState:\"clean\", GoVersion:\"xx\"}\n</code></pre>"},{"location":"13-HelmChart/#step-02-creating-our-helm-chart","title":"Step 02 - Creating our <code>Helm</code> chart","text":"<ul> <li>Creating our custom <code>codewizard-helm-demo</code> <code>Helm</code> chart</li> <li> <p>The custom <code>codewizard-helm-demo</code> <code>Helm</code> chart is build upon the following K8S resources:</p> <ul> <li>ConfigMap</li> <li>Deployment</li> <li>Service</li> </ul> </li> <li> <p>As mentioned above, we will also have the following <code>Helm</code> resources:</p> <ul> <li>Chart.yaml</li> <li>values.yaml</li> <li>templates/_helpers.tpl</li> </ul> </li> </ul>"},{"location":"13-HelmChart/#create-a-new-chart","title":"Create a New Chart","text":"<ul> <li>First, we need to create a <code>Helm</code> chart using the <code>helm create</code> command. </li> <li>This command will generate the necessary file structure for your new chart.</li> </ul> <pre><code>helm create codewizard-helm-demo\n</code></pre> What is the result of this command? <p>Examine the chart structure!</p>"},{"location":"13-HelmChart/#navigate-to-the-chart-directory","title":"Navigate to the Chart Directory","text":"<pre><code>cd codewizard-helm-demo\n</code></pre>"},{"location":"13-HelmChart/#write-the-chart-content","title":"Write the chart content","text":"<ul> <li>Copy the content of the chart folder (in this lab) to the chart directory (overwriting the files).</li> </ul>"},{"location":"13-HelmChart/#step-03-pack-the-chart","title":"Step 03 - Pack the chart","text":"<ul> <li>After we have created or customized our chart, we need to pack it as <code>.tgz</code> file, which can then be shared or installed.</li> </ul>"},{"location":"13-HelmChart/#helm-package","title":"helm package","text":"<p>Helm Package</p> <p><code>helm package</code> packages a chart into a versioned chart archive file. If a path is given, this will \u201clook\u201d at that path for a chart which must contain a <code>Chart.yaml</code> file and then pack that directory.</p> <pre><code>helm package codewizard-helm-demo\n</code></pre> <ul> <li>This command will create a file called <code>codewizard-helm-demo-&lt;version&gt;.tgz</code> inside your current directory.</li> </ul>"},{"location":"13-HelmChart/#step-04-validate-the-chart-content","title":"Step 04 - Validate the chart content","text":""},{"location":"13-HelmChart/#helm-template","title":"<code>helm template</code>","text":"<ul> <li><code>Helm</code> allows you to generate the Kubernetes manifests based on the templates and values files without actually installing the chart. </li> <li>This is useful to preview what the generated resources will look like:</li> </ul> <pre><code>helm template codewizard-helm-demo\n\n## This will output the rendered Kubernetes manifests to your terminal\n</code></pre>"},{"location":"13-HelmChart/#step-05-install-the-chart","title":"Step 05 - Install the chart","text":"<ul> <li>Install the <code>codewizard-helm-demo</code> chart into Kubernetes cluster</li> </ul>"},{"location":"13-HelmChart/#the-helm-install-command","title":"The <code>helm install</code> command","text":"<ul> <li>This command installs a chart archive.</li> <li>The install argument must be a chart reference, a path to a packed chart, a path to an unpacked chart directory or a URL.</li> <li>To override values in a chart, use:<ul> <li><code>--values</code> - pass in a file </li> <li><code>--set</code> - pass configuration from the command line</li> </ul> </li> </ul> <pre><code># Install the packed helm chart\nhelm install codewizard-helm-demo codewizard-helm-demo-0.1.0.tgz\n</code></pre>"},{"location":"13-HelmChart/#step-06-verify-the-installation","title":"Step 06 - Verify the installation","text":"<ul> <li>Examine newly created <code>Helm</code> chart release, and all cluster created resources:</li> </ul> <pre><code># List the installed helms\nhelm ls\n\n# Check the resources\nkubectl get all -n codewizard\n</code></pre>"},{"location":"13-HelmChart/#step-07-test-the-service","title":"Step 07 - Test the service","text":"<ul> <li>Perform an <code>HTTP GET</code> request, send it to the newly created cluster service.</li> <li>Confirm that the response contains the <code>CodeWizard Helm Demo</code> message passed from the <code>values.yaml</code> file.</li> </ul> <pre><code>kubectl run busybox         \\\n        --image=busybox     \\\n        --rm                \\\n        -it                 \\\n        --restart=Never     \\\n        -- /bin/sh -c \"wget -qO- http://codewizard-helm-demo.codewizard.svc.cluster.local\"\n\n### Output: \nCodeWizard Helm Demo\n</code></pre>"},{"location":"13-HelmChart/#step-08-upgrade-the-release-to-newer-version","title":"Step 08 - Upgrade the release to newer version","text":"<ul> <li>Perform a Helm upgrade on the <code>codewizard-helm-demo</code> release:</li> </ul> <pre><code># upgrade and pass a different message than the one from the default values\n# Use the --set to pass the desired value\nhelm  upgrade \\\n  codewizard-helm-demo \\\n  codewizard-helm-demo-0.1.0.tgz \\\n  --set nginx.conf.message=\"Helm Rocks\"\n</code></pre>"},{"location":"13-HelmChart/#step-09-check-the-upgrade","title":"Step 09 - Check the upgrade","text":"<ul> <li>Perform another <code>HTTP GET</code> request.</li> <li>Confirm that the response now has the updated message <code>Helm Rocks</code>:</li> </ul> <pre><code>kubectl run busybox         \\\n        --image=busybox     \\\n        --rm                \\\n        -it                 \\\n        --restart=Never     \\\n        -- /bin/sh -c \"wget -qO- http://codewizard-helm-demo.codewizard.svc.cluster.local\"\n\n### Output: \nHelm Rocks\n</code></pre>"},{"location":"13-HelmChart/#step-10-history","title":"Step 10 - History","text":"<ul> <li>Examine the <code>codewizard-helm-demo</code> release history</li> </ul>"},{"location":"13-HelmChart/#helm-history","title":"<code>helm history</code>","text":"<ul> <li><code>helm history</code> prints historical revisions for a given release.</li> <li>A default maximum of 256 revisions will be returned.</li> </ul> <pre><code>$ helm history codewizard-helm-demo\n\n### Sample output\nREVISION        UPDATED    STATUS          CHART                           APP VERSION     DESCRIPTION     \n1               ...        superseded      codewizard-helm-demo-0.1.0      1.19.7          Install complete\n2               ...        deployed        codewizard-helm-demo-0.1.0      1.19.7          Upgrade complete\n</code></pre>"},{"location":"13-HelmChart/#step-11-rollback","title":"Step 11 - Rollback","text":""},{"location":"13-HelmChart/#helm-rollback","title":"<code>helm rollback</code>","text":"<ul> <li>Rollback the <code>codewizard-helm-demo</code> release to previous version:</li> </ul> <pre><code>$ helm rollback codewizard-helm-demo\n\n### Output:\nRollback was a success! Happy Helming!\n</code></pre> <ul> <li>Check again to verify that you get the original message!</li> </ul>"},{"location":"13-HelmChart/#finalize-cleanup","title":"Finalize &amp; Cleanup","text":"<ul> <li>To remove all resources created by this lab, uninstall the <code>codewizard-helm-demo</code> release:</li> </ul> <pre><code>helm uninstall codewizard-helm-demo\n</code></pre> <ul> <li>(Optional) If you have created a dedicated namespace for this lab, you can delete it by runniung:</li> </ul> <pre><code>kubectl delete namespace codewizard\n</code></pre>"},{"location":"13-HelmChart/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Helm not found:</li> </ul> <p>Make sure <code>Helm</code> is installed and available in your <code>PATH</code>.  Run the following to verify:</p> <pre><code>helm version\n</code></pre> <p></p> <ul> <li>Pods not starting:</li> </ul> <p>Check pod status and logs by running the following commands:</p> <pre><code>kubectl get pods -n codewizard\nkubectl describe pod &lt;pod-name&gt; -n codewizard\nkubectl logs &lt;pod-name&gt; -n codewizard\n</code></pre> <p></p> <ul> <li>Service not reachable:</li> </ul> <p>Ensure the service and pods are running by running the following commands:</p> <pre><code>kubectl get svc -n codewizard\nkubectl get pods -n codewizard\n</code></pre> <p></p> <ul> <li>Values not updated after upgrade:</li> </ul> <p>Double-check your <code>--set</code> or <code>--values</code> flags and confirm the upgrade by running:</p> <pre><code>helm get values codewizard-helm-demo\n</code></pre>"},{"location":"13-HelmChart/#next-steps","title":"Next Steps","text":"<ul> <li>Try creating your own <code>Helm</code> chart for a different application.</li> <li>Explore <code>Helm</code> chart repositories like Artifact Hub.</li> <li>Learn about advanced <code>Helm</code> features, such as: dependencies, hooks, and chart testing.</li> <li>Integrate <code>Helm</code> with CI/CD pipelines for automated deployments.</li> <li>Read more in the official Helm documentation.</li> </ul>"},{"location":"14-Logging/","title":"14 Logging","text":""},{"location":"14-Logging/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"14-Logging/#logging","title":"Logging","text":"<ul> <li>Welcome to the <code>Logging</code> hands-on lab! In this tutorial, we will learn the essentials of <code>Logging</code> in Kubernetes clusters.</li> <li>We will deploy a sample application, configure log collection, and explore logs using popular tools like <code>Fluentd</code>, <code>Elasticsearch</code>, and <code>Kibana</code> (EFK stack).</li> </ul>"},{"location":"14-Logging/#pre-requirements","title":"Pre requirements","text":"<ul> <li>Kubernetes cluster - Setting up minikube cluster instruction</li> <li>kubectl configured to interact with your cluster</li> <li>Helm installed for easier deployment</li> </ul>"},{"location":"14-Logging/#ctrl-click-to-open-in-new-window","title":"CTRL + click to open in new window","text":""},{"location":"14-Logging/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>Why <code>Logging</code> is important in Kubernetes</li> <li>How to deploy a sample app that generates logs</li> <li>How to collect logs using Fluentd</li> <li>How to store and search logs with <code>Elasticsearch</code></li> <li>How to visualize logs with <code>Kibana</code></li> <li>Troubleshooting and best practices</li> </ul>"},{"location":"14-Logging/#introduction","title":"Introduction","text":"<ul> <li><code>Logging</code> is critical for monitoring, debugging, and auditing applications in Kubernetes.</li> <li>Kubernetes does not provide a builtin, centralized <code>Logging</code> solution, but it allows us to integrate with many <code>Logging</code> stacks.</li> <li>We will set up the EFK stack (<code>Elasticsearch</code>, <code>Fluentd</code>, <code>Kibana</code>) to collect, store, and visualize logs from our cluster.</li> </ul>"},{"location":"14-Logging/#lab","title":"Lab","text":""},{"location":"14-Logging/#step-01-deploy-a-sample-application","title":"Step 01 - Deploy a Sample Application","text":"<ul> <li>Deploy a simple <code>Nginx</code> application that generates access logs.</li> </ul> <pre><code>kubectl create deployment nginx --image=nginx\nkubectl expose deployment nginx --port=80 --type=NodePort\n</code></pre> <ul> <li>Check that the pod is running:</li> </ul> <pre><code>kubectl get pods\n</code></pre>"},{"location":"14-Logging/#step-02-deploy-elasticsearch","title":"Step 02 - Deploy <code>Elasticsearch</code>","text":"<ul> <li>Deploy <code>Elasticsearch</code> using <code>Helm</code>:</li> </ul> <pre><code>helm repo add elastic https://helm.elastic.co\nhelm repo update\nhelm install elasticsearch elastic/elasticsearch --set replicas=1 --set minimumMasterNodes=1\n</code></pre> <ul> <li>Wait for the pod to be ready and check its status:</li> </ul> <pre><code>kubectl get pods\n</code></pre>"},{"location":"14-Logging/#step-03-deploy-kibana","title":"Step 03 - Deploy <code>Kibana</code>","text":"<ul> <li>Deploy <code>Kibana</code> using <code>Helm</code>:</li> </ul> <pre><code>helm install kibana elastic/kibana\n</code></pre> <ul> <li>Forward the <code>Kibana</code> port:</li> </ul> <pre><code>kubectl port-forward svc/kibana-kibana 5601:5601 &amp;\n</code></pre> <p>If you are running this lab in Google Cloud Shell:</p> <ol> <li>After running the port-forward command above, click the Web Preview button in the Cloud Shell toolbar (usually at the top right).</li> <li>Enter port <code>5601</code> when prompted.</li> <li>This will open <code>Kibana</code> in a new browser tab at a URL like <code>https://&lt;cloudshell-id&gt;.shell.cloud.google.com/?port=5601</code>.</li> <li>If you see a warning about an untrusted connection, you can safely proceed.</li> </ol> <ul> <li>Access <code>Kibana</code> at http://localhost:5601 (if running locally) or via the Cloud Shell Web Preview, as explained above.</li> </ul>"},{"location":"14-Logging/#step-04-deploy-fluentd","title":"Step 04 - Deploy <code>Fluentd</code>","text":"<ul> <li>Deploy <code>Fluentd</code> as a <code>DaemonSet</code> to collect logs from all nodes and forward them to <code>Elasticsearch</code>.</li> </ul> <pre><code>kubectl apply -f https://raw.githubusercontent.com/fluent/fluentd-kubernetes-daemonset/master/fluentd-daemonset-elasticsearch-rbac.yaml\n</code></pre> <ul> <li>Check that <code>Fluentd</code> pods are running:</li> </ul> <pre><code>kubectl get pods -l app=fluentd\n</code></pre>"},{"location":"14-Logging/#step-05-generate-and-view-logs","title":"Step 05 - Generate and View Logs","text":"<ul> <li>Access the <code>Nginx</code> service to generate logs:</li> </ul> <pre><code>minikube service nginx\n</code></pre> <p>In <code>Kibana</code>, configure an index pattern to view logs:</p> <ol> <li>Open Kibana in your browser (using the Cloud Shell Web Preview as described above).</li> <li>In the left menu, click Stack Management &gt; Kibana &gt; Index Patterns.</li> <li>Click Create index pattern.</li> <li>In the \u201cIndex pattern\u201d field, enter <code>fluentd-*</code> (or <code>logstash-*</code> if your logs use that prefix).</li> <li>Click Next step.</li> <li>For the time field, select <code>@timestamp</code> and click Create index pattern.</li> <li>Go to Discover in the left menu to view and search your logs.</li> </ol> <p>Explore the logs, search, and visualize traffic.</p>"},{"location":"14-Logging/#troubleshooting","title":"Troubleshooting","text":""},{"location":"14-Logging/#pods-not-starting","title":"Pods not starting:","text":"<ul> <li>Check pod status and logs:</li> </ul> <pre><code>kubectl get pods\nkubectl describe pod &lt;pod-name&gt;\nkubectl logs &lt;pod-name&gt;\n</code></pre>"},{"location":"14-Logging/#kibana-not-reachable","title":"Kibana not reachable:","text":"<ul> <li>Ensure port-forward is running and no firewall is blocking port 5601.</li> </ul>"},{"location":"14-Logging/#no-logs-in-kibana","title":"No logs in Kibana:","text":"<ul> <li>Check Fluentd and Elasticsearch pod logs for errors.</li> <li>Ensure index pattern is set up correctly in Kibana.</li> </ul>"},{"location":"14-Logging/#cleanup","title":"Cleanup","text":"<ul> <li>To remove all resources created by this lab:</li> </ul> <pre><code>helm uninstall elasticsearch\nhelm uninstall kibana\nkubectl delete deployment nginx\nkubectl delete service nginx\nkubectl delete -f https://raw.githubusercontent.com/fluent/fluentd-kubernetes-daemonset/master/fluentd-daemonset-elasticsearch-rbac.yaml\n</code></pre>"},{"location":"14-Logging/#next-steps","title":"Next Steps","text":"<ul> <li>Try deploying other logging stacks like <code>Loki</code> + <code>Grafana</code>.</li> <li>Explore log aggregation, alerting, and retention policies.</li> <li>Integrate logging with monitoring and alerting tools.</li> <li>Read more in the Kubernetes logging documentation.</li> </ul>"},{"location":"15-Prometheus-Grafana/","title":"15 Prometheus Grafana","text":""},{"location":"15-Prometheus-Grafana/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"15-Prometheus-Grafana/#prometheus-and-grafana-monitoring-lab","title":"Prometheus and Grafana Monitoring Lab","text":"<ul> <li>In this lab, we will learn how to set up and configure *<code>Prometheus</code>  and <code>Grafana</code> for monitoring a Kubernetes cluster.</li> <li>You will install <code>Prometheus</code> to collect metrics from the cluster and <code>Grafana</code> to visualize those metrics.</li> <li>By the end of this lab, you will have a functional monitoring stack that provides insights into the health and performance of your Kubernetes environment.</li> </ul>"},{"location":"15-Prometheus-Grafana/#pre-requirements","title":"Pre requirements","text":"<ul> <li><code>Helm</code> installed</li> <li>K8S cluster - Setting up minikube cluster instruction</li> <li>kubectl configured to interact with your cluster</li> </ul>"},{"location":"15-Prometheus-Grafana/#ctrl-click-to-open-in-new-window","title":"CTRL + click to open in new window","text":""},{"location":"15-Prometheus-Grafana/#prometheus-and-grafana-setup-and-configuration-guide","title":"Prometheus and Grafana Setup and Configuration Guide","text":"<ul> <li>This guide serves as a comprehensive walkthrough of the steps to set up <code>Prometheus</code> and <code>Grafana</code> on your Kubernetes cluster. </li> <li>It includes hands-on steps for installing <code>Prometheus</code> using <code>Helm</code>, configuring <code>Prometheus</code> to collect metrics, setting up <code>Grafana</code> to visualize key metrics, and automating the setup using a bash script.</li> </ul>"},{"location":"15-Prometheus-Grafana/#introduction-to-prometheus-and-grafana","title":"Introduction to Prometheus and Grafana","text":""},{"location":"15-Prometheus-Grafana/#prometheus","title":"<code>Prometheus</code>","text":"<ul> <li><code>Prometheus</code> is an open-source systems monitoring and alerting toolkit designed for reliability and scalability. </li> <li>It collects and stores metrics as time-series data, providing powerful querying capabilities. </li> <li>It is commonly used in Kubernetes environments for monitoring cluster health, application performance, and infrastructure.</li> </ul>"},{"location":"15-Prometheus-Grafana/#grafana","title":"<code>Grafana</code>","text":"<ul> <li><code>Grafana</code> is a popular open-source data visualization tool that works well with <code>Prometheus</code>. </li> <li>It allows you to create dashboards and visualize metrics in real-time, providing insights into system performance and application health. </li> <li><code>Grafana</code> supports a wide range of visualization options, including <code>graphs</code>, <code>heatmaps</code>, <code>tables</code>, and more.</li> <li>Together, <code>Prometheus</code> and <code>Grafana</code> provide a powerful stack for monitoring and alerting in Kubernetes.</li> </ul>"},{"location":"15-Prometheus-Grafana/#part-01-installing-prometheus-and-grafana","title":"Part 01 - Installing Prometheus and Grafana","text":"<p>Helm Charts</p> <p>We will use Helm, to deploy Prometheus and Grafana.</p>"},{"location":"15-Prometheus-Grafana/#step-01-add-prometheus-and-grafana-helm-repositories","title":"Step 01 - Add Prometheus and Grafana Helm Repositories","text":"<ul> <li>Let\u2019s add the official <code>Helm</code> charts for <code>Prometheus</code> and <code>Grafana</code>:</li> </ul> <pre><code># Add Prometheus community Helm repository\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n# Add Grafana Helm repository\nhelm repo add grafana https://grafana.github.io/helm-charts\n# Update your Helm repositories to make sure they are up-to-date\nhelm repo update\n</code></pre>"},{"location":"15-Prometheus-Grafana/#step-02-install-prometheus-stack","title":"Step 02 - Install Prometheus Stack","text":"<ul> <li><code>Prometheus</code> is installed using the <code>prometheus-stack</code> <code>Helm</code> chart.</li> </ul> <pre><code># Install Prometheus \n#          Alertmanager\n#          Node Exporter\n# Create the `monitoring` namespace if it does not exist.\nhelm install  prometheus                                  \\\n              --namespace monitoring                      \\\n              --create-namespace                          \\\n              prometheus-community/kube-prometheus-stack  \n\n# Verify the status of the release using the following:\nhelm status prometheus -n monitoring\n</code></pre>"},{"location":"15-Prometheus-Grafana/#step-03-install-grafana","title":"Step 03 - Install Grafana","text":"<ul> <li>Now, let\u2019s install <code>Grafana</code>.</li> <li><code>Grafana</code> will be deployed in the same <code>monitoring</code> namespace.</li> </ul> <pre><code>helm install grafana grafana/grafana --namespace monitoring\n\n# Verify the status of the release using the following:\nhelm status grafana -n monitoring\n</code></pre>"},{"location":"15-Prometheus-Grafana/#step-04-access-grafana","title":"Step 04 - Access Grafana","text":"<ul> <li><code>Grafana</code> will expose a service in your Kubernetes cluster.</li> <li>To access it, you need a password and port forwarding.</li> </ul> <pre><code># In order to get the Grafana admin password, run the following command:\nkubectl get secret grafana          \\\n            --namespace monitoring  \\\n            -o jsonpath='{.data.admin-password}' | base64 --decode ; echo\n\n# Set the port forwarding so you can access the service using your browsers\nkubectl port-forward            \\\n        --namespace monitoring  \\\n        service/grafana 3000:80\n</code></pre> <ul> <li>Verify that you can access <code>**Grafana</code></li> <li>Open your browser and navigate to http://localhost:3000</li> <li>The default login is: <ul> <li>Username : <code>admin</code></li> <li>Password : (the password you retrieved earlier)</li> </ul> </li> </ul> <p>Accessing Grafana on Google Cloud Shell</p> <p>If you are running your cluster in Google Cloud Shell, you cannot use <code>localhost</code> for port forwarding. Instead, use the Cloud Shell Web Preview:</p> <ol> <li>Run the port-forward command as usual:     <pre><code>kubectl port-forward --namespace monitoring service/grafana 3000:80\n</code></pre></li> <li>In Google Cloud Shell, click the \u201cWeb Preview\u201d button (top right) and select \u201cPreview on port 3000\u201d.</li> <li>Grafana will open in a new browser tab.  </li> <li>Username: <code>admin</code></li> <li>Password: (the password you retrieved earlier)</li> </ol> <p>Note: You can use any available port (e.g., 3000, 3001) in the port-forward command, just match it in the Web Preview.</p>"},{"location":"15-Prometheus-Grafana/#part-02-configuring-prometheus","title":"Part 02 - Configuring Prometheus","text":"<ul> <li><code>Prometheus</code> can collect various metrics from your Kubernetes cluster automatically if the right exporters are enabled. </li> <li>The kube-prometheus-stack chart that you installed earlier automatically configures <code>Prometheus</code> to scrape a number of Kubernetes components (like <code>kubelet</code>, <code>node-exporter</code>, and <code>kube-state-metrics</code>) for various metrics.</li> </ul>"},{"location":"15-Prometheus-Grafana/#step-01-verify-prometheus-metrics-collection","title":"Step 01 - Verify Prometheus Metrics Collection","text":"<ul> <li>You can check if <code>Prometheus</code> is correctly scraping metrics by navigating to <code>Prometheus</code>\u2019 web UI.</li> </ul> <pre><code># Port-forward the Prometheus service:\nkubectl port-forward            \\\n        --namespace monitoring  \\\n        svc/prometheus-operated 9090:9090\n</code></pre> <ul> <li>Verify that you can access <code>Prometheus</code></li> <li>Open http://localhost:9090</li> <li>In the expression field paste the following:</li> </ul> <pre><code># This query will show the current status of the `kube-state-metrics` job\nup{job=\"kube-state-metrics\"}\n</code></pre> <p>Accessing Prometheus on Google Cloud Shell</p> <p>If you are running your cluster in Google Cloud Shell, you cannot use <code>localhost</code> for port forwarding. Instead, use the Cloud Shell Web Preview:</p> <ol> <li>Run the port-forward command as usual:     <pre><code>kubectl port-forward --namespace monitoring svc/prometheus-operated 9090:9090\n</code></pre></li> <li>In Google Cloud Shell, click the \u201cWeb Preview\u201d button (top right) and select \u201cPreview on port 9090\u201d.</li> <li>Prometheus will open in a new browser tab.</li> </ol> <p>Note: You can use any available port (e.g., 9090, 9091) in the port-forward command, just match it in the Web Preview.</p>"},{"location":"15-Prometheus-Grafana/#part-03-configuring-grafana","title":"Part 03 - Configuring Grafana","text":"<ul> <li>In this part we will set <code>grafana</code> to display the Cluster\u2019s CPUs, Memory, and Requests.</li> <li><code>Grafana</code> dashboards can be configured to display real-time metrics for CPU, memory, and requests.</li> <li><code>Prometheus</code> stores these metrics and <code>Grafana</code> will query <code>Prometheus</code> to display them.</li> </ul>"},{"location":"15-Prometheus-Grafana/#step-01-add-prometheus-as-a-data-source-in-grafana","title":"Step 01 - Add Prometheus as a Data Source in Grafana","text":"<ol> <li>Log into <code>Grafana</code> at: http://localhost:3000, or use the Cloud Shell Web Preview.</li> <li>Click on the hamburger icon on the left sidebar to open the Configuration menu.</li> <li>Click on Data Sources.</li> <li>Click Add data source and choose Prometheus.</li> <li>In the URL field, enter the Prometheus server URL: <code>http://prometheus-operated:9090</code>.</li> <li>Click Save &amp; Test to confirm that the connection is working.</li> </ol>"},{"location":"15-Prometheus-Grafana/#step-02-create-a-dashboard-to-display-metrics","title":"Step 02 - Create a Dashboard to Display Metrics","text":"<ul> <li>Next step is to create a dashboard and panels to display the desired metrics.</li> <li> <p>To create a dashboard in <code>Grafana</code> for CPU, memory, and requests do the following:</p> </li> <li> <p>In <code>Grafana</code>, open the left sidebar menu and select Dashboard.</p> </li> <li>Click Add visualization.</li> <li>Choose <code>Data Source</code> (as we defined it previously).</li> <li>In the panel editor, click on the <code>Code</code> option (right side of the query builder).</li> <li>Enter the below queries to visualize metric(s):       Note: To add new query click on the <code>+ Add query</code></li> <li>Save the dashboard.</li> </ul> <p></p> <ul> <li>CPU Usage</li> </ul> <pre><code>sum(rate(container_cpu_usage_seconds_total{namespace=\"default\", container!=\"\", container!=\"POD\"}[5m])) by (pod, namespace)\n</code></pre> <ul> <li>Memory Usage :</li> </ul> <pre><code>sum(container_memory_usage_bytes{namespace=\"default\", container!=\"\", container!=\"POD\"}) by (pod, namespace)\n</code></pre> <ul> <li>Request Count :</li> </ul> <pre><code>sum(rate(http_requests_total{job=\"kubelet\", cluster=\"\", namespace=\"default\"}[5m])) by (pod, namespace)\n</code></pre>"},{"location":"15-Prometheus-Grafana/#step-03-get-number-of-pods-in-the-cluster","title":"Step 03 - Get Number of Pods in the Cluster","text":"<ul> <li>To track the number of pods running in the cluster, add new panel with the following query:</li> </ul> <pre><code># This query counts the number of pods running in all the namespaces\ncount(kube_pod_info{}) by (namespace)\n</code></pre> <ul> <li>Add another query which will count the number of pods under the namespace <code>monitoring</code>:</li> </ul> <pre><code>count(kube_pod_info{namespace=\"monitoring\"}) by (namespace)\n</code></pre> <p>Tip</p> <p>We have already defined query based upon namespaces before.... You can use the same approach to filter by other labels as well.</p>"},{"location":"15-Prometheus-Grafana/#step-04-customize-the-panel","title":"Step 04: Customize the Panel","text":"<ul> <li>Change the visualization by changing the Graph Style</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/","title":"16 Affinity Taint Tolleration","text":""},{"location":"16-Affinity-Taint-Tolleration/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"16-Affinity-Taint-Tolleration/#node-affinity-taints-and-tolerations-lab","title":"Node Affinity, Taints, and Tolerations Lab","text":"<ul> <li>In this lab, we will explore Kubernetes mechanisms for controlling Pod placement on Nodes.</li> <li>We will learn how to use <code>Node Affinity</code>, <code>Taints</code>, and <code>Tolerations</code> to schedule Pods on specific Nodes based on labels, constraints, and preferences.</li> <li>By the end of this lab, you will understand how to control where Pods run in your cluster and how to reserve Nodes for specific workloads.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#pre-requirements","title":"Pre requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> <li>kubectl configured to interact with your cluster</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#ctrl-click-to-open-in-new-window","title":"CTRL + click to open in new window","text":""},{"location":"16-Affinity-Taint-Tolleration/#introduction-to-pod-scheduling","title":"Introduction to Pod Scheduling","text":"<p>Kubernetes provides several mechanisms to control which Nodes your Pods run on:</p>"},{"location":"16-Affinity-Taint-Tolleration/#node-affinity","title":"Node Affinity","text":"<ul> <li><code>Node Affinity</code> allows us to constrain which Nodes our Pods can be scheduled on based on Node labels.</li> <li>It\u2019s a more expressive and flexible version of <code>nodeSelector</code>.</li> <li>There are two types:</li> <li><code>requiredDuringSchedulingIgnoredDuringExecution</code>: Hard requirement - Pod will not be scheduled unless the rule is met.</li> <li><code>preferredDuringSchedulingIgnoredDuringExecution</code>: Soft preference - Scheduler will try to enforce but will still schedule the Pod if it can\u2019t.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#taints-and-tolerations","title":"Taints and Tolerations","text":"<ul> <li><code>Taints</code> are applied to Nodes and allow a Node to repel a set of Pods.</li> <li><code>Tolerations</code> are applied to Pods and allow (but do not require) Pods to schedule onto Nodes with matching <code>Taints</code>.</li> <li><code>Taints</code> and <code>Tolerations</code> work together to ensure that Pods are not scheduled onto inappropriate Nodes.</li> <li>Use cases include:<ul> <li>Dedicating Nodes to specific workloads</li> <li>Reserving Nodes with special hardware (GPUs, SSDs)</li> <li>Isolating problematic Pods</li> </ul> </li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#part-01-node-affinity","title":"Part 01 - Node Affinity","text":"<ul> <li>In this section, we will learn how to use Node <code>Affinity</code> to schedule Pods on specific Nodes.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#step-01-label-your-nodes","title":"Step 01 - Label Your Nodes","text":"<ul> <li>First, let\u2019s label some Nodes to use with Node <code>Affinity</code>:</li> </ul> <pre><code># Get list of nodes\nkubectl get nodes\n\n# Label a node with environment=production\nkubectl label nodes &lt;node-name&gt; environment=production\n\n# Label another node with environment=development\nkubectl label nodes &lt;node-name&gt; environment=development\n\n# Verify the labels\nkubectl get nodes --show-labels\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-02-create-a-pod-with-required-node-affinity","title":"Step 02 - Create a Pod with Required Node Affinity","text":"<ul> <li>Create a Pod that must run on a Node with <code>environment=production</code>:</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: affinity-required-pod\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: environment\n            operator: In\n            values:\n            - production\n  containers:\n  - name: nginx\n    image: nginx:latest\n</code></pre> <ul> <li>Apply the Pod:</li> </ul> <pre><code># Create the Pod\nkubectl apply -f affinity-required-pod.yaml\n\n# Check which Node the Pod is running on\nkubectl get pod affinity-required-pod -o wide\n\n# Verify it's running on the production Node\nkubectl describe pod affinity-required-pod | grep Node:\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-03-create-a-pod-with-preferred-node-affinity","title":"Step 03 - Create a Pod with Preferred Node Affinity","text":"<ul> <li>Create a Pod that prefers to run on a Node with <code>environment=development</code>:</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: affinity-preferred-pod\nspec:\n  affinity:\n    nodeAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: environment\n            operator: In\n            values:\n            - development\n  containers:\n  - name: nginx\n    image: nginx:latest\n</code></pre> <ul> <li>Apply the Pod:</li> </ul> <pre><code># Create the Pod\nkubectl apply -f affinity-preferred-pod.yaml\n\n# Check which Node the Pod is running on\nkubectl get pod affinity-preferred-pod -o wide\n\n# This Pod will prefer the development Node but can run elsewhere if needed\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-04-experiment-with-node-affinity-operators","title":"Step 04 - Experiment with Node Affinity Operators","text":"<ul> <li> <p><code>Node Affinity</code> supports several operators:</p> <ul> <li><code>In</code>: Label value is in the list of values</li> <li><code>NotIn</code>: Label value is not in the list of values</li> <li><code>Exists</code>: Label key exists (value does not matter)</li> <li><code>DoesNotExist</code>: Label key does not exist</li> <li><code>Gt</code>: Label value is greater than the specified value (numeric comparison)</li> <li><code>Lt</code>: Label value is less than the specified value (numeric comparison)</li> </ul> </li> <li> <p>Example with multiple conditions:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: affinity-multiple-conditions\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: environment\n            operator: In\n            values:\n            - production\n            - staging\n          - key: disk-type\n            operator: Exists\n  containers:\n  - name: nginx\n    image: nginx:latest\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#part-02-taints-and-tolerations","title":"Part 02 - Taints and Tolerations","text":"<ul> <li>In this section, we will learn how to use <code>Taints</code> and <code>Tolerations</code> to control Pod scheduling.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#step-01-understanding-taint-effects","title":"Step 01 - Understanding Taint Effects","text":"<ul> <li> <p><code>Taints</code> have three effects:</p> <ul> <li><code>NoSchedule</code>: Pods without matching <code>tolerations</code> will not be scheduled on the Node.</li> <li><code>PreferNoSchedule</code>: Scheduler will try to avoid placing Pods without <code>tolerations</code>, but it\u2019s not guaranteed.</li> <li><code>NoExecute</code>: Existing Pods without <code>tolerations</code> will be evicted, and new ones won\u2019t be scheduled.</li> </ul> </li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#step-02-apply-a-taint-to-a-node","title":"Step 02 - Apply a Taint to a Node","text":"<ul> <li>Let\u2019s <code>taint</code> a Node to dedicate it for special workloads:</li> </ul> <pre><code># Apply a taint to a Node\nkubectl taint nodes &lt;node-name&gt; dedicated=special-workload:NoSchedule\n\n# Verify the taint\nkubectl describe node &lt;node-name&gt; | grep Taints\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-03-create-a-pod-without-toleration","title":"Step 03 - Create a Pod Without Toleration","text":"<ul> <li>Let\u2019s try to create a Pod without a <code>toleration</code>:</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-without-toleration\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n</code></pre> <pre><code># Create the Pod\nkubectl apply -f pod-without-toleration.yaml\n\n# Check the Pod status - it should not be scheduled on the tainted Node\nkubectl get pod pod-without-toleration -o wide\n\n# If all your Nodes are tainted, the Pod will remain Pending\nkubectl describe pod pod-without-toleration\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-04-create-a-pod-with-toleration","title":"Step 04 - Create a Pod With Toleration","text":"<ul> <li>Now let\u2019s create a Pod that tolerates the <code>taint</code>:</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-toleration\nspec:\n  tolerations:\n  - key: \"dedicated\"\n    operator: \"Equal\"\n    value: \"special-workload\"\n    effect: \"NoSchedule\"\n  containers:\n  - name: nginx\n    image: nginx:latest\n</code></pre> <pre><code># Create the Pod\nkubectl apply -f pod-with-toleration.yaml\n\n# This Pod can now be scheduled on the tainted Node\nkubectl get pod pod-with-toleration -o wide\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-05-understanding-toleration-operators","title":"Step 05 - Understanding Toleration Operators","text":"<ul> <li> <p>Tolerations support two operators:</p> <ul> <li><code>Equal</code>: Requires exact match of key, value, and effect</li> <li><code>Exists</code>: Only checks for key existence (value is ignored)</li> </ul> </li> <li> <p>Example with <code>Exists</code> operator:</p> </li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-exists-toleration\nspec:\n  tolerations:\n  - key: \"dedicated\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n  containers:\n  - name: nginx\n    image: nginx:latest\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-06-noexecute-effect","title":"Step 06 - NoExecute Effect","text":"<ul> <li>The <code>NoExecute</code> effect is special - it evicts running Pods:</li> </ul> <pre><code># Apply a NoExecute taint\nkubectl taint nodes &lt;node-name&gt; maintenance=true:NoExecute\n\n# Any Pods on this Node without matching toleration will be evicted\nkubectl get pods -o wide --watch\n</code></pre> <ul> <li>Let\u2019s add a toleration with <code>tolerationSeconds</code> to delay eviction:</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-delayed-eviction\nspec:\n  tolerations:\n  - key: \"maintenance\"\n    operator: \"Equal\"\n    value: \"true\"\n    effect: \"NoExecute\"\n    tolerationSeconds: 300  # Pod will be evicted after 5 minutes\n  containers:\n  - name: nginx\n    image: nginx:latest\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#part-03-combining-affinity-taints-and-tolerations","title":"Part 03 - Combining Affinity, Taints, and Tolerations","text":"<ul> <li>We can combine <code>Node Affinity</code> with <code>Taints</code> and <code>Tolerations</code> for fine-grained control.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#step-01-create-a-dedicated-node-pool","title":"Step 01 - Create a Dedicated Node Pool","text":"<ul> <li>Let\u2019s simulate a dedicated Node pool for GPU workloads:</li> </ul> <pre><code># Label a Node for GPU workload\nkubectl label nodes &lt;node-name&gt; hardware=gpu\n\n# Taint the Node to prevent non-GPU Pods\nkubectl taint nodes &lt;node-name&gt; nvidia.com/gpu=true:NoSchedule\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-02-deploy-a-gpu-workload","title":"Step 02 - Deploy a GPU Workload","text":"<ul> <li>Let\u2019s create a Pod that requires GPU Nodes:</li> </ul> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-workload\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: hardware\n            operator: In\n            values:\n            - gpu\n  tolerations:\n  - key: \"nvidia.com/gpu\"\n    operator: \"Equal\"\n    value: \"true\"\n    effect: \"NoSchedule\"\n  containers:\n  - name: gpu-app\n    image: nvidia/cuda:11.0-base\n    command: [\"nvidia-smi\"]\n</code></pre> <pre><code># Apply the Pod\nkubectl apply -f gpu-workload.yaml\n\n# Verify it's scheduled on the GPU Node\nkubectl get pod gpu-workload -o wide\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#part-04-cleanup-and-best-practices","title":"Part 04 - Cleanup and Best Practices","text":""},{"location":"16-Affinity-Taint-Tolleration/#step-01-remove-taints","title":"Step 01 - Remove Taints","text":"<pre><code># Remove a taint from a Node (add a minus sign at the end)\nkubectl taint nodes &lt;node-name&gt; dedicated=special-workload:NoSchedule-\n\n# Remove all taints with a specific key\nkubectl taint nodes &lt;node-name&gt; nvidia.com/gpu-\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-02-remove-labels","title":"Step 02 - Remove Labels","text":"<pre><code># Remove a label from a Node (add a minus sign at the end)\nkubectl label nodes &lt;node-name&gt; environment-\nkubectl label nodes &lt;node-name&gt; hardware-\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#step-03-delete-test-pods","title":"Step 03 - Delete Test Pods","text":"<pre><code># Delete all test Pods\nkubectl delete pod affinity-required-pod affinity-preferred-pod\nkubectl delete pod pod-with-toleration pod-without-toleration\nkubectl delete pod gpu-workload\n</code></pre>"},{"location":"16-Affinity-Taint-Tolleration/#best-practices","title":"Best Practices","text":"<ol> <li>Use Node Affinity for preferences, <code>Taints/Tolerations</code> for hard requirements.</li> <li>Label Nodes consistently across your cluster (e.g., <code>node-role</code>, <code>hardware-type</code>, <code>environment</code>).</li> <li>Document your taints - team members need to know why Nodes are tainted.</li> <li>Use <code>PreferNoSchedule</code> for soft isolation instead of <code>NoSchedule</code> when appropriate.</li> <li>Combine with Pod Priority for more sophisticated scheduling strategies.</li> <li>Test eviction behavior before using <code>NoExecute</code> in production.</li> <li>Use tolerationSeconds to gracefully handle Node maintenance.</li> <li>Monitor unschedulable Pods - they indicate scheduling constraint conflicts.</li> </ol>"},{"location":"16-Affinity-Taint-Tolleration/#summary","title":"Summary","text":"<p>In this lab, you learned:</p> <ul> <li>How to use Node Affinity to schedule Pods on specific Nodes based on labels.</li> <li>The difference between <code>required</code> and <code>preferred</code> affinity rules.</li> <li>How to use Taints to repel Pods from Nodes.</li> <li>How to use Tolerations to allow Pods on tainted Nodes.</li> <li>The three taint effects: <code>NoSchedule</code>, <code>PreferNoSchedule</code>, and <code>NoExecute</code>.</li> <li>How to combine affinity, taints, and tolerations for complex scheduling scenarios.</li> <li>Best practices for Pod placement in production clusters.</li> </ul>"},{"location":"16-Affinity-Taint-Tolleration/#additional-resources","title":"Additional Resources","text":"<ul> <li>Kubernetes Node Affinity Documentation</li> <li>Kubernetes Taints and Tolerations Documentation</li> <li>Pod Priority and Preemption</li> </ul>"},{"location":"17-PodDisruptionBudgets-PDB/","title":"17 PodDisruptionBudgets PDB","text":""},{"location":"17-PodDisruptionBudgets-PDB/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"17-PodDisruptionBudgets-PDB/#pod-disruption-budgets-pdb","title":"Pod Disruption Budgets (PDB)","text":"<ul> <li>In this lab, we will learn about <code>Pod Disruption Budgets (PDB)</code> in Kubernetes.</li> <li>We will explore how to define and implement PDBs to ensure application availability during voluntary disruptions, such as node maintenance or cluster upgrades.</li> <li>By the end of this lab, you will understand how to create and manage Pod Disruption Budgets to maintain the desired level of service availability in your Kubernetes cluster.</li> </ul>"},{"location":"17-PodDisruptionBudgets-PDB/#pre-requirements","title":"Pre requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> <li>kubectl configured to interact with your cluster</li> </ul>"},{"location":"17-PodDisruptionBudgets-PDB/#ctrl-click-to-open-in-new-window","title":"CTRL + click to open in new window","text":""},{"location":"17-PodDisruptionBudgets-PDB/#poddisruptionbudgets-budgeting-the-number-of-faults-to-tolerate","title":"<code>PodDisruptionBudgets</code>: Budgeting the Number of Faults to Tolerate","text":"<ul> <li> <p>A <code>pod disruption budget</code> is an indicator of the number of disruptions that can be tolerated at a given time for a class of pods (a budget of faults).</p> </li> <li> <p>Disruptions may be caused by deliberate or accidental Pod deletion.</p> </li> <li> <p>Whenever a disruption to the pods in a service is calculated to cause the service to drop below the budget, the operation is paused until it can maintain the budget. This means that the <code>drain event</code> could be temporarily halted while it waits for more pods to become available such that the budget isn\u2019t crossed by evicting the pods.</p> </li> <li> <p>You can specify Pod Disruption Budgets for Pods managed by these built-in Kubernetes controllers:</p> <ul> <li><code>Deployment</code></li> <li><code>ReplicationController</code></li> <li><code>ReplicaSet</code></li> <li><code>StatefulSet</code></li> </ul> </li> <li> <p>For this tutorial you should get familier with Kubernetes Eviction Policies, as it demonstrates how <code>Pod Disruption Budgets</code> handle evictions.</p> </li> <li> <p>As in the <code>Kubernetes Eviction Policies</code> tutorial, we start with  <pre><code>eviction-hard=\"memory.available&lt;480M\"\n</code></pre></p> </li> </ul>"},{"location":"17-PodDisruptionBudgets-PDB/#sample","title":"Sample","text":"<ul> <li> <p>In the below sample we will configure a <code>Pod Disruption Budget</code> which insure that we will always have at least 1 Nginx instance.</p> </li> <li> <p>First we need an Nginx Deployment:</p> </li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: codewizard\n  labels:\n    app: nginx # &lt;- We will use this name below\n...\n</code></pre> <ul> <li>Now we can create the <code>Pod Disruption Budget</code>:</li> </ul> <pre><code>apiVersion: policy/v1beta1\nkind: PodDisruptionBudget\nmetadata:\n  name: nginx-pdb\nspec:\n  minAvailable: 1 # &lt;--- This will insure that we will have at least 1\n  selector:\n    matchLabels:\n      app: nginx # &lt;- The deployment app label \n</code></pre>"},{"location":"17-PodDisruptionBudgets-PDB/#walkthrough","title":"Walkthrough","text":"<p>01. start minikube with Feature Gates</p> <p>02. Check Node Pressure(s)</p>"},{"location":"17-PodDisruptionBudgets-PDB/#step-01-start-minikube-with-feature-gates","title":"Step 01 - Start minikube with Feature Gates","text":"<ul> <li>Run thwe following command to start minikube with the required <code>Feature Gates</code> and <code>Eviction Signals</code>:</li> </ul> <pre><code>minikube start \\\n    --extra-config=kubelet.eviction-hard=\"memory.available&lt;480M\" \\\n    --extra-config=kubelet.eviction-pressure-transition-period=\"30s\" \\\n    --extra-config=kubelet.feature-gates=\"ExperimentalCriticalPodAnnotation=true\"\n</code></pre> <ul> <li> <p>For more details about <code>Feature Gates</code>, read here.</p> </li> <li> <p>For more details about <code>eviction-signals</code>, read here.</p> </li> </ul>"},{"location":"17-PodDisruptionBudgets-PDB/#step-02-check-node-pressures","title":"Step 02 - Check Node Pressure(s)","text":"<ul> <li>Check to see the Node conditions, if we have any kind of \u201cPressure\u201d, by running the following:</li> </ul> <pre><code>kubectl describe node minikube | grep MemoryPressure\n\n# Output should be similar to :\nConditions:\n  Type             Status  Reason                       Message\n  ----             ------  ------                       -------\n  MemoryPressure   False   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    KubeletReady                 kubelet is posting ready status. AppArmor enabled\n  ...\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                750m (37%)  0 (0%)\n  memory             140Mi (6%)  340Mi (16%)\n  ephemeral-storage  0 (0%)      0 (0%)  \n</code></pre>"},{"location":"17-PodDisruptionBudgets-PDB/#step-03-create-3-pods-using-50-mb-each","title":"Step 03 - Create 3 Pods using 50 MB each","text":"<ul> <li>Create a file named <code>50MB-ram.yaml</code> with the following content:</li> </ul> <pre><code># ./resources/50MB-ram.yaml\n...\n\n# 3 replicas\nspec:\n  replicas: 3\n\n# resources request and limits\nresources:\n  requests:\n    memory: \"50Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"128Mi\"\n    cpu: \"500m\"\n</code></pre> <ul> <li>Create the pods with the following command:</li> </ul> <pre><code>kubectl apply -f resources/50MB-ram.yaml\n</code></pre>"},{"location":"17-PodDisruptionBudgets-PDB/#step-04-check-memory-pressure","title":"Step 04 - Check Memory Pressure","text":"<ul> <li>Now let\u2019s check the Node conditions again to see if we have <code>MemoryPressure</code>:</li> </ul> <p><pre><code>$ kubectl describe node minikube | grep MemoryPressure\n\n# Output should be similar to \nMemoryPressure   False   ...   KubeletHasSufficientMemory   kubelet has sufficient memory available\n</code></pre> - As we can see, we still have <code>sufficient memory available</code>.</p>"},{"location":"18-ArgoCD/","title":"18 ArgoCD","text":""},{"location":"18-ArgoCD/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"18-ArgoCD/#argocd","title":"ArgoCD","text":"<ul> <li>In this tutorial, we will learn the essentials of <code>ArgoCD</code>, a declarative GitOps continuous delivery tool for Kubernetes.</li> <li>We will install <code>ArgoCD</code>, deploy applications, sync resources from Git repositories, and gain practical experience with GitOps workflows.</li> </ul>"},{"location":"18-ArgoCD/#pre-requirements","title":"Pre Requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> <li>kubectl configured to interact with your cluster</li> <li>A <code>Git repository</code> (GitHub, GitLab, or Bitbucket) for storing application manifests</li> <li>Basic understanding of Kubernetes resources (Deployments, Services, etc.)</li> </ul>"},{"location":"18-ArgoCD/#ctrl-click-to-open-in-new-window","title":"CTRL + click to open in new window","text":""},{"location":"18-ArgoCD/#what-will-we-learn","title":"What will we learn?","text":"<ul> <li>What <code>ArgoCD</code> is and why it\u2019s useful for GitOps.</li> <li>How to install and configure <code>ArgoCD</code> on Kubernetes.</li> <li><code>ArgoCD</code> core concepts: Applications, Projects, and Sync.</li> <li>How to deploy applications from <code>Git repositories</code>.</li> <li>Application health and sync status monitoring.</li> <li>Rollback and sync strategies.</li> <li>Best practices for GitOps workflows.</li> </ul>"},{"location":"18-ArgoCD/#what-is-argocd","title":"What is ArgoCD?","text":"<ul> <li><code>ArgoCD</code> is a declarative, GitOps continuous delivery tool for Kubernetes.</li> <li>It follows the <code>GitOps</code> pattern where Git repositories are the source of truth for defining the desired application state.</li> <li><code>ArgoCD</code> automates the deployment of the desired application states in the specified target environments.</li> </ul>"},{"location":"18-ArgoCD/#why-argocd","title":"Why ArgoCD?","text":"<ul> <li>GitOps Workflow: Uses Git as the single source of truth.</li> <li>Automated Deployment: Automatically syncs your Kubernetes cluster with Git repositories.</li> <li>Application Health Monitoring: Continuous monitoring of deployed applications.</li> <li>Rollback Capabilities: Easy rollback to previous versions.</li> <li>Multi-Cluster Support: Manage applications across multiple clusters.</li> <li>SSO Integration: Supports various SSO providers for authentication.</li> <li>RBAC: Fine-grained access control.</li> <li>Audit Trail: Full audit trail of all operations.</li> </ul>"},{"location":"18-ArgoCD/#terminology","title":"Terminology","text":"<ul> <li> <p>Application</p> <ul> <li>An <code>ArgoCD</code> Application is a Kubernetes resource object representing a deployed application instance in an environment.</li> <li>It defines the source repository, target cluster, and sync policies.</li> </ul> </li> <li> <p>Project</p> <ul> <li>An <code>ArgoCD</code> Project provides a logical grouping of applications.</li> <li>Projects are useful for organizing applications and implementing RBAC.</li> </ul> </li> <li> <p>Sync</p> <ul> <li>Sync is the process of making a live cluster state match the desired state defined in Git.</li> <li>Sync can run manually or automatically.</li> </ul> </li> <li> <p>Sync Status</p> <ul> <li>Indicates whether the live state matches the Git state.</li> <li>Status can be: Synced, OutOfSync, or Unknown.</li> </ul> </li> <li> <p>Health Status</p> <ul> <li>Indicates the health of the application resources.</li> <li>Status can be: Healthy, Progressing, Degraded, Suspended, or Missing.</li> </ul> </li> </ul>"},{"location":"18-ArgoCD/#argocd-architecture","title":"ArgoCD Architecture","text":"Component Description API Server Exposes the API consumed by Web UI, CLI, and CI/CD systems Repository Server Maintains a local cache of Git repositories holding application manifests Application Controller Monitors running applications and compares the current live state against the desired state Dex Identity service for integrating with external identity providers Redis Used for caching"},{"location":"18-ArgoCD/#common-argocd-cli-commands","title":"Common ArgoCD CLI Commands","text":"Command Description <code>argocd login &lt;server&gt;</code> Login to <code>ArgoCD</code> server <code>argocd app create &lt;app-name&gt;</code> Create a new application <code>argocd app list</code> List all applications <code>argocd app get &lt;app-name&gt;</code> Get application details <code>argocd app sync &lt;app-name&gt;</code> Sync (deploy) an application <code>argocd app delete &lt;app-name&gt;</code> Delete an application <code>argocd app set &lt;app-name&gt;</code> Update application parameters <code>argocd app diff &lt;app-name&gt;</code> Show differences between Git and live state <code>argocd app history &lt;app-name&gt;</code> Show application deployment history <code>argocd app rollback &lt;app-name&gt; &lt;revision&gt;</code> Rollback to a previous revision"},{"location":"18-ArgoCD/#lab","title":"Lab","text":""},{"location":"18-ArgoCD/#part-01-installing-argocd","title":"Part 01 - Installing ArgoCD","text":""},{"location":"18-ArgoCD/#step-01-create-an-argocd-namespace","title":"Step 01 - Create an ArgoCD Namespace","text":"<ul> <li>Create a dedicated namespace for <code>ArgoCD</code>:</li> </ul> <pre><code>kubectl create namespace argocd\n</code></pre>"},{"location":"18-ArgoCD/#step-02-install-argocd","title":"Step 02 - Install ArgoCD","text":"<ul> <li>Install <code>ArgoCD</code> using the official installation manifest:</li> </ul> <pre><code>kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre>"},{"location":"18-ArgoCD/#step-03-verify-installation","title":"Step 03 - Verify Installation","text":"<ul> <li>Check that all <code>ArgoCD</code> pods are running:</li> </ul> <pre><code>kubectl get pods -n argocd\n</code></pre> <ul> <li>Expected output should show all pods in Running status:</li> </ul> <pre><code>NAME                                  READY   STATUS    RESTARTS   AGE\nargocd-application-controller-0       1/1     Running   0          2m\nargocd-dex-server-xxx                 1/1     Running   0          2m\nargocd-redis-xxx                      1/1     Running   0          2m\nargocd-repo-server-xxx                1/1     Running   0          2m\nargocd-server-xxx                     1/1     Running   0          2m\n</code></pre>"},{"location":"18-ArgoCD/#step-04-expose-argocd-server","title":"Step 04 - Expose ArgoCD Server","text":"<ul> <li>By default, the <code>ArgoCD</code> API server is not exposed externally. </li> <li>We\u2019 will use port-forwarding to access it, by running:</li> </ul> <pre><code>kubectl port-forward svc/argocd-server -n argocd 8080:443\n</code></pre> <ul> <li>Alternatively, you can change the service type to LoadBalancer or create an Ingress:</li> </ul> <pre><code># Change to LoadBalancer (for cloud environments)\nkubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'\n\n# Or use NodePort (for local/development)\nkubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"NodePort\"}}'\n</code></pre>"},{"location":"18-ArgoCD/#step-05-get-initial-admin-password","title":"Step 05 - Get Initial Admin Password","text":"<ul> <li>The initial password for the <code>admin</code> user is auto-generated and stored as a secret by running the following command:</li> </ul> <pre><code># Get the initial admin password\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo\n</code></pre> <p>Note</p> <p>Save this password - you\u2019ll need it to login to the ArgoCD UI later.</p>"},{"location":"18-ArgoCD/#step-06-access-argocd-ui","title":"Step 06 - Access ArgoCD UI","text":"<ul> <li> <p>Open your browser and navigate to:</p> <ul> <li>Port-forward: <code>https://localhost:8080</code></li> <li>LoadBalancer: Use the external IP</li> <li>NodePort: Use <code>http://&lt;node-ip&gt;:&lt;node-port&gt;</code></li> </ul> </li> <li> <p>Login with:</p> <ul> <li>Username: <code>admin</code></li> <li>Password: (from Step 05)</li> </ul> </li> </ul>"},{"location":"18-ArgoCD/#part-02-installing-argocd-cli","title":"Part 02 - Installing ArgoCD CLI","text":""},{"location":"18-ArgoCD/#step-01-download-and-install-argocd-cli","title":"Step 01 - Download and Install ArgoCD CLI","text":"<ul> <li>Install the <code>ArgoCD CLI</code> based on your operating system:</li> </ul> <p>Linux: <pre><code>curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\nrm argocd-linux-amd64\n</code></pre></p> <p>macOS: <pre><code>brew install argocd\n</code></pre></p> <p>Windows (via Chocolatey): <pre><code>choco install argocd-cli\n</code></pre></p>"},{"location":"18-ArgoCD/#step-02-verify-cli-installation","title":"Step 02 - Verify CLI Installation","text":"<pre><code>argocd version --short\n</code></pre>"},{"location":"18-ArgoCD/#step-03-login-via-cli","title":"Step 03 - Login via CLI","text":"<pre><code># If using port-forward\nargocd login localhost:8080 --insecure\n\n# You'll be prompted for username and password\n</code></pre>"},{"location":"18-ArgoCD/#step-04-change-admin-password-optional-but-highly-recommended","title":"Step 04 - Change Admin Password (optional, but highly recommended)","text":"<pre><code>argocd account update-password\n</code></pre>"},{"location":"18-ArgoCD/#part-03-deploying-your-first-application","title":"Part 03 - Deploying Your First Application","text":""},{"location":"18-ArgoCD/#step-01-prepare-a-git-repository","title":"Step 01 - Prepare a Git Repository","text":"<ul> <li>For this lab, we will use a sample Git repository with Kubernetes manifests.</li> <li>You can use the <code>ArgoCD</code> example repository or your own:</li> </ul> <pre><code># Sample repository URL\nhttps://github.com/argoproj/argocd-example-apps.git\n</code></pre>"},{"location":"18-ArgoCD/#step-02-create-an-application-via-cli","title":"Step 02 - Create an Application via CLI","text":"<ul> <li>Create an <code>ArgoCD</code> application that deploys the guestbook app:</li> </ul> <pre><code>argocd app create guestbook \\\n  --repo https://github.com/argoproj/argocd-example-apps.git \\\n  --path guestbook \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace default\n</code></pre> Command Explanation <ul> <li><code>--repo</code>: The Git repository URL</li> <li><code>--path</code>: Path within the repository where manifests are located</li> <li><code>--dest-server</code>: Target Kubernetes cluster (default is the cluster where ArgoCD is installed)</li> <li><code>--dest-namespace</code>: Target namespace for deployment</li> </ul>"},{"location":"18-ArgoCD/#step-03-view-application-status","title":"Step 03 - View Application Status","text":"<pre><code># List all applications\nargocd app list\n\n# Get detailed information about the application\nargocd app get guestbook\n</code></pre>"},{"location":"18-ArgoCD/#step-04-sync-the-application","title":"Step 04 - Sync the Application","text":"<ul> <li>Initially, the application status will be OutOfSync. </li> <li>Sync it to deploy by running:</li> </ul> <pre><code>argocd app sync guestbook\n</code></pre>"},{"location":"18-ArgoCD/#step-05-verify-deployment","title":"Step 05 - Verify Deployment","text":"<pre><code># Check the deployed resources\nkubectl get all -n default\n\n# You should see the guestbook deployment, service, and pods\n</code></pre>"},{"location":"18-ArgoCD/#step-06-access-the-application","title":"Step 06 - Access the Application","text":"<pre><code># Port-forward to access the guestbook service\nkubectl port-forward svc/guestbook-ui -n default 8081:80\n\n# Open browser to http://localhost:8081\n</code></pre>"},{"location":"18-ArgoCD/#part-04-creating-application-via-ui","title":"Part 04 - Creating Application via UI","text":""},{"location":"18-ArgoCD/#step-01-access-argocd-ui","title":"Step 01 - Access ArgoCD UI","text":"<ul> <li>Navigate to the <code>ArgoCD</code> UI at <code>https://localhost:8080</code></li> </ul>"},{"location":"18-ArgoCD/#step-02-create-a-new-application","title":"Step 02 - Create a New Application","text":"<ol> <li>Click on \u201d+ NEW APP\u201d button.</li> <li> <p>Fill in the following details:</p> <ul> <li>Application Name: <code>helm-guestbook</code></li> <li>Project: <code>default</code></li> <li>Sync Policy: <code>Manual</code> (or <code>Automatic</code> for auto-sync)</li> <li>Repository URL: <code>https://github.com/argoproj/argocd-example-apps.git</code></li> <li>Revision: <code>HEAD</code></li> <li>Path: <code>helm-guestbook</code></li> <li>Cluster URL: <code>https://kubernetes.default.svc</code></li> <li>Namespace: <code>default</code></li> </ul> </li> <li> <p>Click \u201cCREATE\u201d.</p> </li> </ol>"},{"location":"18-ArgoCD/#step-03-view-application-in-ui","title":"Step 03 - View Application in UI","text":"<ul> <li>You should now be able to see the <code>helm-guestbook</code> application tile in the UI.</li> <li>Click on it to see the application topology.</li> </ul>"},{"location":"18-ArgoCD/#step-04-sync-via-ui","title":"Step 04 - Sync via UI","text":"<ul> <li>Click the \u201cSYNC\u201d button.</li> <li>Select the resources you want to sync (or select all).</li> <li>Click \u201cSYNCHRONIZE\u201d.</li> </ul>"},{"location":"18-ArgoCD/#step-05-monitor-sync-progress","title":"Step 05 - Monitor Sync Progress","text":"<ul> <li>Watch the sync progress in real-time.</li> <li>The UI will show each resource being created/updated.</li> <li>Once completed, all resources should show as Healthy and Synced.</li> </ul>"},{"location":"18-ArgoCD/#part-05-application-management","title":"Part 05 - Application Management","text":""},{"location":"18-ArgoCD/#step-01-view-application-details","title":"Step 01 - View Application Details","text":"<pre><code># Get full application details\nargocd app get guestbook\n\n# View application parameters\nargocd app get guestbook --show-params\n</code></pre>"},{"location":"18-ArgoCD/#step-02-view-sync-history","title":"Step 02 - View Sync History","text":"<pre><code># View deployment history\nargocd app history guestbook\n</code></pre>"},{"location":"18-ArgoCD/#step-03-view-differences","title":"Step 03 - View Differences","text":"<pre><code># Show differences between Git and live state\nargocd app diff guestbook\n</code></pre>"},{"location":"18-ArgoCD/#step-04-manual-sync-with-options","title":"Step 04 - Manual Sync with Options","text":"<pre><code># Sync with prune (removes resources not in Git)\nargocd app sync guestbook --prune\n\n# Sync specific resources\nargocd app sync guestbook --resource Deployment:guestbook-ui\n\n# Dry-run sync\nargocd app sync guestbook --dry-run\n</code></pre>"},{"location":"18-ArgoCD/#part-06-auto-sync-and-self-healing","title":"Part 06 - Auto-Sync and Self-Healing","text":""},{"location":"18-ArgoCD/#step-01-enable-auto-sync","title":"Step 01 - Enable Auto-Sync","text":"<ul> <li>Enable automatic synchronization so <code>ArgoCD</code> automatically deploys changes from Git:</li> </ul> <pre><code>argocd app set guestbook --sync-policy automated\n</code></pre>"},{"location":"18-ArgoCD/#step-02-enable-self-healing","title":"Step 02 - Enable Self-Healing","text":"<ul> <li>Enable self-healing to automatically fix out-of-sync resources:</li> </ul> <pre><code>argocd app set guestbook --self-heal\n</code></pre>"},{"location":"18-ArgoCD/#step-03-enable-auto-prune","title":"Step 03 - Enable Auto-Prune","text":"<ul> <li>Enable auto-prune to automatically delete resources removed from Git:</li> </ul> <pre><code>argocd app set guestbook --auto-prune\n</code></pre>"},{"location":"18-ArgoCD/#step-04-test-auto-sync","title":"Step 04 - Test Auto-Sync","text":"<ol> <li>Make a change to your Git repository (e.g., change replica count).</li> <li>Commit and push the change.</li> <li>Watch as <code>ArgoCD</code> automatically detects and syncs the change:</li> </ol> <pre><code># Watch the application sync status\nwatch argocd app get guestbook\n</code></pre>"},{"location":"18-ArgoCD/#step-05-test-self-healing","title":"Step 05 - Test Self-Healing","text":"<ol> <li>Manually modify a deployed resource by running:</li> </ol> <pre><code># Manually scale the deployment\nkubectl scale deployment guestbook-ui --replicas=5\n</code></pre> <ol> <li>Watch as <code>ArgoCD</code> detects the drift and automatically restores the desired state:</li> </ol> <pre><code># ArgoCD will revert the replica count to what's in Git\nargocd app get guestbook\n</code></pre>"},{"location":"18-ArgoCD/#part-07-rollback","title":"Part 07 - Rollback","text":""},{"location":"18-ArgoCD/#step-01-view-application-history","title":"Step 01 - View Application History","text":"<pre><code># View all deployment revisions\nargocd app history guestbook\n</code></pre> <p>Example output: <pre><code>ID  DATE                 REVISION\n0   2025-11-10 10:15:30  abc123 (HEAD)\n1   2025-11-10 10:20:45  def456\n2   2025-11-10 10:25:30  ghi789\n</code></pre></p>"},{"location":"18-ArgoCD/#step-02-rollback-to-previous-revision","title":"Step 02 - Rollback to Previous Revision","text":"<pre><code># Rollback to revision ID 1\nargocd app rollback guestbook 1\n</code></pre>"},{"location":"18-ArgoCD/#step-03-verify-rollback","title":"Step 03 - Verify Rollback","text":"<pre><code># Verify the application state\nargocd app get guestbook\n\n# Check deployed resources\nkubectl get all -n default\n</code></pre>"},{"location":"18-ArgoCD/#part-08-working-with-helm-charts","title":"Part 08 - Working with Helm Charts","text":""},{"location":"18-ArgoCD/#step-01-create-helm-based-application","title":"Step 01 - Create Helm-Based Application","text":"<pre><code>argocd app create nginx-helm \\\n  --repo https://charts.bitnami.com/bitnami \\\n  --helm-chart nginx \\\n  --revision 15.1.0 \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace default\n</code></pre>"},{"location":"18-ArgoCD/#step-02-set-helm-values","title":"Step 02 - Set Helm Values","text":"<pre><code># Set Helm values\nargocd app set nginx-helm \\\n  --helm-set service.type=NodePort \\\n  --helm-set replicaCount=3\n</code></pre>"},{"location":"18-ArgoCD/#step-03-sync-helm-application","title":"Step 03 - Sync Helm Application","text":"<pre><code>argocd app sync nginx-helm\n</code></pre>"},{"location":"18-ArgoCD/#step-04-view-helm-parameters","title":"Step 04 - View Helm Parameters","text":"<pre><code># View all Helm parameters\nargocd app get nginx-helm --show-params\n</code></pre>"},{"location":"18-ArgoCD/#part-09-working-with-kustomize","title":"Part 09 - Working with Kustomize","text":""},{"location":"18-ArgoCD/#step-01-create-kustomize-based-application","title":"Step 01 - Create Kustomize-Based Application","text":"<pre><code>argocd app create kustomize-guestbook \\\n  --repo https://github.com/argoproj/argocd-example-apps.git \\\n  --path kustomize-guestbook \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace default\n</code></pre>"},{"location":"18-ArgoCD/#step-02-sync-kustomize-application","title":"Step 02 - Sync Kustomize Application","text":"<pre><code>argocd app sync kustomize-guestbook\n</code></pre>"},{"location":"18-ArgoCD/#step-03-verify-deployment","title":"Step 03 - Verify Deployment","text":"<pre><code>kubectl get all -n default -l app=kustomize-guestbook\n</code></pre>"},{"location":"18-ArgoCD/#part-10-projects-and-rbac","title":"Part 10 - Projects and RBAC","text":""},{"location":"18-ArgoCD/#step-01-create-a-new-project","title":"Step 01 - Create a New Project","text":"<pre><code>argocd proj create my-project \\\n  --description \"My demo project\" \\\n  --src https://github.com/argoproj/argocd-example-apps.git \\\n  --dest https://kubernetes.default.svc,default \\\n  --dest https://kubernetes.default.svc,my-namespace\n</code></pre>"},{"location":"18-ArgoCD/#step-02-list-projects","title":"Step 02 - List Projects","text":"<pre><code>argocd proj list\n</code></pre>"},{"location":"18-ArgoCD/#step-03-view-project-details","title":"Step 03 - View Project Details","text":"<pre><code>argocd proj get my-project\n</code></pre>"},{"location":"18-ArgoCD/#step-04-create-application-in-project","title":"Step 04 - Create Application in Project","text":"<pre><code>argocd app create my-app \\\n  --project my-project \\\n  --repo https://github.com/argoproj/argocd-example-apps.git \\\n  --path guestbook \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace default\n</code></pre>"},{"location":"18-ArgoCD/#part-11-multi-source-applications","title":"Part 11 - Multi-Source Applications","text":""},{"location":"18-ArgoCD/#step-01-create-multi-source-application","title":"Step 01 - Create Multi-Source Application","text":"<ul> <li><code>ArgoCD</code> supports applications with multiple source repositories:</li> </ul> <pre><code># Save as multi-source-app.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: multi-source-app\n  namespace: argocd\nspec:\n  project: default\n  sources:\n    - repoURL: https://github.com/argoproj/argocd-example-apps.git\n      path: guestbook\n      targetRevision: HEAD\n    - repoURL: https://github.com/another-repo/configs.git\n      path: overlays/prod\n      targetRevision: HEAD\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: default\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre>"},{"location":"18-ArgoCD/#step-02-apply-multi-source-application","title":"Step 02 - Apply Multi-Source Application","text":"<pre><code>kubectl apply -f multi-source-app.yaml\n</code></pre>"},{"location":"18-ArgoCD/#part-12-sync-windows-and-waves","title":"Part 12 - Sync Windows and Waves","text":""},{"location":"18-ArgoCD/#step-01-configure-sync-windows","title":"Step 01 - Configure Sync Windows","text":"<ul> <li>Sync windows allow you to define time periods when syncs are allowed or denied:</li> </ul> <pre><code># Add a sync window to allow syncs only during business hours\nargocd proj windows add my-project \\\n  --kind allow \\\n  --schedule \"0 9 * * 1-5\" \\\n  --duration 8h \\\n  --applications \"*\"\n</code></pre>"},{"location":"18-ArgoCD/#step-02-configure-sync-waves","title":"Step 02 - Configure Sync Waves","text":"<ul> <li>Use annotations to control the order of resource synchronization:</li> </ul> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: database\n  annotations:\n    argocd.argoproj.io/sync-wave: \"0\"  # Deploy first\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: backend\n  annotations:\n    argocd.argoproj.io/sync-wave: \"1\"  # Deploy after database\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  annotations:\n    argocd.argoproj.io/sync-wave: \"2\"  # Deploy last\n</code></pre>"},{"location":"18-ArgoCD/#part-13-health-checks-and-hooks","title":"Part 13 - Health Checks and Hooks","text":""},{"location":"18-ArgoCD/#step-01-custom-health-checks","title":"Step 01 - Custom Health Checks","text":"<ul> <li>Define custom health checks for your resources:</li> </ul> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\n  namespace: argocd\ndata:\n  resource.customizations: |\n    cert-manager.io/Certificate:\n      health.lua: |\n        hs = {}\n        if obj.status ~= nil then\n          if obj.status.conditions ~= nil then\n            for i, condition in ipairs(obj.status.conditions) do\n              if condition.type == \"Ready\" and condition.status == \"False\" then\n                hs.status = \"Degraded\"\n                hs.message = condition.message\n                return hs\n              end\n              if condition.type == \"Ready\" and condition.status == \"True\" then\n                hs.status = \"Healthy\"\n                hs.message = condition.message\n                return hs\n              end\n            end\n          end\n        end\n        hs.status = \"Progressing\"\n        hs.message = \"Waiting for certificate\"\n        return hs\n</code></pre>"},{"location":"18-ArgoCD/#step-02-sync-hooks","title":"Step 02 - Sync Hooks","text":"<ul> <li>Use hooks to execute actions during sync:</li> </ul> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: database-migration\n  annotations:\n    argocd.argoproj.io/hook: PreSync\n    argocd.argoproj.io/hook-delete-policy: HookSucceeded\nspec:\n  template:\n    spec:\n      containers:\n      - name: migration\n        image: myapp/migration:latest\n        command: [\"./run-migrations.sh\"]\n      restartPolicy: Never\n</code></pre>"},{"location":"18-ArgoCD/#finalize-cleanup","title":"Finalize &amp; Cleanup","text":""},{"location":"18-ArgoCD/#clean-up-applications","title":"Clean Up Applications","text":"<pre><code># Delete all applications\nargocd app delete guestbook --cascade\nargocd app delete helm-guestbook --cascade\nargocd app delete nginx-helm --cascade\nargocd app delete kustomize-guestbook --cascade\n\n# Or delete via kubectl\nkubectl delete applications -n argocd --all\n</code></pre>"},{"location":"18-ArgoCD/#uninstall-argocd","title":"Uninstall ArgoCD","text":"<pre><code># Delete ArgoCD installation\nkubectl delete -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n\n# Delete the namespace\nkubectl delete namespace argocd\n</code></pre>"},{"location":"18-ArgoCD/#troubleshooting","title":"Troubleshooting","text":""},{"location":"18-ArgoCD/#argocd-server-not-accessible","title":"ArgoCD Server Not Accessible","text":"<ul> <li>Check if the <code>ArgoCD</code> server pod is running:</li> </ul> <pre><code>kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server\n</code></pre> <ul> <li>Check the service:</li> </ul> <pre><code>kubectl get svc -n argocd argocd-server\n</code></pre>"},{"location":"18-ArgoCD/#application-stuck-in-progressing-state","title":"Application Stuck in Progressing State","text":"<ul> <li>Check application details:</li> </ul> <pre><code>argocd app get &lt;app-name&gt;\nkubectl describe application &lt;app-name&gt; -n argocd\n</code></pre> <ul> <li>Check pod logs:</li> </ul> <pre><code>kubectl logs -n argocd -l app.kubernetes.io/name=argocd-application-controller\n</code></pre>"},{"location":"18-ArgoCD/#sync-fails-with-permission-errors","title":"Sync Fails with Permission Errors","text":"<ul> <li>Verify RBAC settings:</li> </ul> <pre><code>argocd proj get &lt;project-name&gt;\n</code></pre> <ul> <li>Check if the destination namespace exists:</li> </ul> <pre><code>kubectl get namespace &lt;namespace&gt;\n</code></pre>"},{"location":"18-ArgoCD/#out-of-sync-status","title":"Out of Sync Status","text":"<ul> <li>View the differences:</li> </ul> <pre><code>argocd app diff &lt;app-name&gt;\n</code></pre> <ul> <li>Force sync:</li> </ul> <pre><code>argocd app sync &lt;app-name&gt; --force\n</code></pre>"},{"location":"18-ArgoCD/#repository-connection-issues","title":"Repository Connection Issues","text":"<ul> <li>Test repository connectivity:</li> </ul> <pre><code>argocd repo add &lt;repo-url&gt; --username &lt;username&gt; --password &lt;password&gt;\nargocd repo list\n</code></pre>"},{"location":"18-ArgoCD/#best-practices","title":"Best Practices","text":""},{"location":"18-ArgoCD/#gitops-workflow","title":"GitOps Workflow","text":"<ol> <li>Single Source of Truth: Keep all Kubernetes manifests in Git.</li> <li>Branch Strategy: Use branches for different environments (dev, staging, prod).</li> <li>Pull Requests: Use PRs for all changes with proper reviews.</li> <li>Automated Testing: Validate manifests before merging.</li> <li>Rollback Strategy: Use Git revert for rollbacks.</li> </ol>"},{"location":"18-ArgoCD/#application-organization","title":"Application Organization","text":"<ol> <li>Use Projects: Organize applications by team or environment.</li> <li>Naming Conventions: Use clear, consistent naming.</li> <li>Sync Policies: Choose appropriate sync policies per environment.</li> <li>Resource Limits: Set proper resource limits in manifests.</li> <li>Health Checks: Define custom health checks for complex resources.</li> </ol>"},{"location":"18-ArgoCD/#security","title":"Security","text":"<ol> <li>RBAC: Implement fine-grained access control.</li> <li>SSO Integration: Use SSO for authentication.</li> <li>Secret Management: Use sealed-secrets or external secret managers.</li> <li>Network Policies: Restrict <code>ArgoCD</code> network access.</li> <li>Audit Logging: Enable and monitor audit logs.</li> </ol>"},{"location":"18-ArgoCD/#monitoring","title":"Monitoring","text":"<ol> <li>Notifications: Configure notifications for sync failures.</li> <li>Metrics: Monitor <code>ArgoCD</code> metrics with <code>Prometheus</code>.</li> <li>Dashboards: Create Grafana dashboards for visibility.</li> <li>Alerts: Set up alerts for critical failures.</li> <li>Regular Reviews: Periodically review application health.</li> </ol>"},{"location":"18-ArgoCD/#next-steps","title":"Next Steps","text":"<ul> <li>Explore ApplicationSets for managing multiple applications.</li> <li>Integrate <code>ArgoCD</code> with CI/CD pipelines.</li> <li>Set up notifications using Slack, email, or webhooks.</li> <li>Implement Progressive Delivery with Argo Rollouts.</li> <li>Configure SSO integration with your identity provider.</li> <li>Set up multi-cluster management.</li> <li>Explore ArgoCD Image Updater for automated image updates.</li> <li>Read the official ArgoCD documentation</li> <li>Join the ArgoCD community</li> </ul>"},{"location":"18-ArgoCD/#additional-resources","title":"Additional Resources","text":"<ul> <li>ArgoCD Official Documentation</li> <li>ArgoCD GitHub Repository</li> <li>ArgoCD Best Practices</li> <li>GitOps Principles</li> <li>Argo Rollouts (Progressive Delivery)</li> <li>ArgoCD ApplicationSet</li> </ul>"},{"location":"19-CustomScheduler/","title":"19 CustomScheduler","text":""},{"location":"19-CustomScheduler/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"19-CustomScheduler/#writing-custom-scheduler","title":"Writing custom Scheduler","text":"<ul> <li><code>Scheduling</code> is the process of selecting a node for a pod to run on.</li> <li>In this lab we will write our own pods <code>scheduler</code>.</li> <li>It is probably not something that you will ever need to do, but still it\u2019s a good practice to understand how scheduling works in K8S and how you can extend it.</li> </ul>"},{"location":"19-CustomScheduler/#pre-requirements","title":"Pre Requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> <li>kubectl configured to interact with your cluster</li> <li>A <code>Git repository</code> (GitHub, GitLab, or Bitbucket) for storing application manifests</li> <li>Basic understanding of Kubernetes resources (Deployments, Services, etc.)</li> </ul>"},{"location":"19-CustomScheduler/#ctrl-click-to-open-in-new-window","title":"CTRL + click to open in new window","text":""},{"location":"19-CustomScheduler/#custom-scheduler","title":"Custom Scheduler","text":"<ul> <li>See further information in the official documentation: Scheduler Configuration</li> <li>To schedule a given pod using a specific scheduler, specify the name of the scheduler in that specification <code>.spec.schedulerName</code>.</li> </ul>"},{"location":"19-CustomScheduler/#a-bit-about-scheduler","title":"A bit about scheduler","text":"<ul> <li>Scheduling happens in a series of stages that are exposed through extension points.</li> <li>We can define several scheduling Profile. A scheduling Profile allows you to configure the different stages of scheduling in the <code>kube-scheduler</code></li> </ul>"},{"location":"19-CustomScheduler/#sample-kubeschedulerconfiguration","title":"Sample <code>KubeSchedulerConfiguration</code>","text":"<pre><code>###\n# Sample KubeSchedulerConfiguration\n###\n#\n# You can configure `kube-scheduler` to run more than one profile.\n# Each profile has an associated scheduler name and can have a different\n# set of plugins configured in its extension points.\n\n# With the following sample configuration, \n# the scheduler will run with two profiles: \n# - default plugins \n# - all scoring plugins disabled\n\napiVersion: kubescheduler.config.k8s.io/v1beta1\nkind: KubeSchedulerConfiguration\nprofiles:\n  - schedulerName: default-scheduler\n  - schedulerName: no-scoring-scheduler\n    plugins:\n      preScore:\n        disabled:\n        - name: '*'\n      score:\n        disabled:\n        - name: '*'\n</code></pre> <ul> <li>Once you have your scheduler code, you can use it in your pod scheduler: </li> </ul> <pre><code># In this sample we use deployment but it will apply to any pod\n...\napiVersion: apps/v1\nkind: Deployment\nspec:\n    spec:\n      # This is the import part of this file.\n      # Here we define our custom scheduler\n      schedulerName: CodeWizardScheduler # &lt;------\n      containers:\n      - name: nginx\n        image: nginx\n</code></pre>"},{"location":"19-CustomScheduler/#sample-bash-scheduler","title":"Sample bash scheduler","text":"<ul> <li>The \u201ctrick\u201d is loop over all the waiting pods and search for the custom scheduler match in <code>spec.schedulerName</code> </li> </ul> <pre><code>...\n  # Get a list of all our pods in pending state\n  for POD in $(kubectl  get pods \\\n                        --server ${CLUSTER_URL} \\\n                        --all-namespaces \\\n                        --output jsonpath='{.items..metadata.name}' \\\n                        --field-selector=status.phase==Pending); \n    do\n\n    # Get the desired schedulerName if th epod has defined any schedulerName\n    CUSTOM_SCHEDULER_NAME=$(kubectl get pod ${POD} \\\n                                    --output jsonpath='{.spec.schedulerName}')\n\n    # Check if the desired schedulerName is our custome one\n    # If its a match this is where our custom scheduler will \"jump in\"\n    if [ \"${CUSTOM_SCHEDULER_NAME}\" == \"${CUSTOM_SCHEDULER}\" ]; \n      then\n        # Do your magic here ......\n        # Schedule the PODS as you wish\n    fi\n    ...\n</code></pre>"},{"location":"20-CronJob/","title":"20 CronJob","text":""},{"location":"20-CronJob/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"20-CronJob/#cronjobs","title":"CronJobs","text":"<ul> <li>In this lab, we will learn how to create and manage <code>CronJobs</code> in Kubernetes.</li> <li>A <code>CronJob</code> creates <code>Jobs</code> on a time-based schedule. It is useful for running periodic and recurring tasks, such as backups or report generation.</li> </ul>"},{"location":"20-CronJob/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> <li>kubectl configured to interact with your cluster</li> </ul> <p> CTRL + click to open in new window</p>"},{"location":"20-CronJob/#what-is-a-cronjob","title":"What is a CronJob?","text":"<ul> <li>A <code>CronJob</code> in Kubernetes runs Jobs on a time-based schedule, similar to Linux cron.</li> <li>Useful for periodic tasks like backups, reports, or cleanup.</li> </ul>"},{"location":"20-CronJob/#step-01-create-a-cronjob-yaml","title":"Step - 01: Create a CronJob YAML","text":"<ul> <li>Create a file named <code>hello-cronjob.yaml</code> with the following content:</li> </ul> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n    name: hello\n    namespace: default\nspec:\n    schedule: \"*/1 * * * *\" # Every 1 minute\n    jobTemplate:\n        spec:\n            template:\n                spec:\n                    containers:\n                    - name: hello\n                        image: busybox\n                        args:\n                        - /bin/sh\n                        - -c\n                        - date; echo Hello from the Kubernetes CronJob!\n                    restartPolicy: OnFailure\n</code></pre>"},{"location":"20-CronJob/#step-02-apply-the-cronjob","title":"Step - 02: Apply the CronJob","text":"<pre><code>kubectl apply -f hello-cronjob.yaml\n</code></pre>"},{"location":"20-CronJob/#step-03-verify-cronjob-creation","title":"Step - 03: Verify CronJob Creation","text":"<pre><code>kubectl get cronjob hello\n</code></pre>"},{"location":"20-CronJob/#step-04-check-cronjob-and-jobs","title":"Step - 04: Check CronJob and Jobs","text":"<ul> <li>List CronJobs:</li> </ul> <pre><code>kubectl get cronjobs\n</code></pre> <ul> <li>List Jobs created by the CronJob:</li> </ul> <pre><code>kubectl get jobs\n</code></pre> <ul> <li>List Pods created by Jobs:</li> </ul> <pre><code>kubectl get pods\n</code></pre>"},{"location":"20-CronJob/#step-05-view-job-output","title":"Step - 05: View Job Output","text":"<ul> <li>Get the name of a pod created by the CronJob, then view its logs:</li> </ul> <pre><code>kubectl logs &lt;pod-name&gt;\n</code></pre> <p>Example output:</p> <pre><code>Mon Nov 10 12:00:00 UTC 2025\nHello from the Kubernetes CronJob!\n</code></pre>"},{"location":"20-CronJob/#step-06-clean-up","title":"Step - 06: Clean Up","text":"<ul> <li>Delete the CronJob and its Jobs:</li> </ul> <pre><code>kubectl delete cronjob hello\nkubectl delete jobs --all\n</code></pre>"},{"location":"20-CronJob/#questions","title":"Questions:","text":"<ul> <li>What happens if the job takes longer than the schedule interval?</li> <li>How would you change the schedule to run every 5 minutes?</li> <li>How can you limit the number of successful or failed jobs to keep?</li> </ul>"},{"location":"21-KubeAPI/","title":"21 KubeAPI","text":""},{"location":"21-KubeAPI/#_1","title":"21 KubeAPI","text":""},{"location":"21-KubeAPI/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"21-KubeAPI/#kube-api-access-from-pod","title":"Kube API Access from Pod","text":"<ul> <li>In this lab, we will learn how to access the Kubernetes API from within a Pod.</li> <li>We will create a simple Pod that runs a script to query the Kubernetes API server and retrieve information about the cluster.</li> </ul>"},{"location":"21-KubeAPI/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> <li>kubectl configured to interact with your cluster</li> </ul> <p> CTRL + click to open in new window</p>"},{"location":"21-KubeAPI/#part-01-build-the-docker-image","title":"Part 01 - Build the docker image","text":"<ul> <li>In order to demonstrate the API query we will build a custom docker image.</li> <li>It is optional to use the pre-build image and skip this step.</li> </ul>"},{"location":"21-KubeAPI/#step-01-the-script-which-will-be-used-for-query-k8s-api","title":"Step 01 - The script which will be used for query K8S API","text":"<ul> <li>In order to be able to access K8S API from within a pod, we will be using the following script:</li> </ul> <pre><code># `api_query.sh`\n\n#!/bin/sh\n\n#################################\n## Access the internal K8S API ##\n#################################\n# Point to the internal API server hostname\nAPI_SERVER_URL=https://kubernetes.default.svc\n\n# Path to ServiceAccount token\n# The service account is mapped by the K8S Api server in the pods\nSERVICE_ACCOUNT_FOLDER=/var/run/secrets/kubernetes.io/serviceaccount\n\n# Read this Pod's namespace if required\n# NAMESPACE=$(cat ${SERVICE_ACCOUNT_FOLDER}/namespace)\n\n# Read the ServiceAccount bearer token\nTOKEN=$(cat ${SERVICE_ACCOUNT_FOLDER}/token)\n\n# Reference the internal certificate authority (CA)\nCACERT=${SERVICE_ACCOUNT_FOLDER}/ca.crt\n\n# Explore the API with TOKEN and the Certificate\ncurl --cacert ${CACERT} --header \"Authorization: Bearer ${TOKEN}\" -X GET ${API_SERVER_URL}/api\n</code></pre>"},{"location":"21-KubeAPI/#step-02-build-the-docker-image","title":"Step 02 - Build the docker image","text":"<ul> <li>For the pod image we will use the following Dockerfile:</li> </ul> <pre><code># `Dockerfile`\n\nFROM    alpine\n\n# Update and install dependencies\nRUN     apk add --update nodejs npm curl\n\n# Copy the endpoint script\nCOPY    api_query.sh .\n\n# Set the execution bit\nRUN     chmod +x api_query.sh .\n</code></pre>"},{"location":"21-KubeAPI/#part-02-deploy-the-pod-to-k8s","title":"Part 02 - Deploy the Pod to K8S","text":"<ul> <li>Once the image is ready, we can deploy it as a pod to the cluster.</li> <li>The required resources are under the k8s folder.</li> </ul>"},{"location":"21-KubeAPI/#step-01-run-kustomization-to-deploy","title":"Step 01 - Run kustomization to deploy","text":"<ul> <li>Deploy to the cluster</li> </ul> <pre><code># Remove old content if any\nkubectl kustomize k8s | kubectl delete -f -\n\n# Deploy the content\nkubectl kustomize k8s | kubectl apply -f -\n</code></pre>"},{"location":"21-KubeAPI/#step-02-query-the-k8s-api","title":"Step 02 - Query the K8S API","text":"<ul> <li>Run the following script to verify that the connection to the API is working:</li> </ul> <pre><code># Get the deployment pod name\nPOD_NAME=$(kubectl get pod -A -l app=monitor-app -o jsonpath=\"{.items[0].metadata.name}\")\n\n# Print out the logs to verify that the pods is connected to the API\nkubectl exec -it -n codewizard $POD_NAME sh ./api_query.sh\n</code></pre>"},{"location":"24-HelmOperator/","title":"22 HelmOperator","text":""},{"location":"24-HelmOperator/#_1","title":"22 HelmOperator","text":""},{"location":"24-HelmOperator/#k8s-hands-on","title":"K8S Hands-on","text":""},{"location":"24-HelmOperator/#helm-operator","title":"Helm Operator","text":"<ul> <li>An in-depth Helm-based operator tutorial.</li> <li>The <code>Helm Operator</code> is a Kubernetes operator, allowing one to declaratively manage Helm chart releases.</li> </ul>"},{"location":"24-HelmOperator/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>K8S cluster - Setting up minikube cluster instruction</li> <li>kubectl configured to interact with your cluster</li> </ul> <p> CTRL + click to open in new window</p> <ul> <li>Docker</li> <li>kubectl</li> <li>operator-sdk installed and configured</li> <li><code>cluster-admin</code> permissions</li> </ul>"},{"location":"24-HelmOperator/#install-operator-sdk","title":"Install <code>operator-sdk</code>","text":"<pre><code># Grab the ARCH and OS\nexport ARCH=$(case $(uname -m) in x86_64) echo -n amd64 ;; aarch64) echo -n arm64 ;; *) echo -n $(uname -m) ;; esac)\nexport OS=$(uname | awk '{print tolower($0)}')\n\n# Get the desired download URL\nexport OPERATOR_SDK_DL_URL=https://github.com/operator-framework/operator-sdk/releases/download/v1.23.0\n\n# Download the Operator binaries\ncurl -LO ${OPERATOR_SDK_DL_URL}/operator-sdk_${OS}_${ARCH}\n\n# Install the release binary in your PATH\nchmod +x operator-sdk_${OS}_${ARCH} &amp;&amp; sudo mv operator-sdk_${OS}_${ARCH} /usr/local/bin/operator-sdk\n</code></pre>"},{"location":"24-HelmOperator/#step-01-create-a-new-project","title":"Step 01 - Create a new project","text":"<ul> <li>Use the CLI to create a new Helm-based nginx-operator project:</li> </ul> <pre><code># Create the desired folder\nmkdir nginx-operator\n\n# Switch to the desired folder\ncd nginx-operator\n\n# Create the helm operator\noperator-sdk                \\\n        init                \\\n        --kind    Nginx     \\\n        --group   demo      \\\n        --plugins helm      \\\n        --version v1alpha1  \\\n        --domain  codewizard.co.il\n</code></pre> <ul> <li>This creates the <code>nginx-operator</code> project specifically for watching the <code>Nginx</code> resource with APIVersion <code>demo.codewizard.co.il/v1alpha1</code> and Kind <code>Nginx</code>.</li> </ul>"},{"location":"24-HelmOperator/#operator-sdk-project-layout","title":"Operator SDK Project Layout","text":"<ul> <li>The command will generate the following structure:</li> </ul> <pre><code>.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 PROJECT\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 crd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bases\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 demo.codewizard.co.il_nginxes.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 default\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 manager_auth_proxy_patch.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 manager_config_patch.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 manager\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 controller_manager_config.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 manager.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 manifests\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 prometheus\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 monitor.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 rbac\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 auth_proxy_client_clusterrole.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 auth_proxy_role.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 auth_proxy_role_binding.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 auth_proxy_service.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 leader_election_role.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 leader_election_role_binding.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 nginx_editor_role.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 nginx_viewer_role.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 role.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 role_binding.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 service_account.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 samples\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 demo_v1alpha1_nginx.yaml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 scorecard\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 bases\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 config.yaml\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 kustomization.yaml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 patches\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 basic.config.yaml\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 olm.config.yaml\n\u251c\u2500\u2500 helm-charts\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 nginx\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 Chart.yaml\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 NOTES.txt\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 _helpers.tpl\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 deployment.yaml\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 hpa.yaml\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 ingress.yaml\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 service.yaml\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 serviceaccount.yaml\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0     \u2514\u2500\u2500 test-connection.yaml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 values.yaml\n\u251c\u2500\u2500 tree.txt\n\u2514\u2500\u2500 watches.yaml\n\n16 directories, 44 files\n</code></pre>"},{"location":"24-HelmOperator/#step-02-customize-the-operator-logic","title":"Step 02 - Customize the operator logic","text":"<ul> <li>For this example the nginx-operator will execute the following reconciliation logic for each Nginx Custom Resource (CR):</li> <li>Create an nginx Deployment, if it doesn\u2019t exist.</li> <li>Create an nginx Service, if it doesn\u2019t exist.</li> <li>Create an nginx Ingress, if it is enabled and doesn\u2019t exist.</li> <li>Update the Deployment, Service, and Ingress, if they already exist but don\u2019t match the desired configuration as specified by the Nginx CR.</li> <li>Ensure that the Deployment, Service, and optional Ingress all match the desired configuration (e.g. replica count, image, service type, etc) as specified by the Nginx CR.</li> </ul>"},{"location":"24-HelmOperator/#watch-the-nginx-cr","title":"Watch the Nginx CR","text":"<ul> <li>By default, the Nginx-operator watches Nginx resource events as shown in <code>watches.yaml</code> and executes Helm releases using the specified chart:</li> </ul> <pre><code># Use the 'create api' subcommand to add watches to this file.\n- group: demo\n  version: v1alpha1\n  kind: Nginx\n  chart: helm-charts/nginx\n</code></pre>"},{"location":"24-HelmOperator/#reviewing-the-nginx-helm-chart","title":"Reviewing the Nginx Helm Chart","text":"<ul> <li> <p>When a Helm operator project is created, the SDK creates an example Helm chart that contains a set of templates for a simple Nginx release.</p> </li> <li> <p>For this example, we have templates for deployment, service, and ingress resources, along with a <code>NOTES.txt</code> template, which Helm chart developers use to convey helpful information about a release.</p> </li> </ul> <p></p>"},{"location":"24-HelmOperator/#understanding-the-nginx-cr-spec","title":"Understanding the Nginx CR spec","text":"<ul> <li> <p>Helm uses a concept called <code>values</code> to provide customizations to a Helm chart\u2019s defaults, which are defined in the Helm chart\u2019s <code>values.yaml</code> file.</p> </li> <li> <p>Overriding these defaults is as simple as setting the desired values in the CR spec.</p> </li> <li> <p>Let\u2019s use the number of replicas value as an example.</p> </li> <li> <p>First, inspecting <code>helm-charts/nginx/values.yaml</code>, we can see that the chart has a value called <code>replicaCount</code> and it is set to <code>1</code> by default.</p> </li> <li> <p>Let\u2019s update the value to 3 - <code>replicaCount: 3</code>.</p> </li> </ul> <pre><code># Update `config/samples/demo_v1alpha1_nginx.yaml` to look like the following:\napiVersion: demo.codewizard.co.il/v1alpha1\nkind: Nginx\nmetadata:\n  name: nginx-sample\nspec:\n  #... (Around line 33)\n  replicaCount: 3 # &lt;------- Adding our replicas count\n</code></pre> <ul> <li>Similarly, we see that the default service port is set to <code>80</code>, but we would like to use <code>8888</code>, so we will again update config/samples/demo_v1alpha1_nginx.yaml by adding the service port override.</li> </ul> <pre><code># Update `config/samples/demo_v1alpha1_nginx.yaml` to look like the following:\napiVersion: demo.codewizard.co.il/v1alpha1\nkind: Nginx\nmetadata:\n  name: nginx-sample\nspec:\n  #... (Around line 36)\n  service:\n    port: 8888 # &lt;------- Updating our service port\n</code></pre>"},{"location":"24-HelmOperator/#step-03-build-the-operators-image","title":"Step 03 - Build the operator\u2019s image","text":"<pre><code># Login to your DockerHub / acr / ecr or any other registry account\n\n# Set the desired image name and tag\n\n# In the Makefile update the following line\n# Image URL to use all building/pushing image targets\nIMG ?= controller:latest\n\n# change it to your registry account\nIMG ?= nirgeier/helm_operator:latest\n</code></pre> <ul> <li>Now let\u2019s build and push the image:</li> </ul> <pre><code>make docker-build docker-push\n</code></pre>"},{"location":"24-HelmOperator/#step-04-deploy-the-operator-to-the-cluster","title":"Step 04 - Deploy the operator to the cluster","text":"<pre><code>make deploy\n\n# Verify that the operator is deployed\nkubectl get deployment -n nginx-operator-system\n</code></pre>"},{"location":"24-HelmOperator/#step-05-create-the-custom-nginx","title":"Step 05 - Create the custom Nginx","text":"<pre><code># Deploy the custom nginx we created earlier\nkubectl apply -f config/samples/demo_v1alpha1_nginx.yaml\n\n# Ensure that the nginx-operator created\nkubectl get deployment | grep nginx-sample\n\n# Check that we have 3 replicas as defined earlier\nkubectl get pods | grep nginx-sample\n\n# Check that the port is set to 8888\nkubectl get svc | grep nginx-sample\n</code></pre>"},{"location":"24-HelmOperator/#step-06-check-the-operator-logic","title":"Step 06 - Check the operator logic","text":"<pre><code># Update the replicaCount and remove the port\n# Once we update the yaml we will check that the operator is working\n# and updating the desired values\n\n# Update the replicaCount in `config/samples/demo_v1alpha1_nginx.yaml`\nreplicaCount: 5\n\n# Remark the service section in the yaml file\n# We wish to see that the operator will use the default values\n36   #service:\n37   #  port: 8888\n38   #  type: ClusterIP\n</code></pre> <ul> <li>Apply the changes:</li> </ul> <pre><code># Apply the changes\nkubectl apply -f config/samples/demo_v1alpha1_nginx.yaml\n</code></pre> <ul> <li>Check to see that the operator is working as expected:</li> </ul> <pre><code># Ensure that the nginx-operator still running\nkubectl get deployment | grep nginx-sample\n\n# Deploy the custom nginx we created earlier\nkubectl apply -f config/samples/demo_v1alpha1_nginx.yaml\n\n# Check that we have 5 replicas as defined earlier\nkubectl get pods | grep nginx-sample\n\n# Check that the port is set back to its default (80)\nkubectl get svc | grep nginx-sample\n</code></pre>"},{"location":"24-HelmOperator/#step07-logging-debugging","title":"Step07 - Logging / Debugging","text":"<ul> <li>We can view the operator\u2019s logs using the following command:</li> </ul> <pre><code># View the operator logs\nkubectl logs deployment.apps/nginx-operator-controller-manager  -n nginx-operator-system -c manager\n</code></pre> <ul> <li>Review the CR status and events:</li> </ul> <pre><code>kubectl describe nginxes.demo.codewizard.co.il\n</code></pre>"}]}